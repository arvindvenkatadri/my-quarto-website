{
  "hash": "a9a427721b8c09d376d53313a9087e1e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: ML - Classification\ndate: 22/July/2022\ndate-modified: \"2025-07-29\"\norder: 30\nabstract: We will look at the basic models for Classification of Data\ncaegories:\n  - Machine Learning\n  - Orange Data Mining\n  - Decision Trees\n  - Random Forests\n  - 20 Questions Game\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## A Childhood Game\n\nHave you played a Childhood Game called **20 Questions**? Someone has a\n\"target\" entity in mind ( a person or a thing or a literary character)\nand the others need to discover that entity by asking 20 questions.\n\n-   How does one create questions in the game?\n    -   Categories?\n    -   Numbers? How?\n    -   Comparisons?\n-   What sort of answers can you expect for each question?\n\n## Twenty Questions Game as a Play with Data...\n\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor\nSwift, or a cartoon character like Thomas the Tank Engine, what would an\nunderlying \"data structure\" look like? We would ask Questions for\ninstance in the following order to find the target of Taylor Swift:\n\n-   Human?(Yes)\n-   Living?(Yes)\n-   Male?(No)\n-   Celebrity?(Yes)\n-   Music?(Yes)\n-   USA?(Yes)\n\nOh...Taylor Swift!!!\n\nLet us try to construct the \"datasets\" underlying this game!\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|name         |Occupation |Sex |Living |Nationality |genre        |pet                |\n|:------------|:----------|:---|:------|:-----------|:------------|:------------------|\n|Taylor Swift |Singer     |F   |TRUE   |USA         |country/rock |Scottish Fold Cats |\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|name                    |Type              |Living |human |Nationality |colour |material |\n|:-----------------------|:-----------------|:------|:-----|:-----------|:------|:--------|\n|Thomas, the Tank Engine |Cartoon Character |FALSE  |FALSE |UK          |blue   |metal    |\n\n\n:::\n:::\n\n\nIt should be fairly clear that the Questions we ask are based on the\nCOLUMNs in the respective **1-row datasets**! The TARGET Column in both\ncases is the **name** column.\n\n## What is a Decision Tree?\n\nCan you imagine how the **20 Questions** Game can be shown as a\n**tree**?\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-2d7e3b57cbf2f1181763\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2d7e3b57cbf2f1181763\">{\"x\":{\"diagram\":\"\\ndigraph tree_diagram {\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = false, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node [shape = circle,\\n        fontname = Helvetica,\\n        label = root]\\n  A; \\n\\n  node [shape = circle,\\n        fixedsize = true,\\n        label = nodes,\\n        fillcolor = purple,\\n        width = 0.9] // sets as circles\\n  B; C; \\n\\nnode [shape = diamond,\\n        fixedsize = true,\\n        label = leaf,\\n        color = springgreen,\\n        width = 0.9] \\n  D;E;\\n\\n  # several \\\"edge\\\" statements\\n  A->B[color = blue, label = yes]; A->C[color = red,label = no]; \\nB->D[color = blue,label = yes]; B->E[color = red,label = no]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nEach Question we ask, based on one of the Feature columns, begets a\nYes/NO answer and we turn the left or right accordingly. When we arrive\nat the leaf, we should be in a position to *guess* the answer !\n\n## Twenty times 20 Questions !!\n\nWhat if the dataset we had contained **many** rows, instead of just one\nrow? How would we play the 20Q Game in this situation? Here is a sample\nof the famous *penguins* dataset:\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|species   |island    | bill_length_mm| bill_depth_mm| flipper_length_mm| body_mass_g|sex    | year|\n|:---------|:---------|--------------:|-------------:|-----------------:|-----------:|:------|----:|\n|Adelie    |Dream     |           36.0|          17.9|               190|        3450|female | 2007|\n|Adelie    |Torgersen |           41.1|          18.6|               189|        3325|male   | 2009|\n|Gentoo    |Biscoe    |           45.2|          16.4|               223|        5950|male   | 2008|\n|Gentoo    |Biscoe    |           50.0|          16.3|               230|        5700|male   | 2007|\n|Adelie    |Biscoe    |           35.9|          19.2|               189|        3800|female | 2007|\n|Gentoo    |Biscoe    |           50.0|          15.3|               220|        5550|male   | 2007|\n|Adelie    |Torgersen |           35.2|          15.9|               186|        3050|female | 2009|\n|Chinstrap |Dream     |           50.6|          19.4|               193|        3800|male   | 2007|\n|Chinstrap |Dream     |           51.3|          18.2|               197|        3750|male   | 2007|\n|Gentoo    |Biscoe    |           45.8|          14.2|               219|        4700|female | 2008|\n|Adelie    |Dream     |           37.0|          16.9|               185|        3000|female | 2007|\n|Gentoo    |Biscoe    |           48.1|          15.1|               209|        5500|male   | 2009|\n\n\n:::\n:::\n\n\nAs before, we would need to look at the dataset as containing a\n**TARGET** column which we want to predict using several other\n**FEATURE** columns. Let us choose **species**.\n\nWhen we look at the FEATURE columns, We would need to formulate\nquestions based on *entire columns* at a time. For instance:\n\n-   *\"Is the bill_length_mm\\* greater than 45mm?\" considers the entire*\n    bill_length_mm\\* FEATURE column\n-   Is the *sex* female? considers the entire *sex* column\n\nIf the specific **FEATURE** column is a **Numerical** (N) variable, the\nquestion would use some \"thresholding\" as shown in the question above,\nto convert the Numerical Variable into a Categorical variable.\n\nIf a specific **FEATURE** column is a **Categorical** (C) variable, the\nquestion would be like a *filter* operation in Excel.\n\nEither way, we end up answering with a **smaller and smaller subset of\nrows in the dataset**, to which the questions are answered with a Yes.\nIt is as if we played **many** 20 Questions games in parallel, ***since\nthere are so many simultaneous \"answers\"***!\n\nOnce we exhaust **all** the FEATURE columns, then what remains is a\nsubset (i.e. rows) of the original dataset and we read off the TARGET\ncolumn, which should now contain a set of identical entries, e.g.\n\"Adelie\". Thus we can extend a *single-target* 20Q game to a\n**multiple-target** one using a larger dataset. ( Note how the multiple\ntargets are all the same: \"Adelie\", or \"Gentoo\", or \"Chinstrap\")\n\nThis forms the basic intuition for a Machine Learning Algorithm called a\n**Decision Tree**.\n\n### Decision Tree in Orange\n\nLet us visualize this Decision Tree in Orange. Look at the now famous\n`penguins` dataset, available here:\n\n<https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv>\n\nWe see that there are three `species` of penguins, that live on three\n`islands`. The measurements for each penguin are `flipper_length_mm`,\n`bill_length_mm`, `bill_depth_mm`, and `body_mass_g`.\n\n1.  **Task 1:** Create a few data visualizations for the variables, and\n    pairs of variables from this dataset.\n\n2.  **Task 2:** Can you inspect the visualizations and imagine how each\n    of this dataset can be used in a `20 Questions Game`, to create a\n    **Decision Tree** for this dataset as shown below?\n\n![Penguins Decision Tree!](penguins.png)\n\n#### What did we learn?\n\n-   The 20Q Game can be viewed as a \"Decision Tree\" of Questions and\n    Answers,\n-   Each **fork** in the game is a Question.\n-   Depending upon whether the current answer is **yes or no**, we turn\n    in one direction or the other.\n-   Each of our questions is **based** on the information available in\n    one or other of the columns!!\n-   We arrive at a final \"answer\" or \"target\" after a particular\n    sequence of yes/no answers. This is the one of the **leaf** nodes in\n    the Tree.\n-   The `island` and the `species` columns are **categories** and are\n    especially suited to being the **targets** for a 20 Questions Game.\n-   We can therefore use an **entire column** of data as our 20Questions\n    **target**, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm,\n**classification** using Decision Trees.\n\n## How do we Make Predictions using our Decision Tree\n\nOur aim is to make `predictions`. Predictions of what? When we are given\nnew unseen data in the same format, we should be able to predict TARGET\nvariable using the same FEATURE columns.\n\nNOTE: This that is usually a **class/category** (We **CAN** also predict\na **numerical value** with a Decision Tree; but we will deal with that\nlater.)\n\nIn order to make predictions with completely unseen data, we need to\nfirst check if the algorithm is working well with **known** data. The\nway to do this is to use a large portion of data to **design the tree**,\nand then use the tree to predict some aspect of the remaining, but\nsimilar, data. Let us split the `penguins` dataset into two pieces: a\n`training set` to design our tree, and a `test set` to check how it is\nworking.\n\nDownload this <u>[**penguin tree file**](penguins-tree.ows)</u> and open\nit in Orange.\n\nHow good are the Predictions? What is the **Classification Error Rate**?\n\n## How Many Trees do we Need? Enter the Random Forest!\n\nCheck all your individual Decision Trees: do they ask the same\nQuestions? Do they fork in the same way? Yes, they all seem to use the\nsame set of parameters to reach the target. So they are capable of being\n\"biased\" and make the **same mistakes**. So we ask: Does it help to use\nmore than one tree, if all the questions/forks in the Trees are similar?\n\nNo...we need different Trees to be able to **ask different questions**,\nbased on different **variables** or **features** in the data. That will\nmake the Trees as different as possible and so...unbiased. This is what\nwe also saw when we played 20Q: offbeat questions opened up some avenues\nfor predicting the answer/target.\n\nA forest of such trees is called ~~**the Wild Wood**~~ **a Random\nForest** !\n\n## An Introduction to Random Forests\n\nIn the Random Forest method, we do as follows:\n\n1.  Split the dataset into `training` and `test` subsets (70::30\n    proportion is very common). Keep aside the `testing` dataset for\n    final testing.\n2.  Decide on a number of trees, say 100-500 in the forest.\n3.  Take the training dataset and repeatedly sample some of the rows in\n    it. Rows can be **repeated** too; this is called **bootstrap\n    sampling**.\n4.  Give this sampled training set to each tree. Each tree develops a\n    question from this dataset, in a **random fashion**, using a\n    **randomly chosen** variable. E.g. with `penguins`, if our target is\n    `species`, then some trees will will use `island`, some others will\n    use `body_mass_g` and some others may use `bill_length_mm`.\n5.  Each tree will \"grow its questions\" in a unique way !! Since the\n    questions are possibly based on a different variable at each time,\n    the trees will grow in very different ways.\n6.  Stop when the required accuracy has been achieved (the sets contain\n    observations/rows from only one `species` predominantly)\n7.  With the `test set` let each tree vote on which `species` it has\n    decided upon. Take the majority vote.\n\nPhew!!\n\nLet's get a visual sense of how all this works:\n\n<https://waternova.github.io/random-forest-viz/>\n\n## Random Forest Classification for Heart Patients\n\nDo you want to develop an ML model for heart patients? We have a dataset\nof heart patients at the [**University of California, ~~Arvind~~ Irvine\nML Dataset\nRepository**](https://archive.ics.uci.edu/ml/datasets/heart+disease)\n\n[**Heart Patient\nData**](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data).\nImport into Orange !!\n\nWhat are the variables?\n\n1.  (age): age in years\n2.  (sex): 1 = male; 0 = female\n3.  (cp): chest-pain type( 4 types, 1/2/3/4)\n4.  (trestbps): resting blood pressure (in mm Hg on admission to the\n    hospital)\n5.  (chol) : serum cholesterol in mg/dl\n6.  (fbs): (fasting blood sugar \\> 120 mg/dl) (1 = true; 0 = false)\n7.  (restecg): resting electrocardiograph results (0 = normal; 1= ST-T\n    wave abnormality; 3 = LV hypertrophy)\n8.  (thalach): maximum heart rate achieved\n9.  (exang): exercise induced angina (1 = yes; 0 = no) (remember\n    **Puneet Rajkumar**)\n10. (oldpeak): ST depression induced by exercise relative to rest\n11. (slope): the slope of the peak exercise ST segment\n    -   Value 1: upsloping\n    -   Value 2: flat\n    -   Value 3: downsloping\n12. (ca): number of major vessels (0-3) colored by fluoroscopy\n13. (thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n14. (num) : **the target attribute**, diagnosis of heart disease\n    (angiographic disease status)\n    -   Value 0: \\< 50% diameter narrowing\n    -   Value 1: \\> 50% diameter narrowing\\\n        (in any major vessel: attributes 59 through 68 are vessels)\n\nWe will create a Random Forest Model for this dataset, and compare with\nthe Decision Tree for the same dataset.\n\n## How good is my Random Forest?\n\n1.  Classification Error\\\n2.  Gini Impurity\\\n3.  Cross Entropy\n\n## References\n\n1.  <https://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa>\n\n2.  The beauty of Random Forests:\n    <https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/>\n\n3.  Pythagorean Trees for Random Forests:\n    <https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/>\n\n4.  *data.tree* sample applications, Christoph Glur, 2020-07-31.\n    <https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html>\n\n5.  <https://ryjohnson09.netlify.app/post/caret-and-tidymodels/>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"../../../../../site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"../../../../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\n<link href=\"../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}