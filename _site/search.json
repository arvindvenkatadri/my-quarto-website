[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Hi, I’m Arvind Venkatadri.",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m external Faculty at the Srishti Manipal Institute of Art and Design, both in Bangalore, INDIA, and a Certified Level #1 TRIZ Professional.\nI have a passion for coding in R / ObservableJS / p5.js, Data Visualization, Complexity Science, Literature, and Creative Thinking / Problem Solving with TRIZ. On this website, I share and teach what I learn.\nTo get started, you can check out my courses. You can also find me on Twitter, GitHub, and on Medium. Feel free to reach out to me via email !\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arvind V.",
    "section": "",
    "text": "I’m an an external Faculty Member at the Srishti Manipal Institute of Art, Design, and Technology (SMI), in Bangalore, INDIA. I am passionate about working on R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses on this website.\nMy other teaching websites are:\n- Foundation Courses at Srishti\n- Teaching R to Artists and Designers\n- Using AI in R\n\nMy student portfolios are here:\nhttps://we-r-us.netlify.app/portfolio/\nhttps://form-and-structure.netlify.app/portfolio/\n\nYou can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html",
    "href": "content/work-related/fsp-discussions/index.html",
    "title": "FSP Discussions 2021",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#introduction",
    "href": "content/work-related/fsp-discussions/index.html#introduction",
    "title": "FSP Discussions 2021",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "href": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "title": "FSP Discussions 2021",
    "section": "This is a summary of the group discussion among:",
    "text": "This is a summary of the group discussion among:\n1. Sadhvi Jawa\n2. Minashshi Singh\n3. Vidhu Gandhi\n4. Yash Bhandari\n5. Arvind Venkatadri"
  },
  {
    "objectID": "content/work-related/fsp-portfolio/index.html",
    "href": "content/work-related/fsp-portfolio/index.html",
    "title": "Teaching in this Pandemic Year 2020-2021",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this pandemic year, 2020-2021, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html",
    "title": "\n Graphs",
    "section": "",
    "text": "“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#setting-up-r-packages",
    "title": "\n Graphs",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\nlibrary(ggformula) # Our plotting package\nlibrary(tidyplots) # New package for publication quality graphs\n\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR)\nlibrary(Lock5Data) # Some neat little datasets from a lovely textbook\nlibrary(kableExtra)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-visualize",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-visualize",
    "title": "\n Graphs",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts from mere statements: for example:\n\n\n\n\n\n\nFigure 1: Rape Capital\n\n\n\n\n\n\n\nFigure 2: Data Reveals Crime\n\n\n\nVisuals are a good starting point to make hypotheses of what may be happening in the situation represented by the data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-analyze",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-analyze",
    "title": "\n Graphs",
    "section": "\n Why Analyze?",
    "text": "Why Analyze?\n\nMerely looking at visualizations may not necessarily tell us the true magnitude or significance of things.\nWe need analytic methods or statistics to assure ourselves, or otherwise, of what we might suspect is happening\nThese methods also help to remove human bias and ensure that we are speaking with the assurance that our problem deserves.\nAnalysis uses numbers, or metrics, that allow us to crystallize our ambiguous words/guesses into quantities that can be calculated with.\nThese metrics are calculable from our data, of course, but are not directly visible, despite often being intuitive.\n\nSo both visuals and analytics. And as we will see, we will not be content with that: we will visualize our analytics, and analyze our visualizations!\nLet us recall first what we meant by tidy data:\n\n\n\n\n\n\n\nFigure 3: Tidy Data\n\n\n\n\n\n\n\n\n\n\nImportantTidy Data\n\n\n\n\nEach variable is a column;\nEach column contains one kind of data.\nEach observation or case is a row.\nEach observations contains one value for each variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#what-is-a-data-visualization",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#what-is-a-data-visualization",
    "title": "\n Graphs",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data + Geometry\nHow many geometric things do we know? Shapes? Lines? Axes? Curves? Angles? Patterns? Textures? Colours? Sizes? Positions? Lengths? Heights? Breadths? Radii? Textures? All these are geometric aspects or aesthetics, each with a unique property. Some “geometric things” which we might consider are shown in the figure below.\n\n\n\n\n\nFigure 4: Common Geometric Aesthetics in Charts\n\n\n\n Mapping\nHow can we manipulate these geometric aesthetics, perhaps like Kandinsky? The aesthetic has a property, an atribute, which we can manipulate in accordance with a data variable! This act of “mapping” a geometric thing to a variable and modifying its essential property is called Data Visualization\nFor instance:\n\n\nlength or height of a bar can be made proportional to theage or income of a person\n\nColour of points can be mapped to gender, with a unique colour for each gender.\n\nPosition along an X-axis can vary in accordance with a height variable, and\n\nPosition along the Y-axis can vary with a bodyWeight variable.\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle, pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nNoteA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and kind of variableQuantitative or Qualitative as seen in Figure 3. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\n\n\n\n\n\n\n\nFigure 5: Data Vis Components and Features\n\n\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered as another axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts.\nAs we will see, Data Variables may be transformed before being mapped to some geometric aesthetic, e.g. we may perform counts with a Qual variable that contains only the entries {S, M, L, XL}. We may also transform the axes (make them logarithmic, or even polar ) to create precisely the shape-meaning we wish. This allows us considerable flexibility in making charts!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#sec-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#sec-data-viz",
    "title": "\n Graphs",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can therefore think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#conclusion",
    "title": "\n Graphs",
    "section": "\n Conclusion",
    "text": "Conclusion\nLet us take a look at Wickham and Grolemund’s Data Science workflow picture:\n\n\n\n\n\nFigure 6: Data Science Workflow\n\n\nSo there we have it:\n\nWe import and clean the data\nQuestions lead us to identify Types of Variables (Quant and Qual)\n\nSometimes we may need to transform the data (long to wide, summarize, create new variables…)\nFurther Questions lead to relationships between variables, which we describe using Data Visualizations\n\nVisualizations may lead to Hypotheses, which we Analyze or Model\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\nWhich is finally Communicated\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter. Other parts of the Workflow (Transformation, Analysis and Modelling) are also following similar grammars, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#ai-generated-summary-and-podcast",
    "title": "\n Graphs",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on data visualization using the R programming language. It introduces concepts such as data types, variables, and visualization techniques. The tutorial utilizes metaphors to explain these concepts, emphasizing the use of geometric aesthetics to represent data. It also highlights the importance of both visual and analytic approaches in understanding data. The tutorial then demonstrates basic chart types, including histograms, scatterplots, and bar charts, and discusses the “Grammar of Graphics” philosophy that guides data visualization in R. The text concludes with a workflow diagram for data science, emphasizing the iterative process of data import, cleaning, transformation, visualization, hypothesis generation, analysis, and communication.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#references",
    "title": "\n Graphs",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\n\nAdam E.M. Eltorai, Jeffrey A. Bakal, Paige C. Newell, Adena J. Osband (editors). (March 22, 2023) Translational Surgery: Handbook for Designing and Conducting Clinical and Translational Research. A very lucid and easily explained set of chapters. ( I have a copy. Yes.)\n\nPart III. Clinical: fundamentals\nPart IV: Statistical principles\n\n\nhttps://safetyculture.com/topics/design-of-experiments/\nEmi Tanaka. https://emitanaka.org/teaching/monash-wcd/2020/week09-DoE.html\n\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\n\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\n\nAlbert Rapp. Adding images to ggplot. https://albert-rapp.de/posts/ggplot2-tips/27_images/27_images\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nLock5Data\n3.0.0\nLock (2021)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLock, Robin. 2021. Lock5Data: Datasets for “Statistics: UnLocking the Power of Data”. https://doi.org/10.32614/CRAN.package.Lock5Data.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#footnotes",
    "title": "\n Graphs",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#setting-up-r-packages",
    "title": "\n Groups and Densities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(tidyverse) # Most Important Last\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-graphs-will-we-see-today",
    "title": "\n Groups and Densities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\n(Qual)\nViolin Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Groups and Densities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#inspiration",
    "title": "\n Groups and Densities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\nWhich is the plots above is more evocative of the underlying data? The violin plots, which looks like a combo box-plot + density, is probably giving us a greater sense of the spread of the data than the good old box plot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#how-do-these-charts-work",
    "title": "\n Groups and Densities",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nOften one needs to view multiple densities at the same time. Ridge plots of course give us one option, where we get densities of a Quant variable split by a Qual variable. Another option is to generate a density plot facetted into small multiples using a Qual variable.\nYet another plot that allows comparison of multiple densities side by side is a violin plot. The violin plot combines the aspects of a boxplot(ranking of values, median, quantiles…) with a superimposed density plot. This allows us to look at medians, means, densities, and quantiles of a Quant variable with respect to another Qual variable. Let us see what this looks like!\n\n\n\n\n\n\n\nFigure 1: Violin Plots for Normal Variables\n\n\n\n\nIn Figure 1, the plots show (very artificial!) distributions of a single Quant variable across levels of another Qual variable. At each level of the Qual variable along the X-axis, we have a violin plot showing the density.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#case-study-1-diamonds-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#case-study-1-diamonds-dataset",
    "title": "\n Groups and Densities",
    "section": "\n Case Study-1: diamonds dataset",
    "text": "Case Study-1: diamonds dataset\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\ngf_violin(price ~ \"All Diamonds\",\n  data = diamonds,\n  draw_quantiles = c(0, .25, .50, .75)\n) %&gt;%\n  gf_labs(title = \"Plot A: Violin plot for Diamond Prices\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_labs(title = \"Plot B: Price by Cut\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    fill = ~cut,\n    color = ~cut,\n    alpha = 0.5,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_labs(title = \"Plot C: Price by Cut\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    fill = ~cut,\n    colour = ~cut,\n    alpha = 0.5,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_facet_wrap(vars(clarity)) %&gt;%\n  gf_labs(title = \"Plot D: Price by Cut facetted by Clarity\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% ggplot() +\n  geom_violin(aes(y = price, x = \"\"),\n    draw_quantiles = c(0, .25, .50, .75)\n  ) + # note: y, not x\n  labs(title = \"Plot A: violin for Diamond Prices\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(aes(cut, price),\n    draw_quantiles = c(0, .25, .50, .75)\n  ) +\n  labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(\n    aes(cut, price,\n      color = cut, fill = cut\n    ),\n    draw_quantiles = c(0, .25, .50, .75),\n    alpha = 0.4\n  ) +\n  labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(\n    aes(cut,\n      price,\n      color = cut, fill = cut\n    ),\n    draw_quantiles = c(0, .25, .50, .75),\n    alpha = 0.4\n  ) +\n  facet_wrap(vars(clarity)) +\n  labs(title = \"Plot D: Price by Cut facetted by Clarity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from diamond Violin Plots\n\n\n\nThe distribution for price is clearly long-tailed (skewed). The distributions also vary considerably based on both cut and clarity. These Qual variables clearly have a large effect on the prices of individual diamonds.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#wait-but-why",
    "title": "\n Groups and Densities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots give us an idea of medians, IQR ranges, and outliers. The shape of the density is not apparent from the box.\nDensities give us shapes of distributions, but do not provide visual indication of other metrics like means or medians ( at least not without some effort)\nViolins help us do both!\nViolins can also be cut in half (since they are symmetric, like Buddhist Prayer Wheels), then placed horizontally, and combined with both a boxplot and a dot-plot to give us raincloud plots that look like this. (Yes, there is code over there, which you can reuse.)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#conclusion",
    "title": "\n Groups and Densities",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nBox Plots “dwell upon” medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\nViolin Plots help us to visualize multiple distributions at the same time, as when we split a Quant variable wrt to the levels of a Qual variable.\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#your-turn",
    "title": "\n Groups and Densities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteDatasets\n\n\n\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\n\n\n\n\n\n\n\n\n\nNoteCalmCode\n\n\n\n\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\n\n\n\n\n\n\n\n\n\nNoteFrom Groups\n\n\n\n\nDatasets from the earlier module on Groups.\n\n\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#references",
    "title": "\n Groups and Densities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggnormalviolin\n0.2.1\nSchneider (2025)\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.1\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSchneider, W. Joel. 2025. ggnormalviolin: A “ggplot2” Extension to Make Normal Violin Plots. https://doi.org/10.32614/CRAN.package.ggnormalviolin.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "We will create Data Visualizations in R to show Parts of a Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g. Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "Package\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "title": "Tutorial on Correlations in R",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr)\nlibrary(GGally)\nlibrary(corrplot) # For Correlogram plots\nlibrary(broom) # to properly format stat test results\n\nlibrary(mosaicData) # package containing datasets\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nAll R functions seen in the code are clickable links that take you to online documentation about the function. Try!\n\n\n\n\n\n\nTipThe Formula interface\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z )\nThe second method may be preferable, especially if you have done some data manipulation first! More about this later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study 1: Galton Dataset from mosaicData\n",
    "text": "Case Study 1: Galton Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\n\n\n Inspecting the Data\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe &lt;- inspect(Galton)\n\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! Let us also look at the output of skim:\n\nskimr::skim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Which variables could have relationships with others? Why? Write down these Questions!\n\n Correlations and Plots\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNoteQuestions\n\n\n\nHow does children’s height correlate with that of father and mother? Is this relationship also affected by sex of the child?\nWith this question, height becomes our target variable, which we should always plot on the dependent y-axis.\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"), # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  title = \"Galton Data Correlations Plot\"\n) +\n\n  theme_bw()\n\n\n\n\n\n\n\nWe note that children’s height is correlated with that of father and mother. The correlations are both positive, and that with father seems to be the larger of the two. ( Look at the slopes of the lines and the values of the correlation scores. )\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat if we group the Quant variables based on a Qual variable, like sex of the child?\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  mapping = aes(colour = sex), # Colour by `sex`\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n\n  diag = list(continuous = \"barDiag\"),\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  title = \"Galton Data Correlations Plot\"\n) +\n\n  theme_bw()\n\n\n\n\n\n\n\nThe split scatter plots are useful, as is the split histogram for height: Clearly the correlation of children’s height with father and mother is positive for both sex-es. The other plots, and even some of the correlations scores are not all useful! Just shows everything we can compute is not necessarily useful immediately.\nIn later modules we will see how to plot correlations when the number of variables is larger still.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCan we plot a Correlogram for this dataset?\n\n\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nLet us confirm with a correlation test:\n\n\nWe will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(height ~ father, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Children vs Fathers\"\n  )\n\n\nChildren vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.28\n8.57\n0\n896\n0.21\n0.33\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Children vs Mothers\"\n  )\n\n\nChildren vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.2\n6.16\n0\n896\n0.14\n0.26\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nImportantCorrelation Scores and Uncertainty\n\n\n\nNote how the mosaic::cor_test() reports a correlation score estimate and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally::ggpairs() too reports the significance of the correlation scores estimates using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor_test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nIn both cases, we used the formula \\(height \\sim other-variable\\), in keeping with our idea of height being the dependent, target variable..\nWe also see the p.value for the estimateed correlation is negligible, and the conf.low/conf.high interval does not straddle \\(0\\). These attest to the significance of the correlation score.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat does this correlation look when split by sex of Child?\n\n\n\n# For the sons\n\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Sons vs Fathers\"\n  )\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"M\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Sons vs Mothers\"\n  )\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Daughters vs Fathers\"\n  )\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Daughters vs Mothers\"\n  )\n\n\nSons vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.39\n9.15\n0\n463\n0.31\n0.47\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nSons vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.33\n7.63\n0\n463\n0.25\n0.41\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.46\n10.72\n0\n431\n0.38\n0.53\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n6.86\n0\n431\n0.23\n0.4\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\nThe same observation as made above ( p.value and confidence intervals) applies here too and tells us that the estimated correlations are significant.\nVisualizing Uncertainty in Correlation Estimates\nWe can also visualize this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse. Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows: we will first plot correlation uncertainty for one pair of variables to develop the intuition, and then for all variables against the one target variable:\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  # We need a graph not a table\n  # So comment out this line from the earlier code\n  # knitr::kable(digits = 2,caption = \"Children vs Mothers\")\n\n  rowid_to_column(var = \"index\") %&gt;% # Need an index to plot with\n\n  # Uncertainty as error-bars\n  gf_errorbar(conf.high + conf.low ~ index, linewidth = 2) %&gt;%\n  # Estimate as a point\n  gf_point(estimate ~ index, color = \"red\", size = 6) %&gt;%\n  # Labels\n  gf_text(estimate ~ index - 0.2,\n    label = \"Correlation Score = estimate\"\n  ) %&gt;%\n  gf_text(conf.high * 0.98 ~ index - 0.25,\n    label = \"Upper Limit = estimate + conf.high\"\n  ) %&gt;%\n  gf_text(conf.low * 1.04 ~ index - 0.25,\n    label = \"Lower Limit = estimate - conf.low\"\n  ) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nWe can now do this for all variables against the target variable height, which we identified in our research question. We will use the iteration capabilities offered by the tidyverse package, purrr:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlation with height in Galton\",\n    caption = \"Significance = - log10(p.value)\"\n  ) %&gt;%\n  gf_refine(\n\n    # Scale for colour\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n      range = c(0.5, 4)\n    )\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nWe can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very little. This kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n\n# For the sons\ngf_point(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\"),\n  title = \"Soms and Fathers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"M\"),\n  title = \"Sons and Mothers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n# For the daughters\ngf_point(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\"),\n  title = \"Daughters and Fathers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\"),\n  title = \"Daughters and Mothers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn approximation to Galton’s famous plot1 (see Wikipedia):\n\n\n\n\n\n\n\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_ellipse(level = 0.95, color = \"red\") %&gt;%\n  gf_ellipse(level = 0.75, color = \"blue\") %&gt;%\n  gf_ellipse(level = 0.5, color = \"green\") %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\nHow would you interpret this plot2?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study#2: Dataset from NHANES\n",
    "text": "Case Study#2: Dataset from NHANES\n\nLet us look at the NHANES dataset from the package NHANES:\n\ndata(\"NHANES\")\n\n\n Inspecting the Data\nNHANES_describe &lt;- inspect(NHANES)\n\nNHANES_describe$categorical\nNHANES_describe$quantitative\nNHANES\nskimr::skim(NHANES)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\nTry help(\"NHANES\") in your Console.\n\nThis is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).\n\nThe dataset is described as: A data frame with 100000 observations on 76 variables. Some of these are:\n- Race1 and Race2: factors with 5 and 6 levels respectively\n- Education a factor with 5 levels\n- HHIncomeMid Total annual gross income for the household in US dollars.\n- Age\n- BMI: Body mass index (weight/height2 in kg/m2)\n- Height: Standing height in cm.\n- Weight: Weight in kg &gt; &gt; - Testosterone: Testosterone total (ng/dL) - PhysActiveDays: Number of days in a typical week that participant does moderate or vigorous-intensity activity.\n- CompHrsDay: Number of hours per day on average participant used a computer or gaming device over the past 30 days.\n\n\n\n\n\n\nImportantMissing Data\n\n\n\nWhy do so many of the variables have missing entries? What could be your guess about the Experiment/Survey`?\n\n\nLet us make some counts of the data, since we have so many factors:\n\nNHANES %&gt;% count(Gender)\n\n\n  \n\n\nNHANES %&gt;% count(Race1)\n\n\n  \n\n\nNHANES %&gt;% count(Race3)\n\n\n  \n\n\nNHANES %&gt;% count(Education)\n\n\n  \n\n\nNHANES %&gt;% count(MaritalStatus)\n\n\n  \n\n\n\nThere is a good mix of factors and counts.\nNow we articulate our Research Questions:\n\n\n\n\n\n\nNoteResearch Questions\n\n\n\n\nDoes Testosterone have a relationship with parameters such as BMI, Weight, Height, PhysActiveDays CompHrsDay and Age?\nDoes HHIncomeMid have a relationship with these same parameters? And with Gender?\nAre there any other pairwise correlations that we should note? (This is especially useful in choosing independent variables for multiple regression)\n\n\n\n( Yes we are concerned with men more than with the women, sadly.)\n\n Correlations and Plots\n\nGGally::ggpairs(NHANES,\n  # Choose the variables we want to plot for\n  columns = c(\n    \"HHIncomeMid\", \"Weight\", \"Height\",\n    \"BMI\", \"Gender\"\n  ),\n\n  # LISTs of graphs needed at different locations\n  # For different combinations of variables\n  diag = list(continuous = \"barDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n  upper = list(continuous = \"cor\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE\n) + # No compute progress bars needed\n  theme_bw()\n\n\n\n\n\n\n\nWe see that HHIncomeMid is Quantitative, discrete valued variable, since it is based on a set of median incomes for different ranges of income. BMI, Weight, Height are continuous Quant variables.\nHHIncomeMid also seems to be relatively unaffected by Weight; And is only mildly correlated with Height and BMI, as seen both by the correlation score magnitudes and the slopes of the trend lines.\nThere is a difference in the median income by Gender, but we will defer that kind of test for later, when we do Statistical Inference.\nUnsurprisingly, BMI and Weight have a strong relationship, as do Height and Weight; the latter is of course non-linear, since the Height levels off at a point.\n\nGGally::ggpairs(NHANES,\n  columns = c(\"Testosterone\", \"Weight\", \"Height\", \"BMI\"),\n  diag = list(continuous = \"barDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n  upper = list(continuous = \"cor\"),\n  switch = \"both\",\n  progress = FALSE\n) +\n  theme_bw()\n\n\n\n\n\n\n\nIt is clear that Testosterone has strong relationships with Height and Weight but not so much with BMI.\n\n Visualizing Uncertainty in Correlation Estimates\nSince the pairs plot is fairly clear for both target variables, let us head to visualizing the significance and uncertainty in the correlation estimates.\n\nHHIncome_corrs &lt;- NHANES %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-HHIncomeMid) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$HHIncomeMid)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nHHIncome_corrs\n\n\n  \n\n\nHHIncome_corrs %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with HouseHold Median Income\",\n    caption = \"Significance = - log10(p.value)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 6, hjust = 1)),\n    coord_flip()\n  )\n\n\n\n\n\n\n\nIf we select just the variables from our Research Question:\n\nHHIncome_corrs_select &lt;- NHANES %&gt;%\n  select(Height, Weight, BMI) %&gt;% # Only change is here!\n\n  # leave off height to get all the remaining ones\n  # select(- HHIncomeMid) %&gt;%\n\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$HHIncomeMid)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nHHIncome_corrs_select\n\n\n  \n\n\nHHIncome_corrs_select %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.000001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with HouseHold Median Income\",\n    caption = \"Significance = - log10(p.value + 0.000001)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 8, hjust = 1)),\n    coord_flip()\n  )\n\n\n\n\n\n\n\nSo we might say taller people make more money? And fatter people make slightly less money? Well, the magnitude of the correlations (aka effect size) are low so we would not imagine this to be a hypothesis that we can defend.\nLet us look at the Testosterone variable: trying all variables shows some paucity of observations ( due to missing data), so we will stick with our chosen variables:\n\nTestosterone_corrs &lt;- NHANES %&gt;%\n  select(Height, Weight, BMI) %&gt;%\n  # leave off height to get all the remaining ones\n  # select(- Testosterone) %&gt;%\n\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$Testosterone)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nTestosterone_corrs\n\n\n  \n\n\nTestosterone_corrs %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.000001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with Testosterone Levels\",\n    caption = \"Significance = - log10(p.value + 0.000001)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 8, hjust = 1)),\n    coord_flip()\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "title": "Tutorial on Correlations in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have a decent Correlations related workflow in R:\n\nLoad the dataset\n\ninspect/skim/glimpse the dataset, identify Quant and Qual variables\nIdentify a target variable based on your knowledge of the data, how it was gathered, who gathered it and what was their intent\nDevelop Pair-Wise plots + Correlations using GGally::ggpairs()\n\nDevelop Correlogram corrplot::corrplot\n\nCheck everything with a cor_test: effect size,significance, confidence intervals\nUse purrr + cor.test to plot correlations and confidence intervals for multiple Quant predictor variables against the target variable\nPlot scatter plots using gf_point.\nAdd extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "title": "Tutorial on Correlations in R",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nmosaicData\n0.20.4\nPruim, Kaplan, and Horton (2023)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nPruim, Randall, Daniel Kaplan, and Nicholas Horton. 2023. mosaicData: Project MOSAIC Data Sets. https://doi.org/10.32614/CRAN.package.mosaicData.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "title": "Tutorial on Correlations in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttp://euclid.psych.yorku.ca/SCS/Gallery/images/galton-corr.jpg&gt;↩︎\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html",
    "title": "\n Change",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs\n\n\n\n\n“The world says: ‘You have needs – satisfy them. You have as much right as the rich and the mighty. Don’t hesitate to satisfy your needs; indeed, expand your needs and demand more.’ This is the worldly doctrine of today. And they believe that this is freedom. The result for the rich is isolation and suicide, for the poor, envy and murder.”\n— Fyodor Dostoevsky",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#sec-slides-and-tutorials",
    "title": "\n Change",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs\n\n\n\n\n“The world says: ‘You have needs – satisfy them. You have as much right as the rich and the mighty. Don’t hesitate to satisfy your needs; indeed, expand your needs and demand more.’ This is the worldly doctrine of today. And they believe that this is freedom. The result for the rich is isolation and suicide, for the poor, envy and murder.”\n— Fyodor Dostoevsky",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#setting-up-r-packages",
    "title": "\n Change",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(GGally) # Corr plots\nlibrary(corrplot) # More corrplots\nlibrary(ggExtra) # Making Combination Plots\n\n# library(devetools)\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR) # Datasets\nlibrary(palmerpenguins) # A famous dataset\n\nlibrary(easystats) # Easy Statistical Analysis and Charts\nlibrary(correlation) # Different Types of Correlations\n# From the easystats collection of packages\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\n\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Our go-to package\nlibrary(skimr) # Another Data inspection package\nlibrary(tidyverse) # Tidy data processing and plotting\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-graphs-will-we-see-today",
    "title": "\n Change",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot\n\n\n\n\n\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot for two variables\nContour Plot\nScatter Plot with Confidence Ellipses\nPairwise Correlation Plots for multiple variables\nCorrelogram for multiple variables\nHeatmap for multiple variables\nErrorbar chart for multiple variables\nCombination chart with marginal densities",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Change",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspiration",
    "title": "\n Change",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: ScatterPlot Inspiration http://www.calamitiesofnature.com/archive/?c=559\n\n\nDoes belief in Evolution depend upon the GSP of of the country? Where is the US in all of this? Does the Bible Belt tip the scales here?\nAnd India?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-is-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-is-correlation",
    "title": "\n Change",
    "section": "\n What is Correlation?",
    "text": "What is Correlation?\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question.\nThe word correlation is used in everyday life to denote some form of association. We might say that we have noticed a correlation between rainy days and reduced sales at supermarkets. However, in statistical terms we use correlation to denote association between two quantitative variables. We also assume that the association is linear, that one variable increases or decreases a fixed amount for a unit increase or decrease in the other. The other technique that is often used in these circumstances is regression, which involves estimating the best straight line to summarise the association.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#pearson-correlation-coefficient",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#pearson-correlation-coefficient",
    "title": "\n Change",
    "section": "\n Pearson Correlation coefficient",
    "text": "Pearson Correlation coefficient\nThe degree of association is measured by a correlation coefficient, denoted by r. It is sometimes called Pearson’s correlation coefficient after its originator and is a measure of linear association. (If a curved line is needed to express the relationship, other and more complicated measures of the correlation must be used.)\nThe correlation coefficient is measured on a scale that varies from + 1 through 0 to – 1. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative.\nIn formal terms, the correlation between two variables \\(x\\) and \\(y\\) is defined as\n\\[\n\\rho = E\\left[\\frac{(x - \\mu_{x}) * (y - \\mu_{y})}{(\\sigma_x)*(\\sigma_y)}\\right]\n\\]\nwhere \\(E\\) is the expectation operator ( i.e taking mean ). Think of this as the average of the products of two scaled variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipPearson Correlation uses z-scores\n\n\n\nWe can see \\((x-\\mu_x)/\\sigma_x\\) is a centering and scaling of the variable \\(x\\). Recall from our discussion on Distributions that this is called the z-score of x.\n\n\nPearson correlation assumes that the relationship between the two variables is linear. There are of course many other types of correlation measures: some which work when this is not so. Type vignette(\"types\", package = \"correlation\") in your Console to see the vignette from the correlation package that discusses various types of correlation measures.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#case-study-1-hollywoodmovies2011-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#case-study-1-hollywoodmovies2011-dataset",
    "title": "\n Change",
    "section": "\n Case Study-1: HollywoodMovies2011 dataset",
    "text": "Case Study-1: HollywoodMovies2011 dataset\nLet us look at the HollywoodMovies2011 dataset from the Lock5withR package. The dataset is also available by clicking the icon below ( in case you are not able to install Lock5withR):\n\n\n Hollywood Movies Dataset",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspecting-the-data",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspecting-the-data",
    "title": "\n Change",
    "section": "\n Inspecting the Data",
    "text": "Inspecting the Data\n\n\n glimpse\n skimr\n mosaic\n web-r\n\n\n\n\nHollywoodMovies2011 -&gt; movies\nglimpse(movies)\n\nRows: 136\nColumns: 14\n$ Movie             &lt;fct&gt; \"Insidious\", \"Paranormal Activity 3\", \"Bad Teacher\",…\n$ LeadStudio        &lt;fct&gt; Sony, Independent, Independent, Warner Bros, Relativ…\n$ RottenTomatoes    &lt;int&gt; 67, 68, 44, 96, 90, 93, 75, 35, 63, 69, 69, 49, 26, …\n$ AudienceScore     &lt;int&gt; 65, 58, 38, 92, 77, 84, 91, 58, 74, 73, 72, 57, 68, …\n$ Story             &lt;fct&gt; Monster Force, Monster Force, Comedy, Rivalry, Rival…\n$ Genre             &lt;fct&gt; Horror, Horror, Comedy, Fantasy, Comedy, Romance, Dr…\n$ TheatersOpenWeek  &lt;int&gt; 2408, 3321, 3049, 4375, 2918, 944, 2534, 3615, NA, 2…\n$ BOAverageOpenWeek &lt;int&gt; 5511, 15829, 10365, 38672, 8995, 6177, 10278, 23775,…\n$ DomesticGross     &lt;dbl&gt; 54.01, 103.66, 100.29, 381.01, 169.11, 56.18, 169.22…\n$ ForeignGross      &lt;dbl&gt; 43.00, 98.24, 115.90, 947.10, 119.28, 83.00, 30.10, …\n$ WorldGross        &lt;dbl&gt; 97.009, 201.897, 216.196, 1328.111, 288.382, 139.177…\n$ Budget            &lt;dbl&gt; 1.5, 5.0, 20.0, 125.0, 32.5, 17.0, 25.0, 80.0, 0.2, …\n$ Profitability     &lt;dbl&gt; 64.672667, 40.379400, 10.809800, 10.624888, 8.873292…\n$ OpeningWeekend    &lt;dbl&gt; 13.27, 52.57, 31.60, 169.19, 26.25, 5.83, 26.04, 85.…\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Data Inspection\n\n\n\nmovies has 136 observations on the following 14 variables.\n\n\nMovie a factor with many levels\n\nLeadStudio a factor with many levels\n\nRottenTomatoes a numeric vector\n\nAudienceScore a numeric vector\n\nStory a factor with many levels\n\nGenre a factor with levels Action, Adventure, Animation, Comedy, Drama, Fantasy, Horror, Romance, Thriller.\n\n\nTheatersOpenWeek a numeric vector. No. of theatres.\n\nBOAverageOpenWeek a numeric vector.\n\nDomesticGross a numeric vector. In million USD.\n\nForeignGross a numeric vector. In million USD.\n\nWorldGross a numeric vector. In million USD.\n\nBudget a numeric vector. In million USD.\n\nProfitability a numeric vector. A ratio\n\nOpeningWeekend a numeric vector. In million USD.\n\nThere are no missing values in the Qual variables; but some entries in the Quant variables are missing. skim throws a warning that we may need to examine later.\n\n\nLet us look at the Quant variables: are these related in anyway? Could the relationship between any two Quant variables also depend upon the level of a Qual variable?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#scatter-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#scatter-plots",
    "title": "\n Change",
    "section": "\n Scatter Plots",
    "text": "Scatter Plots\nWhich are the numeric variables in movies?\n\n\n R\n web-r\n\n\n\n\nmovies_quant &lt;- movies %&gt;%\n  drop_na() %&gt;%\n  select(where(is.numeric))\nmovies_quant\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow let us plot their relationships.\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(DomesticGross ~ WorldGross) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(Profitability ~ OpeningWeekend) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nWe can split some of the scatter plots using one or other of the Qual variables. For instance, is the relationship between the two ratings the same, regardless of movie genre?\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore,\n    color = ~Genre\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = DomesticGross, y = WorldGross)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(OpeningWeekend, Profitability)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(AudienceScore, RottenTomatoes)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(RottenTomatoes, AudienceScore, color = Genre)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from movies scatter plots\n\n\n\nWe have fitted a trend line to each of the scatter plots.\n\n\nDomesticGross and World Gross are related, though there are fewer movies at the high end of DomesticGross…\n\nAudienceScore and RottenTomatoes seem clearly related…both increase together.\n\nOpeningWeek and Profitability are also related in a linear way. There are just two movies which have been extremely profitable..but they do not influence the slope of the trend line too much, because of their location midway in the range of OpeningWeek. Influence is something that is a key concept in Linear Regression.\nBy and large, there are only small variations in slope across Genres.\n\n\n\n\n\n\n\n\n\nImportantIndependent and Dependent Variables\n\n\n\nNote that we have rather arbitrarily taken AudienceScore as the independent variable, to be plotted on the x-axis, and RottenTomatoes on the y-axis. It could easily have been the other way around, based on our Research Question. Datasets are gathered with specific Research Hypotheses in mind, so check the help file and also with the person who gathered the data about what variable they are interested in!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#quantizing-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#quantizing-correlation",
    "title": "\n Change",
    "section": "\n Quantizing Correlation",
    "text": "Quantizing Correlation\nSo we see that there are visible relationships between Quant variables. How do we quantize this relationship, into a correlation score?\nThere are two ways: using the GGally and corplot packages, and doing a formal correlation test with the mosaic package.\n\n\n\nUsing GGally\nUsing corrplot\n\n\n\nBy default, GGally::ggpairs() provides:\n\ntwo different comparisons of each pair of columns\ndisplays either the density or count of the respective variable along the diagonal. \nWith different parameter settings, the diagonal can be replaced with the axis values and variable labels.\n\n\n\n\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"RottenTomatoes\", \"AudienceScore\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #1\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from Pairs Plot#1\n\n\n\n\nAs we saw earlier from the Scatter Plot, AudienceScore and RottenTomatoes are well correlated, with a correlation score of \\(0.833\\)\n\n\nDomesticGross and ForeignGross are also extremely well correlated, with a score of \\(0.873\\).\nBoth these correlation scores are highly significant, with three stars. (We will speak of significance in a while.)\nNone of the other pairs of variables have good correlation scores.\nNote in passing that both the “Gross” related variables have highly skewed distributions. That is the nature of the movie business!\n\n\n\nLet us also try a few other variables, related to budget and profits. For instance, it would be interesting to see the relationship between Budget and Profitability and even either of the “gross” earnings and Profitability.\n\n\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"Budget\", \"Profitability\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #2\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from Pairs Plot #2\n\n\n\n\nThe Budget variable has good correlation scores with DomesticGross and ForeignGross\n\n\nProfitability and Budget seem to have a very slight negative correlation, but this does not appear to be significant.\n\n\n\n\n\nIn this chart, the correlation between pairs of variables is shown symbolically as coloured shapes or colours. Circles, Squares, and Ellipse for example.\n\nThe size, colour, and “orientation” of the shapes in question symbolically represent the strength and polarity of the correlation scores.\nThe direction of the semi-major axis + the colour of the ellipse indicate whether the correlation score is positive or negative;\nAnd the more eccentric the ellipse, the higher is the correlation score in value.\n\n\n\n\n\n\n\nNote\n\n\n\nWhereas GGally computes the correlation scores, corplot “merely” displays them in an evocative way. We need to compute the correlations a priori.\n\n\nNote also:\n\n\n\n\n\n\nTip\n\n\n\nR package corrplot provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables. corrplot is very easy to use and provides a rich array of plotting options in visualization method, graphic layout, color, legend, text labels, etc. It also provides p-values and confidence intervals to help users determine the statistical significance of the correlations.\n\n\n\n# library(corrplot)\nmydata_cor &lt;- cor(movies_quant)\nmydata_cor %&gt;%\n  knitr::kable(caption = \"Correlation Scores Matrix\")\n\n\nCorrelation Scores Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRottenTomatoes\nAudienceScore\nTheatersOpenWeek\nBOAverageOpenWeek\nDomesticGross\nForeignGross\nWorldGross\nBudget\nProfitability\nOpeningWeekend\n\n\n\nRottenTomatoes\n1.0000000\n0.8329740\n-0.0873543\n0.1823480\n0.2085935\n0.0979132\n0.1356232\n-0.0147887\n0.1502764\n0.0986304\n\n\nAudienceScore\n0.8329740\n1.0000000\n0.0259118\n0.1851768\n0.3849406\n0.2557891\n0.3037927\n0.1268649\n0.1047582\n0.2695132\n\n\nTheatersOpenWeek\n-0.0873543\n0.0259118\n1.0000000\n0.0117674\n0.5981162\n0.4850569\n0.5344582\n0.5924941\n0.0547807\n0.5977724\n\n\nBOAverageOpenWeek\n0.1823480\n0.1851768\n0.0117674\n1.0000000\n0.4713164\n0.4522253\n0.4710352\n0.2880262\n0.0964176\n0.5043684\n\n\nDomesticGross\n0.2085935\n0.3849406\n0.5981162\n0.4713164\n1.0000000\n0.8725927\n0.9374780\n0.6497274\n0.1812387\n0.9232259\n\n\nForeignGross\n0.0979132\n0.2557891\n0.4850569\n0.4522253\n0.8725927\n1.0000000\n0.9880383\n0.6707613\n0.1230330\n0.8487202\n\n\nWorldGross\n0.1356232\n0.3037927\n0.5344582\n0.4710352\n0.9374780\n0.9880383\n1.0000000\n0.6830783\n0.1448857\n0.8962294\n\n\nBudget\n-0.0147887\n0.1268649\n0.5924941\n0.2880262\n0.6497274\n0.6707613\n0.6830783\n1.0000000\n-0.1437862\n0.6228180\n\n\nProfitability\n0.1502764\n0.1047582\n0.0547807\n0.0964176\n0.1812387\n0.1230330\n0.1448857\n-0.1437862\n1.0000000\n0.1713962\n\n\nOpeningWeekend\n0.0986304\n0.2695132\n0.5977724\n0.5043684\n0.9232259\n0.8487202\n0.8962294\n0.6228180\n0.1713962\n1.0000000\n\n\n\n\n\n\n\n\n## View the matrix\ncorrplot::corrplot(mydata_cor,\n  method = \"number\",\n  number.cex = 0.6,\n  cl.cex = 0.6, tl.cex = 0.6\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Default plot with circles\ncorrplot(mydata_cor,\n  method = \"circle\",\n  main = \"Correlogram with Circles\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Ellipse plot\ncorrplot(mydata_cor,\n  method = \"ellipse\",\n  main = \"Correlogram with Ellipes\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Heatmap\ncorrplot(mydata_cor,\n  method = \"color\", ## US Spelling only\n  main = \"Correlogram\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Heatmap with numbers\ncorrplot.mixed(mydata_cor,\n  lower = \"color\", number.cex = 0.6,\n  cl.cex = 0.6, tl.cex = 0.6,\n  upper = \"number\",\n  tl.pos = \"l\",\n  main = \"Heatmap?\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from corplots\n\n\n\n\nMost of the variables here have positive correlations, many of them are significant\n\n\n\n\n\n\n\nDoing a Correlation Test\nCorrelations scores can be obtained by conducting a formal test in R. We will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(Profitability ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Profitability vs Budget\"\n  )\n\n\nMovie Profitability vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n-0.08\n-0.96\n0.34\n132\n-0.25\n0.09\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(DomesticGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Domestic Gross vs Budget\"\n  )\n\n\nMovie Domestic Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.7\n11.06\n0\n131\n0.6\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(ForeignGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Foreign Gross vs Budget\"\n  )\n\n\nMovie Foreign Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.69\n10.22\n0\n118\n0.58\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Correlation Tests\n\n\n\nThe budget and profitability are not well correlated, sadly. We see this from the p.value which is \\(0.34\\) and the confidence values for the correlation estimate which also cover \\(0\\).\nHowever, both DomesticGross and ForeignGross are well correlated with Budget. Look at the p.value (=0) and the confidence intervals which are unipolar.\n\n\n\n The ErrorBar Plot for Correlations\nAs stated earlier, in our dataset we have a specific dependent or target variable, which represents the outcome of our experiment or our business situation. The remaining variables are usually independent or predictor variables. A very useful thing to know, and to view, would be the correlations of all independent variables. Using the correlation package from the easystats family of R packages, this can be very easily achieved. Let us quickly do this for the familiar mtcars dataset: we will quickly glimpse it, identify the target variable, and plot the correlations:\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n## Target variable: mpg\n## Calculate all correlations\ncor &lt;- correlation::correlation(mtcars)\ncor\n\n\n  \n\n\n\nWe see correlation between all pairs of variables. We need to choose just those with target variable mpg:\n\n\n\n\ncor %&gt;%\n  # Filter for target variable `mpg` and plot\n  filter(Parameter1 == \"mpg\") %&gt;%\n  gf_point(r ~ reorder(Parameter2, r), size = 4) %&gt;%\n  gf_errorbar(CI_low + CI_high ~ reorder(Parameter2, r),\n    width = 0.5\n  ) %&gt;%\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  gf_labs(\n    title = \"Correlation Errorbar Chart\",\n    subtitle = \"Target variable: mpg\",\n    x = \"Predictor Variable\",\n    y = \"Correlation Score with mpg\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from ErrorBar Plot\n\n\n\n\nSeveral variables are negatively correlated and some are positively correlated with ’mpg`. (The grey line shows “zero correlation”)\nSince none of the error bars straddle zero, the correlations are mostly significant.\n\n\n\n\n\n A New Combination Plot…\nSometimes, a simple scatter, or density alone, or viewed next to one another is not adequate to develop, or convey, our insight. We might just need a combination density + scatter plot. Such a plot can be be constructed from the ground up using ggformula or ggplot; however, there is a nice package called ggExtra that allows the creation of a powerful combination plot:\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  gf_point(body_mass_g ~ flipper_length_mm, colour = ~species) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_labs(x = \"Flipper Length\", y = \"Body Mass in gms\", title = \"Penguins Scatter Plot\", subtitle = \"With Marginal Densities\", caption = \"Using ggExtra\") %&gt;%\n  gf_refine(scale_colour_brewer(palette = \"Accent\")) %&gt;%\n  gf_labs(title = \"Scatter Plot with Marginal Densities\") %&gt;%\n  ggExtra::ggMarginal(\n    type = \"density\", groupColour = TRUE,\n    groupFill = TRUE, margins = \"both\"\n  )\n\n\n\n\n\n\n\nAn Interactive Correlation Game\nHead off to this interactive game website where you can play with correlations!\nhttps://openintro.shinyapps.io/correlation_game/\nSimpson’s Paradox\n\n\nSee how the overall correlation/regression line slopes upward, whereas that for the individual groups slopes downward!! This is an example of Simpson’s Paradox!\n\n Your Turn\n\nTry to play this online Correlation Game.\n\n\n\n\n\n\n\nNote2. School Expenditure and Grades.\n\n\n\n Download the School Data \n\n\n\n\n\n\n\n\nNote3. Gas Prices and Consumption\n\n\n\nAs described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n Download the Gas Consumption Data \n\n\n\n\n\n\n\n\nNote4. Horror Movies (Bah.You awful people..)\n\n\n\n Download the Horror Movie Data \n\n\n\n\n\n\n\n\nNote6. Food Delivery Times\n\n\n\n Download the Food Delivery Data \n\n\n\n Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\n\n\n\n\n\n\n\nImportant\n\n\n\nTarget variables are usually plotted on the Y-axis, while Predictor variables are on the X-Axis, in a Scatter Plot. Why? Because \\(y = mx + c\\) !\n\n\n\nCorrelation scores are good indicators of things that are, well, related. While one variable may not necessarily cause another, a good correlation score may indicate how to chose a good predictor.\nThat is something we will see when we examine Linear Regression\n\nAlways, always, plot and test your data! Both numerical summaries as tables, and graphical summaries as charts, are necessary! See below!!\n\n\n\n\n\n\n\nWarningAnd How about these datasets?\n\n\n\n\n\n\n\n    \n\n      \n\ndataset\n                mean_x\n                mean_y\n                std_dev_x\n                std_dev_y\n                corr_x_y\n              \n\n\naway\n                  54.26610\n                  47.83472\n                  16.76982\n                  26.93974\n                  -0.06412835\n                \n\nbullseye\n                  54.26873\n                  47.83082\n                  16.76924\n                  26.93573\n                  -0.06858639\n                \n\ncircle\n                  54.26732\n                  47.83772\n                  16.76001\n                  26.93004\n                  -0.06834336\n                \n\ndino\n                  54.26327\n                  47.83225\n                  16.76514\n                  26.93540\n                  -0.06447185\n                \n\ndots\n                  54.26030\n                  47.83983\n                  16.76774\n                  26.93019\n                  -0.06034144\n                \n\nh_lines\n                  54.26144\n                  47.83025\n                  16.76590\n                  26.93988\n                  -0.06171484\n                \n\nhigh_lines\n                  54.26881\n                  47.83545\n                  16.76670\n                  26.94000\n                  -0.06850422\n                \n\nslant_down\n                  54.26785\n                  47.83590\n                  16.76676\n                  26.93610\n                  -0.06897974\n                \n\nslant_up\n                  54.26588\n                  47.83150\n                  16.76885\n                  26.93861\n                  -0.06860921\n                \n\nstar\n                  54.26734\n                  47.83955\n                  16.76896\n                  26.93027\n                  -0.06296110\n                \n\nv_lines\n                  54.26993\n                  47.83699\n                  16.76996\n                  26.93768\n                  -0.06944557\n                \n\nwide_lines\n                  54.26692\n                  47.83160\n                  16.77000\n                  26.93790\n                  -0.06657523\n                \n\nx_shape\n                  54.26015\n                  47.83972\n                  16.76996\n                  26.93000\n                  -0.06558334\n                \n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot that cute T-Rex, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)\n\n\n\n\n Conclusions\nScatter Plots give a us sense of change; whether it is linear or non-linear. We can get an idea of correlation between variables with a scatter plot. Our workflow for evaluating correlations between target variable and several other predictor variables uses several packages such as GGally, corrplot, correlation, and of course mosaic for correlation tests.\n\n AI Generated Summary and Podcast\nThis document focusses on correlation between quantitative variables. It examines different ways to visualize correlations, including scatter plots and correlograms. The document provides examples of how to use R packages like GGally and corrplot to create these visualizations and correlation tests to assess the strength and significance of relationships between variables. The tutorial uses the HollywoodMovies2011 and mtcars datasets as examples to demonstrate these concepts. \n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nAntoine Soetewey. Pearson, Spearman and Kendall correlation coefficients by hand https://www.r-bloggers.com/2023/09/pearson-spearman-and-kendall-correlation-coefficients-by-hand/\n\nTaiyun Wei, Viliam Simko. An Introduction to corrplot Package. https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ndatasauRus\n0.1.9\nGillespie et al. (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nlatex2exp\n0.9.6\nMeschiari (2022)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nGillespie, Colin, Steph Locke, Rhian Davies, and Lucy D’Agostino McGowan. 2025. datasauRus: Datasets from the Datasaurus Dozen. https://doi.org/10.32614/CRAN.package.datasauRus.\n\n\nMeschiari, Stefano. 2022. Latex2exp: Use LaTeX Expressions in Plots. https://doi.org/10.32614/CRAN.package.latex2exp.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#an-interactive-correlation-game",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#an-interactive-correlation-game",
    "title": "\n Change",
    "section": "An Interactive Correlation Game",
    "text": "An Interactive Correlation Game\nHead off to this interactive game website where you can play with correlations!\nhttps://openintro.shinyapps.io/correlation_game/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#simpsons-paradox",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#simpsons-paradox",
    "title": "\n Change",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\n\nSee how the overall correlation/regression line slopes upward, whereas that for the individual groups slopes downward!! This is an example of Simpson’s Paradox!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#your-turn",
    "title": "\n Change",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry to play this online Correlation Game.\n\n\n\n\n\n\n\nNote2. School Expenditure and Grades.\n\n\n\n Download the School Data \n\n\n\n\n\n\n\n\nNote3. Gas Prices and Consumption\n\n\n\nAs described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n Download the Gas Consumption Data \n\n\n\n\n\n\n\n\nNote4. Horror Movies (Bah.You awful people..)\n\n\n\n Download the Horror Movie Data \n\n\n\n\n\n\n\n\nNote6. Food Delivery Times\n\n\n\n Download the Food Delivery Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#wait-but-why",
    "title": "\n Change",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\n\n\n\n\n\n\n\nImportant\n\n\n\nTarget variables are usually plotted on the Y-axis, while Predictor variables are on the X-Axis, in a Scatter Plot. Why? Because \\(y = mx + c\\) !\n\n\n\nCorrelation scores are good indicators of things that are, well, related. While one variable may not necessarily cause another, a good correlation score may indicate how to chose a good predictor.\nThat is something we will see when we examine Linear Regression\n\nAlways, always, plot and test your data! Both numerical summaries as tables, and graphical summaries as charts, are necessary! See below!!\n\n\n\n\n\n\n\nWarningAnd How about these datasets?\n\n\n\n\n\n\n\n    \n\n      \n\ndataset\n                mean_x\n                mean_y\n                std_dev_x\n                std_dev_y\n                corr_x_y\n              \n\n\naway\n                  54.26610\n                  47.83472\n                  16.76982\n                  26.93974\n                  -0.06412835\n                \n\nbullseye\n                  54.26873\n                  47.83082\n                  16.76924\n                  26.93573\n                  -0.06858639\n                \n\ncircle\n                  54.26732\n                  47.83772\n                  16.76001\n                  26.93004\n                  -0.06834336\n                \n\ndino\n                  54.26327\n                  47.83225\n                  16.76514\n                  26.93540\n                  -0.06447185\n                \n\ndots\n                  54.26030\n                  47.83983\n                  16.76774\n                  26.93019\n                  -0.06034144\n                \n\nh_lines\n                  54.26144\n                  47.83025\n                  16.76590\n                  26.93988\n                  -0.06171484\n                \n\nhigh_lines\n                  54.26881\n                  47.83545\n                  16.76670\n                  26.94000\n                  -0.06850422\n                \n\nslant_down\n                  54.26785\n                  47.83590\n                  16.76676\n                  26.93610\n                  -0.06897974\n                \n\nslant_up\n                  54.26588\n                  47.83150\n                  16.76885\n                  26.93861\n                  -0.06860921\n                \n\nstar\n                  54.26734\n                  47.83955\n                  16.76896\n                  26.93027\n                  -0.06296110\n                \n\nv_lines\n                  54.26993\n                  47.83699\n                  16.76996\n                  26.93768\n                  -0.06944557\n                \n\nwide_lines\n                  54.26692\n                  47.83160\n                  16.77000\n                  26.93790\n                  -0.06657523\n                \n\nx_shape\n                  54.26015\n                  47.83972\n                  16.76996\n                  26.93000\n                  -0.06558334\n                \n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot that cute T-Rex, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#conclusions",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#conclusions",
    "title": "\n Change",
    "section": "\n Conclusions",
    "text": "Conclusions\nScatter Plots give a us sense of change; whether it is linear or non-linear. We can get an idea of correlation between variables with a scatter plot. Our workflow for evaluating correlations between target variable and several other predictor variables uses several packages such as GGally, corrplot, correlation, and of course mosaic for correlation tests.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#ai-generated-summary-and-podcast",
    "title": "\n Change",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis document focusses on correlation between quantitative variables. It examines different ways to visualize correlations, including scatter plots and correlograms. The document provides examples of how to use R packages like GGally and corrplot to create these visualizations and correlation tests to assess the strength and significance of relationships between variables. The tutorial uses the HollywoodMovies2011 and mtcars datasets as examples to demonstrate these concepts. \n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#references",
    "title": "\n Change",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nAntoine Soetewey. Pearson, Spearman and Kendall correlation coefficients by hand https://www.r-bloggers.com/2023/09/pearson-spearman-and-kendall-correlation-coefficients-by-hand/\n\nTaiyun Wei, Viliam Simko. An Introduction to corrplot Package. https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ndatasauRus\n0.1.9\nGillespie et al. (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nlatex2exp\n0.9.6\nMeschiari (2022)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nGillespie, Colin, Steph Locke, Rhian Davies, and Lucy D’Agostino McGowan. 2025. datasauRus: Datasets from the Datasaurus Dozen. https://doi.org/10.32614/CRAN.package.datasauRus.\n\n\nMeschiari, Stefano. 2022. Latex2exp: Use LaTeX Expressions in Plots. https://doi.org/10.32614/CRAN.package.latex2exp.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate) # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate) # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "title": "🕔 Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHoe might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "title": "🕔 Time Series",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe modern tsibble format: this is a new modern format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using tsbox) to a tibble or a tsibble and work with these.\n\n\n\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with tibble format that we can plot as is. We will then do more after conversion to tsibble format, and then a third example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R and other more recent packages:\n# Base R\nplot(AirPassengers)\n# tsbox static plot\ntsbox::ts_plot(AirPassengers, ylab = \"Passengers\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n  \n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;%\n  filter(Chick == 1, Diet == 1) %&gt;%\n  select(weight, Time)\n\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight, frequency = 2)\nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\nNow we can plot this in many ways:\nplot(ChickWeight_ts) # Using base-R\n# ts_boxable(ChickWeight_ts)\n# Using tsbox\ntsbox::ts_plot(ChickWeight_ts,\n  ylab = \"Weight of Chick #1\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Using TSstudio\nTSstudio::ts_plot(ChickWeight_ts,\n  Xtitle = \"Time\",\n  Ytitle = \"Weight of Chick #1\"\n)\n\n\n\n\n\nWe see that the weights of a young chick specimen increases over time.\n\ntibble data\nThe ts data format can handle only one time series. If we want multiple time series, based on say Qualitative variables, we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\n\ngapminder data\n\n\n\n  \n\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in:\n\nbirths_2000_2014 &lt;- read_csv(\"../data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nbirths_2000_2014\n\n\n  \n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns. There are no Qualitative variables (yet!).\nPlotting tibble time series\n\n\nUsing ggformula\nUsing tsbox and TSstudio\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~day_of_week,\n    color = ~day_of_week\n  ) %&gt;%\n  gf_point(title = \"By Day of Week\") %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~date_of_month,\n    color = ~date_of_month\n  ) %&gt;%\n  gf_point(title = \"By Date of Month\") %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize. Summaries will help, so we could calculate the the mean births on a month basis in each year and plot that:\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;%\n  gf_line(mean_monthly_births ~ year,\n    group = ~month,\n    colour = ~month, linewidth = 1\n  ) %&gt;%\n  gf_point(size = 1.5, title = \"Summaries of Monthly Births over the years\") %&gt;%\n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  gf_line(mean_daily_births ~ year,\n    group = ~day_of_week,\n    colour = ~day_of_week,\n    linewidth = 1,\n    data = .\n  ) %&gt;%\n  gf_point(size = 2) %&gt;%\n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it or using it as a time series. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\n\n\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births, year, month, date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nTipExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  TSstudio::ts_plot(\n    Xtitle = \"Year\",\n    Ytitle = \"Births\",\n    title = \"Births Time Series\",\n    Xgrid = TRUE, Ygrid = TRUE,\n    slider = TRUE,\n    width = 1\n  ) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section.\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    group = day_of_week,\n    color = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"By Day of Week\") +\n  scale_colour_distiller(palette = \"Paired\")\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% ggplot(aes(year, births,\n  group = date_of_month,\n  color = date_of_month\n)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"By Date of Month\") +\n  scale_colour_distiller(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n###\nbirths_2000_2014_monthly %&gt;%\n  ggplot(aes(year, mean_monthly_births,\n    group = month, colour = month\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  labs(title = \"Summaries of Monthly Births over the years\") +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births,\n    group = day_of_week,\n    colour = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary over time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data.  Let us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.Run data(package = \"tsibbledata\") in your Console to find out about these.\n\ndata(\"PBS\")\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\nPBS\n\n\n  \n\n\n\nData Description: This is a large-ish dataset:Run PBS in your console\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. Type help(\"PBS\") in your Console.\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(title = \"PBS Costs vs time\")\n\n\n\n\n\n\n\n\n\n\nPBS %&gt;% ggplot(aes(Month, Cost)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PBS Costs vs time\")\n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy, and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "title": "🕔 Time Series",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe have now arrived at the need to filter, group, and summarize time-series data. We can do this in two ways, with two packages:\n\n\n\n\n\n\nTiptsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\nFiltering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.\n\n\n\n\n\n\n\n\nTiptimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other variables (i.e non-time).\n\n\nLet us first see how many observations there are for each combo of keys:\nPBS %&gt;%\n  count()\n# Grouped Counts\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  dplyr::count()\n# dplyr grouping\nPBS %&gt;%\n  dplyr::group_by(ATC1, ATC2) %&gt;%\n  dplyr::count()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nWe have 336 combinations of Qualitative variables, each combo containing 204 observations (except some! Take a look!): so let us filter for a few such combinations and plot:\n# Costs\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs, per Month\")\n# Scripts\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Scripts ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Scripts, per Month\")\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\",\n    ATC2 == \"A10\"\n  ) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs, per Month for General/A/A10 category patients\")\n# Scripts variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\",\n    ATC2 == \"A10\"\n  ) %&gt;%\n  gf_line(Scripts ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Scripts, per Month for General/A/A10 category patients\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods, and also with Costs and Scripts. Strongly seasonal for both, with seasonal variation increasing over the years, a clear sign of a multiplicative time series. There is a strong upward trend with both types of subsidies, Safety net and Co-payments. But these trends are somewhat different in magnitude for specific combinations of ATC1 and ATC2 categories.\nWe can use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time:\n\n# Original Data\nPBS\n\n\n  \n\n\n# Cost Summary by Month, which is the original data\n# Only grouping happens here\n# New Variable Name to make grouping visible\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  tsibble::index_by(Month_Group = Month) %&gt;%\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n# Cost Summary by Quarter\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  tsibble::index_by(Year_Quarter = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n# Cost Summary by Year\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  index_by(Year_Group = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n\nFinally, it may be a good idea to convert some tibble into a tsibble to leverage some of functions that tsibble offers:\n\nbirths_tsibble &lt;- births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  # Convert to tsibble\n  tsibble::as_tsibble(index = date) # Time Variable\n\nbirths_tsibble\n\n\n  \n\n\n\nThis is DAILY data of course. Let us say we want to group by month and plot mean monthly births as before, but now using tsibble and the index variable:\n\n\ntsibble vs timetk: Basic Plot\ntsibble vs timetk: Grouped Plot 1\ntsibble vs timetk: Grouped Plot 2\n\n\n\nbirths_tsibble %&gt;%\n  gf_line(births ~ date,\n    data = .,\n    title = \"Basic tsibble plotted with ggformula\"\n  )\n# timetk **can** plot tsibbles.\nbirths_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = date,\n    .value = births,\n    .title = \"Tsibble Plotted with timetk\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ month_index,\n    data = .,\n    title = \"Monthly Aggregate with tsibble\"\n  ) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\nbirths_timeseries %&gt;%\n  # timetk cannot wrangle tsibbles\n  # timetk needs tibble or data frame\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, mean,\n    .title = \"Monthly aggregate births with timetk\",\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\n\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ year_index, data = .) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\nbirths_timeseries %&gt;%\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"year\",\n    mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, mean,\n    .title = \"Yearly aggregate births with timetk\",\n    .x_lab = \"year\",\n    .y_lab = \"Mean Yearly Births\"\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "title": "🕔 Time Series",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month / quarter or year?\n\n Monthly Box Plots\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;%\n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date,\n    group = ~month_index,\n    fill = ~month_index, data = .\n  )\n# plot the groups\n# 180 plots!!\n\nbirths_timeseries %&gt;%\n  # timetk::summarise_by_time(.date_var = date,\n  #                           .by = \"month\",\n  #                           mean = mean(births)) %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Monthly births with timetk\",\n    .x_lab = \"year\", .period = \"month\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Quarterly boxplots\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date,\n    group = ~qrtr_index,\n    fill = ~qrtr_index,\n    data = .\n  ) # 60 plots!!\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Quarterly births with timetk\",\n    .x_lab = \"year\", .period = \"quarter\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n Yearwise boxplots\n\nbirths_tsibble %&gt;%\n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n  # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ date,\n    group = ~year_index,\n    fill = ~year_index,\n    data = .\n  ) %&gt;% # plot the groups 15 plots\n  gf_labs(title = \"Yearly aggregate births with ggformula\") %&gt;%\n  gf_theme(scale_fill_distiller(palette = \"Spectral\"))\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Yearly aggregate births with timetk\",\n    .x_lab = \"year\", .period = \"year\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods.\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(\n    births &gt;= 10000 ~ \"high\",\n    births &lt;= 8000 ~ \"low\",\n    TRUE ~ \"fine\"\n  )) %&gt;%\n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~birthrate,\n    color = \"black\"\n  ) %&gt;%\n  gf_theme(scale_x_time(\n    breaks = 1:12,\n    labels = c(\n      \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n      \"May\", \"Jun\", \"Jul\", \"Aug\",\n      \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    )\n  )) %&gt;%\n  gf_theme(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "title": "🕔 Time Series",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them. We have also seen how to decompose time series into periodic and aperiodic components, which can be used to make business decisions."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "title": "🕔 Time Series",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and model-based graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\nTime Series Analysis at Our Coding Club"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "title": "🕔 Time Series",
    "section": "\n Readings",
    "text": "Readings\n\nThe Nuclear Threat—The Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time – A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "title": "🕔 Time Series",
    "section": "Extra Stuff",
    "text": "Extra Stuff\nUsing tsbox and TSstudio\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births, year, month, date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nTipExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  TSstudio::ts_plot(\n    Xtitle = \"Year\",\n    Ytitle = \"Births\",\n    title = \"Births Time Series\",\n    Xgrid = TRUE, Ygrid = TRUE,\n    slider = TRUE,\n    width = 1\n  ) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "title": "🕔 Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate) # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk) # Convert data frames to time series-specific objects\nlibrary(forecast) # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate) # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk) # Convert data frames to time series-specific objects\nlibrary(forecast) # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "title": "Analysis of Time Series in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe have seen how to plot different formats of time series, and how to create summary plots using packages like tsibble and timetk.\nWe will now see how a time series can be broken down to its components so as to systematically understand and analyze it. Thereafter, we examine how to model the timeseries, and make forecasts, a task more like synthesis.\nWe have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\nHow does one process and analyze time series data?\nHow does one plot time series?\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\nWhat is auto correlation etc.\nWhat is a stationary time series?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\nwalmart_sales_weekly\nglimpse(walmart_sales_weekly)\ninspect(walmart_sales_weekly)\n# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\n\n\n\n  \n\n\n\nRows: 1,001\nColumns: 17\n$ id           &lt;fct&gt; 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_…\n$ Store        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Dept         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date         &lt;date&gt; 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-…\n$ Weekly_Sales &lt;dbl&gt; 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3…\n$ IsHoliday    &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ Type         &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Size         &lt;dbl&gt; 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1…\n$ Temperature  &lt;dbl&gt; 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6…\n$ Fuel_Price   &lt;dbl&gt; 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2…\n$ MarkDown1    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown2    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown3    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown4    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown5    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CPI          &lt;dbl&gt; 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380…\n$ Unemployment &lt;dbl&gt; 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7…\n\n\n\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\n\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a “special” holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\n\n\n\n\n\n\nNoteNot yet a time series!\n\n\n\nThis is still a tibble, with a time-oriented variable of course, but not yet a time-series object. The data frame has the YMD columns repeated for each Dept, giving us what is called “long” form data. To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\n\n\n\n#|label: walmart sales tsibble\nwalmart_tsibble &lt;-\n  walmart_sales_weekly %&gt;%\n  as_tsibble(\n    index = Date, # Time Variable\n    key = c(id, Store, Dept, Type)\n  )\n\n#  Identifies unique \"subject\" who are measures\n#  All other variables such as Weekly_sales become \"measured variable\"\n#  Each observation should be uniquely identified by index and key\nwalmart_tsibble\n\n\n  \n\n\n\n\n Basic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfeasts::autoplot(walmart_tsibble, .vars = Weekly_Sales)\n\n\n\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\n\n\n\n\n\n\nNote\n\n\n\nQ.1 How are the weekly sales different for each Department?\nThere are 7 number of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nwalmart_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = Date,\n    .value = Weekly_Sales,\n    .color_var = Dept,\n    .legend_show = TRUE,\n    .title = \"Walmart Sales Data by Department\",\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\n# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_tsibble %&gt;%\n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(\n    .date_var = Date,\n    .start_date = \"2011-12-01\",\n    .end_date = \"2011-12-31\"\n  ) %&gt;%\n  plot_time_series(\n    .date_var = Date,\n    .value = Weekly_Sales,\n    .color_var = Dept,\n    .facet_vars = Dept,\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) # Only 4 points per graph\n\n\n\n\n\nClearly the “unfortunate” Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems…\n\n\n\n Too much noise? How about some averaging?\n\n\n\n\n\n\nNote\n\n\n\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\n# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\":\n\nrolling_avg_month &lt;- timetk::slidify(\n  .period = 4, # every 4 weeks\n  .f = mean, # The function to use\n  .align = \"center\", # Aligned with middle of month\n  .partial = TRUE\n) # To catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n&lt;bytecode: 0x165d62bf8&gt;\n&lt;environment: 0x165d622c8&gt;\n\n\nOK, slidify creates a function! Let’s apply it to the Walmart Sales time series…\n\nwalmart_tsibble %&gt;%\n  # group_by(Dept) %&gt;% # Is this needed?\n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %&gt;%\n  # ungroup() %&gt;% # Is this needed?\n  timetk::plot_time_series(Date, avg_monthly_sales,\n    .color_var = Dept, # Does the grouping!\n    .smooth = FALSE\n  )\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis…should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\n\n\n\n Decomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\n\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\n\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years). Often combined with Trend into “Trend-Cycle”.\n\nError or Noise: Random component\n\nWhen data is non-seasonal this means breaking it up into only trend and irregular components. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nwalmart_tsibble %&gt;%\n  filter(Dept == \"38\") %&gt;%\n  timetk::plot_stl_diagnostics(\n    .data = .,\n    .date_var = Date,\n    .value = Weekly_Sales\n  )\n\n\n\n\n\nWe can do this for all Depts using fable and fabletools:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nwalmart_decomposed &lt;-\n  walmart_tsibble %&gt;%\n  # If we want to filter, we do it here\n  # filter(Dept == \"38\") %&gt;%\n  #\n\n  fabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n  \n\n\nfeasts::autoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study #2: Dataset from nycflights13\n",
    "text": "Case Study #2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets in the nycflights13 package:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\ndata(\"flights\", package = \"nycflights13\")\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nflights_delay_ts &lt;- flights %&gt;%\n  mutate(\n    arr_delay = replace_na(arr_delay, 0),\n    dep_delay = replace_na(dep_delay, 0)\n  ) %&gt;%\n  select(\n    time_hour, arr_delay, dep_delay,\n    carrier, origin, dest,\n    flight, tailnum\n  ) %&gt;%\n  tsibble::as_tsibble(\n    index = time_hour,\n    # All the remaining identify unique entries\n    # Along with index\n    # Many of these variables are common\n    # Need *all* to make unique entries!\n    key = c(carrier, origin, dest, flight, tailnum),\n    validate = TRUE\n  ) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\n\nUsing tsibble\nUsing timetk\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmean_arr_delays_by_carrier &lt;-\n  flights_delay_ts %&gt;%\n  group_by(carrier) %&gt;%\n  index_by(month = ~ yearmonth(.)) %&gt;%\n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n\n  summarise(\n    mean_arr_delay =\n      mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n# colours from the \"I want Hue\" website\ncolour_values &lt;- c(\n  \"#634bd1\", \"#35d25a\", \"#757bff\", \"#fa9011\", \"#72369a\",\n  \"#617d00\", \"#d094ff\", \"#81d8a4\", \"#e63d2d\", \"#0080c0\",\n  \"#9e4500\", \"#98a9ff\", \"#efbd7f\", \"#474e8c\", \"#ffa1e4\",\n  \"#8a3261\", \"#a6c1f8\", \"#a16e96\"\n)\n\n# Plotting with ggformula\nmean_arr_delays_by_carrier %&gt;%\n  gf_hline(yintercept = 0, color = \"grey\") %&gt;%\n  gf_line(\n    mean_arr_delay ~ month,\n    group = ~carrier,\n    color = ~carrier,\n    linewidth = 1.5,\n    title = \"Average Monthly Arrival Delays by Carrier\",\n    caption = \"Using tsibble + ggformula\"\n  ) %&gt;%\n  gf_facet_wrap(vars(carrier), nrow = 4) %&gt;%\n  gf_refine(scale_color_manual(\n    name = \"Airlines\",\n    values = colour_values\n  )) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1,\n    size = 6\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nmean_arr_delays_by_carrier2 &lt;-\n  flights_delay_ts %&gt;%\n  as_tibble() %&gt;%\n  group_by(carrier) %&gt;%\n  # summarize_by_time REQUIRES a tibble\n  # Cannot do this with a tsibble\n  timetk::summarise_by_time(\n    .date_var = time_hour,\n    .by = \"month\",\n    mean_arr_delay = mean(arr_delay)\n  )\n\n\nmean_arr_delays_by_carrier2\ncolour_values &lt;- c(\n  \"#634bd1\", \"#35d25a\", \"#757bff\", \"#fa9011\", \"#72369a\",\n  \"#617d00\", \"#d094ff\", \"#81d8a4\", \"#e63d2d\", \"#0080c0\",\n  \"#9e4500\", \"#98a9ff\", \"#efbd7f\", \"#474e8c\", \"#ffa1e4\",\n  \"#8a3261\", \"#a6c1f8\", \"#a16e96\"\n)\np &lt;- mean_arr_delays_by_carrier2 %&gt;%\n  timetk::plot_time_series(\n    # .data = .,\n    .date_var = time_hour, # no change to time variable name!\n    .value = mean_arr_delay,\n    .color_var = carrier,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .interactive = FALSE, .line_size = 2,\n    .facet_ncol = 4, .legend_show = FALSE, .facet_scales = \"fixed\",\n    .title = \"Average Monthly Arrival Delays by Carrier\",\n    .y_lab = \"Arrival Delays over Time\", .x_lab = \"Time\"\n  ) +\n  geom_hline(yintercept = 0, color = \"grey\") +\n  scale_colour_manual(\n    name = \"Airline\",\n    values = colour_values\n  ) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(caption = \"Using timetk\")\n\n# Reverse the layers in the plot\n# Zero line BELOW, time series above\n# https://stackoverflow.com/questions/20249653/insert-layer-underneath-existing-layers-in-ggplot2-object\n\np$layers &lt;- rev(p$layers)\np\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nInsights: - Clearly airline OO has had some serious arrival delay problems in the first half of 2013… - most other delays are around the zero-line, with some variations in both directions\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. Plot a candlestick chart for total flight delays for a particular month for each origin across airlines!\n\n\nUsing ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nflights_delay_ts %&gt;%\n  mutate(\n    total_delay = arr_delay + dep_delay,\n    month = lubridate::month(time_hour,\n      # Makes ordinal factor with month labels\n      label = TRUE\n    )\n  ) %&gt;%\n  filter(month == \"Dec\") %&gt;%\n  gf_boxplot(total_delay ~ .,\n    color = ~origin,\n    fill = ~origin, alpha = 0.3\n  ) %&gt;%\n  gf_facet_wrap(vars(carrier), nrow = 3, scales = \"free_y\") %&gt;%\n  gf_theme(theme(axis.text.x = element_blank()))\n\n\n\n\n\n\n\n\n\n\nflights_delay_ts %&gt;%\n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  timetk::filter_by_time(\n    .start_date = \"2013-12\",\n    .end_date = \"2013-12\"\n  ) %&gt;%\n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = carrier,\n    .period = \"month\",\n    .interactive = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\nInsights: - JFK has more outliers in total_delay than EWR and LGA - Just a hint of more delays in April… and July?\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.3. Plot a heatmap chart for total flight delays by origin, aggregated by month\n\n\nUsing tsibble + ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\navg_delays_month &lt;- flights_delay_ts %&gt;%\n  group_by(origin) %&gt;%\n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  index_by(month = ~ yearmonth(.)) %&gt;%\n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  summarise(mean_monthly_delay = mean(total_delay, na.rm = TRUE))\n\navg_delays_month\n\n\n  \n\n\n# three origins 12 months therefore 36 rows\n# Tsibble index_by + summarise also gives us a  month` column\n\n\n\nggformula::gf_tile(origin ~ month,\n  fill = ~mean_monthly_delay,\n  color = \"black\", data = avg_delays_month,\n  title = \"Mean Flight Delays from NY Airports in 2013\"\n) %&gt;%\n  gf_theme(scale_fill_viridis_c(option = \"A\"))\n\n\n\n\n\n\n# \"magma\" (or \"A\") inferno\" (or \"B\") \"plasma\" (or \"C\")\n# \"viridis\" (or \"D\") \"cividis\" (or \"E\")\n# \"rocket\" (or \"F\") \"mako\" (or \"G\") \"turbo\" (or \"H\")\n\n\n\n\n\n\n\nInsights: - TBD - TBD"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "title": "Analysis of Time Series in R",
    "section": "\n Seasons, Trends, Cycles, and Random Changes",
    "text": "Seasons, Trends, Cycles, and Random Changes\nHere are how the different types of patterns in time series are as follows:\n\nTrend: A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend.\n\n\nSeasonal: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of drugs (with the PBS data) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n\n\nCyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.\n\nThe function feasts::STL allows us to create these decompositions.\nLet us try to find and plot these patterns in Time Series.\n\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n##\nbirths_tsibble &lt;-\n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n##\nbirths_STL_yearly &lt;-\n  births_tsibble %&gt;%\n  fabletools::model(STL(births ~ season(period = \"year\")))\n\nfabletools::components(births_STL_yearly)\n\n\n  \n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nfeasts::autoplot(components(births_STL_yearly))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "title": "Analysis of Time Series in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can plot most of the common Time Series Plots with the help of the tidyverts packages: ( tsibble, feast, fable and fabletools) , along with timetk and ggformula.\nThere are other plot packages to investigate, such as dygraphs\nRecall that we have used the tsibble format for the data. There are other formats such as ts, xts and others which are meant for time series analysis. But for our present purposes, we are able to do things with the capabilities of timetk."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "title": "Analysis of Time Series in R",
    "section": "\n References",
    "text": "References\n\nRob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice (3rd ed), Available Online https://otexts.com/fpp3/\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html",
    "title": "\n Summaries",
    "section": "",
    "text": "“The most certain sign of wisdom is cheerfulness.”\n— Michel de Montaigne, Writer and philosopher",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#using-web-r",
    "title": "\n Summaries",
    "section": "\n Using web-R",
    "text": "Using web-R\nThis tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\nKeyboard Shortcuts\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#setting-up-r-packages",
    "title": "\n Summaries",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(tidyverse)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-we-grasp-data",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-we-grasp-data",
    "title": "\n Summaries",
    "section": "\n How do we Grasp Data?",
    "text": "How do we Grasp Data?\nWe spoke of Experiments and Data Gathering in the first module Nature of Data. This helped us to obtain data.\nAs we discussed in that same Module, for us to grasp the significance of the data, we need to describe it; the actual data is usually too vast for us to comprehend in its entirety. Anything more than a handful of observations in a dataset is enough for us to require other ways of grasping it.\nThe first thing we need to do, therefore, is to reduce it to a few salient numbers that allow us to summarize the data.\n\n\n\n\n\n\nImportantReduction is Addition\n\n\n\nSuch a reduction may seem paradoxical but is one of the important tenets of statistics: reduction, while taking away information, ends up adding to insight.\nSteven Stigler (2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. One of the Big Ideas in Statistics from that book is: Aggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\nLet us get some inspiration from Brad Pitt, from the movie Moneyball, which is about applying Data Analytics to the game of baseball.\n\n And then, an example from a more sombre story:\n\n\n\n\n\n\nYear\nBelow Level #1\nLevel #1\nLevel #2\nLevel #3\nLevels #4 and #5\n\n\n\nNumber in millions (2012/2014)\n8.35\n26.49\n65.10\n71.41\n26.57\n\n\nNumber in millions (2017)\n7.59\n29.23\n66.07\n68.81\n26.75\n\n\n\n\nNote: \n\n\n\n\n\n\n\n\n SOURCE: U.S. Department of Education, National Center for Education Statistics, Program for the International Assessment of Adult Competencies (PIAAC), U.S. PIAAC 2017, U.S. PIAAC 2012/2014.\n\n\n\n\n\n\n\n\n\n\n\nTable 1\n\n\n\nThis ghastly-looking Table 1 examines U.S. adults with low English literacy and numeracy skills—or low-skilled adults—at two points in the 2010s, in the years 2012/20141 and 2017, using data from the Program for the International Assessment of Adult Competencies (PIAAC). As can be seen the summary table is quite surprising in absolute terms, for a developed country like the US, and the numbers have increased from 2012/2014 to 2017!\nSo why do we need to summarise data? Summarization is an act of throwing away data to make more sense, as stated by (Stigler 2016) and also in the movie by Brad Pitt aka Billy Beane. To summarize is to understand. Add to that the fact that our Working Memories can hold maybe 7 items, so it means information retention too.\nAnd if we don’t summarise? Jorge Luis Borges, in a fantasy short story published in 1942, titled “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes, there were only details.” (emphasis mine)\nAggregation can yield great gains above the individual components in data. Funes was big data without Statistics.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-graphs-numbers-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-graphs-numbers-will-we-see-today",
    "title": "\n Summaries",
    "section": "\n What graphs / numbers will we see today?",
    "text": "What graphs / numbers will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\n“Chart Shape”\n\n\nAll\nAll\nTables and Stat Measures\n\n\n\n\n\nBefore we plot a single chart, it is wise to take a look at several numbers that summarize the dataset under consideration. What might these be? Some obviously useful numbers are:\n\nDataset length: How many rows/observations?\nDataset breadth: How many columns/variables?\nHow many Quant variables?\nHow many Qual variables?\nQuant variables: min, max, mean, median, sd\nQual variables: levels, counts per level\nBoth: means, medians for each level of a Qual variable…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Summaries",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nWe will obviously choose all variables in the dataset, unless they are unrelated ones such as row number or ID which (we think) may not contribute any information and we can disregard.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-these-summaries-work",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-these-summaries-work",
    "title": "\n Summaries",
    "section": "\n How do these Summaries Work?",
    "text": "How do these Summaries Work?\nQuant variables: Inspecting the min, max, mean, median, variance and sd of each of the Quant variables tells us straightaway what the ranges of the variables are, and if there are some outliers, which could be normal, or maybe due to data entry error! Comparing two Quant variables for their ranges also tells us that we may have to \\(scale/normalize\\) them for computational ease, if one variable has large numbers and the other has very small ones.\nQual variables: With Qual variables, we understand the levels within each, and understand the total number of combinations of the levels across these. Counts across levels, and across combinations of levels tells us whether the data has sufficient readings for graphing, inference, and decision-making, of if certain levels/classes of data are under or over represented.\nTogether?: We can use Quant and Qual together, to develop the above summaries (min, max,mean, median and sd) for Quant variables, again across levels, and across combinations of levels of single or multiple Quals, along with counts if we are interested in that.\nFor both types of variables, we need to keep an eye open for data entries that are missing! This may point to data gathering errors, which may be fixable. Or we will have to take a decision to let go of that entire observation (i.e. a row). Or even do what is called imputation to fill in values that are based on the other values in the same column, which sounds like we are making up data, but isn’t so really.\nAnd this may also tell us if we are witnessing a Simpson’s Paradox situation. You may have to decide on what to do with this data sparseness, or just check your biases!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#some-quick-definitions",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#some-quick-definitions",
    "title": "\n Summaries",
    "section": "\n Some Quick Definitions",
    "text": "Some Quick Definitions\n\n\n\n\n\n\nImportantMean\n\n\n\nThe sample mean, or average, of a Quantitative data variable can be calculated as the sum of the observed values divided by the number of observations:\n\\[\nmean = \\bar{x} = \\frac{x_1 + x_2+ x_3....+x_n}{n}\n\\]\n\n\n\n\n\n\n\n\nImportantVariance and Standard Deviation\n\n\n\nObservations can be on either side of the mean, naturally. To measure the extent of these differences, we square and sum the differences between individual values and their mean, and take their average to obtain the (sample) variance:\n\\[\nvariance = s^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_2 - \\bar{x})^2 +...(x_n - \\bar{x})^2 +}{n-1}\n\\] The standard deviation \\(s\\) is just the square root of the variance.\n\n\n(The \\(n-1\\) is a mathematical nuance to allow for the fact that we have used the data to calculate the mean before we get to \\(s^2\\), and hence have “used up” one degree of randomness in the data. It gets us more robust results.)\n\n\n\n\n\n\nImportantMedian\n\n\n\nWhen the observations in a Quant variable are placed in order of their maginitude, the observation in tke middle is the median. Half the observations are below, and half are above, the median",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-1",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-1",
    "title": "\n Summaries",
    "section": "\n Case Study-1",
    "text": "Case Study-1\nWe will first use a dataset mpg that is available in R as part of one of the R packages that we have loaded with the library() command.\n\n Examine the Data\nIt is usually a good idea to make crisp business-like tables, for the data itself, and the schema as revealed by one of the outputs of the three methods to be presented below. There are many methods to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr and kableExtra packagepackage:\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    # add Human Readable column names\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\nNext we will look at a few favourite statistics or “favstats” that we can derive from data. R is full of packages that can provide very evocative and effective summaries of data. We will first start with the dplyr package from the tidyverse, the skimr package, then the mosaic package. We will look at the summary outputs from these and learn how to interpret them.\n\n\nUsing dplyr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n web-r\n\n\n\nThe dplyr package offers a convenient command called glimpse:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nVery crisp output, giving us the size of the dataset (234 X 11) and the nature of the variable columns, along with their first few entries.\nThe chr variables are usually Categorical/Qualitative.\nThe int or dbl (double precision) are usually Numerical/Quantitative.\nBut be careful! Verify that this is as per your intent, interpret the variables and modify their encoding as needed.\n\n\n\n\n\n\nLet us look at mpgusing skimr::skim().\nFrom the output of ?skimr:\n\nThe format of the results are a single wide data frame combining the results, with some additional attributes and two metadata columns:\n\n\n\nskim_variable: name of the original variable\n\nskim_type: class of the variable\n\nWe can use skim(dataset) directly as shown below:\n\nskimr::skim(mpg) # explicitly stating package name\n\n\nData summary\n\n\nName\nmpg\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\n\n\nmodel\n0\n1\n2\n22\n0\n38\n0\n\n\ntrans\n0\n1\n8\n10\n0\n10\n0\n\n\ndrv\n0\n1\n1\n1\n0\n3\n0\n\n\nfl\n0\n1\n1\n1\n0\n5\n0\n\n\nclass\n0\n1\n3\n10\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ndispl\n0\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nyear\n0\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\ncyl\n0\n1\n5.89\n1.61\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\ncty\n0\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nhwy\n0\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\nTaken together, we have the following:\n\n\n\n\n\n\nNote\n\n\n\n\nA Data Summary: it lists the dimensions of the mpg dataset: 234 rows and 11 columns. 6 columns are character formatted, the remaining 5 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each. From the histogram we can see that year is two-valued, cyl is three-valued, and cty and hwy are continuous… Again check that this is as you intend them to be. We may need to modify the encoding if needed.\n\n\n\n\n\nWe get very similar output from mosaic::inspect():\n\ninspect(mpg)\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the output of mosaic::inspect() is organized as follows:\n\nThere are two dataframes/tables in the output, one describing the Qualitative Variables and the other describing the Quantitative Variables.\nIn the table describing the Qual variables, we have:\n\n\nname: Name of the variable in the (parent) dataset. i.e Column Names\n\nclass: format of that column\n\nlevels: All these variables are factors, with levels shown here. Some for example, manufacturer has 15 levels, and there are 234 rows\n\n\n\ninspect also conveniently shows how much data is missing and in which variables. This is a very important consideration in the use of the data for analytics purposes.\n\n\nWe can save and see the outputs separately:\n\nmpg_describe &lt;- inspect(mpg)\nmpg_describe$categorical\nmpg_describe$quantitative\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary and Munging\nUsing skim/inspect/glimpse, we can put together a (brief) data dictionary as follows:\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nmodel(chr): Car model name\n\nmanufacturer(chr): Car maker name\n\nfl(chr): fuel type\n\ndrv(chr): type of drive(front, rear, 4W)\n\nclass(chr): type of vehicle ( sedan, pickup…)\n\ntrans(chr): type of transmission ( auto, manual..)\n\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nhwy(int): Highway Mileage\n\ncty(int): City Mileage\n\ncyl(int): Number of Cylinders. How do we understand this variable? Should this be Qual?\n\ndispl(dbl): Engine piston displacement\n\nyear(int): Year of model\n\n\n\nWe see that there are certain variables that must be converted to factors for analytics purposes, since they are unmistakably Qualitative in nature. Let us do that now, for use later:\n\nmpg_modified &lt;- mpg %&gt;%\n  dplyr::mutate(\n    cyl = as_factor(cyl),\n    fl = as_factor(fl),\n    drv = as_factor(drv),\n    class = as_factor(class),\n    trans = as_factor(trans)\n  )\nglimpse(mpg_modified)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;fct&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;fct&gt; auto(l5), manual(m5), manual(m6), auto(av), auto(l5), man…\n$ drv          &lt;fct&gt; f, f, f, f, f, f, f, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, r, …\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;fct&gt; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, r, …\n$ class        &lt;fct&gt; compact, compact, compact, compact, compact, compact, com…\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-2",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-2",
    "title": "\n Summaries",
    "section": "\n Case Study-2",
    "text": "Case Study-2\nInstead of taking a “built-in” dataset , i.e. one that is part of an R package that we can load with library(), let us try the above process with a data set that we obtain from the internet. We will use this superb repository of datasets created by Vincent Arel-Bundock: https://vincentarelbundock.github.io/Rdatasets/articles/data.html\nLet us choose a modest-sized dataset, say this dataset on Doctor Visits, which is available online https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv and read it into R.\n\n\n\n\n\n\nImportantReading external data into R\n\n\n\nThe read_csv() command from R package readr allows us to read both locally saved data on our hard disk, or data available in a shared folder online. Avoid using the read.csv() from base R , though it will show up in your code auto-complete set of options!\n\n\n\n# From Vincent Arel-Bundock's dataset website\n# https://vincentarelbundock.github.io/Rdatasets\n#\n# read_csv can read data directly from the net\n# Don't use read.csv()\ndocVisits &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv\")\n\nRows: 5190 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): gender, private, freepoor, freerepat, nchronic, lchronic\ndbl (7): rownames, visits, age, income, illness, reduced, health\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo, a data frame containing 5,190 observations on 12 variables.\n\n\n\n\n\n\nNoteHow about a locally stored CSV file?\n\n\n\nWe can also use a locally downloaded and stored CSV file. Assuming the file is stored in a subfolder called data inside your R project folder, we can proceed as follows:\n\n```{r}\n#| eval: false\ndocVisits &lt;- read_csv(\"data/DoctorVisits.csv\")\n```\n\n\n\nLet us quickly report the data itself, as in a real report. Note that we can use the features of the kableExtra package to dress up this table too!!\n\ndocVisits %&gt;%\n  head(10) %&gt;%\n  kbl(\n    caption = \"Doctor Visits Dataset\",\n    # Add Human Readable Names if desired\n    # col.names(..names that you may want..)\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nDoctor Visits Dataset\n\nrownames\nvisits\ngender\nage\nincome\nillness\nreduced\nhealth\nprivate\nfreepoor\nfreerepat\nnchronic\nlchronic\n\n\n\n1\n1\nfemale\n0.19\n0.55\n1\n4\n1\nyes\nno\nno\nno\nno\n\n\n2\n1\nfemale\n0.19\n0.45\n1\n2\n1\nyes\nno\nno\nno\nno\n\n\n3\n1\nmale\n0.19\n0.90\n3\n0\n0\nno\nno\nno\nno\nno\n\n\n4\n1\nmale\n0.19\n0.15\n1\n0\n0\nno\nno\nno\nno\nno\n\n\n5\n1\nmale\n0.19\n0.45\n2\n5\n1\nno\nno\nno\nyes\nno\n\n\n6\n1\nfemale\n0.19\n0.35\n5\n1\n9\nno\nno\nno\nyes\nno\n\n\n7\n1\nfemale\n0.19\n0.55\n4\n0\n2\nno\nno\nno\nno\nno\n\n\n8\n1\nfemale\n0.19\n0.15\n3\n0\n6\nno\nno\nno\nno\nno\n\n\n9\n1\nfemale\n0.19\n0.65\n2\n0\n5\nyes\nno\nno\nno\nno\n\n\n10\n1\nmale\n0.19\n0.15\n1\n0\n0\nyes\nno\nno\nno\nno\n\n\n\n\n\n\n\n Examine the Data\n\n\nUsing dplyr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n web-r\n\n\n\n\nglimpse(docVisits)\n\nRows: 5,190\nColumns: 13\n$ rownames  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ visits    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ gender    &lt;chr&gt; \"female\", \"female\", \"male\", \"male\", \"male\", \"female\", \"femal…\n$ age       &lt;dbl&gt; 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, …\n$ income    &lt;dbl&gt; 0.55, 0.45, 0.90, 0.15, 0.45, 0.35, 0.55, 0.15, 0.65, 0.15, …\n$ illness   &lt;dbl&gt; 1, 1, 3, 1, 2, 5, 4, 3, 2, 1, 1, 2, 3, 4, 3, 2, 1, 1, 1, 1, …\n$ reduced   &lt;dbl&gt; 4, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 13, 7, 1, 0, 0, 1, 0, 0,…\n$ health    &lt;dbl&gt; 1, 1, 0, 0, 1, 9, 2, 6, 5, 0, 0, 2, 1, 6, 0, 7, 5, 0, 0, 0, …\n$ private   &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"ye…\n$ freepoor  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ freerepat &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ nchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"…\n$ lchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from dplyr::glimpse()\n\n\n\nVery crisp output, giving us the size of the dataset (5190 X 13) and the nature of the variable columns, along with their first few entries. There are several Quantitative variables: visits, age, income, illness, reduced and health; the rest seem to be Qualitative variables.\nAlways document your data with variable descriptions when you share it, a data dictionary!\n\n\n\n\n\nskim(docVisits) %&gt;% kbl()\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\ngender\n0\n1\n4\n6\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nprivate\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreepoor\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreerepat\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nnchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nlchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nrownames\n0\n1\nNA\nNA\nNA\nNA\nNA\n2595.5000000\n1498.3682792\n1.00\n1298.25\n2595.50\n3892.75\n5190.00\n▇▇▇▇▇\n\n\nnumeric\nvisits\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.3017341\n0.7981338\n0.00\n0.00\n0.00\n0.00\n9.00\n▇▁▁▁▁\n\n\nnumeric\nage\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.4063854\n0.2047818\n0.19\n0.22\n0.32\n0.62\n0.72\n▇▂▁▂▅\n\n\nnumeric\nincome\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.5831599\n0.3689067\n0.00\n0.25\n0.55\n0.90\n1.50\n▇▆▅▅▂\n\n\nnumeric\nillness\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.4319846\n1.3841524\n0.00\n0.00\n1.00\n2.00\n5.00\n▇▂▂▁▁\n\n\nnumeric\nreduced\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.8618497\n2.8876284\n0.00\n0.00\n0.00\n0.00\n14.00\n▇▁▁▁▁\n\n\nnumeric\nhealth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.2175337\n2.1242665\n0.00\n0.00\n0.00\n2.00\n12.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from skimr::skim()\n\n\n\n\nA Data Summary: it lists the dimensions of the docVisits dataset: 5190 rows and 13 columns. 6 columns are character formatted, the remaining 7 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each.\nCan we consider the health Goldberg score a Qualitative variable, to be understood as “ranks” between a minimum and maximum? It is just possible…\n\n\n\n\n\n\ninspect(docVisits)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1    gender character      2 5190       0\n2   private character      2 5190       0\n3  freepoor character      2 5190       0\n4 freerepat character      2 5190       0\n5  nchronic character      2 5190       0\n6  lchronic character      2 5190       0\n                                   distribution\n1 female (52.1%), male (47.9%)                 \n2 no (55.7%), yes (44.3%)                      \n3 no (95.7%), yes (4.3%)                       \n4 no (79%), yes (21%)                          \n5 no (59.7%), yes (40.3%)                      \n6 no (88.3%), yes (11.7%)                      \n\nquantitative variables:  \n      name   class  min      Q1  median      Q3     max         mean\n1 rownames numeric 1.00 1298.25 2595.50 3892.75 5190.00 2595.5000000\n2   visits numeric 0.00    0.00    0.00    0.00    9.00    0.3017341\n3      age numeric 0.19    0.22    0.32    0.62    0.72    0.4063854\n4   income numeric 0.00    0.25    0.55    0.90    1.50    0.5831599\n5  illness numeric 0.00    0.00    1.00    2.00    5.00    1.4319846\n6  reduced numeric 0.00    0.00    0.00    0.00   14.00    0.8618497\n7   health numeric 0.00    0.00    0.00    2.00   12.00    1.2175337\n            sd    n missing\n1 1498.3682792 5190       0\n2    0.7981338 5190       0\n3    0.2047818 5190       0\n4    0.3689067 5190       0\n5    1.3841524 5190       0\n6    2.8876284 5190       0\n7    2.1242665 5190       0\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from mosaic::inspect()\n\n\n\nWe see that the output of mosaic::inspect() is organized very similarly to the output from skim. Is there any missing data? Both skim and mosaic report on the data completion for each variable in the dataset.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nvisits\nNumber of doctor visits in past 2 weeks.\n\n\ngender\nFactor indicating gender.\n\n\nage\nAge in years divided by 100.\n\n\nincome\nAnnual income in tens of thousands of dollars.\n\n\nillness\nNumber of illnesses in past 2 weeks.\n\n\nreduced\nNumber of days of reduced activity in past 2 weeks due to illness or injury.\n\n\nhealth\nGeneral health questionnaire score using Goldberg’s method.\n\n\nprivate\nFactor. Does the individual have private health docVisits?\n\n\nfreepoor\nFactor. Does the individual have free government health docVisits due to low income?\n\n\nfreerepat\nFactor. Does the individual have free government health docVisits due to old age, disability or veteran status?\n\n\nnchronic\nFactor. Is there a chronic condition not limiting activity?\n\n\nlchronic\nFactor. Is there a chronic condition limiting activity?\n\n\n\nHere too, we should convert the variables that are obviously Qualitative into factors, ordered or otherwise:\n\ndocVisits_modified &lt;- docVisits %&gt;%\n  mutate(\n    gender = as_factor(gender),\n    private = as_factor(private),\n    freepoor = as_factor(freepoor),\n    freerepat = as_factor(freerepat),\n    nchronic = as_factor(nchronic),\n    lchronic = as_factor(lchronic)\n  )\ndocVisits_modified\n\n\n  \n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-counts-of-qualitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-counts-of-qualitative-variables",
    "title": "\n Summaries",
    "section": "\n Groups and Counts of Qualitative Variables",
    "text": "Groups and Counts of Qualitative Variables\nWhat is the most important dialogue uttered in the movie “Sholay”?\nRecall our discussion in Types of Data Variables. We have looked at means, limits, and percentiles of Quantitative variables. Another good idea to examine datasets is to look at counts, proportions, and frequencies with respect to Qualitative variables.\nWe typically do this with the dplyr package from the tidyverse.\n\n\nmpg dataset\ndocVisits dataset\n web-r\n\n\n\n\nmpg_modified %&gt;% dplyr::count(cyl)\n\n\n  \n\n\nmpg_modified %&gt;% mosaic::count(drv) # does the same thing! Counts!\n\n\n  \n\n\nmpg_modified %&gt;% count(fl)\n\n\n  \n\n\n### All combinations of cut, color, clarity\n### Overwhelming??\nmpg_modified %&gt;%\n  count(across(where(is.factor)))\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Groups and Counts (mpg)\n\n\n\n\nWe see that the groups for each level of cyl, drv, and fl are not the same size: for instance the group with the “r”-level fl type is largest at 168 observations, and the “r”-level in drv has only 25 observations.\nGroup counts based on cyl are also not balanced.\nCounting all combinations of these three factors shows counts of 1 for several combination and does not lead to any decent amount of aggregation.\n\nThese aspects may need to be factored into downstream modelling or machine learning tasks. (Usually by stratification wrt levels of the Qualitative variables)\nThe levels are not too many, so tables work, and so would bar charts, which we will examine next. If there are too many levels in any factor, tables are a better option. Bar charts can still be plotted, but it may be preferable to lump smaller categories/levels together. (Using the forcats package)\n\n\n\n\n\n## Counting by the obvious factor variables\ndocVisits %&gt;% count(gender)\n\n\n  \n\n\ndocVisits %&gt;% count(private)\n\n\n  \n\n\ndocVisits %&gt;% count(freepoor)\n\n\n  \n\n\ndocVisits %&gt;% count(freerepat)\n\n\n  \n\n\ndocVisits %&gt;% count(lchronic)\n\n\n  \n\n\ndocVisits %&gt;% count(nchronic)\n\n\n  \n\n\n\n\n# Now for all Combinations...\n# Maybe too much to digest...\ndocVisits %&gt;% count(across(where(is.character)))\n\n\n  \n\n\n# Shall we try counting by some variables that might be factors?\n# Even if they are labeled as &lt;dbl&gt;?\n#\ndocVisits %&gt;% count(illness)\n\n\n  \n\n\ndocVisits %&gt;% count(health)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Groups and Counts (docVisits)\n\n\n\n\nMost of the counts are roughly balanced across the levels of the factors; however, freepoor and lchronic show unbalanced counts…\nThe factors are too numerous for a combination count table to very useful..\nCounting by illness and health does show that these two columns have a limited set of integer entries across over 5000 rows!! So these can be thought of as factors if needed in the analysis. So not every integer variable is necessaily a number!!\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-summaries-of-quantitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-summaries-of-quantitative-variables",
    "title": "\n Summaries",
    "section": "\n Groups and Summaries of Quantitative Variables",
    "text": "Groups and Summaries of Quantitative Variables\nWe saw that we could obtain numerical summary stats such as means, medians, quartiles, maximum/minimum of entire Quantitative variables, i.e the complete column. However, we often need identical numerical summary stats of parts of a Quantitative variable. Why?\nNote that we have Qualitative variables as well in a typical dataset. These Qual variables help us to group the entire dataset based on their combinations of levels. We can now think of summarizing Quant variables within each such group.\nLet us work through these ideas for both our familiar datasets.\n\n\nmpg dataset\ndocvisits dataset\n web-r\n\n\n\n\nmpg_modified %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n\n  \n\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n\n  \n\n\n# Perhaps the best method for us!\nmpg_modified %&gt;%\n  mosaic::favstats(hwy ~ cyl, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n# Be aware of the first column format here!\nmpg_modified %&gt;%\n  mosaic::favstats(hwy ~ cyl + fl, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Grouped Quant Summaries (mpg)\n\n\n\n\nWe have quite some variation of mean_hwy mileage over cyl , though the groups/level are quite imbalanced. This is of course a small dataset.\nThe number of groups are large enough (&gt;&gt; 7!) to warrant a chart, which we will make in our next module on Distributions.\nMean price varies quite some based on cyland fl. Some groups are non-existent, hence we see “NA” and “NaN” in the output of mosaic::favstats.; and also on combinations of cyl:fl. This should point to the existence of some interaction effect when modelling for price.\n\n\n\n\n\n\ndocVisits_modified %&gt;%\n  group_by(gender) %&gt;%\n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  group_by(gender) %&gt;%\n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  group_by(freepoor, nchronic) %&gt;%\n  summarise(\n    mean_income = mean(income),\n    average_visits = mean(visits),\n    count = n()\n  )\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  mosaic::favstats(income ~ gender, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  mosaic::favstats(income ~ freepoor + nchronic, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Grouped Quant Summaries (docVisits)\n\n\n\nClearly the people who are freepoor ( On Govt Insurance) AND with a chronic condition are those who have lower average income and a higher average number of visits to the doctor…but there are relatively few of them (n = 55) in this dataset.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#more-on-dplyr",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#more-on-dplyr",
    "title": "\n Summaries",
    "section": "\n More on dplyr",
    "text": "More on dplyr\nThe dplyr package is capable of doing much more than just count, group_by and summarize. We will encounter this package many times more as we build our intuition about data visualization. A full tutorial on dplyr is linked to the icon below:\n\n\n\ndplyr Tutorial",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#reporting-tables-for-data-and-the-data-schema",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#reporting-tables-for-data-and-the-data-schema",
    "title": "\n Summaries",
    "section": "\n Reporting Tables for Data and the Data Schema",
    "text": "Reporting Tables for Data and the Data Schema\n\n\n\n\n\n\nImportantData and the Data Schema are Different!!\n\n\n\nNote that all the three methods (dplyr::glimpse(), skimr::skim(), and mosaic::inspect()) report the schema of the original dataframe. The schema are also formatted as data frames! However they do not “contain” the original data! Do not confuse between the data and it’s reported schema!\n\n\nAs stated earlier, it is usually a good idea to make crisp business-like tables, for the data itself, and of the schema as revealed by one of the outputs of the three methods (glimpse/skim/inspect) presented above. There are many table-making methods in R to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr and kableExtra packages that we have installed already:\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    longtable = FALSE, centering = TRUE,\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\nAnd for the schema from skim(), with some extra bells and whistles on the table:\n\nskim(mpg) %&gt;%\n  kbl(align = \"c\", caption = \"Skim Output for mpg Dataset\") %&gt;%\n  kable_paper(full_width = F)\n\n\nSkim Output for mpg Dataset\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nmodel\n0\n1\n2\n22\n0\n38\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ntrans\n0\n1\n8\n10\n0\n10\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ndrv\n0\n1\n1\n1\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfl\n0\n1\n1\n1\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nclass\n0\n1\n3\n10\n0\n7\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\ndispl\n0\n1\nNA\nNA\nNA\nNA\nNA\n3.471795\n1.291959\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nnumeric\nyear\n0\n1\nNA\nNA\nNA\nNA\nNA\n2003.500000\n4.509646\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\nnumeric\ncyl\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.888889\n1.611535\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\nnumeric\ncty\n0\n1\nNA\nNA\nNA\nNA\nNA\n16.858974\n4.255946\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nnumeric\nhwy\n0\n1\nNA\nNA\nNA\nNA\nNA\n23.440171\n5.954643\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\nSee https://haozhu233.github.io/kableExtra/ for more options on formatting the table with kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#a-quick-quiz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#a-quick-quiz",
    "title": "\n Summaries",
    "section": "\n A Quick Quiz",
    "text": "A Quick Quiz\n\n\n\n\n\n\nWarning\n\n\n\nIt is always a good idea to look for variables in data that may be incorrectly formatted. For instance, a variable marked as numerical may have the values 1-2-3-4 which represent options, sizes, or say months. in which case it would have to be interpreted as a factor.\n\n\nLet us take a small test with the mpg dataset:\n\nWhat is the number of qualitative/categorical variables in the mpg data?    \n\nHow many manufacturers are named in this dataset?    \n\nHow many levels does the variable drv have?    \n\nHow many quantitative/numerical variables shown in the mpg data?    \n\nBut the variable hwy\ncty\ncyl\ndispl   is actually a qualitative variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#your-turn",
    "title": "\n Summaries",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry your hand at these datasets. Look at the data, state the data dictionary, contemplate a few Research Questions and answer them with Summaries and Tables in Quarto!\nTry adding more summary functions to the summary table? Which might you choose? Why?\n\n\n\n\n\n\n\nNoteStar Trek Books\n\n\n\n\n\n Start Trek Book data\n\n\nWhich would be the Group By variables here? And what would you summarize? With which function?\n\n\n\n\n\n\n\n\nNoteMath Anxiety! Hah!\n\n\n\n\n\n Math Anxiety data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#wait-but-why",
    "title": "\n Summaries",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nData Summaries give you the essentials, without getting bogged down in the details(just yet).\n\nSummaries help you “live with your data”; this is an important step in understanding it, and deciding what to do with it.\n\nSummaries help evoke Questions and Hypotheses, which may lead to inquiries, analysis, and insights\n\n\nGrouped Summaries should tell you if:\n\ncounts of groups in your target audience are lopsided/imbalanced; Go and Get your data again.\n\nthere are visible differences in Quant data across groups, so your target audience could be nicely fractured;\netc.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#conclusion",
    "title": "\n Summaries",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nThe three methods (glimpse/skim/inspect) given here give us a very comprehensive look into the structure of the dataset.\nThe favstats method allows us to compute a whole lot of metrics for Quant variables for each level of one or more *Qual variables.\nUse the kable set of commands to make a smart-looking of the data and the outputs of any of the three methods.\n\nMake these part of your Workflow.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#ai-generated-summary-and-podcast",
    "title": "\n Summaries",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on using the R programming language to perform descriptive statistical analysis on data sets. The tutorial focuses on summarizing data using various R packages like dplyr, skimr, and mosaic. It emphasizes the importance of understanding the data’s structure, identifying different types of variables (qualitative and quantitative), and calculating summary statistics such as means, medians, and frequencies. The tutorial provides examples using real datasets and highlights the significance of data summaries in gaining initial insights, formulating research questions, and identifying potential issues with the data.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#references",
    "title": "\n Summaries",
    "section": "\n References",
    "text": "References\n\nLock, Lock, Lock, Lock, and Lock. (2021). Statistics: Unlocking the Power of Data, 3rd Edition). https://media.wiley.com/product_data/excerpt/69/11196821/1119682169-32.pdf\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\npalmerpenguins\n0.1.1\nHorst, Hill, and Gorman (2020)\n\n\nskimr\n2.1.5\nWaring et al. (2022)\n\n\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nStigler, Stephen M. 2016. “The Seven Pillars of Statistical Wisdom,” March. https://doi.org/10.4159/9780674970199.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://doi.org/10.32614/CRAN.package.skimr.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#what-other-free-tools-are-there-on-the-web",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#what-other-free-tools-are-there-on-the-web",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#references",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "References",
    "text": "References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish’s policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#papers",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#papers",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "Papers",
    "text": "Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html",
    "title": "\n Networks",
    "section": "",
    "text": "“The beginnings and endings of all human undertakings are untidy.”\n— John Galsworthy, author, Nobel laureate (14 Aug 1867-1933)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#setting-up-r-packages",
    "title": "\n Networks",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(igraph) # Network Analysis Library (Handle data and Viz)\nlibrary(tidygraph) # For Network \"Manipulation\"\nlibrary(ggraph) # For Network Visualization\nlibrary(graphlayouts) # For Network Visualization, more layouts and themes\nlibrary(visNetwork) # For Interactive Network Visualization\nlibrary(igraphdata) # For \"Network\" Datasets\nlibrary(sand) # Statistical Analysis of Networks Data\n\nlibrary(tinytable) # Elegant Tables for our data\n\n# For repeatable layouts, some can be random!!\nset.seed(12345)\n\nlibrary(tidyverse)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#introduction",
    "title": "\n Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "title": "\n Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#goals",
    "title": "\n Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#pedagogical-note",
    "title": "\n Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#graph-metaphors",
    "title": "\n Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nTipExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#predictruninfer-1",
    "title": "\n Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"files/data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"files/data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(\n  nodes = grey_nodes,\n  edges = grey_edges,\n  directed = FALSE\n)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line\n# About what that line does for the overall graph\n\nggraph(graph = ga, layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(\n    shape = 21, size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n\n  labs(\n    title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\"\n  ) +\n\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc.\n\nggraph(graph = ga, layout = \"kk\") +\n  geom_edge_link0(width = 2) +\n  geom_node_point(\n    shape = 21, size = 4,\n    fill = \"moccasin\",\n    color = \"firebrick\",\n    stroke = 2\n  ) +\n  labs(\n    title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\"\n  ) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n\n  geom_node_point(aes(fill = race),\n    colour = \"black\",\n    size = 4, shape = 21\n  ) + # change variable here\n\n  labs(\n    title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n    subtitle = \"Colouring Nodes by Attribute\",\n    caption = \"Grey's Anatomy\"\n  ) +\n\n  scale_edge_width(range = c(0.2, 2)) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?\n\n\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(\n    edge_width = \"Weight\", title = \"Grey's Anatomy\",\n    subtitle = \"Arc Layout\"\n  ) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n\n  geom_node_point(size = 3, colour = \"red\") +\n  geom_node_text(aes(label = name),\n    repel = TRUE, size = 2, check_overlap = TRUE,\n    max.overlaps = 25\n  ) +\n  labs(\n    edge_width = \"Weight\",\n    title = \"Grey's Anatomy\",\n    subtitle = \"Arc Layout\"\n  ) +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#hierarchical-layouts",
    "title": "\n Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class\n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;- tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") +\n  geom_edge_diagonal() +\n  labs(title = \"Dendrogram\")\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) +\n  geom_edge_diagonal0() +\n  geom_node_point(aes(filter = leaf)) +\n  coord_fixed() +\n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth), size = 0.25) +\n  scale_fill_distiller(palette = \"Pastel1\") +\n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) +\n  scale_fill_distiller(palette = \"Accent\") +\n  coord_fixed() +\n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") +\n  geom_node_tile(aes(y = -y, fill = depth)) +\n  scale_fill_distiller(palette = \"Set3\") +\n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") +\n  coord_fixed() +\n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#faceting",
    "title": "\n Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\nset_graph_style(family = \"Roboto Condensed\", size = 11)\n##\n# facet edges by type\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_edges(~type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1) +\n  labs(\n    title = \"Grey's Anatomy Network\",\n    subtitle = \"Types of Relationships\"\n  )\n\n# facet nodes by sex\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0() +\n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1) +\n  labs(\n    title = \"Grey's Anatomy Network\",\n    subtitle = \"Race\"\n  )\n\n\n# facet both nodes and edges\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_graph(type ~ race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\") +\n  labs(\n    title = \"Grey's Anatomy Network\",\n    subtitle = \"Race and Relationship\"\n  )\n\nunset_graph_style()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#network-analysis-with-tidygraph",
    "title": "\n Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\n\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;%\n  filter(degree &gt; 0) %&gt;%\n  activate(edges) %&gt;%\n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\nggraph(ga, layout = \"nicely\") +\n  geom_edge_link0(aes(width = centrality_edge_betweenness())) +\n\n  geom_node_point(aes(\n    colour = centrality_degree(),\n    size = centrality_degree()\n  )) +\n\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n\n  scale_size(name = \"Degree\", range = c(2, 5)) +\n\n  scale_color_fermenter(\n    name = \"Degree\", # USE SAME NAME to Merge legends!!\n    palette = \"Set1\",\n    aesthetics = c(\"colour\", \"fill\"),\n    guide = guide_legend(reverse = FALSE)\n  ) +\n\n  scale_edge_width(name = \"Betweenness\", range = c(0.25, 1)) +\n  labs(\n    title = \"Grey's Anatomy\",\n    subtitle = \"Nodes Scaled by Degree, Edge-width scaled by Betweenness\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\n# visualize communities of nodes\nga %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(aes(fill = community), size = 3, shape = 21) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;%\n  rowid_to_column(var = \"id\") %&gt;%\n  rename(\"label\" = name) %&gt;%\n  mutate(sex = case_when(\n    sex == \"F\" ~ \"Female\",\n    sex == \"M\" ~ \"Male\"\n  )) %&gt;%\n  replace_na(., list(sex = \"Transgender?\")) %&gt;%\n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;%\n  select(from, to) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"from\" = \"label\")\n  ) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"to\" = \"label\")\n  ) %&gt;%\n  select(\"from\" = id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;%\n  visNodes(font = list(size = 40)) %&gt;%\n  # Colour and icons for each of the gender-groups\n  visGroups(\n    groupname = \"Female\", shape = \"icon\",\n    icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\", shape = \"icon\",\n    icon = list(code = \"f183\", size = 75, color = \"slateblue\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\", shape = \"icon\",\n    icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  # visLegend() %&gt;%\n  # Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;%\n  # Add Interaction Controls\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis, ) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"files/data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"files/data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;-\n  starwars_nodes %&gt;%\n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;-\n  starwars_edges %&gt;%\n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;%\n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;%\n  # Select \"id.x\" and \"id.y\" ONLY\n  # Rename them as \"from\" and \"to\"\n  # keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(font = list(size = 30)) %&gt;%\n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(\n    font = list(size = 30), shape = \"icon\",\n    icon = list(code = \"f1e3\", size = 75)\n  ) %&gt;%\n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  ) %&gt;%\n  addFontAwesome(version = \"4.7.0\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#your-turn",
    "title": "\n Networks",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;%\n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nNoteThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\", package = \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nNoteGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nNoteOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#references",
    "title": "\n Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.2.2\nDavid Schoch (2023)\n\n\nigraph\n2.1.4\n\nCsardi and Nepusz (2006); Csárdi et al. (2025)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://doi.org/10.32614/CRAN.package.visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://doi.org/10.32614/CRAN.package.igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://doi.org/10.32614/CRAN.package.sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://doi.org/10.32614/CRAN.package.showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html",
    "title": "\n Counts",
    "section": "",
    "text": "“No matter what happens in life, be good to people. Being good to people is a wonderful legacy to leave behind.”\n— Taylor Swift",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#slides-and-tutorials",
    "title": "\n Counts",
    "section": "",
    "text": "“No matter what happens in life, be good to people. Being good to people is a wonderful legacy to leave behind.”\n— Taylor Swift",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#setting-up-r-packages",
    "title": "\n Counts",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(tidyverse) # Most Important Last\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-graphs-will-we-see-today",
    "title": "\n Counts",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQual\nNone\nBar Chart",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Counts",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#inspiration",
    "title": "\n Counts",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Capital Cities\n\n\nHow much does the (financial) capital of a country contribute to its GDP? Which would be India’s city? What would be the reduction in percentage? And these Germans are crazy.(Toc, toc, toc, toc!)\nNote how the axis variable that defines the bar locations is a …Qual variable!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#graphing-packages-in-r",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#graphing-packages-in-r",
    "title": "\n Counts",
    "section": "\n Graphing Packages in R",
    "text": "Graphing Packages in R\nThere are several Data Visualization packages, even systems, within R.\n\nBase R supports graph making out of the box; (fast, very flexible, but a bit complex)(Update July 2025: There is a new(?) package called tinyplots that creates base R plots with syntax that is much more intuitive. (We will include some here for reference.)\nThe most well known is ggplot https://ggplot2-book.org/ which uses Leland Wilkinson’s concept of a “Grammar of Graphics”; ggformula is a wrapper around ggplot that makes the syntax a little more concise and intuitive.\nThere is the lattice package https://lattice.r-forge.r-project.org/ which uses the “Trellis Graphics” concept framework for data visualization developed by R. A. Becker, W. S. Cleveland, et al.;\nAnd the grid package https://bookdown.org/rdpeng/RProgDA/the-grid-package.html that allows extremely fine control of shapes plotted on the graph.\n\nEach system has its benefits and learning complexities. We will look at plots created using ggformula, and the recently introduced tidyplots package, which allows intuitive creation of publication-ready charts based on the famous and established ggplot framework. We will, where appropriate state ggplot code too for comparison.\nA quick reminder on how mosaic and ggformula and ggplot work in a very similar fashion:\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith mosaic, one can create a statistical correlation test between two variables as: cor_test(y ~ x, data = ______ )\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____) In practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()\n\n\n\n\n\n\n\n\nTiptidyplots command template\n\n\n\ntidyplot(data = ---, x = ---, y = ---, color = ---) |&gt; add_***_***()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-charts-and-histograms",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-charts-and-histograms",
    "title": "\n Counts",
    "section": "\n Bar Charts and Histograms",
    "text": "Bar Charts and Histograms\nBar Charts show counts of observations with respect to a Qualitative variable. For instance, a shop inventory with shirt-sizes. Each bar has a height proportional to the count per shirt-size, in this example.\nAlthough Histograms may look similar to Bar Charts, the two are different. First, histograms show continuous Quant data. By contrast, bar charts show categorical data, such as shirt-sizes, or apples, bananas, carrots, etc. Visually speaking, histograms do not usually show spaces between bars because these are continuous values, while column charts must show spaces to separate each category.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#how-do-bar-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#how-do-bar-charts-work",
    "title": "\n Counts",
    "section": "\n How do Bar Chart(s) Work?",
    "text": "How do Bar Chart(s) Work?\nBar are used to show “counts” and “tallies” with respect to Qual variables: they answer the question How Many?. For instance, in a survey, how many people vs Gender? In a Target Audience survey on Weekly Consumption, how many low, medium, or high expenditure people?\nEach Qual variable potentially has many levels as we saw in the Nature of Data. For instance, in the above example on Weekly Expenditure, low, medium and high were levels for the Qual variable Expenditure. Bar charts perform internal counts for each level of the Qual variable under consideration. The Bar Plot is then a set of disjoint bars representing these counts; see the icon above, and then that for histograms!! The X-axis is the set of levels in the Qual variable, and the Y-axis represents the counts for each level.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#case-study-1-chicago-taxi-rides-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#case-study-1-chicago-taxi-rides-dataset",
    "title": "\n Counts",
    "section": "\n Case Study-1: Chicago Taxi Rides dataset",
    "text": "Case Study-1: Chicago Taxi Rides dataset\nWe will first look at at a dataset that speaks about taxi rides in Chicago in the year 2022. This is available on Vincent Arel-Bundock’s superb repository of datasets.Let us read into R directly from the website.\n\n R\n\n\n\ntaxi &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/taxi.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\ndplyr::glimpse(taxi)\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;chr&gt; \"Chicago Independents\", \"City Service\", \"other\", \"Chicago Ind…\n$ local    &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\",…\n$ dow      &lt;chr&gt; \"Thu\", \"Thu\", \"Mon\", \"Mon\", \"Sun\", \"Sat\", \"Fri\", \"Sun\", \"Fri\"…\n$ month    &lt;chr&gt; \"Feb\", \"Mar\", \"Feb\", \"Apr\", \"Mar\", \"Apr\", \"Mar\", \"Jan\", \"Apr\"…\n$ hour     &lt;dbl&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\n\n\nskimr::skim(taxi)\n\n\nData summary\n\n\nName\ntaxi\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ntip\n0\n1\n2\n3\n0\n2\n0\n\n\ncompany\n0\n1\n5\n28\n0\n7\n0\n\n\nlocal\n0\n1\n2\n3\n0\n2\n0\n\n\ndow\n0\n1\n3\n3\n0\n7\n0\n\n\nmonth\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1\n5000.50\n2886.90\n1\n2500.75\n5000.50\n7500.25\n10000.0\n▇▇▇▇▇\n\n\ndistance\n0\n1\n6.22\n7.38\n0\n0.94\n1.78\n15.56\n42.3\n▇▁▂▁▁\n\n\nhour\n0\n1\n14.18\n4.36\n0\n11.00\n15.00\n18.00\n23.0\n▁▃▅▇▃\n\n\n\n\n\n\n\n\nmosaic::inspect(taxi)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1     tip character      2 10000       0\n2 company character      7 10000       0\n3   local character      2 10000       0\n4     dow character      7 10000       0\n5   month character      4 10000       0\n                                   distribution\n1 yes (92.1%), no (7.9%)                       \n2 other (27.1%) ...                            \n3 no (81.2%), yes (18.8%)                      \n4 Thu (19.6%), Wed (17.5%), Tue (16.3%) ...    \n5 Apr (31.8%), Mar (31.4%), Feb (20.4%) ...    \n\nquantitative variables:  \n      name   class min      Q1  median        Q3     max        mean\n1 rownames numeric   1 2500.75 5000.50 7500.2500 10000.0 5000.500000\n2 distance numeric   0    0.94    1.78   15.5625    42.3    6.224144\n3     hour numeric   0   11.00   15.00   18.0000    23.0   14.177300\n           sd     n missing\n1 2886.895680 10000       0\n2    7.381397 10000       0\n3    4.359904 10000       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\ndistance: Continuous Quant variable, the distance of the trip in miles.\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ntip: Yes/No type Qual variable, whether a tip was given or not.\n\ncompany: 7 levels, the cab company that was used for the ride.\n\nlocal: 2 levels, whether the trip was local or not.\n\nhour : 24 levels, the hour of the day when the trip started.\n\ndow: 7 levels, the day of the week.\n\nmonth: 12 levels, the month of the year.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the taxi dataset\n\n\n\n\nThis is a large dataset (10K rows), 8 columns/variables.\nThere are several Qualitative variables: tip(2), company(7) and local(2), dow(7), and month(12). These have levels as shown in the parenthesis.\nNote that hour despite being a discrete/numerical variable, it can be treated as a Categorical variable too.\n\ndistance is Quantitative.\nThere are no missing values for any variable, all are complete with 10K entries.\n\n\n\n\n Data Munging\nWe will convert the tip, company, dow, local, hour, and month variables into factors beforehand.\n\n## Convert `dow`, `local`, `month`, and `hour` into ordered factors\ntaxi_modified &lt;- taxi %&gt;%\n  mutate(\n    ##\n    tip = factor(tip,\n      levels = c(\"yes\", \"no\"),\n      labels = c(\"yes\", \"no\"),\n      ordered = TRUE\n    ),\n    ##\n    company = factor(company), # Any order is OK.\n    ##\n    dow = factor(dow,\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      ordered = TRUE\n    ),\n    ##\n    local = factor(local,\n      levels = c(\"yes\", \"no\"),\n      labels = c(\"yes\", \"no\"),\n      ordered = TRUE\n    ),\n    ##\n    month = factor(month,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      ordered = TRUE\n    ),\n    ##\n    hour = factor(hour,\n      levels = c(0:23), labels = c(0:23),\n      ordered = TRUE\n    )\n  )\ntaxi_modified %&gt;% glimpse()\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;ord&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, y…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;fct&gt; Chicago Independents, City Service, other, Chicago Independen…\n$ local    &lt;ord&gt; no, yes, no, no, no, yes, no, no, no, no, no, no, no, yes, no…\n$ dow      &lt;ord&gt; Thu, Thu, Mon, Mon, Sun, Sat, Fri, Sun, Fri, Tue, Tue, Sun, W…\n$ month    &lt;ord&gt; Feb, Mar, Feb, Apr, Mar, Apr, Mar, Jan, Apr, Mar, Mar, Apr, A…\n$ hour     &lt;ord&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n Hypothesis and Research Questions\nThe target variable for an experiment that resulted in this data might be the tip variable, since that looks like a response, or an outcome. It is a binary i.e. Yes/No type Qual variable.\nWe will use the tip variable to ask questions about the data, and then plot the answers to these questions.\n\n\n\n\n\n\nNoteResearch Questions:\n\n\n\n\nDo more people tip than not?\nDoes a tip depend upon whether the trip is local or not?\nDo some cab company-ies get more tips than others?\nAnd does a tip depend upon the distance, hour of day, and dow and month?\n\nTry and think of more Questions!\n\n\n\n Plotting Barcharts\nLet’s plot some bar graphs: recall that for bar charts, we need to choose Qual variables to count with! In each case, we will state a Hypothesis/Question and try to answer it with a chart.\n\n Question-1: Do more people tip than not?\n\n\n\n\n\n\nNoteQuestion-1: Do more people tip than not?\n\n\n\n\n\nggformula-1\nggplot-1\ntidyplots-1\ntinyplot-1\n web-r-1\n\n\n\n\nShow the Codetheme_set(new = theme_custom())\n\ngf_bar(~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 1A: Counts of Tips\")\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nShow the Codetheme_set(new = theme_custom())\n\nggplot(taxi_modified) +\n  geom_bar(aes(x = tip)) +\n  labs(title = \"Plot 1A: Counts of Tips\")\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\nShow the Codetidyplot(x = tip, data = taxi_modified) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 1A: Counts of Tips\") %&gt;%\n  adjust_size(height = 50, width = 85, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\nShow the Codetinyplot(~tip,\n  data = taxi_modified,\n  type = \"barplot\",\n  main = \"Plot 1A: Counts of Tips\"\n)\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nFar more people do tip than not. Which is nice.\n(Future) The counts of tip are very imbalanced and if we are to setup a model for that (logistic regression) we would need to very carefully subset the data for training and testing our model.\n\n\n\n\n Question-2: Does the tip depend upon whether the trip is local or not?\n\n\n\n\n\n\nNoteQuestion-2: Does the tip depend upon whether the trip is local or not?\n\n\n\n\n\nggformula-2\nggplot-2\ntidyplots-2\ntinyplot-2\n web-r-2\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_labs(title = \"Plot 2A: Dodged Bar Chart\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  gf_props(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"dodge\") +\n  labs(title = \"Plot 2A:Dodged Bar Chart\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"stack\") +\n  labs(\n    title = \"Plot 2B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"fill\") +\n  labs(title = \"Plot 2C: Filled Bar Chart\", subtitle = \"Shows Per group differences in Proportions!\") +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 2D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\",\n    y = \"Proportion\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 2A: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local, colour = tip, data = taxi_modified) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 2B: Stacked Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 2C: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntinyplot(~ local | tip,\n  data = taxi_modified,\n  type = \"barplot\", palette = \"tableau\",\n  beside = TRUE, # for placing bars beside one another\n  main = \"Plot 2A: Dodged Bar Chart\",\n  legend = \"right!\"\n) # Outside, to the right\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntinyplot(~ local | tip,\n  data = taxi_modified,\n  type = \"barplot\", palette = \"tableau\",\n  beside = FALSE,\n  xlevels = c(\"no\", \"yes\"), # try reversing\n  main = \"Plot 2B: Stacked Bar Chart\",\n  sub = \"Can we spot per group differences in proportions??\",\n  legend = \"right!\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\nCounting the frequency of tip by local gives us grouped counts, but we cannot tell the percentage per group (local or not) of those who tip and those who do not.\nWe need per-group percentages because the number of local trips are not balanced\nHence with tidyplots, we find that add_barstack_relative gives the clearest visual indication of a difference in proportion.\nLikewise with ggformula, we tried bar charts with position = stack, but finally it is the position = fill that works best.\nWe see that the percentage of tippers is somewhat higher with people who make non-local trips. Not surprising.\n\n\n\n\n Question-3: Do some cab company-ies get more tips than others?\n\n\n\n\n\n\nNoteQuestion-3: Do some cab company-ies get more tips than others?\n\n\n\n\n\nggformula-3\nggplot-3\ntidyplots-3\ntinyplot-3\n web-r-3\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"dodge\") %&gt;%\n  gf_labs(title = \"Plot 3A: Dodged Bar Chart\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    size = 6,\n    angle = 45, hjust = 0.5\n  ))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"stack\") %&gt;%\n  gf_labs(\n    title = \"Plot 3B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  gf_percents(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  gf_props(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"dodge\") +\n  labs(title = \"Plot 3A: Dodged Bar Chart\") +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n##\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"stack\") +\n  labs(\n    title = \"Plot 3B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 3C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 3D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\",\n    y = \"Proportions\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 3A: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(rotate_labels = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 3B: Stacked Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(rotate_labels = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 3C: Stacked Bar Chart\") %&gt;%\n  add_annotation_text(\n    text = \"Proportions of Tippers per Company\",\n    x = 4.25, y = 1.25\n  ) %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(title = \"\", rotate_labels = 45) %&gt;%\n  adjust_y_axis(title = \"Proportions\") # Not happening??\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo be Coded!!\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\nUsing stack-ed, dodge-ed, and fill-ed in ggformula (and bars, absolute-stacked-bars, and relative-stacked-bars in tidyplots) in bar plots gives us different ways of looking at the sets of counts;\n\nfill: gives us a per-group proportion of another Qual variable for a chosen Qual variable. This chart view is useful in Inference for Proportions;\nMost cab company-ies have similar usage, if you neglect the other category of company;\nDoes seem that of all the company-ies, tips are not so good for the Flash Cab company. A driver issue? Or are the cars too old? Or don’t they offer service everywhere?\n\n\n\n\n Question-4: Does a tip depend upon the distance, hour of day, and dow and month?\n\n\n\n\n\n\nNoteQuestion-4: Does a tip depend upon the distance, hour of day, and dow and month?\n\n\n\n\n\nggformula-4\nggplot-4\ntidyplots-4\ntinyplot-4\n web-r-4\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4B: Counts of Tips by Day of Week\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~month, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4C: Counts of Tips by Month\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~ month | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## This may be too busy a graph...\ngf_bar(~ dow | hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Plot 4E: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Is this plot arrangement easy to grasp?\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## This is better!\ngf_bar(~ hour | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Plot 4F: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Facetted by Day of Week\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = dow, fill = tip)) +\n  labs(title = \"Plot 4B: Counts of Tips by Day of Week\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = month, fill = tip)) +\n  labs(title = \"Plot 4C: Counts of Tips by Month\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = month, fill = tip)) +\n  facet_wrap(~dow) +\n  labs(title = \"Plot 4D: Counts of Tips by Day of Week and Month\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = dow, fill = tip)) +\n  facet_wrap(~hour) +\n  labs(\n    title = \"Plot 4E: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Is this plot arrangement easy to grasp?\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = hour, fill = tip)) +\n  facet_wrap(~dow) +\n  labs(\n    title = \"Plot 4F: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Swapped the Facets\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4B: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 4C: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##\ntidyplots::tidyplot(month,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly) %&gt;%\n  split_plot(\n    by = dow,\n    ncol = 3, nrow = 3, guides = \"collect\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##\ntidyplots::tidyplot(dow,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly) %&gt;%\n  adjust_x_axis(rotate_labels = 45) %&gt;%\n  split_plot(\n    by = hour,\n    ncol = 3, nrow = 8, guides = \"collect\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo be Coded.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-4\n\nNote: We were using fill = ~ tip here! Why is that a good idea?\n\ntips vs hour: There are always more people who tip than those who do not. Of course there are fewer trips during the early morning hours and the late night hours, based on the very small bar-pairs we see at those times\n\ntips vs dow: Except for Sunday, the tip count patterns (Yes/No) look similar across all days.\n\ntips vs month: We have data for 4 months only. Again, the tip count patterns (Yes/No) look similar across all months. Perhaps slightly fewer trips in Jan, when it is cold in Chicago and people may not go out much.\n\ntips vs dow vs month: Very similar counts for tips(Yes/No) across day-of-week and month.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-plot-extras",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-plot-extras",
    "title": "\n Counts",
    "section": "\n Bar Plot Extras",
    "text": "Bar Plot Extras\n\n\n\n\n\n\nNotegf-bar and gf-col\n\n\n\nNote also that gf_bar/geom_bar takes only ONE variable (for the x-axis), whereas gf_col/geom_col needs both X and Y variables since it simply plots columns. Both are useful!\n\n\n\n\n\n\n\n\nNoteAnd we can plot Proportions and Percentages too!\n\n\n\n\n\n\nWe have already seen gf_props in our two case studies above. Also check out gf_percents ! These are both very useful ggformula functions!\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_props(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"dodge\"\n) %&gt;%\n  gf_labs(\n    title = \"Plotting Proportions using gf_props\",\n    subtitle = \"Option = dodge\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_props(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"fill\"\n) %&gt;%\n  gf_labs(\n    title = \"Plotting Proportions using gf_props\",\n    subtitle = \"Option = fill\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_percents(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"dodge\"\n) %&gt;%\n  gf_refine(\n    scale_y_continuous(\n      labels = scales::label_percent(scale = 1)\n    )\n  ) %&gt;%\n  gf_labs(title = \"Plotting Percentages using gf_percents\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#are-the-differences-in-proportion-significant",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#are-the-differences-in-proportion-significant",
    "title": "\n Counts",
    "section": "\n Are the Differences in Proportion Significant?",
    "text": "Are the Differences in Proportion Significant?\nWhen we see situations such as this, where data has one or more Qual variables that are binary(Yes/No), we are always interested in whether these proportions of Yes/No are really different, or if we are just seeing the result of random chance. This is usually mechanized by a Stat Test called a Single Proportion Test or, when we have more than one, a Multiple Proportion Test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#your-turn",
    "title": "\n Counts",
    "section": "\n Your Turn",
    "text": "Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make Bar plots with each of them, using one or more Qual variables.\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\n\nAiRbnb Price Data on the French Riviera:\n\n\n\n AiRbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n\n\nFertility: This rather large and interesting Fertility related dataset from https://vincentarelbundock.github.io/Rdatasets/csv/AER/Fertility.csv\n\n\n Download the Fertility Data \nglimpse / skim / inspect the dataset in each case, state that Data Dictionary, and develop a set of Questions that can be answered by appropriate stat measures, or by using a chart to show the distribution.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#wait-but-why",
    "title": "\n Counts",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nAlways count your chickens count your data before you model or infer!\nCounts first give you an absolute sense of how much data you have.\nCounts by different Qual variables give you a sense of the combinations you have in your data: \\((Male/Female) * (Income-Status) * (Old/Young) * (Urban/Rural)\\) (Say 2 * 3 * 2 * 2 = 24 combinations of data)\nCounts then give an idea whether your data is lop-sided: do you have too many observations of one category(level) and too few of another category(level) in a given Qual variable?\nBalance is important in order to draw decent inferences\nAnd for ML algorithms, to train them properly.\nSince the X-axis in bar charts is Qualitative (the bars don’t touch, remember!) it is possible to sort the bars at will, based on the levels within the Qualitative variables. See the approx Zipf’s Law distribution for the English alphabet below:\n\n\n\n\n\n\nFigure 6: Zipf’s Law\n\n\nIn Figure 6, the letters of the alphabet are “levels” within a Qualitative variable, and these levels have been sorted based on the frequency or count! This is what Sherlock Holmes might have done, or the method how they cracked the code to the treasure in this story.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#conclusion",
    "title": "\n Counts",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nQualitative data variables can be plotted as counts, using Bar Charts\n\ngf_col and gf_bar provide Bar charts; gf_bar performs counts internally, whereas gf_col requires pre-counted data.\nUsing facets allows us to view counts of one Qual variable split over two other Qual variables",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#ai-generated-summary-and-podcast",
    "title": "\n Counts",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis text excerpt focuses on bar charts and histograms as visualization tools for qualitative and quantitative data, respectively. It walks the reader through the creation of bar charts using the R programming language, illustrating the concept through a case study using the Chicago taxi rides dataset. The author explores various scenarios and questions related to taxi tipping, such as the frequency of tips and their dependence on trip locality, company, hour of the day, and day of the week. Finally, the excerpt highlights the importance of understanding data counts before undertaking data modeling or inference, emphasizing the role of bar charts in revealing data distribution and potential imbalances.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#references",
    "title": "\n Counts",
    "section": "\n References",
    "text": "References\n\nDaniel Kaplan and Randall Pruim. ggformula: Formula Interface for ggplot2 (full version). https://www.mosaic-web.org/ggformula/articles/pkgdown/ggformula-long.html\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntinyplot\n0.4.1\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\n\n\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html",
    "title": "\n Experiments",
    "section": "",
    "text": "Important\n\n\n\n Guys carry more cash than Gals.\n\n\nHave you got money? Open your wallet and show me how much. Guys and Gals. Will this work in these UPI days? And well-dressed Gals don’t have pockets in their clothing, uncle… So then ask “How much did you spend today?” Again, Guys and Gals.\nAnd then a two independent sample test for means. And Permutation and all that stuff.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-1-guys-have-more-pocket-money-than-gals",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-1-guys-have-more-pocket-money-than-gals",
    "title": "\n Experiments",
    "section": "",
    "text": "Important\n\n\n\n Guys carry more cash than Gals.\n\n\nHave you got money? Open your wallet and show me how much. Guys and Gals. Will this work in these UPI days? And well-dressed Gals don’t have pockets in their clothing, uncle… So then ask “How much did you spend today?” Again, Guys and Gals.\nAnd then a two independent sample test for means. And Permutation and all that stuff.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-2-i-will-eat-my-tip-thank-you.",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-2-i-will-eat-my-tip-thank-you.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #2: I will eat my tip, thank you.",
    "text": "Free Hunch #2: I will eat my tip, thank you.\n\n\n\n\n\n\nImportant The average tip people give is higher for people who are non-vegetarians. Regardless of whether you are going Dutch or not.\n\n\n\n\n\n\nAre vegetarians more kanjoos? Or it is the meat-eaters?\nSo Swiggy/Zomato/Dining Out bills. For both sets of people. And then the t-t-t-t-t-test…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-3-art-design-and-vocation-are-all-diff-different.",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-3-art-design-and-vocation-are-all-diff-different.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #3: Art, Design, and Vocation are all diff-different.",
    "text": "Free Hunch #3: Art, Design, and Vocation are all diff-different.\n\n\n\n\n\n\nImportant Grades are very different between B.Voc, B.Cra, and B.Des folks.\n\n\n\n\n\n\nSo? Grades of course, for a good sample from all three groups of people..and then? ANOVA of course.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-4-chhota-bheem-vs-doraemon-vs-dragon-tales",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-4-chhota-bheem-vs-doraemon-vs-dragon-tales",
    "title": "\n Experiments",
    "section": "\n Free Hunch #4: Chhota Bheem vs Doraemon vs Dragon Tales",
    "text": "Free Hunch #4: Chhota Bheem vs Doraemon vs Dragon Tales\n\n\n\n\n\n\nImportant Doraeomon &gt;&gt; Dragon Tales &gt;&gt; Chhota Bheem!\n\n\n\n\n\n\nThe Anywhere Door makes Doraemon the greatest children’s show on earth. But Dragon Tales also has that Dragon Stone thingy…“I wish, I wish..”\nAnd that flute musical in Chhota Bheem…Uff!\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\nSo? Get Opinion scores from people. Scale of 10. And then? Oh, ANOVA, peasants!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-5-i-am-an-intj",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-5-i-am-an-intj",
    "title": "\n Experiments",
    "section": "\n Free Hunch #5: I am an INTJ",
    "text": "Free Hunch #5: I am an INTJ\n\n\n\n\n\n\nImportant Srishti kids are predominantly introverted\n\n\n\n\n\n\nWhat are we looking at, data-wise? A proportion, which if more than 50% would justify our hunch. So we do an MBTI on some unsuspecting sample of people, and try to generalize that result to the population.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-6-lets-go-to-chefstouch",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-6-lets-go-to-chefstouch",
    "title": "\n Experiments",
    "section": "\n Free Hunch #6: Let’s Go to ChefsTouch(?)",
    "text": "Free Hunch #6: Let’s Go to ChefsTouch(?)\n\n\n\n\n\n\nImportant Most people think the food in the mess/cafeteria is ordinary.\n\n\n\n\n\n\nAgain, a survey of a sample. Opinions, yes or no. A Proportion for the sample, and an extension to the population. A proportion test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#references",
    "title": "\n Experiments",
    "section": "References",
    "text": "References\n\nFacing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html",
    "title": "\n Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n\n\n\n\n\n\n\n“My stories run up and bite me in the leg – I respond by writing them down – everything that goes on during the bite. When I finish, the idea lets go and runs off.”\n— Ray Bradbury, science-fiction writer (22 Aug 1920-2012)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#slides-and-tutorials",
    "title": "\n Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n\n\n\n\n\n\n\n“My stories run up and bite me in the leg – I respond by writing them down – everything that goes on during the bite. When I finish, the idea lets go and runs off.”\n— Ray Bradbury, science-fiction writer (22 Aug 1920-2012)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#setting-up-r-packages",
    "title": "\n Evolution and Flow",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(ggstream)\nlibrary(ggformula)\n# remotes::install_github(\"corybrunson/ggalluvial@main\", build_vignettes = TRUE)\nlibrary(ggalluvial)\nlibrary(ggsankeyfier)\n# install.packages(\"devtools\")\n# devtools::install_github(\"davidsjoberg/ggsankey\")\nlibrary(ggsankey)\nlibrary(networkD3)\nlibrary(echarts4r) # Interactive graphs\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nlibrary(tidyverse)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-time-evolution-charts-can-we-plot",
    "title": "\n Evolution and Flow",
    "section": "\n What Time Evolution Charts can we plot?",
    "text": "What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time…and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada ).\n\n\n\n\n\n\nImportantAnd Introducting echarts4r\n\n\n\nWe will also build interactive versions of these charts using echarts4r!\n\n\nDownload this data by clicking on the button below:\n Download the Health data \n\nhealth &lt;-\n  read_csv(\"data/public-health-expenditure-share-GDP-OWID.csv\")\n\nhealth_filtered &lt;- health %&gt;%\n  filter(Entity %in% c(\n    \"India\",\n    \"China\",\n    \"United States\",\n    \"United Kingdom\",\n    \"Russia\",\n    \"Sweden\"\n  ))\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\ngf_point(\n  data = health_filtered,\n  public_health_expenditure_pc_gdp ~ Year,\n  colour = ~Entity,\n  ylab = \"Healthcare Budget\\n as % of GDP\",\n  title = \"Line Charts to show Evolution (over Time )\"\n) %&gt;%\n  gf_line()\n###\ngf_area(\n  data = health_filtered,\n  public_health_expenditure_pc_gdp ~ Year,\n  fill = ~Entity, alpha = 0.3,\n  ylab = \"Healthcare Budget\\n as % of GDP\",\n  title = \"Area Charts to show Evolution (over Time )\"\n) %&gt;%\n  gf_line(colour = ~Entity)\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  e_charts(Year) %&gt;%\n  e_scatter(public_health_expenditure_pc_gdp) %&gt;%\n  e_line(public_health_expenditure_pc_gdp) %&gt;%\n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;%\n  e_y_axis(\n    name = \"Public Health Expenditure\",\n    nameLocation = \"middle\", nameGap = 25\n  ) %&gt;%\n  e_tooltip()\n###\nhealth_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  e_charts(Year) %&gt;%\n  e_scatter(public_health_expenditure_pc_gdp) %&gt;%\n  e_area(public_health_expenditure_pc_gdp) %&gt;%\n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;%\n  e_y_axis(\n    name = \"Public Health Expenditure\",\n    nameLocation = \"middle\", nameGap = 25\n  ) %&gt;%\n  e_tooltip()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-space-evolution-charts-can-we-plot",
    "title": "\n Evolution and Flow",
    "section": "\n What Space Evolution Charts can we plot?",
    "text": "What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nHere is what Thomas Lin Pedersen says:\n\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables. If the variables have an intrinsic order the representation can be thought of as a Sankey Diagram. If each variable is a point in time it will resemble an Alluvial diagram.\n\n\n\n\n\n\n\n\n\n\n(a) ggsankey aesthetics\n\n\n\n\n\n\n\n\n\n(b) ggsankeyfier aesthetics\n\n\n\n\n\n\nFigure 1: Geometric Aesthetics from two Sankey Plot Packages\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / edges / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students pursruing different degrees going through multiple courses with multiple departments during a semester of study. Here students, degrees, courses, departments would be some variables we would plot and we would visualize the number of students moving across courses and deparments based on their degree etc.\nHere is an example of a Sankey Diagram: This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. (Data: Department of Energy & Climate Change via Tom Counsell)1:\n\n\n\n\n\n\n\n\nNoteSwitching to ggplot here\n\n\n\nFor the next few charts, there are (as yet) no equivalents in ggformula. Hence we will use ggplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#case-study-1-titanic-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#case-study-1-titanic-dataset",
    "title": "\n Evolution and Flow",
    "section": "\n Case Study-1: Titanic Dataset",
    "text": "Case Study-1: Titanic Dataset\n\n\n\ndata(Titanic, package = \"datasets\")\nTitanic &lt;- Titanic %&gt;% as_tibble()\nTitanic\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNoteTable Form Data\n\n\n\nNote that this data is in tidy wide / table form, with separate columns for each Qualitative variable and a separate count column, which we saw when we examined Categorical Data. This is, in my opinion, intuitively the best form of data to plot a Sankey plot with. Each variable gives us “one part in the flow”. But there are other forms such as the tidy long form which we have been using practically all this while. You will find examples of on the ggalluvial website using tidy long form data. https://corybrunson.github.io/ggalluvial/\n\n\n\n\n\nUsing ggplot\nUsing ggsankeyfier\nUsing echarts4r\n\n\n\n\n\n\n##\nTitanic %&gt;% ggplot(\n  data = .,\n\n  # Select the Categorical Variables for the vertical Axes / Stages\n  aes(\n    axis1 = Class,\n    axis2 = Sex,\n    axis3 = Age,\n    axis4 = Survived,\n    y = n\n  ), fill = \"white\"\n) +\n\n  # Alluvials between Categorical Axes\n  geom_alluvium(aes(fill = Survived),\n    colour = \"black\",\n    linewidth = 0.25\n  ) +\n\n  # Vertical segments for each Categorical Variable2\n  geom_stratum(\n    colour = \"black\",\n    linewidth = 1,\n    fill = \"white\"\n  ) +\n\n  # Labels for each \"level\" of the Categorical Axes\n  geom_text(\n    stat = \"stratum\", size = 3,\n    aes(label = after_stat(stratum))\n  ) +\n\n\n\n  # Scales and Colours\n  scale_x_discrete(\n    limits = c(\"Class\", \"Sex\", \"Age\", \"Survived\"),\n    expand = c(0.1, 0.1)\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  xlab(\"Demographic\") +\n  ggtitle(\n    \"Passengers on the maiden voyage of the Titanic\",\n    \"Stratified by demographics and survival\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is how the package ggalluvial defines the elements of a typical alluvial plot:\n\nAn axis is a dimension (variable) along which the data are vertically arranged at a fixed horizontal position. The plot above uses three categorical axes: Class, Sex, and Age.\nThe groups at each axis are depicted as opaque blocks called strata. For example, the Class axis contains four strata: 1st, 2nd, 3rd, and Crew.\nHorizontal (x-) splines called alluvia span the entire width of the plot. In this plot, each alluvium corresponds to a fixed strata value of each axis variable, indicated by its vertical position at the axis, as well as of the Survived variable, indicated by its fill color.\nThe segments of the alluvia between pairs of adjacent axes are flows.\nThe alluvia intersect the strata at lodes. The lodes are not visualized in the above plot, but they can be inferred as filled rectangles extending the flows through the strata at each end of the plot or connecting the flows on either side of the center stratum.\n\n\n\n\nThe ggsankeyfier also plots alluvial and sankey diagrams. This package takes data in long-form. See this article.. ggsankeyfier has builtin commands to convert data from wide to long:\n\n\n\nTitanic %&gt;%\n  as_tibble() %&gt;%\n  ggsankeyfier::pivot_stages_longer(\n    data = .,\n    stages_from = c(\"Class\", \"Sex\", \"Age\", \"Survived\"),\n    values_from = \"n\",\n    additional_aes_from = \"Survived\"\n  ) -&gt; Titanic_long\nTitanic_long\n\n\n\n\n\n  \n\n\n\n\n\nThis data is in long form, with stages defining the axes in the graph, and the node variable giving us levels within each (Qualitative) axis. The edge_id labels both ends (from and to) of each connector or edge/flow/alluvium.\nLet us plot this now:\n\n\n\nTitanic_long %&gt;%\n  ggplot(aes(\n    x = stage, y = n,\n    group = node, connector = connector,\n    edge_id = edge_id\n  )) +\n  geom_sankeynode(v_space = \"auto\") +\n  geom_sankeyedge(aes(fill = Survived), v_space = \"auto\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"\", title = \"Titanic Survival\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us make an interactive graph for this dataset using echarts4.\n\nClassSex &lt;-\n  Titanic %&gt;%\n  group_by(Class, Sex) %&gt;%\n  summarise(cs = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Class, \"target\" = Sex, \"value\" = cs)\n\nSexAge &lt;-\n  Titanic %&gt;%\n  group_by(Sex, Age) %&gt;%\n  summarise(sa = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Sex, \"target\" = Age, \"value\" = sa)\n\nAgeSurvived &lt;-\n  Titanic %&gt;%\n  group_by(Age, Survived) %&gt;%\n  summarise(as = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Age, \"target\" = Survived, \"value\" = as)\n\nCombo &lt;- rbind(ClassSex, SexAge, AgeSurvived)\nCombo\n\n\n  \n\n\nCombo %&gt;%\n  e_charts() %&gt;%\n  e_sankey(source, target, value) %&gt;%\n  e_title(\"Titanic: Who lived, and who didn't?\") %&gt;%\n  e_tooltip()\n\n\n\n\n\nThe process with echarts4r is quite different, since the data structure used by this package is different:\n\nThe echarts4r package needs to have source and target columns for axes, along with a value to determine the width of the alluvium. \nThe names in the source and target can repeat, and can appear in both source and target columns in order to create a multi-axis diagram. Hence the data needs to be inherently in long form.\nHowever, for the values, we need to manually calculate the aggregate totals for alluvia between each consecutive pairs of axes (i.e Qual variables). This is not done automatically in echarts4r, but it is with ggalluvial.\nSo we create grouped aggregate summaries for each pair of Qualitative variables that we wish to plot consecutively ( i.e as axis1, axis2…)\nStack these pair-wise alluvia totals into one combo data frame using rbind(), after renaming the variables to “source”, “target” and “value”.\n\nPhew! seems like too much work to do…I wonder if good, old-fashioned pivot-longer will get us here…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#chord-diagram",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#chord-diagram",
    "title": "\n Evolution and Flow",
    "section": "\n Chord Diagram",
    "text": "Chord Diagram\n\n\n\n\nWe will explore this diagram when we explore network graphs with the tidygraph and ggraph packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#dumbbell-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#dumbbell-plots",
    "title": "\n Evolution and Flow",
    "section": "\n Dumbbell Plots",
    "text": "Dumbbell Plots\nA simple plot that can quickly indicate changes in multiple variables/aspects over either a time or a space variable is a dumbbell plot. This is a combination of scatter plot + a segment plot. Let us take our previously loaded health dataset and plot just the change in expenditure for multiple countries, across a time span of 8 years (2010 - 2018)\n\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\nhealth_2010_2018\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  gf_segment(Entity + Entity ~ Year2010 + Year2018,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_point(Entity ~ Year2018,\n    colour = ~\"2018\"\n  ) %&gt;%\n  gf_point(Entity ~ Year2010,\n    colour = ~\"2010\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Can we do better? Sort the bars, improve axis ticks, title..\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  # plot segments first\n  gf_segment(\n    reorder(Entity, Year2018) + reorder(Entity, Year2018) ~\n      Year2010 + Year2018,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  # Then plot points\n  gf_point(reorder(Entity, Year2018) ~ Year2018,\n    colour = ~\"2018\",\n    size = 3\n  ) %&gt;%\n  gf_point(\n    reorder(Entity, Year2018) ~ Year2010,\n    colour = ~\"2010\", size = 3,\n    xlab = \"Health Expenditure as Percentage of GDP\",\n    ylab = \"Country\",\n    title = \"Healthcare Budgets Changes between 2010 to 2018\",\n    subtitle = \"Bars are Sorted\",\n    caption = \"And the X-Axis is in percentage\"\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(\n      breaks = scales::breaks_width(2),\n      labels = scales::label_percent(suffix = \"%\", scale = 1)\n    ),\n    scale_colour_manual(name = \"Year\", values = c(\"red\", \"green\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  ggplot() +\n  geom_segment(\n    aes(\n      y = Entity, yend = Entity,\n      x = Year2010, xend = Year2018\n    ),\n    colour = \"grey\",\n    linewidth = 2\n  ) +\n  geom_point(aes(y = Entity, x = Year2018, colour = \"2018\")) +\n  geom_point(aes(y = Entity, x = Year2010, colour = \"2010\"))\n## Can we do better?\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  ggplot() +\n  # plot segments first\n  geom_segment(\n    aes(\n      y = reorder(Entity, Year2018), yend = reorder(Entity, Year2018),\n      x = Year2010, xend = Year2018\n    ),\n    colour = \"grey\",\n    linewidth = 2\n  ) +\n\n  # Then plot points\n  geom_point(aes(\n    y = reorder(Entity, Year2018), x = Year2018,\n    colour = \"2018\"\n  ), size = 3) +\n  geom_point(aes(\n    y = reorder(Entity, Year2018), x = Year2010,\n    colour = \"2010\"\n  ), size = 3) +\n  labs(\n    x = \"Health Expenditure as Percentage of GDP\",\n    y = \"Country\", title = \"Healthcare Budgets\",\n    subtitle = \"Changes between 2010 to 2018\"\n  ) +\n  scale_x_continuous(\n    breaks = scales::breaks_width(2),\n    labels = scales::label_percent(suffix = \"%\", scale = 1)\n  ) +\n  scale_colour_manual(name = \"Year\", values = c(\"red\", \"green\"))",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#wait-but-why",
    "title": "\n Evolution and Flow",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nChanges can be over time, or over “space”\nIn the latter case, we can think of some Quantity changing over (multiple levels of) multiple Qualitative variables. E.g. Sales over Product Type over Showroom Location over Festival Season…\nWhen a single Quant varies over a single multi-level Qual, the Chord Diagram may be simpler than the Sankey/Alluvial. E.g Bird migration across Multiple Locations. This can even show bidirectional changes. ( Sankeys with loops are also possible, however)\nWhen you have a Quant that changes over only one two-level Qual variable, the Dumbbell plot becomes an option.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#conclusion",
    "title": "\n Evolution and Flow",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe see that we can visualize “evolutions” over time and space. The evolutions can represent changes in the quantities of things, or their categorical affiliations or groups.\nWhat business/design data would you depict in this way? Revenue streams? Employment? Expenditures over time and market? Migration? App usage patterns? There are many possibilities!\nNote also that the Bump Charts are a special case of Alluvial/Sankey charts where each node connects/flows to only one other node.\n\n\n\n\nlogsUserNetworkAPI ServerCell TowerData ProcessorOnline PortalsatellitestransmitterStorageUIphone logsMake callpersistdisplayaccess",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#your-turn",
    "title": "\n Evolution and Flow",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package are two datasets, majors and vaccinations. Plot alluvial charts for both of these.\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#references",
    "title": "\n Evolution and Flow",
    "section": "\n References",
    "text": "References\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram.\n\n\nggalluvial cheatsheet,https://cheatography.com/seleven/cheat-sheets/ggalluvial/\n\nJohn Coene, Sankey plots with echarts4r, https://echarts4r.john-coene.com/articles/chart_types.html#sankey\n\nOther packages: Sankey plot | the R Graph Gallery (r-graph-gallery.com)\n\nAnother package: Sankey diagrams in ggplot2 with ggsankey | RCHARTS (r-charts.com)\n\nSankey Charts using networkD3: http://christophergandrud.github.io/networkD3\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggalluvial\n0.12.5\n\nBrunson (2020); Brunson and Read (2023)\n\n\n\nggsankey\n0.0.99999\nSjoberg (2025)\n\n\nggsankeyfier\n0.1.8\nde Vries (2024)\n\n\nggstream\n0.1.0\nSjoberg (2021)\n\n\nnetworkD3\n0.4.1\nAllaire et al. (2025)\n\n\nscales\n1.4.0\nWickham, Pedersen, and Seidel (2025)\n\n\n\n\n\n\nAllaire, J. J., Christopher Gandrud, Kenton Russell, and CJ Yetman. 2025. networkD3: D3 JavaScript Network Graphs from r. https://doi.org/10.32614/CRAN.package.networkD3.\n\n\nBrunson, Jason Cory. 2020. “ggalluvial: Layered Grammar for Alluvial Plots.” Journal of Open Source Software 5 (49): 2017. https://doi.org/10.21105/joss.02017.\n\n\nBrunson, Jason Cory, and Quentin D. Read. 2023. “ggalluvial: Alluvial Plots in ‘ggplot2’.” http://corybrunson.github.io/ggalluvial/.\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://doi.org/10.32614/CRAN.package.echarts4r.\n\n\nde Vries, Pepijn. 2024. ggsankeyfier: Create Sankey and Alluvial Diagrams Using “ggplot2”. https://doi.org/10.32614/CRAN.package.ggsankeyfier.\n\n\nSjoberg, David. 2021. ggstream: Create Streamplots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggstream.\n\n\n———. 2025. ggsankey: Sankey, Alluvial and Sankey Bump Plots. https://github.com/davidsjoberg/ggsankey.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2025. scales: Scale Functions for Visualization. https://doi.org/10.32614/CRAN.package.scales.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#footnotes",
    "title": "\n Evolution and Flow",
    "section": "Footnotes",
    "text": "Footnotes\n\nD3 JavaScript Network Graphs from R: christophergandrud.github.io/networkD3/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4. How are people Ages distributed across levels of Education?\n\n\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html",
    "href": "content/courses/Analytics/Descriptive/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Winston Chang (2024). R Graphics Cookbook.https://r-graphics.org",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html#references",
    "href": "content/courses/Analytics/Descriptive/listing.html#references",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Winston Chang (2024). R Graphics Cookbook.https://r-graphics.org",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "library(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference\nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference, Permutation/Bootstrap\nlibrary(patchwork) # Arranging Plots\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(supernova) # Beginner-Friendly ANOVA Tables\nlibrary(paletteer) # Color Palettes\nlibrary(tidyverse) # Tidy Data Processing\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#setting-up-r-packages",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "library(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference\nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference, Permutation/Bootstrap\nlibrary(patchwork) # Arranging Plots\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(supernova) # Beginner-Friendly ANOVA Tables\nlibrary(paletteer) # Color Palettes\nlibrary(tidyverse) # Tidy Data Processing\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#introduction",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Introduction",
    "text": "Introduction\nSuppose we have three sales strategies on our website, to sell a certain product, say men’s shirts. We have observations of customer website interactions over several months. How do we know which strategy makes people buy the fastest ?\nIf there is a University course that is offered in parallel in three different classrooms, is there a difference between the average marks obtained by students in each of the classrooms?\nIn each case we have a set of Quant observations in each Qual category: Interaction Time vs Sales Strategy in the first example, and Student Marks vs Classroom in the second. We can take mean scores in each category and decide to compare them. How do we make the comparisons? One way would be to compare them pair-wise, doing as many t-tests as there are pairs. But with this rapidly becomes intractable and also dangerous: with increasing number of groups, the number of mean-comparisons becomes very large \\(N\\choose 2\\) and with each comparison the possibility of some difference showing up, just by chance, increases! And we end up making the wrong inference and perhaps the wrong decision. The trick is of course to make comparisons all at once and ANOVA is the technique that allows us to do just that.\nIn this tutorial, we will compare the Hatching Time of frog spawn1, at three different lab temperatures.\nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nBased on the sample dataset at hand, how does frogspawn hatching time vary with different temperature settings?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-read-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\nDownload the data by clicking the button below.\n Download the frogs data \n\n\n\n\n\n\nImportantData Folder\n\n\n\nSave the CSV in a subfolder titled “data” inside your R work folder.\n\n\n\nfrogs_orig &lt;- read_csv(\"data/frogs.csv\")\nfrogs_orig\n\n\n  \n\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13°C, 18°C and 25°C. Different samples of spawn were subject to each of these temperatures respectively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-clean-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-clean-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Clean the Data",
    "text": "Workflow: Clean the Data\nThe data is in wide-format, with a separate column for each Temperature, and a common column for Sample ID. This is good for humans, but poor for a computer: there are NA entries since not all samples of spawn can be subject to all temperatures. (E.g. Sample ID #1 was maintained at 13°C, and there are NAs in the other two columns, which we don’t need).\nWe will first stack up the Temperature columns into a single column, separate that into pieces and then retain just the number part (13, 18, 25), getting rid of the word Temperature from the column titles. Then the remaining numerical column with temperatures (13, 18, 25) will be converted into a factor.\nWe will use pivot_longer()and separate_wider_regex() to achieve this. [See this animation for pivot_longer(): https://haswal.github.io/pivot/ ]\nfrogs_orig %&gt;%\n  pivot_longer(\n    .,\n    cols = starts_with(\"Temperature\"),\n    cols_vary = \"fastest\",\n    # new in pivot_longer\n    names_to = \"Temp\",\n    values_to = \"Time\"\n  ) %&gt;%\n  drop_na() %&gt;%\n  ##\n  separate_wider_regex(\n    cols = Temp,\n    # knock off the unnecessary \"Temperature\" word\n    # Just keep the digits thereafter\n    patterns = c(\"Temperature\", TempFac = \"\\\\d+\"),\n    cols_remove = TRUE\n  ) %&gt;%\n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(\n    x = TempFac,\n    levels = c(13, 18, 25),\n    labels = c(\"13\", \"18\", \"25\")\n  )) %&gt;%\n  rename(\"Id\" = `Frogspawn sample id`) -&gt; frogs_long\nfrogs_long\n##\nfrogs_long %&gt;% count(TempFac)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we have cleaned up our data and have 20 samples for Hatching Time per TempFac setting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-eda",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-eda",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nLet us plot some histograms and boxplots of Hatching Time:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\ngf_histogram(~Time,\n  fill = ~TempFac,\n  data = frogs_long, alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Histograms of Hatching Time Distributions vs Temperature\",\n    x = \"Hatching Time\", y = \"Count\"\n  ) %&gt;%\n  gf_text(7 ~ (mean(Time) + 2),\n    label = \"Overall Mean\"\n  ) %&gt;%\n  gf_refine(\n    scale_fill_paletteer_d(\"ggthemes::colorblind\"),\n    guides(fill = guide_legend(title = \"Temperature level (°C)\"))\n  )\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\ngf_boxplot(\n  data = frogs_long,\n  Time ~ TempFac,\n  fill = ~TempFac,\n  alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Boxplots of Hatching Time Distributions vs Temperature\",\n    x = \"Temperature\", y = \"Hatching Time\",\n    caption = \"Using ggprism\"\n  ) %&gt;%\n  gf_refine(\n    scale_fill_paletteer_d(\"ggthemes::colorblind\"),\n    scale_x_discrete(guide = \"prism_bracket\"),\n    guides(fill = guide_legend(title = \"Temperature level (°C)\"))\n  )\n\n\n\n\n\n\n\nThe histograms look well separated and the box plots also show very little overlap. So we can reasonably hypothesize that Temperature has a significant effect on Hatching Time.\nLet’s go ahead with our ANOVA test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA",
    "text": "Workflow: ANOVA\nWe will first execute the ANOVA test with code and evaluate the results. Then we will do an intuitive walkthrough of the process and finally, hand-calculate entire analysis for clear understanding. For now, a little faith!\n\n\nCode\nIntuitive\nFrogs Demonstrated\n\n\n\nR offers a very simple command aov to execute an ANOVA test: Note the familiar formula of stating the variables:\n\nfrogs_anova &lt;- aov(Time ~ TempFac, data = frogs_long)\n\nThis creates an ANOVA model object, called frogs_anova. We can examine the ANOVA model object best with a package called supernova2:\n\n# library(supernova)\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nsupernova::pairwise(frogs_anova,\n  correction = \"Bonferroni\", # Try \"Tukey\"\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, # We'll see\n  plot = TRUE\n)\n\n\n\n\n\n\n\n\n  group_1 group_2    diff pooled_se       t    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -5.861 -4.739 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.661 -9.539 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.361 -4.239 .0000\n\n\nThis table + error-bar plot gives us a clear comparison between each pair of the three groups of observations defined by TempFac. The differences in spawn hatching Time between each pair of TempFac settings are given by the diff column. Also shown are the confidence intervals for each of these differences (none of which include \\(0\\)); the p-values for each of these differences is also negligible. Thus we can conclude that the effect of temperature on hatching time is significant.\n\n\n\n\n\n\nNote\n\n\n\nTo find which specific value of TempFac has the most effect will require pairwise comparison of the group means, using a standard t-test. The confidence level for such repeated comparisons will need what is called Bonferroni correction3 to prevent us from detecting a significant (pair-wise) difference simply by chance. To do this we take \\(\\alpha = 0.05\\), the confidence level used and divide it by \\(K\\), the number of pair-wise comparisons we intend to make. This new value is used to decide on the significance of the estimated parameter. So the pairwise comparisons in our current data will have to use \\(\\alpha/3 = 0.0166\\) as the confidence level. The supernova::pairwise() function did this for us very neatly!\nThere are also other ways, such as the “Tukey correction” for multiple tests.\n\n\n\n\nAll that is very well, but what is happening under the hood of the aov() command?\nConsider a data set with a single Quant and a single Qual variable. The Qual variable has two levels, the Quant data has 20 observations per Qual level.\n\n\n\n\n  \n\n\n\n\n\n\n\nAll Data: In Fig A, the horizontal black line is the overall mean of quant, denoted as \\(\\mu_{tot}\\). The vertical black lines to the points show the departures of each point from this overall mean. The sum of squares of these vertical black lines in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{1}\\]\nGrouped Data: In Fig B, the horizontal green and red lines are the means of the individual groups, respectively \\(\\mu_A\\) and \\(\\mu_B\\). The green and red vertical lines are the departures, or errors, of each point from its own group-mean. The sum of the squares of the green and red lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - \\mu_A)^2] + \\Sigma (y - \\mu_B)^2]\n\\tag{2}\\]\nImprovement: We take the difference in the squared error sums:\n\\[\nSSA = SST - SSE\n\\tag{3}\\]\n\\(SSA\\) is called the Treatment Sum of Squares, the “improvement” in going from believing in one mean to believing in two.\nImprovement Ratio: \\(SSA/SSE\\) might now help us decide whether two means are better than one.\nLet us compute these numbers for our toy dataset:\n\ndemo_anova &lt;- aov(quant ~ qual, data = toydata)\nsupernova::supernova(demo_anova)\n\n Analysis of Variance Table (Type III SS)\n Model: quant ~ qual\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) |  823.407  1 823.407 139.356 .7857 .0000\n Error (from model)    |  224.529 38   5.909                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1047.935 39  26.870                    \n\n\nWhat do we see?\n\n\nAll Data: \\(SST = 1047.935\\).\n\nGrouped Data: \\(SSE = 224.529\\).\n\nImprovement: \\(SSA = SST-SSE\\) = \\(823.407\\).\n\nImprovement Ratio: Before we set up this ratio, we must realize that each of these measures uses a different number of observations! So the comparison is done after scaling each of \\(SSA\\) and \\(SSE\\) by the number of observations influencing them. (a sort of per capita, or average, squared error, an idea we saw when we defined Standard Errors): \\(F_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\\), where \\(df_{SSA} = 1\\) and \\(df_{SSE} = 38\\) are respectively the degrees of freedom in \\(SSA\\) and \\(SSE\\).\n\nLarge Enough Ratio?: The value of the F-statistic from the table above is \\(\\frac{823.407}{5.909} = 139.356\\). Is this ratio big enough? F-statistic is compared with a critical value of the F-critical to help us decide. (Here, it is.)\n\nBelief: So we now believe in the idea of two means.\n\nBack to Mean Differences: Finally, in order to find which of the means is significantly different from others (if there are more than two!), we need to make a pair-wise comparison of the means, applying the Bonferroni correction as stated before. This means we divide the critical p.value we expect by the number of comparisons we make between levels of the Qual variable. supernova did this for us in the error-bar plot above.\n\n\n\n\n\n\n\nImportantWhy “ANOVA”?\n\n\n\nWhen divide each of \\(SSA\\) and \\(SSE\\) by their degrees of freedom, this gives us a ratio of variances, the F-statistic. And so we are in effect deciding if means are significantly different by analyzing (a ratio of) variances! Hence the name, AN-alysis O-f VA-riance, ANOVA.\n\n\nSo this may seem like a great Hero’s Journey, where we start with means and differences, go into sums of squares, differences and comparisons of error ratios, and return to the means where we started, only to know them properly now.\n\n\nNow that we understand what aov() is doing, let us hand-calculate the numbers for our frogs dataset and check. Let us visualize our calculations first.\n\n\n\n\n\nSST\n\n\n\n\n\nSSE\n\n\n\n\nLet us get the ready table from supernova first, and then systematically calculate all numbers with understanding:\n\nsupernova::supernova(frogs_anova)\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\nHere are the SST, SSE, and the SSA:\n\n# Calculate overall sum squares SST\nfrogs_overall &lt;- frogs_long %&gt;%\n  summarise(\n    overall_mean_time = mean(Time),\n    # Overall mean across all readings\n    # The Black Line\n\n    SST = sum((Time - overall_mean_time)^2),\n    n = n()\n  ) # Always do this with `summarise`\n\nfrogs_overall\n\n\n  \n\n\n##\nSST &lt;- frogs_overall$SST\nSST\n\n[1] 1096.333\n\n\n\n# Calculate sums of square errors *within* each group\n# with respect to individual group means\nfrogs_within_groups &lt;- frogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(\n    grouped_mean_time = mean(Time), # The Coloured Lines\n    grouped_variance_time = var(Time),\n    group_error_squares = sum((Time - grouped_mean_time)^2),\n    n = n()\n  )\nfrogs_within_groups\n##\nfrogs_SSE &lt;- frogs_within_groups %&gt;%\n  summarise(SSE = sum(group_error_squares))\n##\nSSE &lt;- frogs_SSE$SSE\nSSE\n\n\n  \n\n\n\n[1] 75.4\n\n\n\nSST\nSSE\nSSA &lt;- SST - SSE\nSSA\n\n[1] 1096.333\n[1] 75.4\n[1] 1020.933\n\n\nWe have \\(SST = 1096\\), \\(SSE = 75.4\\) and therefore \\(SSA = 1020.9\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSA and SSE.\nLet us calculate these Degrees of Freedom.\nWith \\(k = 3\\) levels in the factor TempFac, and \\(n = 20\\) points per level, \\(SST\\) clearly has degree of freedom \\(kn-1 = 3*20~ -1 = 59\\), since it uses all observations but loses one degree to calculate the global mean. (If each level did not have the same number of points \\(n\\), we simply take all observations less one as the degrees of freedom for \\(SST\\)).\n\\(SSE\\) has \\(k*(n-1) = 3 * (20 -1) = 57\\) as degrees of freedom, since each of the \\(k\\) groups there are \\(n\\) observations and each group loses one degree to calculate its own group mean.\nAnd therefore \\(SSA\\), being their difference, has \\(kn-1 -k*(n-1) = k-1 = 2\\) degrees of freedom.\nThese are, of course, as shown in the df column in the supernova tabel above. We can still calculate these in R, for the sake of method and clarity (and pedantry):\n\n# Error Sum of Squares SSE\ndf_SSE &lt;- frogs_long %&gt;%\n  # Takes into account \"unbalanced\" situations\n  # Where groups are not equal in size\n  group_by(TempFac) %&gt;%\n  summarise(per_group_df_SSE = n() - 1) %&gt;%\n  summarise(df_SSE = sum(per_group_df_SSE)) %&gt;%\n  as.numeric()\n\n\n## Overall Sum of Squares SST\ndf_SST &lt;- frogs_long %&gt;%\n  summarise(df_SST = n() - 1) %&gt;%\n  as.integer()\n\n\n# Treatment Sum of Squares SSA\nk &lt;- length(unique(frogs_long$TempFac))\ndf_SSA &lt;- k - 1\n\nThe degrees of freedom for the quantities are:\ndf_SST\ndf_SSE\ndf_SSA\n\n\n\n[1] 59\n[1] 57\n[1] 2\n\n\n\nNow we are ready to compute the F-statistic: dividing each sum-of-squares byt its degrees of freedom gives us variances which we will compare, using the F-statistic as a ratio:\n\n# Finally F_Stat!\n# Combine the sum-square_error for each level of the factor\n# Weighted by degrees of freedom **per level**\n# Which are of course equal here ;-D\n\nMSE &lt;- frogs_within_groups %&gt;%\n  summarise(mean_square_error = sum(group_error_squares / df_SSE)) %&gt;%\n  as.numeric()\nMSE\n\n[1] 1.322807\n\n##\nMSA &lt;- SSA / df_SSA # This is OK\nMSA\n\n[1] 510.4667\n\n##\nF_stat &lt;- MSA / MSE\nF_stat\n\n[1] 385.8966\n\n\nThe F-stat is compared with a critical value of the F-statistic, F_crit which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to \\(\\alpha = 0.95\\), but here with the Bonferroni correction, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value F_critical as a quartile:\n\nF_crit &lt;-\n  qf(\n    p = (1 - 0.05 / 3), # Significance level is 5% + Bonferroni Correction\n    df1 = df_SSA, # Numerator degrees of freedom\n    df2 = df_SSE # Denominator degrees of freedom\n  )\nF_crit\nF_stat\n\n[1] 4.403048\n[1] 385.8966\n\n\nThe F_crit value can also be seen in a plot4,5:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmosaic::xpf(\n  q = F_crit,\n  df1 = df_SSA, df2 = df_SSE, method = \"gg\",\n  log.p = FALSE, lower.tail = TRUE,\n  return = \"plot\"\n) %&gt;%\n  gf_vline(xintercept = F_crit) %&gt;%\n  gf_label(0.75 ~ 5.5,\n    label = \"F_critical\",\n    inherit = F, show.legend = F\n  ) %&gt;%\n  gf_labs(\n    title = \"F distribution for Frogs Data\",\n    subtitle = \"F_critical = 4.403\"\n  )\n\n\n\n\n\n\n\nAny value of F more than the F_crit occurs with smaller probability than \\(0.05/3 = 0.017\\). Our F_stat is much higher than F_crit, by orders of magnitude! And so we can say with confidence that Temperature has a significant effect on spawn Time.\nAnd that is how ANOVA computes!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#stating-the-model",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#stating-the-model",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Stating the Model",
    "text": "Stating the Model\nAnd supernova gives us a nice linear equation relating Hatching_Time to TempFac:\n\nsupernova::equation(frogs_anova)\n\nFitted equation:\nTime = 26.3 + -5.3*TempFac18 + -10.1*TempFac25 + e\n\n\nTempFac18 and TempFac25 are binary {0,1} coded variables, representing the test situation. e is the remaining error. The equation models the means at each value of TempFac.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-checking-anova-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-checking-anova-assumptions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Checking ANOVA Assumptions",
    "text": "Workflow: Checking ANOVA Assumptions\nANOVA makes 3 fundamental assumptions:\n\nData (and errors) are normally distributed.\nVariances are equal.\nObservations are independent.\n\nWe can check these using checks and graphs.\n\n Checks for Normality\nThe shapiro.wilk test tests if a vector of numeric data is normally distributed and rejects the hypothesis of normality when the p-value is less than or equal to 0.05. \n\nshapiro.test(x = frogs_long$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n\n\nThe p-value is very low and we cannot reject the (alternative) hypothesis that the overall data is not normal. How about normality at each level of the factor?\n\n\n\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  group_modify(~ .x %&gt;%\n    select(Time) %&gt;%\n    as_vector() %&gt;%\n    shapiro.test() %&gt;%\n    broom::tidy())\n\n\n\n\n\n  \n\n\n\n\n\nThe shapiro.wilk test makes a NULL Hypothesis that the data are normally distributed and estimates the probability that the given data could have happened by chance. Except for TempFac = 18 the p.values are less than 0.05 and we can reject the NULL hypothesis that each of these is normally distributed. Perhaps this is a sign that we need more than 20 samples per factor level. Let there be more frogs !!! இன்னும தவளைகள் வேண்டும்!! !!\nWe can also check the residuals post-model:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;%\n  gf_dhistogram(~value, data = .) %&gt;%\n  gf_labs(\n    title = \"Residuals Histogram\",\n    x = \"Residuals\", y = \"Count\"\n  ) %&gt;%\n  gf_fitdistr()\n##\nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;%\n  gf_qq(~value, data = .) %&gt;%\n  gf_qqstep() %&gt;%\n  gf_labs(\n    title = \"Residuals Q-Q Plot\",\n    x = \"Theoretical Quantiles\", y = \"Sample Quantiles\"\n  ) %&gt;%\n  gf_qqline()\n##\nshapiro.test(frogs_anova$residuals)\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\n\n Check for Similar Variance\nResponse data with different variances at different levels of an explanatory variable are said to exhibit heteroscedasticity. This violates one of the assumptions of ANOVA.\nTo check if the Time readings are similar in variance across levels of TempFac, we can use the Levene Test, or since our per-group observations are not normally distributed, a non-parametric rank-based Fligner-Killeen Test. The NULL hypothesis is that the data are with similar variances. The tests assess how probable this is with the given data assuming this NULL hypothesis:\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(variance = var(Time))\n# Not too different...OK on with the test\nDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n##\nfligner.test(Time ~ TempFac, data = frogs_long)\n\n\n\n\n  \n\n\n  \n\n\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n\n\n\nIt seems that there is no cause for concern here; the data do not have significantly different variances.\n\n Independent Observations\nThis is an experiment design concern; the way the data is gathered must be specified such that data for each level of the factors ( factor combinations if there are more than one) should be independent.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-effect-size",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-effect-size",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Effect Size",
    "text": "Workflow: Effect Size\nThe simplest way to find the actual effect sizes detected by an ANOVA test is something we have already done, with the supernova package: Here is the table and plot again:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfrogs_supernova &lt;-\n  supernova::pairwise(frogs_anova,\n    plot = TRUE,\n    alpha = 0.05,\n    correction = \"Bonferroni\"\n  )\n\n\n\n\n\n\nfrogs_supernova\n\n\n  group_1 group_2    diff pooled_se       t    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -5.861 -4.739 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.661 -9.539 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.361 -4.239 .0000\n\n\nThis table, the plot, and the equation we set up earlier all give us the sense of how the TempFac affects Time. The differences are given pair-wise between levels of the Qual factor, TempFac, and the standard error has been declared in pooled fashion (all groups together).\n\nWe can also use (paradoxically) the summary.lm() command:\n\ntidy_anova &lt;-\n  frogs_anova %&gt;%\n  summary.lm() %&gt;%\n  broom::tidy()\ntidy_anova\n\n\n  \n\n\n\nIt may take a bit of effort to understand this. First the TempFac is arranged in order of levels, and the mean at the \\(TempFac = 13\\) is titled Intercept. That is \\(26.3\\). The other two means for levels \\(18\\) and \\(25\\) are stated as differences from this intercept, \\(-5.3\\) and \\(-10.1\\) respectively. The p.value for all these effect sizes is well below the desired confidence level of \\(0.05\\).\n\n\n\n\n\n\nNoteStandard Errors\n\n\n\nObserve that the std.error for the intercept is \\(0.257\\) while that for TempFac18 and TempFac25 is \\(0.257 \\times \\sqrt2 = 0.363\\) since the latter are differences in means, while the former is a single mean. The Variance of a difference is the sum of the individual variances, which are equal here.\n\n\nWe can easily plot bar-chart with error bars for the effect size:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ntidy_anova %&gt;%\n  mutate(\n    hi = estimate + std.error,\n    lo = estimate - std.error\n  ) %&gt;%\n  gf_hline(\n    data = ., yintercept = 0,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_col(estimate ~ term,\n    fill = \"grey\",\n    color = \"black\",\n    width = 0.15\n  ) %&gt;%\n  gf_errorbar(hi + lo ~ term,\n    color = \"blue\",\n    width = 0.2\n  ) %&gt;%\n  gf_point(estimate ~ term,\n    color = \"red\",\n    size = 3.5\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\"Temperature (°C)\",\n    labels = c(\"13°C\", \"18°C\", \"25°C\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Effect Size of Temperature on Spawn Time\",\n    subtitle = \"ANOVA: Frogs Spawn Time vs Temperature Setting\",\n    caption = \"Relative Effect Values\",\n    x = \"Temperature (°C)\", y = \"Effect Size (Spawn Time)\"\n  )\n\n\n\n\n\n\n\nIf we want an “absolute value” plot for effect size, it needs just a little bit of work:\n\n# Merging group averages with `std.error`\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(mean = mean(Time)) %&gt;%\n  cbind(std.error = tidy_anova$std.error) %&gt;%\n  mutate(\n    hi = mean + std.error,\n    lo = mean - std.error\n  ) %&gt;%\n  gf_hline(\n    data = ., yintercept = 0,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_col(mean ~ TempFac,\n    fill = \"grey\",\n    color = \"black\", width = 0.15\n  ) %&gt;%\n  gf_errorbar(hi + lo ~ TempFac,\n    color = \"blue\",\n    width = 0.2\n  ) %&gt;%\n  gf_point(mean ~ TempFac,\n    color = \"red\",\n    size = 3.5\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\"Temperature (°C)\",\n    labels = c(\"13°C\", \"18°C\", \"25°C\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Effect Size of Temperature on Spawn Time\",\n    subtitle = \"ANOVA: Frogs Spawn Time vs Temperature Setting\",\n    caption = \"Absolute Effect Values\",\n    x = \"Temperature (°C)\", y = \"Effect Size (Spawn Time)\"\n  )\n\n\n\n\n\n\n\nIn both graphs, note the difference in the error-bar heights.\nThe ANOVA test does not tell us that the “treatments” (i.e. levels of TempFac) are equally effective. We need to use a multiple comparison procedure to arrive at an answer to that question. We compute the pair-wise differences in effect-size:\n\nfrogs_anova %&gt;% stats::TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Time ~ TempFac, data = frogs_long)\n\n$TempFac\n       diff        lwr       upr p adj\n18-13  -5.3  -6.175224 -4.424776     0\n25-13 -10.1 -10.975224 -9.224776     0\n25-18  -4.8  -5.675224 -3.924776     0\n\n\nWe see that each of the pairwise differences in effect-size is significant, with p = 0 !\n\nUsing other packages\n\n\nUsing ggstatsplot\nUsing supernova\n\n\n\nThere is a very neat package called ggstatsplot6 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nlibrary(ggstatsplot)\nfrogs_long %&gt;%\n  ggstatsplot::ggbetweenstats(\n    x = TempFac, y = Time,\n    colour = TempFac, alpha = 0.8,\n    type = \"parametric\",\n    pairwise.comparisons = TRUE,\n    p.adjust.method = \"bonferroni\",\n    conf.level = 0.95,\n    # Plot parameters for the points\n    point.args = list(\n      position = ggplot2::position_jitterdodge(dodge.width = 0.6), alpha =\n        0.8, size = 3, stroke = 0, na.rm = TRUE\n    ),\n\n    # Plot parameters for the boxplots\n    boxplot.args = list(width = 0.3, alpha = 0.2, na.rm = TRUE),\n\n    # Plot parameters for the violin plots\n    violin.args = list(width = 0.5, alpha = 0.2, na.rm = TRUE),\n    title = \"ANOVA : Frogs Spawn Time vs Temperature Setting\"\n  ) +\n  scale_colour_paletteer_d(\"ggthemes::colorblind\") +\n  labs(x = \"Temperature (°C)\", y = \"Spawn Time (seconds)\")\n\n\n\n\n\n\n\nProbably a case of too much in one plot, but it does give us a lot of information, including the pairwise comparisons and the effect sizes. The ggstatsplot package is very useful for quick visualizations of statistical tests.\n\n\nWe can also obtain crisp-looking anova tables from the new supernova package 7, which is based on the methods discussed in Judd et al. Section 14\nlibrary(supernova)\nsupernova::supernova(frogs_anova)\nsupernova::pairwise(frogs_anova)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\n\n\n\n  group_1 group_2    diff pooled_se       q    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -6.175 -4.425 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.975 -9.225 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.675 -3.925 .0000\n\n\n\nThe supernova table clearly shows the reduction the Sum of Squares as we go from a NULL (empty) model to a full ANOVA model.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova-using-permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova-using-permutation-tests",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA using Permutation Tests",
    "text": "Workflow: ANOVA using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst8, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp.\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nWe will use infer to do this: We calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\n\nobserved_infer &lt;-\n  frogs_long %&gt;%\n  specify(Time ~ TempFac) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\nobserved_infer\n\n\n  \n\n\n\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\n\nnull_dist_infer &lt;- frogs_long %&gt;%\n  specify(Time ~ TempFac) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 4999, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\n##\nnull_dist_infer\n\n\n  \n\n\n##\nnull_dist_infer %&gt;%\n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") +\n  scale_x_continuous(trans = \"log10\", expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0.2, 500), clip = \"off\") +\n  annotation_logticks(outside = FALSE) +\n  theme_custom()\n\n\n\n\n\n\n\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#wait-but-why",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nIn marketing, design, or business research, similar quantities may be measured across different locations, or stores, or categories of people, for instance.\nANOVA is the tool to decide if the Quant variable has differences across the Qual categories.\nThis approach can be extended to more than one Qual variable, and also if there is another Quant variable in the mix.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#conclusions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have discussed ANOVA as a means of modelling the effects of a Categorical variable on a Continuous (Quant) variable. ANOVA can be carried out using the standard formula aov when assumptions on distributions, variances, and independence are met. Permutation ANOVA tests can be carried out when these assumptions do not quite hold.\n\n\n\n\n\n\nNoteTwo-Way ANOVA\n\n\n\nWhat if we have two Categorical variables as predictors?\nWe then need to perform a Two-Way ANOVA analysis, where we look at the predictors individually (main effects) and together (interaction effects). Here too, we need to verify if the number of observations are balanced across all combinations of factors of the two Qualitative predictors. There are three different classical approaches (Type1, Type2, and Type3 ANOVA) for testing hypotheses in ANOVA for unbalanced designs, as they are called. (Langsrud 2003).\n\n\n\n\n\n\n\n\nNoteInformative Hypothesis Testing: Models which incorporate a priori Beliefs\n\n\n\nNote that when we specified our research question, we had no specific hypothesis about the means, other than that they might be different. In many situations, we may have reason to believe in the relative “ordering” of the means for different levels of the Categorical variable. The one-sided t-test is the simplest example (e.g., \\(\\mu_1 &gt;= 0\\) and \\(\\mu_1 &gt;= \\mu_2\\)); this readily extends to the multi-parameter setting, where more than one inequality constraint can be imposed on the parameters (e.g., \\(\\mu_1 &lt;= \\mu_2 &lt;= \\mu_3\\).\nIt is possible to incorporate these beliefs into the ANOVA model, using what is called as informative hypothesis testing, which have certain advantages compared to unconstrained models. The R package called restriktor has the capability to develop such models with beliefs.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#your-turn",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the simple datasets at https://www.performingmusicresearch.com/datasets/\nCan you try to ANOVA-analyse the datasets we dealt with in plotting Groups with Boxplots?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#sec-references",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n References",
    "text": "References\n\nThe ANOVA tutorial at Our Coding Club\n\nAntoine Soetewey. How to: one-way ANOVA by hand. https://statsandr.com/blog/how-to-one-way-anova-by-hand/\n\nANOVA in R - Stats and R https://statsandr.com/blog/anova-in-r/\n\nMichael Crawley.(2013) The R Book,second edition. Chapter 11.\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan.(2017). “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1.\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\nLangsrud, Øyvind. (2003). ANOVA for unbalanced data: Use type II instead of type III sums of squares. Statistics and Computing. 13. 163-167. https://doi.org/10.1023/A:1023260610025. https://www.researchgate.net/publication/220286726_ANOVA_for_unbalanced_data_Use_type_II_instead_of_type_III_sums_of_squares\n\nKim TK. (2017). Understanding one-way ANOVA using conceptual figures. Korean J Anesthesiol. 2017 Feb;70(1):22-26. https://ekja.org/upload/pdf/kjae-70-22.pdf\n\n\nAnova – Type I/II/III SS explained.https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/\n\nBidyut Ghosh (Aug 28, 2017). One-way ANOVA in R. https://datascienceplus.com/one-way-anova-in-r/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nDescTools\n0.99.60\nSignorell (2025)\n\n\nggprism\n1.0.6\nDawson (2025)\n\n\nggstatsplot\n0.13.1\nPatil (2021)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\nrestriktor\n0.6.10\nVanbrabant and Kuiper (2024)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://doi.org/10.32614/CRAN.package.supernova.\n\n\nDawson, Charlotte. 2025. ggprism: A “ggplot2” Extension Inspired by “GraphPad Prism”. https://doi.org/10.32614/CRAN.package.ggprism.\n\n\nLangsrud, Øyvind. 2003. Statistics and Computing 13 (2): 163–67. https://doi.org/10.1023/a:1023260610025.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nSignorell, Andri. 2025. DescTools: Tools for Descriptive Statistics. https://doi.org/10.32614/CRAN.package.DescTools.\n\n\nVanbrabant, Leonard, and Rebecca Kuiper. 2024. restriktor: Restricted Statistical Estimation and Inference for Linear Models. https://doi.org/10.32614/CRAN.package.restriktor.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#footnotes",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe ANOVA tutorial at Our Coding Club.↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nhttps://www.openintro.org/go/?id=anova-supplement↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎\nmosaic::xpf() gives both a graph and the probabilities.↩︎\nggplot2 Based Plots with Statistical Details • ggstatsplot https://indrajeetpatil.github.io/ggstatsplot/↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(mosaic)\nlibrary(ggformula)\nlibrary(infer)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)\n\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#setting-up-r-packages",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(mosaic)\nlibrary(ggformula)\nlibrary(infer)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)\n\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#introduction",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Introduction",
    "text": "Introduction\nOften we hear reports that a certain percentage of people support a certain political party, or that a certain proportion of people are in favour of a certain policy. Such statements are the result of a desire to infer a proportion in the population, which is what we will investigate here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#workflow-sampling-theory-for-proportions",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#workflow-sampling-theory-for-proportions",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Workflow: Sampling Theory for Proportions",
    "text": "Workflow: Sampling Theory for Proportions\nWe have seen how sampling from a population works when we wish to estimate means:\n\nThe sample means \\(\\bar{x}\\) are centred around the population mean \\(\\mu\\);\nThe samples means are normally distributed\n\nThe uncertainty in using \\(\\bar{x}\\) as an estimate for \\(\\mu\\) is given by a Confidence interval defined by some constant times the Standard Error of the sample \\(\\frac{s}{\\sqrt(n)}\\);\nThe larger the size of the sample, the tighter the Confidence Interval.\n\nNow then: does a similar logic work for proportions too, as for means?\n\n The CLT for Proportions\n\nSample proportions are also centred around population proportions\n\nSuccess-failure condition: If \\(\\hat{p} *n &gt;= 10\\) and \\((1-\\hat{p})*n &gt;= 10\\) are both satisfied, then the we can assume that the sampling distribution of the proportion is normal. And so:\nThe Standard Error for a sample proportion is given by \\(SE = \\sqrt\\frac{\\hat{p}(1-\\hat{p})}{n}\\), where \\(\\hat{p}\\) is the sample proportion\nWe would calculate the Confidence Intervals in a similar fashion, based on the desired probability of error, as:\n\n\\[\np = \\hat{p} \\pm 1.96*{SE}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-1-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-1-yrbss-survey",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #1: YRBSS Survey",
    "text": "Case Study #1: YRBSS Survey\nWe will be analyzing the same dataset called the Youth Risk Behavior Surveillance System (YRBSS) survey from the openintro package, which uses data from high schoolers to help discover health patterns. The dataset is called yrbss.\n\n Workflow: Read the Data\n\ndata(yrbss, package = \"openintro\")\nyrbss\n\n\n  \n\n\n\nWhen summarizing the YRBSS data, the Centers for Disease Control and Prevention seeks insight into the population parameters. Accordingly, in this tutorial, our research questions are:\n\n\n\n\n\n\nNoteResearch Questions\n\n\n\n\nWhat are the counts within each category for the amount of days these students have texted while driving within the past 30 days?\nWhat proportion of people on earth have texted while driving each day for the past 30 days without wearing helmets?\n\n\n\nQuestion 1 pertains to the data set yrbss, our “sample”. To answer this, you can answer the question, “What proportion of people in your sample reported that they have texted while driving each day for the past 30 days?” with a statistic. Question 2 is an inference we need to make about the population of highschoolers. While the question “What proportion of people on earth have texted while driving each day for the past 30 days?” is answered with an estimate of the parameter.\nFor our first Research Question, we will choose the column helmet_12m: Remember that you can use filter to limit the dataset to just non-helmet wearers. Here, we will name the (filtered ) dataset no_helmet.\n\nyrbss %&gt;%\n  group_by(helmet_12m) %&gt;%\n  count()\n\n\n  \n\n\n##\nyrbss %&gt;%\n  group_by(text_while_driving_30d) %&gt;%\n  count()\n\n\n  \n\n\n\nAlso, it may be easier to calculate the proportion if we create a new variable that specifies whether the individual has texted every day while driving over the past 30 days or not. We will call this variable text_ind.\n\nno_helmet_text &lt;- yrbss %&gt;%\n  filter(helmet_12m == \"never\") %&gt;%\n  mutate(text_ind = ifelse(text_while_driving_30d == \"30\", \"yes\", \"no\")) %&gt;%\n  # removing most of the other variables\n  select(age, gender, text_ind)\nno_helmet_text\n\n\n  \n\n\n##\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  count(text_ind)\n\n\n  \n\n\n##\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  summarize(prop = prop(text_ind, success = \"yes\"), n = n())\n\n\n  \n\n\n\nThis is the observed_statistic: the proportion of people in this sample who do text when they drive without a helmet.\nVisualizing a Single Proportion\nWe can quickly plot this, just for the sake of visual understanding of the proportions:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  gf_bar(~text_ind) %&gt;%\n  gf_labs(\n    x = \"texted?\",\n    title = \"High-Schoolers who texted every day\",\n    subtitle = \"While driving with no helmet on!!\"\n  )",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#inference-for-a-single-proportion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#inference-for-a-single-proportion",
    "title": "🃏 Testing a Single Proportion",
    "section": "Inference for a Single Proportion",
    "text": "Inference for a Single Proportion\nBased on this sample in the yrbss data, we wish to infer proportions for the population of high-schoolers.\nHypothesis Testing for a Single Proportion\nConsider the inference we did for a single mean. What was our NULL Hypothesis? That the population mean \\(\\mu = 0\\). for two means? That they might be equal. What might a suitable NULL Hypothesis be for a single proportion? What attitude of ain’t nothing happenin’ might we adopt?\n\n\n\n\n\n\nImportant\n\n\n\nWith proportions, we usually look for a “no difference” situation, i.e. a ratio of unity!! So our NULL hypothesis would be a proportion of 1:1 for texters and no-texters, so a proportion of \\(0.5\\)!!\n\n\n\n\nClassical Test\nUncertainty in Estimation\n Bootstrap test\n\n\n\nThe simplest test in R for a single proportion is the binom.test:\n\nmosaic::binom.test(~text_ind, data = no_helmet_text, success = \"yes\")\n\n\n\n\ndata:  no_helmet_text$text_ind  [with success = yes]\nnumber of successes = 463, number of trials = 6503, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.06506429 0.07771932\nsample estimates:\nprobability of success \n            0.07119791 \n\nmosaic::binom.test(~text_ind, data = no_helmet_text, success = \"yes\") %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nHow do we understand this result? That the sample tells us the \\(\\hat{p} = 0.07119\\) and that based on this the population proportion of those who text while driving without a helmet is also not 0.5, since the p-value is \\(2.2e-16\\). So we reject the NULL hypothesis and accept the alternative hypothesis.\n\n\nThe Confidence Intervals from the binom.test inform us about our population proportion estimate: It lies within the interval [0.06506429, 0.07771932]. We know that this is also given by:\n\\[\n\\begin{eqnarray}\nCI &=& \\hat{p} ~ \\pm 1.96*SE\\\\\n&=& \\hat{p} ~ \\pm 1.96*\\sqrt{\\hat{p}* (1-\\hat{p})/n}\\\\\n&=& 0.0711 \\pm 1.96*\\sqrt{0.0711 * (1- 0.0711)/6847}\\\\\n&=& 0.0711 \\pm 0.006\\\\\n&=& [0.065, 0.771]\n\\end{eqnarray}\n\\]\n\n Permutation Visually Demonstrated\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. We will first look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators! Now we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n  \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nAs can be seen, the ratio is different!\nWe can now check out our Hypothesis that there is no bias. We can shuffle the data many many times, calculating the ratio each time, and plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with the originally observed figure from Mother Nature.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist &lt;- do(4999) * diff(mean(\n  candidate_selected ~ shuffle(evaluator),\n  data = data\n))\n# null_dist %&gt;% names()\nnull_dist %&gt;%\n  gf_histogram(~M,\n    fill = ~ (M &lt;= obs_difference),\n    bins = 25, show.legend = FALSE,\n    xlab = \"Bias Proportion\",\n    ylab = \"How Often?\",\n    title = \"Permutation Test on Difference between Groups\",\n    subtitle = \"\"\n  ) %&gt;%\n  gf_vline(xintercept = ~obs_difference, color = \"red\") %&gt;%\n  gf_label(500 ~ obs_difference,\n    label = \"Observed\\n Bias\",\n    show.legend = FALSE\n  )\nmean(~ M &lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n \n\n\n[1] 0.00220044\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.0022\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.\n\n\n\nThe inferential tools for estimating a single population proportion are analogous to those used for estimating single population means: the bootstrap confidence interval and the hypothesis test.\n\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  specify(response = text_ind, success = \"yes\") %&gt;%\n  generate(reps = 999, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\") %&gt;%\n  get_ci(level = 0.95)\n\n\n  \n\n\n\nNote that since the goal is to construct an interval estimate for a proportion, it’s necessary to both include the success argument within specify, which accounts for the proportion of non-helmet wearers than have consistently texted while driving the past 30 days, in this example, and that stat within calculate is here “prop”, signaling that we are trying to do some sort of inference on a proportion.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-2-tbd",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-2-tbd",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #2: TBD",
    "text": "Case Study #2: TBD\nTo be Written up in the foreseeable future.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#an-interactive-app",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#an-interactive-app",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n An interactive app",
    "text": "An interactive app\nhttps://openintro.shinyapps.io/CLT_prop/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#wait-but-why",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nIn business, or “design research”, one encounters things that are proportions in a target population:\n\nAdoption of a service or an app\nPeople preferring a particular product\nBeliefs which are of Yes/No type: Is this Govt. doing the right thing with respect to taxes?\nKnowing what this population proportion is a necessary step to take a decision about what you will do about it.\n(Other than plot a *&%#$$%^& pie chart)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#conclusion",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nWe have seen how the CLT works with proportions, in a manner similar to that with means\nThe Standard Error (and therefore the CI) for the inference of a proportion is related to the actual population proportion, which is very different behaviour from that with means, where SE was just a number that depended on the sample size\nBootstrap procedures work with inference for a single proportion. (Permutation when there are two)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#your-turn",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nType data(package = \"resampledata\") and data(package = \"resampledata3\") in your RStudio console. This will list the datasets in both these package. Try loading a few of these and infering for single proportions.\nNational Health and Nutrition Examination Survey (NHANES) dataset. Install the package NHANES and explore the dataset for proportions that might be interesting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#references",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n References",
    "text": "References\n\nStackExchange. prop.test vs binom.test in R. https://stats.stackexchange.com/q/551329\n\nMine Çetinkaya-Rundel and Johanna Hardin, OpenIntro Modern Statistics: Chapter 17\n\nLaura M. Chihara, Tim C. Hesterberg, Mathematical Statistics with Resampling and R. 3 August 2018.© 2019 John Wiley & Sons, Inc.\n\nOpenIntro Statistics Github Repo: https://github.com/OpenIntroStat/openintro-statistics\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggbrace\n0.1.2\nHuber (2025)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nHuber, Nicolas. 2025. ggbrace: Curly Braces for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggbrace.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE, fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\n\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?:\n\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nB.  Check for Variances\n\n\n\n\n\n\nImportantConditions:\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType this in your console: help(yrbss)Type help(wilcox.test) in your Console.\n\nUsing the Parametric t.test\nUsing the non-parametric wilcox.test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\nAll Tests Together\nA.  Check for Normality\nB.  Check for Variances\nAll Tests Together\nA.  Check for Normality\nB.  Check for Variances\n Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey\nEvery two years, the Centers for Disease Control and Prevention in the USA conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from highschoolers (9th through 12th grade), to analyze health patterns. We will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\n\n Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. \nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?\n\n\n\n Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\nyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender, physically_active_7d) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\n\n\n\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nNoteShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"male\") %&gt;%\n  select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"female\") %&gt;%\n  select(weight)\n# shapiro.test(male_student_weights$weight)\n# shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_facet_grid(~gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nWe can plot Q-Q plots3 for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_gender %&gt;%\n  gf_qq(~ weight | gender) %&gt;%\n  gf_qqline(ylab = \"scores\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nNo real evidence (visually) of the variables being normally distributed.\n\n\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089 \n\n\n\n\n Inference\n\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\nnull_dist_weight &lt;-\n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\n\n\n\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!\n\n Case Study #3: Weight vs Exercise in the YRBSS Survey\nNext, consider the possible relationship between a highschooler’s weight and their physical activity.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss_select_phy &lt;- yrbss %&gt;%\n  drop_na(physically_active_7d, weight) %&gt;%\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 3, \"yes\", \"no\"),\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  ) %&gt;%\n  select(weight, physical_3plus)\n\n# Let us check\nyrbss_select_phy %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight vary based on whether students exercise on more or less than 3 days a week? (physically_active_7d &gt;= 3 days)\n\n\n\n Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \"\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\n\n\n\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nyrbss_select_phy %&gt;%\n  gf_density(~weight,\n    fill = ~physical_3plus,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Exercise Frequency\"\n  ) %&gt;%\n  gf_facet_grid(~physical_3plus) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_phy %&gt;%\n  gf_qq(~ weight | physical_3plus, color = ~physical_3plus) %&gt;%\n  gf_qqline(ylab = \"Weight\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\n\n\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test(weight ~ physical_3plus,\n  data = yrbss_select_phy,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# Critical F value\nqf(0.975, 4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]\n\n Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\nUsing parametric t.test\nUsing non-parametric paired Wilcoxon test\n\n\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n  var.equal = FALSE, # Welch Correction\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n  conf.int = TRUE,\n  conf.level = 0.95,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)\n\n\n  \n\n\n\n\n\n\n\nYour Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. \nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\nyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender, physically_active_7d) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\n\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\nnull_dist_weight &lt;-\n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \"\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test",
    "text": "Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear.\n\n\nTry sample sizes of 25, 50, 100, 500.\n\n\n\nWrite your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "title": "Sampling",
    "section": "",
    "text": "Try sample sizes of 25, 50, 100, 500."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "title": "Sampling",
    "section": "",
    "text": "Write your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#introduction",
    "title": "🧭 Basics of Statistical Inference",
    "section": " Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "title": "🧭 Basics of Statistical Inference",
    "section": "The Big Ideas in Stats",
    "text": "The Big Ideas in Stats\nSteven Stigler(Stigler 2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. The Big Ideas in Statistics from that book are:\n\n1.Aggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\n2.Information\n\nIn the early eighteenth century it was discovered that in many situations the amount of information in a set of data was only proportional to the square root of the number n of observations, not the number n itself.\n\n\n\n3.Likelihood\n\nBy the name I give to the third pillar, Likelihood, I mean the calibration of inferences with the use of probability. The simplest form for this is in significance testing and the common P-value.\n\n\n\n4.Intercomparison\n\nIt represents what was also once a radical idea and is now commonplace: that statistical comparisons do not need to be made with respect to an exterior standard but can often be made in terms interior to the data themselves. The most commonly encountered examples of intercomparisons are Student’s t-tests and the tests of the analysis of variance.\n\n\n\n5.Regression\n\nI call the fifth pillar Regression, after Galton’s revelation of 1885, explained in terms of the bivariate normal distribution. Galton arrived at this by attempting to devise a mathematical framework for Charles Darwin’s theory of natural selection, overcoming what appeared to Galton to be an intrinsic contradiction in the theory: selection required increasing diversity, in contradiction to the appearance of the population stability needed for the definition of species.\n\n\n\n6.Design of Experiments and Observations\n\nThe sixth pillar is Design, as in “Design of Experiments,” but conceived of more broadly, as an ideal that can discipline our thinking in even observational settings.Starting in the late nineteenth century, a new understanding of the topic appeared, as Charles S. Peirce and then Fisher discovered the extraordinary role randomization could play in inference.\n\n\n\n7.Residuals\n\nThe most common appearances in Statistics are our model diagnostics (plotting residuals), but more important is the way we explore high-dimensional spaces by fitting and comparing nested models.\n\nIn our work with Statistical Models, we will be working with all except Idea 6 above.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "title": "🧭 Basics of Statistical Inference",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nFrom Daniel Kaplan’s book:\n\n“Modeling” is a process of asking questions. “Statistical” refers in part to data – the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don’t know and that doing so contributes to your understanding.\n\nThe conclusions you reach from data depend on the specific questions you ask. The word “modeling” highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data. You examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#types-of-statistical-models-based-on-purpose",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#types-of-statistical-models-based-on-purpose",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Types of Statistical Models Based on Purpose",
    "text": "Types of Statistical Models Based on Purpose\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what’s a “normal” white blood cell count? Sometimes you want to describe the relationship between things. Example: What’s the relationship between the price of gasoline and consumption by automobiles?\nClassification or Prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can’t directly observe. For instance, you know a patient’s white blood-cell count and other laboratory measurements and want to diagnose the patient’s illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause (polarity) and effect (magnitude) correct in your models.\n\n\n\n\n\n\nNote\n\n\n\nAn effect size tells how the output of a model changes when a simple change is made to the input.Effect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBut for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling – with tremendous consequences for science, medicine, government, and commerce – is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\n\n\n\nTypes of Models Based on Data Variables\nLet us look at the famous dataset pertaining to Francis Galton’s work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the types of variables in that dataset.\n\n\n\n\n\n\n\nVariables and Models",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#linear-models-everywhere",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#linear-models-everywhere",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Linear Models Everywhere",
    "text": "Linear Models Everywhere\nOne method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression \\(y = \\beta_1 *x + \\beta_0\\) . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model, with the dependent variable modelled as \\(y\\) and the independent one as \\(x\\). We will try to work up to the intuition that this model can be used to understand all the models in the Figure.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#a-flowchart-of-statistical-inference-tests",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#a-flowchart-of-statistical-inference-tests",
    "title": "🧭 Basics of Statistical Inference",
    "section": "A Flowchart of Statistical Inference Tests",
    "text": "A Flowchart of Statistical Inference Tests\n\n\n\n\n\nflowchart TD\n    A[Inference for Means] --&gt;|Check Assumptions|B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test\\n Outliers: Box Plots]\n    B --&gt; M[Means]\n\n subgraph Means\n    direction TB\n      subgraph Single-Mean\n        direction LR\n        OM[Single Mean]&lt;--&gt;|p| TT[t.test]\n        TWT[t.test \\n Welch]&lt;--&gt;|p diff var|OM[Single Mean]\n        WT[wilcox.test]&lt;--&gt;|np|OM[Single Mean]\n      end\n      \n      subgraph Paired-Means\n        direction LR\n        TM[Paired Means]&lt;--&gt;|p| TTP[t.test with pairs]\n        WTP[wilcox.test with pairs\\n Mann-Whitney U Test]--&gt;|np|TM\n      end\n      \n      subgraph Multiple-Means\n        direction LR\n        MM[Multiple Means] --&gt;|p| ANO[ANOVA]\n        KW[kruskal.test]&lt;--&gt;|np indep| MM\n        FT[friedman.test]&lt;--&gt;|np dep| MM\n      end\n      \nM --&gt;Single-Mean\nSingle-Mean--&gt;Paired-Means\nPaired-Means--&gt;Multiple-Means\n\nend\n\n%%subgraph LM\n%%  direction BT\n%%  LM[Linear Model]--&gt;Means\n%%end",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#references",
    "title": "🧭 Basics of Statistical Inference",
    "section": "References",
    "text": "References\n\nChester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Available Online https://moderndive.com/index.html\nhttp://drafts.jsvine.com/the-magic-criteria/\nTihamér von Ghyczy, The Fruitful Flaws of Strategy Metaphors. Harvard Business Review, 2003. https://hbr.org/2003/09/the-fruitful-flaws-of-strategy-metaphors\nDaniel T. Kaplan, Statistical Models (second edition). Available online. https://dtkaplan.github.io/SM2-bookdown/\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. Available Online. https://dtkaplan.github.io/CompactInference/\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. Available online https://www.mosaic-web.org/go/SM2-technique/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Case Study-1: IceCream!!",
    "text": "Case Study-1: IceCream!!\nWhat is there to not like about icecreams!! Here is a dataset that has data on Sugar and Calories between Vanilla and Chocolate icecreams, across several brands of icecreams. Is this a sample of paired data? Let us check:\n\n Inspecting and Charting Data\ndata(\"IceCream\")\nIceCream\ninspect(IceCream)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n   name  class levels  n missing                                  distribution\n1 Brand factor     39 39       0 Baskin Robbins (2.6%) ...                    \n\nquantitative variables:  \n               name   class   min    Q1 median    Q3 max      mean        sd  n\n1   VanillaCalories integer 120.0 140.0    160 240.0 307 191.41026 58.644207 39\n2        VanillaFat numeric   4.5   7.5      9  15.5  21  11.28718  4.431655 39\n3      VanillaSugar numeric  10.0  12.5     17  21.0  27  17.13077  4.841333 39\n4 ChocolateCalories integer 120.0 140.0    170 260.0 320 198.74359 63.063342 39\n5      ChocolateFat numeric   5.0   7.5      9  14.7  21  11.12051  4.597378 39\n6    ChocolateSugar numeric  12.0  15.0     18  22.3  33  18.97436  5.402812 39\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\n\nHmm…the data are about calories, fat, and sugar between two flavours of icecream sold by each brand. There are 39 brands.\nLet us plot the data first:\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaCalories) ~ VanillaCalories,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaCalories) ~ -ChocolateCalories,\n    fill = \"green\",\n    xlab = \"Calories\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaFat) ~ VanillaFat,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaFat) ~ -ChocolateFat,\n    fill = \"green\",\n    xlab = \"Fat\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaSugar) ~ VanillaSugar,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaSugar) ~ -ChocolateSugar,\n    fill = \"green\",\n    xlab = \"Sugar\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe may hypothesize that say, the fat content in the two flavours might be similar on a per brand basis. That is, if say Baskin Robbins has high sugar in the vanilla flavour, it is likely to have high sugar also in its chocolate flavour.\nLet us see what are the observed differences in the mean values of calories, sugar, and fat across brands:\n\nIceCream %&gt;%\n  mutate(\n    diff_calories = VanillaCalories - ChocolateCalories,\n    diff_fat = VanillaFat - ChocolateFat,\n    diff_sugar = VanillaSugar - ChocolateSugar\n  ) %&gt;%\n  summarise(\n    mean_diff_calories = mean(diff_calories),\n    mean_diff_fat = mean(diff_fat),\n    mean_diff_sugar = mean(diff_sugar)\n  )\n\n\n  \n\n\n\nHmm…while the numbers showing difference in means are quite different, we need to perform tests to infer whether these difference are statistically significant.\n\n Hypothesis\nHow do we specify our Hypotheses? (Of course, there is more than one!)\nWrite the Null and Alternate hypotheses here.\n\n Null Distribution Computations\nHow do we compute the NULL distributions, for each of the three components of the ice creams, using pair-wise analysis?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Conclusions",
    "text": "Conclusions\nSo are there significant differences in sugar, fat, and calorie content across the two flavours?\nIs this conclusion different if you don’t use paired-data, and just treat the data as independent readings?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "title": "Simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "The mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nShow the Code# library(mosaic)\nshuffle(1:10)\n\n [1]  3  8  1  2  6  5 10  4  9  7\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0. (Correlations between two variables) For example:\n\nCoupled with mosaic::do() we can repeat a shuffle many times, computing a desired statistic each time we shuffle. The distribution of this computed statistic is a NULL distribution, which can then be compared with the observed statistic to decide upon the Hypothesis Test and p-value."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "title": "Permutation Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nCase Study-1: Hot Wings Orders vs Gender\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nShow the CodeBeerwings &lt;- read.csv(\"../../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nShow the Codemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nShow the Codeobs_diff_wings &lt;- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings\n\ndiffmean \n     5.2 \n\n\n\nShow the Codegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 =&gt; No\\ difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =&gt;Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nShow the Codenull_dist_wings &lt;- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_wings, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.001998002 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\nTo test whether eating more hotwings would lead to increased beer consumption, we need a regression model, which we can again test with a permutation test.\n\nShow the Codelm(Beer ~ Hotwings, data = Beerwings)\n\n\nCall:\nlm(formula = Beer ~ Hotwings, data = Beerwings)\n\nCoefficients:\n(Intercept)     Hotwings  \n      3.040        1.941  \n\n\nCase Study-2: Verizon\nThe following example is used throughout this article. Verizon was an Incumbent Local Exchange Carrier (ILEC), responsible for maintaining land-line phone service in certain areas. Verizon also sold long-distance service, as did a number of competitors, termed Competitive Local Exchange Carriers (CLEC). When something went wrong, Verizon was responsible for repairs, and was supposed to make repairs as quickly for CLEC long-distance customers as for their own. The New York Public Utilities Commission (PUC) monitored fairness by comparing repair times for Verizon and different CLECs, for different classes of repairs and time periods. In each case a hypothesis test was performed at the 1% significance level, to determine whether repairs for CLEC’s customers were significantly slower than for Verizon’s customers. There were hundreds of such tests. If substantially more than 1% of the tests were significant, then Verizon would pay large penalties. These tests were performed using t tests; Verizon proposed using permutation tests instead.\n\nShow the Codeverizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nShow the Codemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nShow the Codeobs_diff_verizon &lt;- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nShow the Codenull_dist_verizon &lt;- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_verizon)\n\n prop_TRUE \n0.01098901 \n\n\nCase Story-3: Recidivism\nDo criminals released after a jail term commit crimes again?\n\nShow the Coderecidivism &lt;- read.csv(\"../../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‘Age25’  variable.\n\nShow the Coderecidivism_na &lt;- recidivism %&gt;% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We convert it to a numeric variable of 1’s and 0’s.\n\nShow the Coderecidivism_na &lt;- recidivism_na %&gt;% mutate(Recid2 = ifelse(Recid == \"Yes\", 1, 0))\n\nobs_diff_recid &lt;- diffmean(Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nShow the Codenull_dist_recid &lt;- do(1000) * diffmean(Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram(~diffmean, data = null_dist_recid) %&gt;%\n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\n\n\n\nCase Study-4: Matched Pairs: Results from a diving championship.\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n  \n\n\nShow the Codeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nShow the CodeDiving2017\n\n\n  \n\n\nShow the CodeDiving2017 %&gt;% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nShow the Codeobs_diff_swim &lt;- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nShow the Codepolarity &lt;- c(rep(1, 6), rep(-1, 6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nShow the Codenull_dist_swim &lt;- do(100000) * mean(\n  data = Diving2017,\n  ~ (Final - Semifinal) * resample(polarity,\n    replace = TRUE\n  )\n)\nnull_dist_swim %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_swim, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\n\n\n\nCase Study #5: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the CodeflightDelays &lt;- read.csv(\"../../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nShow the Codeprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nShow the Codeobs_diff_delay &lt;- diffprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays&gt;= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nShow the Codenull_dist_delay &lt;- do(10000) * diffprop(data = flightDelays, Delay &gt;= 20 ~ shuffle(Carrier))\nnull_dist_delay %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_delay, ~diffprop) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nShow the Code2 * mean(null_dist_delay &gt;= obs_diff_delay)\n\n[1] 2e-04\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nShow the Codevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nShow the Code# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nShow the Codeobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var &lt;-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %&gt;% head()\n\n\n  \n\n\nShow the Code# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~UA) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\nShow the Code2 * mean(null_dist_var &gt;= obs_diff_var)\n\n[1] 0.3022\n\n\nClearly there is no case for a significant difference in variances!\nCase Study #6: Walmart vs Target\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nShow the Codegroceries &lt;- read.csv(\"../../../../../../materials/data/resampling/Groceries.csv\") %&gt;% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n  \n\n\nShow the Codeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nShow the Codegf_col(\n  data = groceries,\n  Target ~ Product,\n  fill = \"#0073C299\",\n  width = 0.5\n) %&gt;%\n  gf_col(\n    data = groceries,\n    -Walmart ~ Product,\n    fill = \"#EFC00099\",\n    ylab = \"Prices\",\n    width = 0.5\n  ) %&gt;%\n  gf_col(\n    data = groceries %&gt;% filter(Product == \"Quaker Oats Life Cereal Original\"),\n    -Walmart ~ Product,\n    fill = \"red\",\n    width = 0.5\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %&gt;%\n  gf_theme(ggplot2::coord_flip())\n\n\n\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nShow the Codediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nShow the Codeobs_diff_price &lt;- mean(~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nShow the Codepolarity &lt;- c(rep(1, 15), rep(-1, 15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nShow the Codenull_dist_price &lt;- do(100000) * mean(\n  data = groceries,\n  ~ (Walmart - Target) * resample(polarity,\n    replace = TRUE\n  )\n)\nnull_dist_price %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nShow the Code2 * (sum(null_dist_price &gt;= obs_diff_price + 1) / (100000 + 1)) # P-value\n\n[1] 0\n\n\nDoes not seem to be any significant difference in prices…\nSuppose we knock off the Quaker Cereal data item…\n\nShow the Codewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nShow the Codegroceries_less &lt;- groceries[-2, ]\ngroceries_less\n\n\n  \n\n\nShow the Codeobs_diff_price_less &lt;- mean(~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nShow the Codepolarity_less &lt;- c(rep(1, 15), rep(-1, 14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;- do(100000) * mean(\n  data = groceries_less,\n  ~ (Walmart - Target) * resample(polarity_less,\n    replace = TRUE\n  )\n)\nnull_dist_price_less %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price_less, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\n\n\n\nShow the Code1 - mean(null_dist_price_less &gt;= obs_diff_price_less) # P-value\n\n[1] 0.01584\n\n\nCase Study 7: Proportions between Categorical Variables\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nShow the CodeGSS2002 &lt;- read.csv(\"../../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nShow the CodeGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nShow the CodeGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nShow the Codegss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nShow the Codegss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  #\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n\ngss_summary\n\n\n  \n\n\nShow the Code# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes(x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n  # if labels are desired\n  facet_grid(~Education, scales = \"free_x\", space = \"free_x\") +\n  theme(scale_fill_brewer(palette = \"RdYlGn\")) +\n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void()\n\n\n\n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nShow the Codegss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nShow the Code# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\nShow the Codegf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nShow the Codegf_histogram(~p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "…neither let us despair over how small our successes are. For however much our successes fall short of our desire, our efforts aren’t in vain when we are farther along today than yesterday.\n— John Calvin",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#setting-up-r-packages",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Setting up R packages",
    "text": "Setting up R packages\n\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(infer)\nlibrary(broom) # Clean test results in tibble form\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # More datasets\nlibrary(tidyverse)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#introduction",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module, we will answer a basic Question: What is the mean \\(\\mu\\) of the population?\nRecall that the mean is the first of our Summary Statistics. We wish to know more about the mean of the population from which we have drawn our data sample.\nWe will do this is in several ways, based on the assumptions we are willing to adopt about our data. First we will use a toy dataset with one “imaginary” sample, normally distributed and made up of 50 observations. Since we “know the answer” we will be able to build up some belief in the tests and procedures, which we will dig into to form our intuitions.\nWe will then use a real-world dataset to make inferences on the means of Quant variables therein, and decide what that could tell us.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#statistical-inference-is-almost-an-attitude",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#statistical-inference-is-almost-an-attitude",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Statistical Inference is almost an Attitude!",
    "text": "Statistical Inference is almost an Attitude!\nAs we will notice, the process of Statistical Inference is an attitude: ain’t nothing happenin’! We look at data that we might have received or collected ourselves, and look at it with this attitude, seemingly, of some disbelief. We state either that:\n\nthere is really nothing happening with our research question, and that anything we see in the data is the outcome of random chance.\nthe value/statistic indicated by the data is off the mark and ought to be something else.\n\nWe then calculate how slim the chances are of the given data sample showing up like that, given our belief. It is a distance measurement of sorts. If those chances are too low, then that might alter our belief. This is the attitude that lies at the heart of Hypothesis Testing.\n\n\n\n\n\n\nImportant\n\n\n\nThe calculation of chances is both a logical, and a possible procedure since we are dealing with samples from a population. If many other samples give us quite different estimates, then we would discredit the one we derive from it.\n\n\nEach test we perform will mechanize this attitude in different ways, based on assumptions and conveniences. (And history)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-1-toy-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-1-toy-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #1: Toy data",
    "text": "Case Study #1: Toy data\nSince the CLT assumes the sample is normally-distributed, let us generate a sample that is just so:\n\n\n\nset.seed(40) # for replication\n#\n# Data as individual vectors\n# ( for t.tests etc)\n# Generate normally distributed data with mean = 2, sd = 2, length = 50\ny &lt;- rnorm(n = 50, mean = 2, sd = 2)\n\n# And as tibble too\nmydata &lt;- tibble(y = y)\nmydata",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inspecting-and-charting-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmydata %&gt;%\n  gf_density(~y) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_labs(\n    title = \"Densities of Original Data Variables\",\n    subtitle = \"Compared with Normal Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservations from Density Plots\n\n\n\n\nThe variable \\(y\\) appear to be centred around\nIt does not seem to be normally distributed…\nSo assumptions are not always valid…\n\n\n\nResearch Question\nResearch Questions are always about the population! Here goes:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nCould the mean of the population \\(\\mu\\), from which y has been drawn, be zero?\n\n\nAssumptions\n\n\n\n\n\n\nNoteTesting for Normality\n\n\n\nThe y-variable does not appear to be normally distributed. This would affect the test we can use to make inferences about the population mean.\nThere are formal tests for normality too. We will do them in the next case study. For now, let us proceed naively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference",
    "title": "🃏 Inference for a Single Mean",
    "section": "Inference",
    "text": "Inference\n\n\nThe t-test\nWilcoxon’s Signed-Rank Test\nUsing Permutation and Bootstrap\nIntuitive\n\n\n\nA. Model\nWe have \\(mean(y) = \\bar{y}.\\) We formulate “our disbelief” of \\(\\bar{y}\\) with a NULL Hypothesis, about the population as follows:\n\\[\n\\ H_0: \\mu = 0\n\\] And the alternative hypothesis, again about the population as\n\\[\nH_a:\\mu \\ne 0\n\\]\nB. Code\n\n# t-test\nt1 &lt;- mosaic::t_test(\n  y, # Name of variable\n  mu = 0, # belief of population mean\n  alternative = \"two.sided\"\n) %&gt;% # Check both sides\n\n  broom::tidy() # Make results presentable, and plottable!!\nt1\n\n\n  \n\n\n\n\n\n\n\n\n\nImportantRecall Confidence Intervals\n\n\n\nRecall how we calculated means, standard deviations from data (samples). If we could measure the entire population, then there would be no uncertainty in our estimates for means and sd-s. Since we are forced to sample, we can only estimate population parameters based on the sample estimates and state how much off we might be.\nConfidence intervals for population means are given by:\n\\[\n\\begin{eqnarray*}\nCI & = & \\bar{y} ~ {\\pm ~ constant * Standard Error}\\\\\n& = & \\bar{y} ~ {\\pm ~ 1.96 * {sd/\\sqrt{n}}}\n\\end{eqnarray*}\n\\]\n\nAssuming the y is normally-distributed, the \\(constant = 1.96\\) for confidence level of 95%. What that means is that if we take multiple such samples like \\(y\\) from the population, their means (which are random) will land within \\(CI\\) of the population mean (which is fixed!) 95% of the time. Uff…! May remind you of Buffon’s Needle…\n\n\n\n\n\n\n\n\n\n\n\n\nSo \\(\\bar{y}\\) i.e. the estimate is \\(2.045689\\). The confidence intervals do not straddle zero. The chances that this particular value of mean (\\(2.045689\\)) would randomly occur under the assumption that \\(\\mu\\) is zero, are exceedingly slim, \\(p.value = 1.425495e-08\\). Hence we can reject the NULL hypothesis that the true population, of which y is a sample, could have mean \\(\\mu = 0\\).\n\n\n“Signed Rank” Values: A Small Digression\nWhen the Quant variable we want to test for is not normally distributed, we need to think of other ways to perform our inference. Our assumption about normality has been invalidated.\nMost statistical tests use the actual values of the data variables. However, in these cases where assumptions are invalidated, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself. The signed ranks are then tested to see if there are more of one polarity than the other, roughly speaking, and how probable this could be.\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\n\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n\nGive each of the ranks the sign of the original observation ( + or -)\n\n\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nSince we are dealing with the mean, the sign of the rank becomes important to use.\nA. Model\n\\[\nmean(signed\\_rank(y)) = \\beta_0\n\\]\n\\[\nH_0: \\mu_0 = 0\n\\] \\[\nH_a: \\mu_0 \\ne 0\n\\]\nB. Code\n\n# Standard Wilcoxon Signed_Rank Test\nt2 &lt;- wilcox.test(y, # variable name\n  mu = 0, # belief\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\nt2\n\n\n  \n\n\n# Can also do this equivalently\n# t-test with signed_rank data\nt3 &lt;- t.test(signed_rank(y),\n  mu = 0,\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\nt3\n\n\n  \n\n\n\nAgain, the confidence intervals do not straddle \\(0\\), and we need to reject the belief that the mean is close to zero.\n\n\n\n\n\n\nNote\n\n\n\nNote how the Wilcoxon Test reports results about \\(y\\), even though it computes with \\(signed-rank(y)\\). The “equivalent t-test” with signed-rank data cannot do this, since it uses “rank” data, and reports the same result.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test 1! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe can use two packages in R, mosaic to develop our intuition for what are called permutation based statistical tests; and a more recent package called infer in R which can do pretty much all of this, including visualization.\nWe will stick with mosaic for now. We will do a permutation test first, and then a bootstrap test. In subsequent modules, we will use infer also.\nFor the Permutation test, we mechanize our belief that \\(\\mu = 0\\) by shuffling the polarities of the y observations randomly 4999 times to generate other samples from the population \\(y\\) could have come from2. If these samples can frequently achieve \\(\\bar{y_i} \\leq 0\\), then we might believe that the population mean may be 0!\nWe see that the means here that chances that the randomly generated means can exceed our real-world mean are about \\(0\\)! So the mean is definitely different from \\(0\\).\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Calculate exact mean\nobs_mean &lt;- mean(~y, data = mydata)\nbelief1 &lt;- 0 # What we think the mean is\nobs_diff_mosaic &lt;- obs_mean - belief1\nobs_diff_mosaic\n\n[1] 2.045689\n\n## Steps in Permutation Test\n## Repeatedly Shuffle polarities of data observations\n## Take means\n## Compare all means with the real-world observed one\nnull_dist_mosaic &lt;-\n  mosaic::do(9999) * mean(\n    ~ abs(y) *\n      sample(c(-1, 1), # +/- 1s multiply y\n        length(y), # How many +/- 1s?\n        replace = T\n      ), # select with replacement\n    data = mydata\n  )\n##\nrange(null_dist_mosaic$mean)\n\n[1] -1.754293  1.473298\n\n##\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_dist_mosaic,\n  fill = ~ (mean &gt;= obs_diff_mosaic),\n  bins = 50, title = \"Distribution of Permutation Means under Null Hypothesis\",\n  subtitle = \"Why is the mean of the means zero??\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\n# Null distributions are always centered around zero. Why?\nprop(~ mean &gt;= obs_diff_mosaic,\n  data = null_dist_mosaic\n)\n\nprop_TRUE \n        0 \n\n\nLet us try the bootstrap test now: Here we simulate samples, similar to the one at hand, using repeated sampling the sample itself, with replacement, a process known as bootstrapping, or bootstrap sampling.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\n## Resample with replacement from the one sample of 50\n## Calculate the mean each time\nnull_toy_bs &lt;- mosaic::do(4999) *\n  mean(\n    ~ sample(y,\n      replace = T\n    ), # select with replacement\n    data = mydata\n  )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_toy_bs,\n  bins = 50,\n  title = \"Distribution of Bootstrap Means\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = ~belief1, colour = \"red\")\n\n\n\n\n\n\nprop(~ mean &gt;= belief1,\n  data = null_toy_bs\n) +\n  prop(~ mean &lt;= -belief1,\n    data = null_toy_bs\n  )\n\nprop_TRUE \n        1 \n\n\n\n\n\n\n\n\nNotePermutation vs Bootstrap\n\n\n\nThere is a difference between the two. The bootstrap test uses the sample at hand to generate many similar samples without access to the population, and calculates the statistic needed (i.e. mean). No Hypothesis is stated. The distribution of bootstrap samples looks “similar” to that we might obtain by repeatedly sampling the population itself. (centred around a population parameter, i.e. \\(\\mu\\))\nThe permutation test generates many permutations of the data and generates appropriates measures/statistics under the NULL hypothesis. Which is why the permutation test has a NULL distribution centered at \\(0\\) in this case, our NULL hypothesis.\nAs student Sneha Manu Jacob remarked in class, Permutation flips the signs of the data values in our sample; Bootstrap flips the number of times each data value is (re)used. Good Insight!!\n\n\n\n\nYes, the t-test works, but what is really happening under the hood of the t-test? The inner mechanism of the t-test can be stated in the following steps:\n\nCalculate the mean of the sample \\(\\bar{y}\\).\nCalculate the sd of the sample, and, assuming the sample is normally distributed, calculate the standard error (i.e. \\(\\frac{sd}{\\sqrt{n}}\\))\nTake the difference between the sample mean \\(\\bar{y}\\) and our expected/believed population mean \\(\\mu\\).\nWe expect that the population mean ought to be within the confidence interval of the sample mean \\(\\bar{y}\\).\nFor a normally distributed sample, the confidence interval is given by \\(\\pm1.96 * standarderror\\), to be 95% sure that the sample mean is a good estimate for the population mean.\nTherefore if the difference between actual and believed is far beyond the confidence interval, hmm…we cannot think our belief is correct and we change our opinion.\n\nLet us translate that mouthful into calculations!\n\nmean_belief_pop &lt;- 0.0 # Assert our belief\n# Sample Mean\nmean_y &lt;- mean(y)\nmean_y\n\n[1] 2.045689\n\n## Sample standard error\nstd_error &lt;- sd(y) / sqrt(length(y))\nstd_error\n\n[1] 0.3014752\n\n## Confidence Interval of Observed Mean\nconf_int &lt;- tibble(ci_low = mean_y - 1.96 * std_error, ci_high = mean_y + 1.96 * std_error)\nconf_int\n\n\n  \n\n\n## Difference between actual and believed mean\nmean_diff &lt;- mean_y - mean_belief_pop\nmean_diff\n\n[1] 2.045689\n\n## Test Statistic\nt &lt;- mean_diff / std_error\nt\n\n[1] 6.785596\n\n\nWe see that the difference between means is 6.78 times the std_error! At a distance of 1.96 (either way) the probability of this data happening by chance already drops to 5%!! At this distance of 6.78, we would have negligible probability of this data occurring by chance!\nHow can we visualize this?\n\n\n\n\n\n\n\n\n\n\n\n\nIf X ~ N(2.046, 0.3015), then \n\n\n    P(X &lt;= 1.443e-07) = P(Z &lt;= -6.786) = 5.78e-12\n\n\n    P(X &gt;  1.443e-07) = P(Z &gt;  -6.786) = 1\n\n\n\n\n\n\n\n\n\n\n[1] 5.780412e-12",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-2-exam-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-2-exam-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #2: Exam data",
    "text": "Case Study #2: Exam data\nLet us now choose a dataset from the openintro package:\n\ndata(\"exam_grades\")\nexam_grades\n\n\n  \n\n\n\nResearch Question\nThere are quite a few Quant variables in the data. Let us choose course_grade as our variable of interest. What might we wish to find out?\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nIn general, the Teacher in this class is overly generous with grades unlike others we know of, and so the average course-grade is equal to 80% !!\n\n\n\n Inspecting and Charting Data\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nexam_grades %&gt;%\n  gf_density(~course_grade) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_labs(\n    title = \"Density of Course Grade\",\n    subtitle = \"Compared with Normal Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nHmm…data looks normally distributed. But this time we will not merely trust our eyes, but do a test for it.\nTesting Assumptions in the Data\n\n\n\n\n\n\nNoteIs the data normally distributed?\n\n\n\n\nstats::shapiro.test(x = exam_grades$course_grade) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe Shapiro-Wilkes Test tests whether a data variable is normally distributed or not. Without digging into the maths of it, let us say it makes the assumption that the variable is so distributed and then computes the probability of how likely this is. So a high p-value (\\(0.47\\)) is a good thing here.\nWhen we have large Quant variables ( i.e. with length &gt;= 5000), the shapiro.test does not work, and we use an Anderson-Darling3 test to confirm normality:\n\nlibrary(nortest)\n# Especially when we have &gt;= 5000 observations\nnortest::ad.test(x = exam_grades$course_grade) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nSo course_grade is a normally-distributed variable. There are no exceptional students! Hmph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference-1",
    "title": "🃏 Inference for a Single Mean",
    "section": "Inference",
    "text": "Inference\n\n\nt.test\nWilcoxon test\nUsing Permutation and Bootstrap\n\n\n\nA. Model\nWe have that \\(mean(course\\_grade) = \\beta_0\\). As before, we formulate “our (dis)belief” in this sample mean with a NULL Hypothesis about the population, as follows:\n\\[\n\\ H_0: \\mu= 80\n\\]\n\\[\nH_a: \\mu \\ne 80\n\\]\nB. Code\n\n# t-test\nt4 &lt;- mosaic::t_test(\n  exam_grades$course_grade, # Name of variable\n  mu = 80, # belief\n  alternative = \"two.sided\"\n) %&gt;% # Check both sides\n  broom::tidy()\nt4\n\n\n  \n\n\n\nSo, we can reject the NULL Hypothesis that the average grade in the population of students who have taken this class is 80, since there is a minuscule chance that we would see an observed sample mean of 72.238, if the population mean \\(\\mu\\) had really been \\(80\\).\n\n\n\n# t-test\nt5 &lt;- wilcox.test(\n  exam_grades$course_grade, # Name of variable\n  mu = 90, # belief\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;% # Check both sides\n\n  broom::tidy() # Make results presentable, and plottable!!\nt5\n\n\n  \n\n\n\nThis test too suggests that the average course grade is different from 80.\n\n\n\n\n\n\nNoteWhy compare on both sides?\n\n\n\nNote that we have computed whether the average course_grade is generally different from 80 for this Teacher. We could have computed whether it is greater, or lesser than 80 ( or any other number too). Read this article for why it is better to do a “two.sided” test in most cases.\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Calculate exact mean\nobs_mean_grade &lt;- mean(~course_grade, data = exam_grades)\nbelief &lt;- 80\nobs_grade_diff &lt;- belief - obs_mean_grade\n## Steps in a Permutation Test\n## Repeatedly Shuffle polarities of data observations\n## Take means\n## Compare all means with the real-world observed one\nnull_dist_grade &lt;-\n  mosaic::do(4999) *\n    mean(\n      ~ (course_grade - belief) *\n        sample(c(-1, 1), # +/- 1s multiply y\n          length(course_grade), # How many +/- 1s?\n          replace = T\n        ), # select with replacement\n      data = exam_grades\n    )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_dist_grade,\n  fill = ~ (mean &gt;= obs_grade_diff),\n  bins = 50,\n  title = \"Distribution of Permuted Difference-Means under Null Hypothesis\",\n  subtitle = \"Why is the mean of the means zero??\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = obs_grade_diff, colour = \"red\") %&gt;%\n  gf_vline(xintercept = -obs_grade_diff, colour = \"red\")\n\n\n\n\n\n\n# p-value\n# Permutation distributions are always centered around zero. Why?\nprop(~ mean &gt;= obs_grade_diff,\n  data = null_dist_grade\n) +\n  prop(~ mean &lt;= -obs_grade_diff,\n    data = null_dist_grade\n  )\n\nprop_TRUE \n        0 \n\n\nAnd let us now do the bootstrap test:\n\nnull_grade_bs &lt;- mosaic::do(4999) *\n  mean(\n    ~ sample(course_grade,\n      replace = T\n    ), # select with replacement\n    data = exam_grades\n  )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_grade_bs,\n  fill = ~ (mean &gt;= obs_grade_diff),\n  bins = 50,\n  title = \"Distribution of Bootstrap Means\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = ~belief, colour = \"red\")\n\n\n\n\n\n\nprop(~ mean &gt;= belief,\n  data = null_grade_bs\n) +\n  prop(~ mean &lt;= -belief,\n    data = null_grade_bs\n  )\n\nprop_TRUE \n        0 \n\n\nThe permutation test shows that we are not able to “generate” the believed mean-difference with any of the permutations. Likewise with the bootstrap, we are not able to hit the believed mean with any of the bootstrap samples.\nHence there is no reason to believe that the belief (80) might be a reasonable one and we reject our NULL Hypothesis that the mean is equal to 80.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#workflow-for-inference-for-a-single-mean",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#workflow-for-inference-for-a-single-mean",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Workflow for Inference for a Single Mean",
    "text": "Workflow for Inference for a Single Mean\nA series of tests deal with one mean value of a sample. The idea is to evaluate whether that mean is representative of the mean of the underlying population. Depending upon the nature of the (single) variable, the test that can be used are as follows:\n\n\n\n\n\nflowchart TD\n    A[Inference for Single Mean] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n or\\n Anderson-Darling Test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes\\n Parametric| D[t.test]\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[t.test\\n with\\n Signed-Ranks of Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap]\n    C --&gt;|No\\n Non-Parametric| Q[Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#wait-but-why",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can only sample from a population, and calculate sample statistics\nBut we still want to know about population parameters\n\nAll our tests and measures of uncertainty with samples are aimed at obtaining a confident measure of a population parameter.\nMeans are the first on the list!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#conclusion",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nIf samples are normally distributed, we use a t.test.\nElse we try non-parametric tests such as the Wilcoxon test.\nSince we now have compute power at our fingertips, we can leave off considerations of normality and simply proceed with either a permutation or a boostrap test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#sec-references",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n References",
    "text": "References\n\nOpenIntro Modern Statistics, Chapter #17\n\nBootstrap based Inference using the infer package: https://infer.netlify.app/articles/t_test\n\nMichael Clark & Seth Berry. Models Demystified: A Practical Guide from t-tests to Deep Learning. https://m-clark.github.io/book-of-models/\n\nUniversity of Warwickshire. SAMPLING: Searching for the Approximation Method use to Perform rational inference by Individuals and Groups. https://sampling.warwick.ac.uk/#Overview",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#additional-readings",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#additional-readings",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Additional Readings",
    "text": "Additional Readings\n\nhttps://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/posts/21-diagrams/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.5\nKrasser (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nKrasser, Roland. 2025. explore: Simplifies Exploratory Data Analysis. https://doi.org/10.32614/CRAN.package.explore.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#footnotes",
    "title": "🃏 Inference for a Single Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎\nhttps://stats.stackexchange.com/q/171748↩︎\nhttps://www.r-bloggers.com/2021/11/anderson-darling-test-in-r-quick-normality-check/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "",
    "text": "library(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#setting-up-r-packages",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "",
    "text": "library(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#introduction",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nMany experiments gather qualitative data across different segments of a population, for example, opinion about a topic among people who belong to different income groups, or who live in different parts of a city. This should remind us of the Likert Plots that we plotted earlier. In this case the two variables, dependent and independent, are both Qualitative, and we can calculate counts and proportions.\nHow does one Qual variable affect the other? How do counts/proportions of the dependent variable vary with the levels of the independent variable? This is our task for this module.\nHere is a quick example of the kind of data we might look at here, taken from the British Medical Journal:\n\n\n\n\n\nFigure 1: Breast Feeding\n\n\nClearly, we can see differences in counts/proportions of women who breast-fed their babies for three months or more, based on whether they were “printers wives” or “farmers’ wives”!\nIs there a doctor in the House?\n\n The CLT for Two Proportions\nWe first need to establish some model assumptions prior to making our analysis. As before, we wish to see if the CLT applies here, and if so, in what form. The difference between two proportions \\(\\hat{p_1}-\\hat{p_2}\\) can be modeled using a normal distribution when:\n\n\nIndependence (extended): The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.\n\nSuccess-failure condition: The success-failure condition holds for both groups, where we check successes and failures in each group separately. That is, we should have at least 10 successes and 10 failures in each of the two groups.\n\nWhen these conditions are satisfied, the standard error of \\(\\hat{p_1}-\\hat{p_2}\\) is well-approximated by:\n\\[\nSE(\\hat{p_1}-\\hat{p_2}) = \\sqrt{\\frac{\\hat{p_1}*(1-\\hat{p_1})}{n_1}} + \\sqrt{\\frac{\\hat{p_2}*(1-\\hat{p_2})}{n_2}}\n\\]\nwhere \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) represent the sample proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes.\nWe can represent the Confidence Intervals as:\n\\[\n\\begin{eqnarray}\nCI(p_1 - p_2) &=& (\\hat{p_1} - \\hat{p_2}) \\pm 1.96 * SE(\\hat{p_1}-\\hat{p_2})\\\\\n&=& (\\hat{p_1} - \\hat{p_2}) \\pm 1.96 * \\left(\\sqrt{\\frac{\\hat{p_1}*(1-\\hat{p_1})}{n_1}} + \\sqrt{\\frac{\\hat{p_2}*(1-\\hat{p_2})}{n_2}}\\right)\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#case-study-1-gss2002-dataset",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#case-study-1-gss2002-dataset",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Case Study-1: GSS2002 dataset",
    "text": "Case Study-1: GSS2002 dataset\nWe saw how we could perform inference for a single proportion. We can extend this idea to multiple proportions too.\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset from the resampledata package, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002, package = \"resampledata\")\nglimpse(GSS2002)\n\nRows: 2,765\nColumns: 21\n$ ID            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Region        &lt;fct&gt; South Central, South Central, South Central, South Centr…\n$ Gender        &lt;fct&gt; Female, Male, Female, Female, Male, Male, Female, Female…\n$ Race          &lt;fct&gt; White, White, White, White, White, White, White, White, …\n$ Education     &lt;fct&gt; HS, Bachelors, HS, Left HS, Left HS, HS, Bachelors, HS, …\n$ Marital       &lt;fct&gt; Divorced, Married, Separated, Divorced, Divorced, Divorc…\n$ Religion      &lt;fct&gt; Inter-nondenominational, Protestant, Protestant, Protest…\n$ Happy         &lt;fct&gt; Pretty happy, Pretty happy, NA, NA, NA, Pretty happy, NA…\n$ Income        &lt;fct&gt; 30000-34999, 75000-89999, 35000-39999, 50000-59999, 4000…\n$ PolParty      &lt;fct&gt; \"Strong Rep\", \"Not Str Rep\", \"Strong Rep\", \"Ind, Near De…\n$ Politics      &lt;fct&gt; Conservative, Conservative, NA, NA, NA, Conservative, NA…\n$ Marijuana     &lt;fct&gt; NA, Not legal, NA, NA, NA, NA, NA, NA, Legal, NA, NA, NA…\n$ DeathPenalty  &lt;fct&gt; Favor, Favor, NA, NA, NA, Favor, NA, NA, Favor, NA, NA, …\n$ OwnGun        &lt;fct&gt; No, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, N…\n$ GunLaw        &lt;fct&gt; Favor, Oppose, NA, NA, NA, Oppose, NA, NA, Oppose, NA, N…\n$ SpendMilitary &lt;fct&gt; Too little, About right, NA, About right, NA, Too little…\n$ SpendEduc     &lt;fct&gt; Too little, Too little, NA, Too little, NA, Too little, …\n$ SpendEnv      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ SpendSci      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ Pres00        &lt;fct&gt; Bush, Bush, Bush, NA, NA, Bush, Bush, Bush, Bush, NA, NA…\n$ Postlife      &lt;fct&gt; Yes, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, …\n\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\nskimr::skim(GSS2002)\n\n\nData summary\n\n\nName\nGSS2002\n\n\nNumber of rows\n2765\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n20\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nRegion\n0\n1.00\nFALSE\n7\nNor: 684, Sou: 486, Sou: 471, Mid: 435\n\n\nGender\n0\n1.00\nFALSE\n2\nFem: 1537, Mal: 1228\n\n\nRace\n0\n1.00\nFALSE\n3\nWhi: 2188, Bla: 410, Oth: 167\n\n\nEducation\n5\n1.00\nFALSE\n5\nHS: 1485, Bac: 443, Lef: 400, Gra: 230\n\n\nMarital\n0\n1.00\nFALSE\n5\nMar: 1269, Nev: 708, Div: 445, Wid: 247\n\n\nReligion\n19\n0.99\nFALSE\n13\nPro: 1460, Cat: 673, Non: 379, Chr: 65\n\n\nHappy\n1396\n0.50\nFALSE\n3\nPre: 784, Ver: 415, Not: 170\n\n\nIncome\n890\n0.68\nFALSE\n24\n400: 170, 300: 166, 250: 140, 500: 136\n\n\nPolParty\n36\n0.99\nFALSE\n8\nInd: 528, Not: 515, Not: 449, Str: 408\n\n\nPolitics\n1434\n0.48\nFALSE\n7\nMod: 522, Con: 210, Sli: 209, Sli: 159\n\n\nMarijuana\n1914\n0.31\nFALSE\n2\nNot: 545, Leg: 306\n\n\nDeathPenalty\n1457\n0.47\nFALSE\n2\nFav: 899, Opp: 409\n\n\nOwnGun\n1841\n0.33\nFALSE\n3\nNo: 605, Yes: 310, Ref: 9\n\n\nGunLaw\n1849\n0.33\nFALSE\n2\nFav: 737, Opp: 179\n\n\nSpendMilitary\n1441\n0.48\nFALSE\n3\nAbo: 615, Too: 414, Too: 295\n\n\nSpendEduc\n1422\n0.49\nFALSE\n3\nToo: 992, Abo: 278, Too: 73\n\n\nSpendEnv\n1443\n0.48\nFALSE\n3\nToo: 793, Abo: 439, Too: 90\n\n\nSpendSci\n1499\n0.46\nFALSE\n3\nAbo: 629, Too: 461, Too: 176\n\n\nPres00\n1016\n0.63\nFALSE\n5\nBus: 885, Gor: 781, Nad: 57, Oth: 16\n\n\nPostlife\n1554\n0.44\nFALSE\n2\nYes: 975, No: 236\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nID\n0\n1\n1383\n798.33\n1\n692\n1383\n2074\n2765\n▇▇▇▇▇\n\n\n\n\n\nNote how all variables are Categorical !! Education has five levels, and of course DeathPenalty has three:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty and set up a Contingency Table.\n\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\n##\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  addmargins()\ngss_table\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nContingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaicplot using several packages. Let us do a quick recap:\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\n\n\n\n\nmosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  vcd::mosaic(gp = shading_hsv)\n\n\n\n\n\n\n\n\n\n\n# library(ggmosaic)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nggplot(data = gss2002) +\n  geom_mosaic(aes(\n    x = product(DeathPenalty, Education),\n    fill = DeathPenalty\n  )) +\n  scale_fill_brewer(name = \"Death Penalty\", palette = \"Set1\") +\n  labs(title = \"Mosaic Plot of Death Penalty by Education\")\n\n\n\n\n\n\n\n\n\nAs seen before, it needs a little more work, to convert the Contingency Table into a tibble:\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n###\ngf_col(edu_prop ~ Education,\n  data = gss_summary,\n  width = ~edu_count,\n  fill = ~DeathPenalty,\n  stat = \"identity\",\n  position = \"fill\",\n  color = \"black\"\n) %&gt;%\n  gf_text(edu_prop ~ Education,\n    label = ~ scales::percent(edu_prop),\n    position = position_stack(vjust = 0.5)\n  ) %&gt;%\n  gf_facet_grid(~Education,\n    scales = \"free_x\",\n    space = \"free_x\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(\n    name = \"Death Penalty\",\n    palette = \"Set1\"\n  )) %&gt;%\n  gf_labs(\n    title = \"Mosaic Plot of Death Penalty by Education\",\n    x = \"Education Level\",\n    y = \"Proportion of Votes for Death Penalty\"\n  )",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#hypotheses-definition",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#hypotheses-definition",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Hypotheses Definition",
    "text": "Hypotheses Definition\nWhat would our Hypotheses be relating to the proportions of votes for or against the Death Penalty?\n\\(H_0: \\text{Education does not affect votes for Death Penalty}\\\\\\)\n\\(H_a: \\text{Education affects votes for Death Penalty}\\\\\\)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-two-proportions",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-two-proportions",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Inference for Two Proportions",
    "text": "Inference for Two Proportions\nWe are now ready to perform our statistical inference. We will use the standard Pearson chi-square test, and develop and intuition for it. We will then do a permutation test to have an alternative method to complete the same task.\n\n\nCode\nIntuitive Explanation\n\n\n\nLet us now perform the base chisq test: We need a contingency table and then the chisq test: We will calculate the observed-chi-squared value, and compare it with the critical value.\n\n# Chi-square test\nmosaic::xchisq.test(mosaic::tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  x\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n  117      511       71      135       64   \n(129.86) (488.51) ( 59.78) (141.54) ( 78.33)\n [1.27]   [1.04]   [2.11]   [0.30]   [2.62] \n&lt;-1.13&gt;  &lt; 1.02&gt;  &lt; 1.45&gt;  &lt;-0.55&gt;  &lt;-1.62&gt; \n         \n   72      200       16       71       50   \n( 59.14) (222.49) ( 27.22) ( 64.46) ( 35.67)\n [2.79]   [2.27]   [4.63]   [0.66]   [5.75] \n&lt; 1.67&gt;  &lt;-1.51&gt;  &lt;-2.15&gt;  &lt; 0.81&gt;  &lt; 2.40&gt; \n         \nkey:\n    observed\n    (expected)\n    [contribution to X-squared]\n    &lt;Pearson residual&gt;\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(mosaic::tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Determine the Chi-Square critical value\nX_squared_critical &lt;- qchisq(\n  p = .05,\n  df = (5 - 1) * (2 - 1), # (nrows-1) * (ncols-1)\n  lower.tail = FALSE\n)\nX_squared_critical\n\n[1] 9.487729\n\n\nWe see that our observed \\(X^2 = 23.45\\); the critical value X_squared_critical is \\(9.48\\), which is much smaller! The p-value is \\(0.0001029\\), very low as we would expect, indicating that the NULL Hypothesis should be rejected in favour of the alternate hypothesis, that opinions about the DeathPenalty are related to Education.\n\n\n\n\n\n\n\n\nLet us now dig into that cryptic-looking table above!\n\n\nLet us look at the Contingency Table that we have:\n\n\n\n\n\n    \n\n      \n\nDeathPenalty\n                Left HS\n                HS\n                Jr Col\n                Bachelors\n                Graduate\n                Sum\n              \n\n\nFavor\n                  117\n                  511\n                  71\n                  135\n                  64\n                  898\n                \n\nOppose\n                  72\n                  200\n                  16\n                  71\n                  50\n                  409\n                \n\nSum\n                  189\n                  711\n                  87\n                  206\n                  114\n                  1307\n                \n\n\n\n\n\n\nFigure 2: Contingency Table\n\n\n\n In the chi-square test, we check whether the two (or more) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect if there was no relationship between the variables?\nConsider the entry in location (1,1): 117. The number of expected entries there is the probability of an entry landing in that square times the total number of entries:\n\\[\n\\begin{align}\n\\text{Expected Value[1,1]}\n&= p_{row_1} * p_{col_1} * Total~Scores\\\\\\\n&= \\Large{\\frac{\\sum_{r_{1}}}{\\sum_{r_{all}c_{all}}} * \\frac{\\sum_{c_{1}}}{\\sum_{r_{all}c_{all}}} * \\sum_{r_{all}c_{all}}} \\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\\\n&= 130\n\\end{align}\n\\]\nProceeding in this way for all the 15 entries in the Contingency Table, we get the “Expected” Contingency Table. Here are both tables for comparison:\n\n\n\nExpected Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n130\n489\n60\n142\n78\n898\n\n\nOppose\n59\n222\n27\n64\n36\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\n\n\nActual Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\nAnd here are the mosaic plots for the actual and expected Contingency Tables, along with the association plot showing the differences, as we did when plotting Proportions:\n\n\n\n\n\nActual\n\n\n\n\n\nExpected\n\n\n\n\n\nTile-Wise Differences\n\n\n\n\n\nNow, the Pearson Residual in each cell is equivalent to the z-score of that cell. Recall the z-score idea: we subtract the mean and divide by the std. deviation to get the z-score.\nIn the Contingency Table, we have counts which are usually modeled as an (integer) Poisson distribution, for which mean (i.e Expected value) and variance are identical. Thus we get the Pearson Residual as:\n\\[\nr_{i,j} = \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\n\\]\nand therefore:\n\\[\nr_{i,j} = \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\]\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows.\n\\[\nχ2 = \\sum_{i=1}^R\\sum_{j=1}^C{r_{i,j}^2}\n\\]\nwhere R and C are number of rows and columns in the Contingency Table, the levels in the two Qual variables.\nFor location [1,1], its contribution to χ2 would be: \\((117-130)^2/130 = 1.3\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\nAll right, what of all this? How did this \\(X^2\\) distribution come from? Here is a lovely, brief explanation from this StackOverflow Post:\n\n\nIn a Contingency Table the Null Hypothesis states that the variables in the rows and the variable in the columns are independent.\n\nThe cell counts \\(E_{ij}\\) are assumed to be Poisson distributed with mean = \\(E_{ij}\\) and as they are Poisson, their variance is also \\(E_{ij}\\).\n\nAsymptotically the Poisson distribution approaches the normal distribution, with mean = \\(E_{ij}\\) and standard deviation with \\(\\sqrt{E_{ij}}\\) so, asymptotically \\(\\large{\\frac{(X_{ij} - E_{ij})}{\\sqrt{E_{ij}}}}\\) is approximately standard normal \\(N(0,1)\\).\n\nIf you square standard normal variables and sum these squares then the result is a chi-square random variable so \\(\\sum_{i,j}\\left(\\frac{(X_{ij}-E_{ij})}{\\sqrt{E_{ij}}}\\right)^2\\) has a (asymptotically) a chi-square distribution.\n\nAsymptotics must hold and that is why most textbooks state that the result of the test is valid when all expected cell counts \\(E_{ij}\\) are larger than 5, but that is just a rule of thumb that makes the approximation ‘’good enough’’.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#permutation-test-for-education",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#permutation-test-for-education",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Permutation Test for Education\n",
    "text": "Permutation Test for Education\n\nWe will now perform the permutation test for the difference between proportions. We will first get an intuitive idea of the permutation, and then perform it using both mosaic and infer.\n\n\n Permutation Visually Demonstrated\nCode\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. We will first look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators! Now we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n  \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nAs can be seen, the ratio is different!\nWe can now check out our Hypothesis that there is no bias. We can shuffle the data many many times, calculating the ratio each time, and plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with the originally observed figure from Mother Nature.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist &lt;- do(4999) * diff(mean(\n  candidate_selected ~ shuffle(evaluator),\n  data = data\n))\n# null_dist %&gt;% names()\nnull_dist %&gt;%\n  gf_histogram(~M,\n    fill = ~ (M &lt;= obs_difference),\n    bins = 25, show.legend = FALSE,\n    xlab = \"Bias Proportion\",\n    ylab = \"How Often?\",\n    title = \"Permutation Test on Difference between Groups\",\n    subtitle = \"\"\n  ) %&gt;%\n  gf_vline(xintercept = ~obs_difference, color = \"red\") %&gt;%\n  gf_label(500 ~ obs_difference,\n    label = \"Observed\\n Bias\",\n    show.legend = FALSE\n  )\nmean(~ M &lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n \n\n\n[1] 0.00220044\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.0022\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.\n\n\nWe should now repeat the test with permutations on Education:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nnull_chisq &lt;- do(4999) *\n  chisq.test(mosaic::tally(DeathPenalty ~ shuffle(Education),\n    data = gss2002\n  ))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(\n    xintercept = observedChi2,\n    color = \"red\"\n  ) %&gt;%\n  gf_refine(annotate(\"text\",\n    y = 500, x = observedChi2,\n    label = \"Observed\\n Chi-Square\"\n  )) %&gt;%\n  gf_labs(\n    title = \"Permutation Test on Chi-Square Statistic\",\n    x = \"Chi-Square Statistic\",\n    y = \"How Often?\"\n  )\n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n    2e-04 \n\n\nThe p-value is well below our threshold of \\(0.05\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-proportions-case-study-2-tbd-dataset",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-proportions-case-study-2-tbd-dataset",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Inference for Proportions Case Study-2: TBD dataset",
    "text": "Inference for Proportions Case Study-2: TBD dataset\nTo be Written Up. Yes, but when, Arvind?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#conclusion",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Conclusion",
    "text": "Conclusion\nIn our basic \\(X^2\\) test, we calculate the test statistic of \\(X^2\\) and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWhy would a permutation test be a good idea here? With a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( &gt;= 5) the resulting NULL distribution is of the \\(X^2\\) variety.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#wait-but-why",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#your-turn",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Your Turn",
    "text": "Your Turn",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#references",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n References",
    "text": "References\n\n\nOpenIntro Modern Statistics: Chapter 17\n\nChapter 8: The Chi-Square Test, from Statistics at Square One. The British Medical Journal. https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/8-chi-squared-tests. Very readable and easy to grasp. Especially if you like watching Grey’s Anatomy and House.\nExploring the underlying theory of the chi-square test through simulation - part 1 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence/\n\nExploring the underlying theory of the chi-square test through simulation - part 2 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence-part-2/\n\nAn Online \\(\\Xi^2\\)-test calculator. https://www.statology.org/chi-square-test-of-independence-calculator/\n\nhttps://saylordotorg.github.io/text_introductory-statistics/s13-04-comparison-of-two-population-p.html\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\nscales\n1.4.0\nWickham, Pedersen, and Seidel (2025)\n\n\nvcd\n1.4.13\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2024)\n\n\n\n\n\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2024. vcd: Visualizing Categorical Data. https://doi.org/10.32614/CRAN.package.vcd.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2025. scales: Scale Functions for Visualization. https://doi.org/10.32614/CRAN.package.scales.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html",
    "href": "content/courses/Analytics/Inference/listing.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "href": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "href": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "title": "Statistical Inference",
    "section": "An Idea to Encourage You: Stats Lessons from Sholay!!",
    "text": "An Idea to Encourage You: Stats Lessons from Sholay!!\n\nGabbar: “Kitne Aadmi thay?\nStats Teacher: How many observations do you have? n &lt; 30 is a joke.\n\nGabbar: Kya Samajh kar aaye thay? Gabbar khus hoga? Sabaasi dega kya?\nStats Teacher: What are the levels in your Factors? Are they binary? Don’t do ANOVA just yet!\n\nGabbar: (Fires off three rounds ) Haan, ab theek hai!\nStats Teacher: Yes, now the dataset is balanced wrt the factor (Treatment and Control).\n\nGabbar: Is pistol mein teen zindagi aur teen maut bandh hai. Dekhte hain kisko kya milega.\nStats Teacher: This is our Research Question, for which we will Design an Experiment.\n\nGabbar: (Twirls the chambers of his revolver) “Hume kuchh nahi pataa!”\nStats Teacher: Let us perform a non-parametric Permutation Test for this Factor!\n\nGabbar: “Kamaal ho gaya!”\nStats Teacher: Fantastic! Our p-value is so small that we can reject the NULL Hypothesis!!\n\nGo and like this post at: https://www.linkedin.com/pulse/stat-lessons-from-sholay-arvind-venkatadri-wgtrf/?trackingId=c0b4UCTLRea6U%2Bj%2Bm4TCtw%3D%3D",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#references",
    "href": "content/courses/Analytics/Inference/listing.html#references",
    "title": "Statistical Inference",
    "section": "References",
    "text": "References\n\nhttps://www.openintro.org/book/os/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#modules",
    "href": "content/courses/Analytics/Inference/listing.html#modules",
    "title": "Statistical Inference",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html",
    "title": "\n Facing the Abyss",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#a-data-analytics-process",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#a-data-analytics-process",
    "title": "\n Facing the Abyss",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#set-up-your-project",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#set-up-your-project",
    "title": "\n Facing the Abyss",
    "section": "\n Set up your Project",
    "text": "Set up your Project\n\nCreate a new Project in RStudio. File -&gt; New Project -&gt; Quarto Blog\nCreate a new Quarto document: all your Quarto documents should be in the posts/ folder. See the samples therein to get an idea.\nSave the document with a meaningful name, e.g. EDA-Workflow.qmd\n\nCreate a new folder in the Project for your data files, e.g. data/. This can be at the inside the posts/ folder.\nStore all datasets within this folder, and refer to them with relative paths, e.g. ../data/mydata.csv in any other Quarto document in the Project. (../ means “go up one level from the current folder”.)\n\nNow edit the `.qmd file which you are editing for this report to include the following sections, YAML, code chunks, and text as needed.\n\n\n\n\n\n\nNoteDownload this document as a Work Template\n\n\n\nHit the &lt;/&gt;Code button at upper right to copy/save this very document as a Quarto Markdown template for your work. Delete the text that you don’t need, but keep most of the Sections as they are!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#setting-up-r-packages",
    "title": "\n Facing the Abyss",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nInstall packages using install.packages() in your Console.\nLoad up your libraries in a setup chunk:\nAdd knitr options to your YAML header, so that all your plots are rendered in high quality PNG format.\n\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk:\n    dev: \"ragg_png\"\n    \n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n##\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(crosstable) # Summary stats tables\nlibrary(kableExtra)\n##\nlibrary(paletteer) # Colour Palettes for Peasants\n##\n## Add other packages here as needed, e.g.:\n## scales/ggprism;\n## ggstats/correlation;\n## vcd/vcdExtra/ggalluvial/ggpubr;\n## sf/tmap/osmplotr/rnaturalearth;\n## igraph/tidygraph/ggraph/graphlayouts;\n\n\n Themes and Fonts\nSet up a theme for your plots. This is a good time to set up your own theme, or use an existing one, e.g. ggprism, ggthemes, ggpubr, etc. If you have a Company logo, you can use that as a theme too.\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n\nUse Namespace based Code\n\n\n\n\n\n\nWarning\n\n\n\nTry always to name your code-command with the package from whence it came! So use dplyr::filter() / dplyr::summarize() and not just filter() or summarize(), since these commands could exist across multiple packages, which you may have loaded last.\n(One can also use the conflicted package to set this up, but this is simpler for beginners like us. )",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#read-data",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#read-data",
    "title": "\n Facing the Abyss",
    "section": "\n Read Data",
    "text": "Read Data\n\nUse readr::read_csv(); or data(...) if the data is in a package\n\n\ndata(penguins, package = \"palmerpenguins\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#examine-data",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#examine-data",
    "title": "\n Facing the Abyss",
    "section": "\n Examine Data",
    "text": "Examine Data\n\nUse dplyr::glimpse()\n\nUse mosaic::inspect() or skimr::skim()\n\nUse dplyr::summarise() and crosstable::crosstable()\n\nFormat your tables with knitr::kable()\n\nHighlight any interesting summary stats or data imbalances\n\n\ndplyr::glimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-dictionary-and-experiment-description",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-dictionary-and-experiment-description",
    "title": "\n Facing the Abyss",
    "section": "\n Data Dictionary and Experiment Description",
    "text": "Data Dictionary and Experiment Description\n\n\nData Dictionary: A table containing the variable names, their interpretation, and their nature(Qual/Quant/Ord…)\nIf there are wrongly coded variables in the original data, state them in their correct form, so you can munge the in the next step\nDeclare what might be target and predictor variables, based on available information of the experiment, or a description of the data.\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\nCategorical variables, e.g. species, island, sex\n\nUse dplyr::count() to get counts of each category\n\n\n\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\nContinuous variables, e.g. body_mass_g, flipper_length_mm, bill_length_mm\n\nUse dplyr::summarise() to get summary statistics of each variable",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-munging",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-munging",
    "title": "\n Facing the Abyss",
    "section": "\n Data Munging",
    "text": "Data Munging\n\nConvert variables to factors as needed\nReformat / Rename other variables as needed\nClean badly formatted columns (e.g. text + numbers) using tidyr::separate_**_**()\n\nSave the data as a modified file\nDo not mess up the original data file\n\n\n```{r}\n#| label: data-munging\n#| eval: false\n\ndataset_modified &lt;- data %&gt;%\n  dplyr::mutate(across(where(is.character), as.factor))\n# And so on\n```\n\nMunge the variables separately if you need to specify factor labels and levels for each variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#form-hypotheses",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#form-hypotheses",
    "title": "\n Facing the Abyss",
    "section": "\n Form Hypotheses",
    "text": "Form Hypotheses\nQuestion-1\n\nState the Question or Hypothesis\n(Temporarily) Drop variables using dplyr::select()\n\nCreate new variables if needed with dplyr::mutate()\n\nFilter the data set using dplyr::filter()\n\nReformat data if needed with tidyr::pivot_longer() or tidyr::pivot_wider()\n\nAnswer the Question with a Table, a Chart, a Test, using an appropriate Model for Statistical Inference\nUse title, subtitle, legend and scales appropriately in your chart\nPrefer ggformula unless you are using a chart that is not yet supported therein (eg. ggbump() or plot_likert())\n\n\n## Set graph theme\n## Idiotic that we have to repeat this every chunk\n## Open issue in Quarto\ntheme_set(new = theme_custom())\n\npenguins %&gt;%\n  tidyr::drop_na() %&gt;%\n  gf_point(body_mass_g ~ flipper_length_mm,\n    colour = ~species\n  ) %&gt;%\n  gf_labs(\n    title = \"My First Penguins Plot\",\n    subtitle = \"Using ggformula with fonts\",\n    x = \"Flipper Length mm\", y = \"Body Mass gms\",\n    caption = \"I love penguins, and R\"\n  )\n\n\n\n\n\n\n\nInference-1\n. . . .\nQuestion-n\n….\nInference-n\n….",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#one-most-interesting-graph",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#one-most-interesting-graph",
    "title": "\n Facing the Abyss",
    "section": "\n One Most Interesting Graph",
    "text": "One Most Interesting Graph",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#conclusion",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#conclusion",
    "title": "\n Facing the Abyss",
    "section": "\n Conclusion",
    "text": "Conclusion\nDescribe what the graph shows and why it so interesting. What could be done next?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#references",
    "title": "\n Facing the Abyss",
    "section": "\n References",
    "text": "References\n\nhttps://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/\nColour Palettes\n\nOver 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton? You could also find/define palettes that are in line with your Company’s logo / colour schemes.\n Here are the Qualitative Palettes: (searchable) \n\n\n\n\n\n\n And the Quantitative/Continuous palettes: (searchable) \n\n\n\n\n\n\n Use the commands:\n\n## For Qual variable-&gt; colour/fill:\nscale_colour_paletteer_d(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)\n\n## For Quant variable-&gt; colour/fill:\nscale_colour_paletteer_c(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "title": "Using FlexDashboard in R",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "title": "Using FlexDashboard in R",
    "section": "References",
    "text": "References\n\nFlexdashboard Basics https://rstudio.github.io/flexdashboard/articles/flexdashboard.html\nFlexdashboard Examples https://rstudio.github.io/flexdashboard/articles/examples.html\nShannon Haymond,Create laboratory business intelligence dashboards for free using R: A tutorial using the flexdashboard package, Journal of Mass Spectrometry and Advances in the Clinical Lab, Volume 23, 2022,Pages 39-43, ISSN 2667-145X, https://doi.org/10.1016/j.jmsacl.2021.12.002.\nhttps://posit.co/blog/flexdashboard-easy-interactive-dashboards-for-r/"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "title": "📐 Intro to Linear Programming",
    "section": "",
    "text": "library(blogdown)\nlibrary(gMOIP)\n# See: https://relund.github.io/gMOIP/index.html\nlibrary(knitr)\nlibrary(rgl)\nrgl::setupKnitr()\noptions(rgl.useNULL = TRUE)\nopts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  # cache = TRUE, autodep = TRUE,\n  comment = \"#&gt;\",\n  fig.show = \"asis\",\n  warning = FALSE, message = FALSE, include = TRUE,\n  out.width = \"99%\", fig.width = 8, fig.align = \"center\", fig.asp = 0.62\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "title": "📐 Intro to Linear Programming",
    "section": "Introduction",
    "text": "Introduction\nWhat is Linear Programming?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "title": "📐 Intro to Linear Programming",
    "section": "Demonstration of Level Curve",
    "text": "Demonstration of Level Curve",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Solver",
    "text": "Linear Programming Solver",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming in 3D view",
    "text": "Linear Programming in 3D view",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Interactive",
    "text": "Linear Programming Interactive\nLet us say we have a Linear Programming problem with 3 variables: We define the model:\n\\[\nMaximise : 20x_1 + 10x_2 + 15x_3\\\\\nSubject \\ to \\\\\n\\\\\nx_1 + x_2 + x_3 &lt;= 10\\\\\n3x_1 + x_3 &lt;= 24\n\\]\nHere is the interactive LP Polytope:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "title": "📐 Intro to Linear Programming",
    "section": "References",
    "text": "References\n\nVirginia Postrel, Operations Everything, Boston Globe, Hune 27, 2004. http://archive.boston.com/news/globe/ideas/articles/2004/06/27/operation_everything?pg=full",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "",
    "text": "knitr::opts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  # cache = TRUE, autodep = TRUE,\n  comment = \"#\",\n  fig.show = \"asis\",\n  warning = FALSE, message = FALSE, fig.align = \"center\",\n  scipen = 1, digits = 2\n)\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gMOIP)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "What is the Simplex Method?",
    "text": "What is the Simplex Method?\nTo be written up.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "A Random Walk with the Simplex Method",
    "text": "A Random Walk with the Simplex Method\nLet us try to form a geometric intuition for the Simplex method.\nWe will define an LP problem, and geometrically traverse the steps the Simplex method might take to solve for the optimum solution.\nLet us define a problem:\n\\[\nMaximise\\ 7.75x_1 + 10x_2\\\\\n\\] \\[\nSubject\\ to\\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3\\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27\\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90\\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]\nThe Objective function is: \\(7.75x_1 + 10x_2\\)\nThe Constraints are defined by the three inequalities \\(C1::C3\\). In order to plot these, we convert the inequalities to equalities and plot these as lines. Each line splits the \\(x_1:x_2\\) plane into two half-planes. The inequality part is then taken into account by choosing the appropriate half-plane created by the equation. The intersection of all the half-planes defined by the constraints is the Feasibility Region.\nThe Feasibility region for this LP problem is plotted below:\n\n\n\n\n\n\n\n\nThe corner points of the Feasibility Region are:\n\n\n\n  \n\n\n\nRecall that:\n\nThe optimum in an LP problem is found on the boundary, at one of the vertices\nAt each of these vertices one or more constraints (\\(C1::C_n\\)) is tight, i.e. there is no slack.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Procedure",
    "text": "Procedure\n\nWe start with an arbitrary point on the edge of the Feasibility Region. \\(A = (0,0)\\) is a common choice. At this point, since all variables are \\(0\\), the objective function is also \\(0\\).\nWe (arbitrarily) decide to move along the boundary of the Feasibility Region, to another FSP. We arbitrarily chose the \\(x_1\\) axis, and set/keep \\(x_2 = 0\\). We now wish to find out the \\(x_1\\) coordinate of the next FSP point. This would be at the intersection of the \\(x_1\\) axis and one of the Constraint lines.\nAll the three Constraint Lines would possibly intersect the \\(x_1\\) axis. We need to choose that intercept point that has the smallest, non-negative \\(x_1\\) intercept value. (Why?)\nSo, which Constraint Line intersects the \\(x_1\\) axis at the smallest value? Is it point B, or point F?\nTo find out, we substitute \\(x_2 = 0\\) in each of the Constraint equations, and solve for the \\(x_1\\):\\[\n\\begin{cases}\nC1: -3x_1 + 2 \\times 0 = 3 \\ =&gt; x_1 = \\color{red}{-1}\\\\\nC2: 2x_1 + 4\\times0 = 27 \\ =&gt; x_1 = 13.5\\ Point\\ F\\\\\n   {\\mathbf{ \\color{lightgreen}{C3}: 9x_1 + 10\\times0 = 90 \\ =&gt; x_1 = 10\\  \\color{lightgreen}{Point\\ B}}}\n\\end{cases}\n\\]\nNegative values for any variable are not permitted. So the smallest value of intercept is \\(x_1 = 10\\) for Constraint \\(C3\\). We therefore move to point \\(B(10,0)\\). At this point the objective function has improved to:\n\n\\[\nObjective = 7.75\\times 10 + 10\\times0 = 77.5\\ at\\ Point\\ B\n\\]\n\nWe now start from Point B, and move to the next nearest point. In identical fashion to Step2, we “imagine” that we move along a new axis defined by:\\[\nIntercept = Point\\ B(10,0)\\\\\n\\] \\[\nEquation = Constraint\\ C3: 9x_1 + 10x_2 = 90\\\\\n\\] We express \\(x_1\\) in terms of \\(x_2\\) with \\(C3\\): \\[\n\\hat C3: x_1 = \\frac{90 - 10x_2}{9}\n\\] As in Step 2, we substitute this equation \\(\\hat C3\\) into the other two constraints, \\(C1\\) and \\(C2\\): \\[\n\\begin{cases}\nC1: -3\\times \\frac{90 - 10x_2}{9} + 2x_2 = 3 \\ =&gt; x_2 = 6.18\\ Point\\ K\\\\\n{\\mathbf{ \\color{lightgreen}{C2}: 2\\times \\frac{90 - 10x_2}{9}+ 4x_2 = 27 =&gt; x_2 = 3.93\\ \\color{lightgreen}{Point\\ C}}}\n\\end{cases}\n\\] As before we choose the smaller of the two intercepts, so \\(x_2 = 3.93\\). Calculating for \\(x_1\\), we get point \\(C(5.63, 3.93)\\). At this point the objective function has improved to:\n\n\\[\n7.75\\times 5.63 + 10\\times 3.93 = 82.9\\ at\\ Point\\ C\n\\]\n\nWe now proceed along the constraint line \\(C2\\) towards the next point. In identical fashion to Step 2 and 3, we “imagine” that we move along a new axis defined by: \\[\nIntercept = Point\\ C(5.63,3.93)\n\\] \\[\nEquation = Constraint\\ C2: 2x_1 + 4x_2 = 27 \\\\\n\\] Again, We express \\(x_1\\) in terms of \\(x_2\\) with \\(C2\\) this time: \\[\n\\hat C2: x_1 = \\frac{27 - 4x_2}{2}\n\\] As in Step 2 and, we substitute this equation \\(\\hat C2\\) into the other constraint, the only remaining \\(C1\\): \\[\n{\\mathbf C1: -3\\times \\frac{27 - 4x_2}{2} + 2x_2 = 3 \\ =&gt; x_2 = 5.44\\ Point\\ D\\\\}\n\\] Calculating for \\(x_1\\), we get point \\(C(2.63, 5.44)\\). At this point the objective function has improved decreased to: \\[\n7.75\\times 2.63 + 10\\times 5.44 = 74.8\\ at\\ Point\\ D\n\\] Since this value for the Objective function is smaller than that at the previous point, our search terminates and we decide that Point \\(C(5.63,3.93)\\) is the optimal point.\nSo the final result is: \\[\n   x_1(max) = 5.63\\\\\n\\] \\[\n   x_2(max) = 3.93\\\\\n\\] \\[\n   Maximum\\ Objective\\ Function\\ Value = 82.9\n\\] The final result is plotted below:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Summary",
    "text": "Summary\nThe essence of this “intuitive method” can be captured as follows:\n\nStart from a known simple point on the edge of Feasibility Region, e.g. (0,0), since the two coordinate axes frequently form two edges to the Feasibility Region.\n\nMove along one of the axis to find a first adjacent edge point. This adjacent point corresponds to the “tightening” of one or other of the Constraint equations(i.e. slack = 0 for that Constraint)\n\nCalculate the Objective function at that point.\n\nUse this new point as the next starting point and move along the Constraint line from the previous step.\n\nRepeat step 2 and 3, calculating the Objective function each time.\n\nKeep the solution point where the objective function hits a maximum, i.e. when moving to the next point reduces the value of the Objective function.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text\n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }\n\nmy_theme &lt;- function() { # Creating a function\n  theme_classic() + # Using pre-defined theme as base\n    theme(\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(), # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\", # Customizing legend position\n      plot.caption = element_text(size = 8)\n    ) # Customizing plot caption\n}\n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text\n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }\n\nmy_theme &lt;- function() { # Creating a function\n  theme_classic() + # Using pre-defined theme as base\n    theme(\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(), # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\", # Customizing legend position\n      plot.caption = element_text(size = 8)\n    ) # Customizing plot caption\n}\n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min       Q1   median       Q3      max     mean\n1    tract integer   1.00000 1303.250 3393.500 3739.750 5082.000 2700.356\n2      lon numeric -71.28950  -71.093  -71.053  -71.020  -70.810  -71.056\n3      lat numeric  42.03000   42.181   42.218   42.252   42.381   42.216\n4     medv numeric   5.00000   17.025   21.200   25.000   50.000   22.533\n5    cmedv numeric   5.00000   17.025   21.200   25.000   50.000   22.529\n6     crim numeric   0.00632    0.082    0.257    3.677   88.976    3.614\n7       zn numeric   0.00000    0.000    0.000   12.500  100.000   11.364\n8    indus numeric   0.46000    5.190    9.690   18.100   27.740   11.137\n9      nox numeric   0.38500    0.449    0.538    0.624    0.871    0.555\n10      rm numeric   3.56100    5.886    6.208    6.623    8.780    6.285\n11     age numeric   2.90000   45.025   77.500   94.075  100.000   68.575\n12     dis numeric   1.12960    2.100    3.207    5.188   12.127    3.795\n13     rad integer   1.00000    4.000    5.000   24.000   24.000    9.549\n14     tax integer 187.00000  279.000  330.000  666.000  711.000  408.237\n15 ptratio numeric  12.60000   17.400   19.050   20.200   22.000   18.456\n16       b numeric   0.32000  375.377  391.440  396.225  396.900  356.674\n17   lstat numeric   1.73000    6.950   11.360   16.955   37.970   12.653\n          sd   n missing\n1  1380.0368 506       0\n2     0.0754 506       0\n3     0.0618 506       0\n4     9.1971 506       0\n5     9.1822 506       0\n6     8.6015 506       0\n7    23.3225 506       0\n8     6.8604 506       0\n9     0.1159 506       0\n10    0.7026 506       0\n11   28.1489 506       0\n12    2.1057 506       0\n13    8.7073 506       0\n14  168.5371 506       0\n15    2.1649 506       0\n16   91.2949 506       0\n17    7.1411 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Correlations",
    "text": "Workflow: Correlations\nWe can use purrr to evaluate all pair-wise correlations in one shot:\n\nall_corrs &lt;- housing %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off cmedv/medv to get all the remaining ones\n  select(-cmedv, -medv) %&gt;%\n  # perform a cor.test for all variables against cmedv\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, housing$cmedv)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with Median House Price\") %&gt;%\n  gf_theme(my_theme()) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3)),\n    guides(linewidth = guide_legend(reverse = TRUE))\n  )\n\n\n\n\n\n\n\nThe variables rm, lstat seem to have high correlations with cmedv which are also statistically significant."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Maximal Multiple Regression Model",
    "text": "Workflow: Maximal Multiple Regression Model\nWe will create a regression model for cmedv using all the other numerical predictor features in the dataset.\n\nhousing_numeric &lt;- housing %&gt;% select(\n  where(is.numeric),\n\n  # remove medv\n  # an older version of cmedv\n  -c(medv)\n)\nnames(housing_numeric) # 16 variables, one target, 15 predictors\n\n [1] \"tract\"   \"lon\"     \"lat\"     \"cmedv\"   \"crim\"    \"zn\"      \"indus\"  \n [8] \"nox\"     \"rm\"      \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\"\n[15] \"b\"       \"lstat\"  \n\nhousing_maximal &lt;- lm(cmedv ~ ., data = housing_numeric)\nsummary(housing_maximal)\n\n\nCall:\nlm(formula = cmedv ~ ., data = housing_numeric)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.934  -2.752  -0.619   1.711  26.120 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.45e+02   3.23e+02   -1.07  0.28734    \ntract       -7.52e-04   4.46e-04   -1.69  0.09231 .  \nlon         -6.79e+00   3.44e+00   -1.98  0.04870 *  \nlat         -2.35e+00   5.36e+00   -0.44  0.66074    \ncrim        -1.09e-01   3.28e-02   -3.32  0.00097 ***\nzn           4.40e-02   1.39e-02    3.17  0.00164 ** \nindus        2.75e-02   6.20e-02    0.44  0.65692    \nnox         -1.55e+01   4.03e+00   -3.85  0.00014 ***\nrm           3.81e+00   4.20e-01    9.07  &lt; 2e-16 ***\nage          5.82e-03   1.34e-02    0.43  0.66416    \ndis         -1.38e+00   2.10e-01   -6.59  1.1e-10 ***\nrad          2.36e-01   8.47e-02    2.78  0.00558 ** \ntax         -1.48e-02   3.74e-03   -3.96  8.5e-05 ***\nptratio     -9.49e-01   1.41e-01   -6.73  4.7e-11 ***\nb            9.55e-03   2.67e-03    3.57  0.00039 ***\nlstat       -5.46e-01   5.06e-02  -10.80  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.73 on 490 degrees of freedom\nMultiple R-squared:  0.743, Adjusted R-squared:  0.735 \nF-statistic: 94.3 on 15 and 490 DF,  p-value: &lt;2e-16\n\n\nThe maximal model has an R.squared of \\(0.7426\\) which is much better than that we obtained for a simple model based on rm alone. How much can we simplify this maximal model, without losing out on R.squared?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Model Reduction",
    "text": "Workflow: Model Reduction\nWe now proceed naively by removing one predictor after another. We will resort to what may amount to p-hacking by sorting the predictors based on their p-value1 in the maximal model and removing them in decreasing order of their p-value.\nWe will also use some powerful features from the purrr package (also part of the tidyverse), to create all these models all at once. Then we will be able to plot their R.squared values together and decide where we wish to trade off Explainability vs Complexity for our model.\n\n# No of Quant predictor variables in the dataset\nn_vars &lt;- housing %&gt;%\n  select(where(is.numeric), -c(cmedv, medv)) %&gt;%\n  length()\n\n# Maximal Model, now tidied\nhousing_maximal_tidy &lt;-\n  housing_maximal %&gt;%\n  broom::tidy() %&gt;%\n  # Obviously remove \"Intercept\" ;-D\n  filter(term != \"(Intercept)\") %&gt;%\n  # And horrors! Sort variables by p.value\n  arrange(p.value)\n\nhousing_maximal_tidy\n\n\n  \n\n\n\nThe last 5 variables are clearly statistically insignificant.\nAnd now we unleash the purrr package to create all the simplified models at once. We will construct a dataset containing three columns:\n\nA list of all quantitative predictor variables\nA sequence of numbers from 1 to N(predictor)\n\nA “list” column containing the housing data frame itself\n\nWe will use the iteration capability of purrr to sequentially drop one variable at a time from the maximal(15 predictor) model, build a new reduced model each time, and compute the r.squared:\nhousing_model_set &lt;- tibble(\n  all_vars =\n    list(housing_maximal_tidy$term), # p-hacked order!!\n  keep_vars = seq(1, n_vars),\n  data = list(housing_numeric)\n)\nhousing_model_set\n# Unleash purrr in a series of mutates\nhousing_model_set &lt;- housing_model_set %&gt;%\n  # list of predictor variables for each model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[1:keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(\n    formula =\n      map(\n        .x = mod_vars,\n        .f = \\(mod_vars) as.formula(paste(\n          \"cmedv ~\", paste(mod_vars, collapse = \"+\")\n        ))\n      )\n  ) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n# Check everything after the operation\nhousing_model_set\n# Tidy up the models using broom to expose their metrics\nhousing_models_tidy &lt;- housing_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      )\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_models_tidy %&gt;%\n  gf_line(\n    r.squared ~ keep_vars,\n    ylab = \"R.Squared\",\n    xlab = \"No. params in the Linear Model\",\n    title = \"Model Explainability vs Complexity\",\n    subtitle = \"Model r.squared vs No. of Predictors\",\n    data = .\n  ) %&gt;%\n  # Plot r.squared vs predictor count\n  gf_point(r.squared ~ keep_vars,\n    size = 3.5, color = \"grey\"\n  ) %&gt;%\n  # Show off the selected best model\n  gf_point(\n    r.squared ~ keep_vars,\n    size = 3.5,\n    color = \"red\",\n    data = housing_models_tidy %&gt;% filter(keep_vars == 4)\n  ) %&gt;%\n  gf_hline(yintercept = 0.7, linetype = \"dashed\") %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAt the loss of some 5% in the r.squared, we can drop our model complexity from 15 predictors to say 4! Our final model will then be:\n\nhousing_model_final &lt;-\n  housing_model_set %&gt;%\n  # filter for best model, with 4 variables\n  filter(keep_vars == 4) %&gt;%\n  # tidy up the model\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::tidy(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      )\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, models, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_model_final\n\n\n  \n\n\nhousing_model_final %&gt;%\n  # And plot the model\n  # Remove the intercept term\n  filter(term != \"(Intercept)\") %&gt;%\n  gf_col(estimate ~ term, fill = ~term, width = 0.25) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_errorbar(conf.low + conf.high ~ term,\n    width = 0.1,\n    title = \"Multiple Regression\",\n    subtitle = \"Model Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n\n\nOur current best model can be stated as:\n\\[\n\\widehat{cmedv} \\sim 24.459 - 0.563 * dis - 0.673 * lstat - 0.957 * ptratio  + 4.199 * rm\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Diagnostics",
    "text": "Workflow: Diagnostics\nLet us use broom::augment to calculate residuals and predictions to arrive at a quick set of diagnostic plots.\n\nhousing_model_final_augment &lt;-\n  housing_model_set %&gt;%\n  filter(keep_vars == 4) %&gt;%\n  # augment the model\n  mutate(\n    augment_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::augment(x)\n      )\n  ) %&gt;%\n  unnest(augment_models) %&gt;%\n  select(cmedv:last_col())\n\nhousing_model_final_augment\n\n\n  \n\n\n\nhousing_model_final_augment %&gt;%\n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_qq(~.std.resid, title = \"Q-Q Residuals\") %&gt;%\n  gf_qqline() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_point(sqrt(.std.resid) ~ .fitted,\n    title = \"Scale-Location Plot\"\n  ) %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_point(.std.resid ~ .hat,\n    title = \"Residuals vs Leverage\"\n  ) %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals plot shows a curved trend, and certainly does not resemble the stars at night, so it is possible that we have left out some possible richness in our model-making, a “systemic inadequacy”.\nThe Q-Q plot of residuals also shows a J-shape which indicates a non-normal distribution of residuals.\nThese could indicate that more complex model ( e.g. linear model with interactions between variables ( i.e. product terms ) may be necessary."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have used a multiple-regression workflow that takes all predictor variables into account in a linear model, and then systematically simplified that model such that the performance was just adequate.\nThe models we chose were all linear of course, but without interaction terms : each predictor was used only for its main effect. When the diagnostic plots were examined, we did see some shortcomings in the model. This could be overcome with a more complex model. These might include selected interactions, transformations of target(\\(cmedv^2\\), or \\(sqrt(cmedv)\\)) and some selected predictors."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n References",
    "text": "References\n\nJames, Witten, Hastie, Tibshirani, An Introduction to Statistical Learning. Chapter 3. Linear Regression. https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n1.0.0\nIannone et al. (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. gt: Easily Create Presentation-Ready Display Tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\nJames, Witten, Hastie, Tibshirani,An Introduction to Statistical Learning. Chapter 3. Linear Regression https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow Plan",
    "text": "Workflow Plan\nOur target variable is Salary.\nWe will start with an examination of correlations between Salary and other Quant predictors.\nWe will use a null model for our Linear Regression at first, keeping just an intercept term. Based on the examination of the r-square improvement offered by each predictor individually, we will add another quantitative predictor. We will follow this process through up to a point where the gains in model accuracy are good enough to justify the additional model complexity.\n\n\n\n\n\n\nNote\n\n\n\nThis approach is the exact opposite of the earlier tutorial on multiple linear regression, where we started with a maximal model and trimmed it down based on an assessment of r.squared."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nThe Hitters dataset has the following variables:\n\ndata(\"Hitters\")\ninspect(Hitters)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1    League factor      2 322       0\n2  Division factor      2 322       0\n3 NewLeague factor      2 322       0\n                                   distribution\n1 A (54.3%), N (45.7%)                         \n2 W (51.2%), E (48.8%)                         \n3 A (54.7%), N (45.3%)                         \n\nquantitative variables:  \n      name   class  min    Q1 median     Q3   max    mean      sd   n missing\n1    AtBat integer 16.0 255.2  379.5  512.0   687  380.93  153.40 322       0\n2     Hits integer  1.0  64.0   96.0  137.0   238  101.02   46.45 322       0\n3    HmRun integer  0.0   4.0    8.0   16.0    40   10.77    8.71 322       0\n4     Runs integer  0.0  30.2   48.0   69.0   130   50.91   26.02 322       0\n5      RBI integer  0.0  28.0   44.0   64.8   121   48.03   26.17 322       0\n6    Walks integer  0.0  22.0   35.0   53.0   105   38.74   21.64 322       0\n7    Years integer  1.0   4.0    6.0   11.0    24    7.44    4.93 322       0\n8   CAtBat integer 19.0 816.8 1928.0 3924.2 14053 2648.68 2324.21 322       0\n9    CHits integer  4.0 209.0  508.0 1059.2  4256  717.57  654.47 322       0\n10  CHmRun integer  0.0  14.0   37.5   90.0   548   69.49   86.27 322       0\n11   CRuns integer  1.0 100.2  247.0  526.2  2165  358.80  334.11 322       0\n12    CRBI integer  0.0  88.8  220.5  426.2  1659  330.12  333.22 322       0\n13  CWalks integer  0.0  67.2  170.5  339.2  1566  260.24  267.06 322       0\n14 PutOuts integer  0.0 109.2  212.0  325.0  1378  288.94  280.70 322       0\n15 Assists integer  0.0   7.0   39.5  166.0   492  106.91  136.85 322       0\n16  Errors integer  0.0   3.0    6.0   11.0    32    8.04    6.37 322       0\n17  Salary numeric 67.5 190.0  425.0  750.0  2460  535.93  451.12 263      59\n\n\n\n Scatter Plots and Correlations\nWe should examine scatter plots and Correlations of Salary against these variables. Let us select a few sets of Quantitative and Qualitative features, along with the target variable Salary and do a pairs-plots with them:\nHitters %&gt;%\n  select(Salary, AtBat, Hits, HmRun) %&gt;%\n  GGally::ggpairs(title = \"Plot 1\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, Runs, RBI, Walks, Years) %&gt;%\n  GGally::ggpairs(title = \"Plot 2\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, CRBI, CAtBat, CHits, CHmRun, CRuns, CWalks) %&gt;%\n  GGally::ggpairs(title = \"Plot 3\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, PutOuts, Assists, Errors) %&gt;%\n  GGally::ggpairs(title = \"Plot 4\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, League, Division, NewLeague) %&gt;%\n  GGally::ggpairs(title = \"Plot 5\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtBat and Hits seem relevant predictors for Salary. So are Runs, RBI,Walks, and Years. From Plot 2, both RBI and Walks are also inter-correlated with Runs. All the C* variables are well correlated with Salary and also among one another. (Plot3). Plot 4 has no significant correlations at all. Plot 5 shows Salary nearly equally distributed across League, Division, and NewLeague.\n\n Correlation Error-Bars\nWe can also plot all correlations in one graph using cor.test and purrr:\nall_corrs &lt;-\n  Hitters %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off Salary and year to get all the remaining ones\n  select(-Salary) %&gt;%\n  # perform a cor.test for all variables against Salary\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Hitters$Salary)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") %&gt;%\n  arrange(desc(estimate))\n\nall_corrs\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    linewidth = 2,\n    color = \"grey\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    linewidth = ~ -log10(p.value),\n    width = 0.5,\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = NULL, y = \"Correlation with Salary\") %&gt;%\n  # gf_theme(theme = my_theme()) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3))\n  ) %&gt;%\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    theme(axis.text.x = element_text(hjust = 1))\n  ) %&gt;%\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    coord_flip()\n  )\n\n\n\n\n  \n\n\n\n\n\n\n\nThere are a good many predictors which have statistically significant correlations with Salary, such as CRuns , CHmRun. The darker the colour, the higher is the correlation score; the fatter the bar, the higher is the significance of the correlation.\nWe now start with setting up simple Linear Regressions with no predictors, only an intercept. We then fit separate Linear Models using each predictor individually. Then based on the the improvement in r.squared offered by each predictor, we progressively add it to the model, until we are “satisfied” with the quality of the model ( using rsquared and other means).\nLet us now do this."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Minimal Multiple Regression Model",
    "text": "Workflow: Minimal Multiple Regression Model\nNote the formula structure here: we want just and intercept.\n\nlm_min &lt;- lm(data = Hitters, Salary ~ 1)\nsummary(lm_min)\n\n\nCall:\nlm(formula = Salary ~ 1, data = Hitters)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -468   -346   -111    214   1924 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    535.9       27.8    19.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 451 on 262 degrees of freedom\n  (59 observations deleted due to missingness)\n\n\nlm_min %&gt;% broom::tidy()\nlm_min %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOK, so the intercept is highly significant, the t-statistic is also high, but the intercept contributes nothing to the r.squared!! It is of no use at all!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round#1)",
    "text": "Workflow: Predictor Addition (Round#1)\nWe will now set up individual models for each predictor and look at the p.value and r.squared offered by each separate model:\nnames &lt;- names(Hitters %&gt;%\n  select(\n    where(is.numeric),\n    -c(Salary)\n  ))\n\nn_vars &lt;- length(names)\n\nHitters_model_set &lt;- tibble(\n  all_vars = list(names),\n  keep_vars = seq(1, n_vars),\n  data = list(Hitters)\n)\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  # Select Single Predictor for each Simple Model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(formula = map(\n    .x = mod_vars,\n    .f = \\(mod_vars) as.formula(paste(\n      \"Salary ~\", paste(mod_vars, collapse = \"+\")\n    ))\n  )) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;-\n  Hitters_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      ),\n    predictor_name = names[keep_vars]\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;%\n  arrange(desc(r.squared))\n\n# Check everything after the operation\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;%\n  gf_point(r.squared ~ reorder(predictor_name, r.squared),\n    size = 3.5,\n    color = \"black\",\n    ylab = \"R.Squared\",\n    xlab = \"Params in the Linear Model\", data = .\n  ) %&gt;%\n  # gf_theme(my_theme()) %&gt;%\n  gf_refine(theme(axis.text.x = element_text(\n    angle = 30,\n    hjust = 1\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Which is the winning Predictor?\nwinner &lt;- Hitters_model_set %&gt;%\n  arrange(desc(r.squared)) %&gt;%\n  select(predictor_name) %&gt;%\n  head(1) %&gt;%\n  as.character()\nwinner\n\n[1] \"CRBI\"\n\n\n# Here is the Round 1 Model\n# Minimal model updated to included winning predictor\nlm_round1 &lt;- update(lm_min, ~ . + CRBI)\nlm_round1 %&gt;% broom::tidy()\nlm_round1 %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we can add CRBI as a predictor to our model as a predictor gives us an improved r.squared of \\(0.321\\), which is the square of the correlation between Salary and CRBI, \\(.567\\).\nAnd the model itself is: \\[\nSalary \\sim 274.580 + 0.791 \\times CRBI\n\\tag{1}\\]\nLet’s press on to Round 2."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round #2)",
    "text": "Workflow: Predictor Addition (Round #2)\nWe will set up a round-2 model using CRBI as the predictor, and then proceed to add each of the other predictors as an update to the model.\n# Preliminaries\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), -c(Salary, winner)))\n# names\n\nn_vars &lt;- length(names)\n# n_vars\n# names &lt;- names %&gt;% str_remove(winner)\n# names\n# n_vars &lt;- n_vars-1\n\n\n# Round 2 Iteration\nHitters_model_set &lt;- tibble(\n  all_vars = list(names),\n  keep_vars = seq(1, n_vars),\n  data = list(Hitters)\n)\n# Hitters_model_set\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  # list of predictor variables for each model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(formula = map(\n    .x = mod_vars,\n    .f = \\(mod_vars) as.formula(paste(\n      \"Salary ~ CRBI +\", paste(mod_vars, collapse = \"+\")\n    ))\n  )) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n# Check everything after the operation\n# Hitters_model_set\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;-\n  Hitters_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      ),\n    predictor_name = names[keep_vars]\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;%\n  arrange(desc(r.squared))\n\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;%\n  gf_point(r.squared ~ reorder(predictor_name, r.squared),\n    size = 3.5,\n    ylab = \"R.Squared\",\n    xlab = \"Param in the Linear Model\"\n  ) %&gt;%\n  # gf_theme(my_theme()) %&gt;%\n  gf_refine(theme(axis.text.x = element_text(\n    angle = 30,\n    hjust = 1\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n# Which is the winning Predictor?\n#\nwinner &lt;- Hitters_model_set %&gt;%\n  arrange(desc(r.squared)) %&gt;%\n  select(predictor_name) %&gt;%\n  head(1) %&gt;%\n  as.character()\nwinner\n# Here is the Round 1 Model\nlm_round2 &lt;- update(lm_round1, ~ . + Hits)\nlm_round2 %&gt;% broom::tidy()\nlm_round2 %&gt;% broom::glance()\n\n\n\n[1] \"Hits\"\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nAnd now the model itself is: \\[\nSalary \\sim -47.96 + 0.691 \\times CRBI + 3.30 \\times Hits\n\\tag{2}\\]\nNote the change in both intercept and the slope for CRBI when the new predictor Hits is added!!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Visualization",
    "text": "Workflow: Visualization\nLet us quickly see how this model might look. We know that with simple regression, we obtain a straight line as our model. Here, with two (or more) predictors, we should obtain a ….(hyper)plane! Play with the interactive plot below!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Discussion",
    "text": "Discussion\nIt is interesting that the second variable to be added was Hits which has a lower correlation of \\(r = 0.439\\) with Salary compared to some other Quant predictors such as Chits( \\(r = 0.525\\) ). This is because CRBI is hugely correlated with all of these predictors, so CRBI effectively acts as a proxy for all of these. See Plot 3.\nWe see that adding Hits to the model gives us an improved r.squared of \\(0.425\\)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can proceed in this way to subsequent rounds and decide to stop when the model complexity (no. of predictors ) and the resulting gain in r.squared does not seem worth it.\n\n\n\n\n\n\nNoteAutomatic Iteration Method\n\n\n\nWe ought to convert the above code into an R function and run it that way for a specific number of rounds to see how things pan out. That is in the next version of this Tutorial! It appears that there is, what else, an R Package, called reghelper that allows us to do this! 😇 The reghelper::build_model() function can be used to:\n\nStart with only an intercept\n\nSequentially add each of the other predictor variables into the model “blocks”\n\nBlocks will be added in the order they are passed to the function, and variables from previous blocks will be included with each subsequent block, so they do not need to be repeated.\n\n\nType help(rehelper) in your Console.\nlibrary(reghelper)\n\nbig_model &lt;- build_model(\n  dv = Salary,\n  # Start with only an intercept lm(Salary ~ 1, data = .)\n\n  # Sequentially add each of the other predictor variables\n  # Pass through variable names (or interaction terms) to add for each block.\n  # To add one term to a block, just pass it through directly;\n  # to add multiple terms at a time to a block, pass it through in a vector or list.\n  # Interaction Terms can be specified using the vector/list\n  # Blocks will be added in the order they are passed to the function\n  # Variables from previous blocks will be included with each subsequent block,  so they do not need to be repeated.\n\n  1, AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits,\n  CHmRun, CRuns, CRBI, CWalks, PutOuts, Assists, Errors,\n  data = Hitters,\n  model = \"lm\"\n)\n\n\n\nThis multiple model is a list object with 4 items. Type summary(big_model) in your Console.\nWe can clean it up a wee bit:\n\nlibrary(gt)\n# big_model has 4 parts: formulas, residuals, coefficients, overall\n\noverall_clean &lt;- summary(big_model)$overall %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names()\n\nformulas_clean &lt;- summary(big_model)$formulas %&gt;%\n  as.character() %&gt;%\n  as_tibble() %&gt;%\n  rename(\"model_formula\" = value)\n\nall_models &lt;- cbind(formulas_clean, overall_clean) %&gt;%\n  dplyr::select(1, 2, 8)\nall_models %&gt;%\n  gt::gt() %&gt;%\n  tab_style(\n    style = cell_fill(color = \"grey\"),\n    locations = cells_body(rows = seq(1, 18, 2))\n  )\n\n\n\n\n\nmodel_formula\nr_squared\ndelta_r_sq\n\n\n\nSalary ~ 1\nNA\nNA\n\n\nSalary ~ 1 + AtBat\n0.156\nNA\n\n\nSalary ~ 1 + AtBat + Hits\n0.204\n0.047748\n\n\nSalary ~ 1 + AtBat + Hits + HmRun\n0.227\n0.023530\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs\n0.227\n0.000137\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI\n0.244\n0.016807\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks\n0.307\n0.062553\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years\n0.409\n0.101919\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat\n0.455\n0.046369\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits\n0.471\n0.016234\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun\n0.488\n0.016771\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns\n0.488\n0.000333\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI\n0.489\n0.001157\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks\n0.498\n0.008575\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts\n0.522\n0.023739\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists\n0.527\n0.005401\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists + Errors\n0.528\n0.000814\n\n\n\n\n\n\nSo we have a list of all models with main effects only. We could play with the build_model function to develop interaction models too! Slightly weird that the NULL model of Salary~1 does not show an r.squared value with build_model…??"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n References",
    "text": "References\n\nhttps://ethanwicker.com/2021-01-11-multiple-linear-regression-002/\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n1.0.0\nIannone et al. (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. gt: Easily Create Presentation-Ready Display Tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-linreg",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-linreg",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#setting-up-r-packages",
    "title": "Modelling with Linear Regression",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(ggstatsplot)\nlibrary(tidyverse)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#introduction",
    "title": "Modelling with Linear Regression",
    "section": "\n Introduction",
    "text": "Introduction\nOne of the most common problems in Prediction Analytics is that of predicting a Quantitative response variable, based on one or more Quantitative predictor variables or features. This is called Linear Regression. We will use the intuitions built up during our study of ANOVA to develop our ideas about Linear Regression.\nSuppose we have data on salaries in a Company, with years of study and previous experience. Would we be able to predict the prospective salary of a new candidate, based on their years of study and experience? Or based on the mileage done, could we predict the resale price of a used car? These are typical problems in Linear Regression.\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#the-linear-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#the-linear-regression-model",
    "title": "Modelling with Linear Regression",
    "section": "\n The Linear Regression Model",
    "text": "The Linear Regression Model\nThe premise here is that many common statistical tests are special cases of the linear model.\nA linear model estimates the relationship between one continuous or ordinal variable (dependent variable or “response”) and one or more other variables (explanatory variable or “predictors”). It is assumed that the relationship is linear:1\n\\[\n\\Large{y_i \\sim \\beta_1*x_i + \\beta_0\\\\}\n\\tag{1}\\]\nor\n\\[\n\\Large{y_1 \\sim exp(\\beta_1)*x_i + \\beta_0}\n\\tag{2}\\]\nbut not:\n\\[\n\\color{red}{y_i \\sim \\beta_1*exp(\\beta_2*x_i) + \\beta_0\\\\}\n\\]\nor\n\\[\n\\color{red}{y_i \\sim \\beta_1 *x^{\\beta_2} + \\beta_0}\n\\]\nIn Equation 1, \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x. Each prediction leaves a small “residual” error between the actual and predicted values. \\(\\beta_0\\) and \\(\\beta_1\\) are calculated based on minimizing the sum of squares of these residuals, and hence this method is called “ordinary least squares” (OLS) regression.\n\n\n\n\n\nFigure 1: Least Squares\n\n\nThe net area of all the shaded squares is minimized in the calculation of \\(\\beta_0\\) and \\(\\beta_1\\). As per Lindoloev, many statistical tests, going from one-sample t-tests to two-way ANOVA, are special cases of this system. Also see Jeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-models-as-hypothesis-tests",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Models as Hypothesis Tests",
    "text": "Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as “there is no relationship” between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows:\n\n\n\n\n\n\nNoteModelling Process\n\n\n\nWith \\(y = \\beta_0 + \\beta_1 *x\\)\n\nMake the following hypotheses:\n\n\\[\n    NULL\\ Hypothesis\\ H_0 =&gt; x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\]\n\\[\n    Alternate\\ Hypothesis\\ H_1 =&gt; x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe “assume” that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p(\\(\\beta_1 = Estimated\\ Value\\) when the NULL Hypothesis is assumed TRUE). This is the p-value. If that probability is p&gt;=0.05, we say we “cannot reject” \\(H_0\\) and there is unlikely to be significant linear relationship.\nHowever, if p&lt;= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship, because the probability p that \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-assumptions-in-linear-models",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-assumptions-in-linear-models",
    "title": "Modelling with Linear Regression",
    "section": "\n Assumptions in Linear Models",
    "text": "Assumptions in Linear Models\nWhen does a Linear Model work? We can write the assumptions in Linear Regression Models as an acronym, LINE:\n1. L: \\(\\color{blue}{linear}\\) relationship between variables 2. I: Errors are independent (across observations)\n3. N: \\(y\\) is \\(\\color{red}{normally}\\) distributed at each “level” of \\(x\\).\n4. E: \\(y\\) has the same variance at all levels of \\(x\\). No heteroscedasticity.\n\n\n\n\n\nFigure 2: OLS Assumptions\n\n\nHence a very concise way of expressing the Linear Model is:\n\\[\n\\Large{y \\sim N(x_i^T * \\beta, ~~\\sigma^2)}\n\\]\n\n\n\n\n\n\nImportantGeneral Linear Models\n\n\n\nThe target variable \\(y\\) is modelled as a normally distribute variable whose mean depends upon a linear combination of predictor variables \\(x\\), and whose variance is \\(\\sigma^2\\).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-model-workflow",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-model-workflow",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Model Workflow",
    "text": "Linear Model Workflow\nOK, on with the computation!\n\n Workflow: Read the Data\nLet us now read in the data and check for these assumptions as part of our Workflow.\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax.\n\n Workflow: EDA\nIn order to fit the linear model, we need to choose predictor variables that have strong correlations with the target variable. We will first do this with GGally, and then with the tidyverse itself. Both give us a very unique view into the correlations that exist within this dataset.\n\n\n Workflow: Correlations with GGally\n Correlations using cor.test and purrr\n\n\n\nLet us select a few sets of Quantitative and Qualitative features, along with the target variable cmedv and do a pairs-plots with them:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access\n  select(cmedv, rm, age, dis) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 1\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n##\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000\n  select(cmedv, rad, zn, indus, tax) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 2\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n##\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population\n  select(cmedv, crim, nox, rad, b, lstat) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 3\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n\nSee the top row of the pairs plots. Clearly, rm (avg. number of rooms) is a big determining feature for median price cmedv. This we infer based on the large correlation of rm withcmedv, \\(0.696\\). The variableage (proportion of owner-occupied units built prior to 1940) may also be a significant influence on cmedv, with a correlation of \\(-0.378\\).\nNone of the Quant variables rad, zn, indus, tax have a overly strong correlation with cmedv. .\nThe variable lstat (proportion of lower classes in the neighbourhood) as expected, has a strong (negative) correlation with cmedv; rad(index of accessibility to radial highways), nox(nitrous oxide) and crim(crime rate) also have fairly large correlations with cmedv, as seen from the pairs plots.\n\n\n\n\n\n\nImportantCorrelation Scores and Uncertainty\n\n\n\nRecall that cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally too reports the significance of the correlation scores using stars, *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nLet us plot (again) scatter plots of Quant Variables that have strong correlation with cmedv:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_point(\n  data = housing,\n  cmedv ~ age,\n  title = \"Price vs Proportion of houses older than 1940\",\n  ylab = \"Median Price\",\n  xlab = \"Proportion of older-than-1940 buildings\")\n\n##\ngf_point(\n  data = housing,\n  cmedv ~ lstat,\n  title = \"Price vs Proportion of lower classes...\"\n  subtitle = \"...In the neighbourhood\",\n  ylab = \"Median Price\",\n  xlab = \"proportion of lower classes in the neighbourhood\")\n##\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"(cmedv) Median Price\",\n  xlab = \"(rm) Avg. No. of Rooms\")\n\n\n\nError in parse(text = input): &lt;text&gt;:16:3: unexpected symbol\n15:   title = \"Price vs Proportion of lower classes...\"\n16:   subtitle\n      ^\n\n\n\nSo, rm does have a positive effect on cmedv, and age may have a (mild?) negative effect on cmedv; lstat seems to have a pronounced negative effet on cmedv. We have now managed to get a decent idea which Quant predictor variables might be useful in modelling cmedv: rm, lstat for starters, then perhapsage.\nLet us also check the Qualitative predictor variables: Access to the Charles river (chas) does seem to affect the prices somewhat.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictor Access to Charles River\n  select(cmedv, chas) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 4\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n\nLook at the bar plot above. While not too many properties can be near the Charles River (for obvious reasons) the box plots do seem to show some dependency of cmedv on chas.\n\n\n\n\n\n\nNote\n\n\n\nQualitative predictors for a Quantitative target can be included in the model using what is called dummy variables, where each level of the Qualitative variable is given a one-hot kind of encoding. See for example https://www.statology.org/dummy-variables-regression/\n\n\n\n\nThis is somewhat advanced material: We will use the purrr package to develop all correlations with respect to our target variable in one shot and also plot these correlation test scores in an error-bar plot. See Tidy Modelling with R. This has the advantage of being able to depict all correlations in one plot. (We will use this approach again here when we trim our linear models down from the maximal one to a workable one of lesser complexity.). Let us do this.\nWe develop a list object containing all correlation test results with respect to cmedv, tidy these up using broom::tidy, and then plot these:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nall_corrs &lt;- housing %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off target variable cmedv and IDs\n  # get all the remaining ones\n  select(-cmedv, -medv) %&gt;%\n  purrr::map(\n    .x = ., # All numeric variables selected in the previous step\n    .f = \\(.x) cor.test(.x, housing$cmedv)\n  ) %&gt;% # Apply the cor.test with `cmedv`\n\n  # Tidy up the cor.test outputs into neat columns\n  # Need \".id\" column to keep track of predictor variable name\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2,\n    title = \"Correlations: Target Variable vs All Predictors\",\n    subtitle = \"Boston Housing Dataset\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  # Plot points(smallest geom) last!\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with cmedv\") %&gt;%\n  # gf_theme(theme_minimal()) %&gt;%\n\n  # tilt the x-axis labels for readability\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  # Colour and linewidth scales + legends\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\",\n      range = c(0.25, 3),\n\n      # guide_legend(reverse = TRUE): Fat Lines mean higher significance\n    )\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\n\n\n\nWe can clearly see that rm and lstat have strong correlations with cmedv and should make good choices for setting up a minimal linear regression model. (medv is the older errored version of cmedv)\n\n\n\n\n Model Building\nWe will first execute the lm test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\n Model Code\n Forecasting with the Linear Model\n Linear Model Intuitive\n Linear Models Manually Demonstrated (Apologies to Spinoza)\n Using Other Packages\n\n\n\nR offers a very simple command lm to execute an Linear Model: Note the familiar formula of stating the variables: ( \\(y \\sim x\\); where \\(y\\) = target, \\(x\\) = predictor)\n\nhousing_lm &lt;- lm(cmedv ~ rm, data = housing)\nsummary(housing_lm)\n\n\nCall:\nlm(formula = cmedv ~ rm, data = housing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.336  -2.425   0.093   2.918  39.434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -34.6592     2.6421  -13.12   &lt;2e-16 ***\nrm            9.0997     0.4178   21.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.597 on 504 degrees of freedom\nMultiple R-squared:  0.4848,    Adjusted R-squared:  0.4838 \nF-statistic: 474.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe model for \\(\\widehat{cmedv}\\) , the prediction for cmedvcan be written in the form of \\(y = mx + c\\), as:\n\\[\n\\widehat{cmedv} \\sim -34.65924 + 9.09967* rm\n\\tag{3}\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nThe effect size of rm on predicting cmedv a (slope) value of \\(9.09967\\) which is significant at p-value of \\(&lt;2.2e-16\\); for every one room increase in rm, we have a \\(USD~90997\\) increase in median price cmedv.\nThe F-statistic for the Linear Model is given by \\(F = 474.3\\), which is very high. (We will use the F-statistic again when we do Multiple Regression.)\nThe R-squared value is \\(R^2 = 0.48\\) which means that rm is able to explain about half of the trend in cmedv; there is substantial variation in cmedv that is still left to explain, an indication that we should perhaps use a richer model, with more predictors. These aspects are explored in the Tutorials.\n\n\n\nWe can plot the scatter plot of these two variables with the model also over-plotted.\n\n#| layout-ncol: 3\n#| fig-width: 5\n#| fig-height: 4\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Tidy Data frame for the model using `broom`\nhousing_lm_tidy &lt;-\n  housing_lm %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\nhousing_lm_tidy\n\n\n  \n\n\n##\nhousing_lm_augment &lt;-\n  housing_lm %&gt;%\n  broom::augment(\n    se_fit = TRUE,\n    interval = \"confidence\"\n  )\nhousing_lm_augment\n\n\n  \n\n\n##\nintercept &lt;-\n  housing_lm_tidy %&gt;%\n  filter(term == \"(Intercept)\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n##\nslope &lt;-\n  housing_lm_tidy %&gt;%\n  filter(term == \"rm\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n##\nhousing %&gt;%\n  drop_na() %&gt;%\n  gf_point(\n    cmedv ~ rm,\n    title = \"Price vs Average no. of Rooms\",\n    ylab = \"Median Price\",\n    xlab = \"Avg. No. of Rooms\",\n    alpha = 0.2\n  ) %&gt;%\n  # Plot the model equation\n  gf_abline(\n    slope = slope, intercept = intercept,\n    colour = \"lightcoral\",\n    linewidth = 2\n  ) %&gt;%\n  # Plot the model prediction points on the line\n  gf_smooth(\n    method = \"lm\", geom = \"point\",\n    color = \"grey30\",\n    size = 0.5\n  ) %&gt;%\n  gf_refine(\n    annotate(\n      geom = \"segment\",\n      y = 0, yend = 29, x = 7, xend = 7, # manually calculated\n      linetype = \"dashed\",\n      color = \"dodgerblue\",\n      arrow = arrow(\n        angle = 30,\n        length = unit(0.25, \"inches\"),\n        ends = \"last\",\n        type = \"closed\"\n      )\n    ),\n    annotate(\n      geom = \"segment\",\n      y = 29, yend = 29, x = 2.5, xend = 7, # manually calculated\n      linetype = \"dashed\",\n      arrow = arrow(\n        angle = 30,\n        length = unit(0.25, \"inches\"),\n        ends = \"first\",\n        type = \"closed\"\n      ),\n      color = \"dodgerblue\"\n    )\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(\n      limits = c(2.5, 10),\n      expand = c(0, 0)\n    ),\n    # removes plot panel margins\n    scale_y_continuous(\n      limits = c(0, 55),\n      expand = c(0, 0)\n    )\n  ) %&gt;%\n  gf_theme(theme = theme_custom())\n\n\n\n\n\n\n\nFor any new value of rm, we go up to the vertical blue line and read off the predicted median price by following the horizontal blue line. That is how the model is used (by hand).\n\n\nIn practice, we use the broom package functions (tidy, glance and augment) to obtain a clear view of the model parameters and predictions of cmedv for all existing values of rm. We see estimates for the intercept and slope (rm) for the linear model, along with the standard errors and p.values for these estimated parameters. And we see the fitted values of cmedv for the existing rm; these values will naturally lie on the straight-line depicting the model. We will examine this augment-ed data more the section on Diagnostics.\nTo predict cmedv with new values of rm, we use predict. Let us now try to make predictions with some new data:\n\nnew &lt;- tibble(rm = seq(3, 10)) # must be named \"rm\"\nnew %&gt;% mutate(\n  predictions =\n    stats::predict(\n      object = housing_lm,\n      newdata = .,\n      se.fit = FALSE\n    )\n)\n\n\n  \n\n\n\nNote that “negative values” for predicted cmedv would have no meaning!\n\n\nAll that is very well, but what is happening under the hood of the lm command? Consider the cmedv (target) variable and the rm feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(cmedv ~ rm, housing)\n\nFind a line that, in some way, gives us some prediction of cmedv for any given rm\n\nCalculate the errors in prediction and use those to find the “best” line.\nUse that “best” line henceforth as a model for prediction.\n\nHow does one fit the “best” line? Consider a choice of “lines” that we can use to fit to the data. Here are 6 lines of varying slopes (and intercepts ) that we can try as candidates for the best fit line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be apparent that while we cannot determine which line may be the best, the worst line seems to be the one in the final plot, which ignores the x-variable rm altogether. This corresponds to the NULL Hypothesis, that there is no relationship between the two variables. Any of the other lines could be a decent candidate, so how do we decide?\n\n\n\n\n\n\n\n\n\n\nIn Fig A, the horizontal blue line is the overall mean of cmedv, denoted as \\(\\mu_{tot}\\). The vertical green lines to the points show the departures of each point from this overall mean, called residuals. The sum of squares of these residuals in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{4}\\]\nIn Fig B, the vertical red lines are the residuals of each point from the potential line of fit. The sum of the squares of these lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - a - b * rm)^2]\n\\tag{5}\\]\nIt should be apparent that if there is any positive linear relationship between cmedv and rm,then \\(SSE &lt; SST\\).\nHow do we get the optimum slope + intercept? If we plot the \\(SSE\\) as a function of varying slope, we get:\n\n#| echo: false\nsim_model &lt;- tibble(\n  b = slope + seq(-5, 5),\n  a = intercept,\n  dat = list(tibble(\n    cmedv = housing_sample$cmedv,\n    rm = housing_sample$rm\n  ))\n) %&gt;%\n  mutate(r_squared = pmap_dbl(\n    .l = list(a, b, dat),\n    .f = \\(a, b, dat) sum((dat$cmedv - (b * dat$rm + a))^2)\n  ))\nmin_r_squared &lt;- sim_model %&gt;%\n  select(r_squared) %&gt;%\n  min()\nmin_slope &lt;- sim_model %&gt;%\n  filter(r_squared == min_r_squared) %&gt;%\n  select(b) %&gt;%\n  as.numeric()\nsim_model %&gt;%\n  gf_point(r_squared ~ b, data = ., size = 2) %&gt;%\n  gf_line(ylab = \"SSE\", xlab = \"slope\", title = \"Error vs Slope\") %&gt;%\n  gf_hline(yintercept = min_r_squared, color = \"red\") %&gt;%\n  gf_segment(min_r_squared + 0 ~ min_slope + min_slope,\n    colour = \"red\",\n    arrow = arrow(ends = \"last\", length = unit(1, \"mm\"))\n  ) %&gt;%\n  gf_refine(\n    coord_cartesian(expand = FALSE),\n    expand_limits(y = c(0, 20000), x = c(3.5, 15))\n  )\n\n\n\n\n\n\n\nWe see that there is a quadratic minimum \\(SSE\\) at the optimum value of slope and at all other slopes, the \\(SSE\\) is higher. We can use this to find the optimum slope, which is what the function lm does.\n\n\nLet us hand-calculate the numbers so we know what the test is doing. Here is the SST: we pretend that there is no relationship between cmedv ans rm and compute a NULL model:\n\n# Calculate overall sum squares SST\n\nSST &lt;- deviance(lm(cmedv ~ 1, data = housing))\nSST\n\n[1] 42577.74\n\n\nAnd here is the SSE:\n\nSSE &lt;- deviance(housing_lm)\nSSE\n\n[1] 21934.39\n\n\nGiven that the model leaves unexplained variations in cmedv to the extent of \\(SSE\\), we can compute the \\(SSR\\), the Regression Sum of Squares, the amount of variation in cmedv that the linear model does explain:\n\nSSR &lt;- SST - SSE\nSSR\n\n[1] 20643.35\n\n\nWe have \\(SST = 42577.74\\), \\(SSE = 21934.39\\) and therefore \\(SSR = 20643.35\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSR / df_{SSR}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSR}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSR and SSE.\nLet us calculate these Degrees of Freedom. If we have \\(n=\\) 506 observations of data, then:\n\n\n\\(SST\\) clearly has degree of freedom \\(n-1 = 505\\), since it uses all observations but loses one degree to calculate the global mean.\n\n\\(SSE\\) was computed using the slope and intercept, so it has \\((n-2) = 504\\) as degrees of freedom.\nAnd therefore \\(SSR\\) being their difference has just \\(1\\) degree of freedom.\n\nNow we are ready to compute the F-statistic:\n\nn &lt;- housing %&gt;%\n  count() %&gt;%\n  as.numeric()\ndf_SSR &lt;- 1\ndf_SSE &lt;- n - 2\nF_stat &lt;- (SSR / df_SSR) / (SSE / df_SSE)\nF_stat\n\n[1] 474.3349\n\n\nThe F-stat is compared with a critical value of the F-statistic, which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to 0.95, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;- qf(\n  p = 0.95, # Significance level is 5%\n  df1 = df_SSR, # Numerator degrees of freedom\n  df2 = df_SSE\n) # Denominator degrees of freedom\nF_crit\n\n[1] 3.859975\n\nF_stat\n\n[1] 474.3349\n\n\nThe F_crit value can also be seen in a plot2:\n\nmosaic::pdist(\n  dist = \"f\",\n  q = F_crit,\n  df1 = df_SSR, df2 = df_SSE\n)\n\n\n\n\n\n\n\n[1] 0.95\n\n\nAny value of F more than the \\(F_{crit}\\) occurs with smaller probability than 0.05. Our F_stat is much higher than \\(F_{crit}\\), by orders of magnitude! And so we can say with confidence that rm has a significant effect on cmedv.\nThe value of R.squared is also calculated from the previously computed sums of squares:\n\\[\nR.squared = \\frac{SSR}{SST} = \\frac{SSY-SSE}{SST}\n\\tag{6}\\]\n\nr_squared &lt;- (SST - SSE) / SST\nr_squared\n\n[1] 0.484839\n\n# Also computable by\n# mosaic::rsquared(housing_lm)\n\nSo R.squared = 0.484839\nThe value of Slope and Intercept are computed using a maximum likelihood derivation and the knowledge that the means square error is a minimum at the optimum slope: for a linear model \\(y \\sim mx + c\\)\n\\[\nslope = \\frac{\\Sigma[(y - y_{mean})*(x - x_{mean})]}{\\Sigma(x - x_{mean})^2}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that the slope is equal to the ratio of the covariance of x and y to the variance of x.\n\n\nand\n\\[\nIntercept = y_{mean} - slope * x_{mean}\n\\]\n\nslope &lt;- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~rm, data = housing)\nslope\n\n[1] 9.09967\n\n##\nintercept &lt;- mosaic::mean(~cmedv, data = housing) - slope * mosaic::mean(~rm, data = housing)\nintercept\n\n[1] -34.65924\n\n\nSo, there we are! All of this is done for us by one simple formula, lm()!\n\n\nThere is a very neat package called ggstatsplot3 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\nlibrary(ggstatsplot)\nhousing_lm %&gt;%\n  ggstatsplot::ggcoefstats(\n    title = \"Linear Model for Boston Housing\",\n    subtitle = \"Using ggstatsplot\"\n  )\n\n\n\n\n\n\n\nThis chart shows the estimates for the intercept and rm along with their error bars, the t-statistic, degrees of freedom, and the p-value.\nWe can also obtain crisp-looking model tables from the new supernova package 4, which is based on the methods discussed in Judd et al.\nlibrary(supernova)\nsupernova::supernova(housing_lm)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: cmedv ~ rm\n\n                                SS  df        MS       F   PRE     p\n ----- --------------- | --------- --- --------- ------- ----- -----\n Model (error reduced) | 20643.347   1 20643.347 474.335 .4848 .0000\n Error (from model)    | 21934.392 504    43.521                    \n ----- --------------- | --------- --- --------- ------- ----- -----\n Total (empty model)   | 42577.739 505    84.312                    \n\n\n\nThis table is very neat in that it gives the Sums of Squares for both the NULL (empty) model, and the current model for comparison. The PRE entry is the Proportional Reduction in Error, a measure that is identical with r.squared, which shows how much the model reduces the error compared to the NULL model(48%). The PRE idea is nicely discussed in Judd et al Section 10.\n\n\n\n\n Workflow: Model Checking and Diagnostics\nWe will follow much of the treatment on Linear Model diagnostics, given here on the STHDA website.\n\nA first step of this regression diagnostic is to inspect the significance of the regression beta coefficients, as well as, the R.square that tells us how well the linear regression model fits to the data.\nFor example, the linear regression model makes the assumption that the relationship between the predictors (x) and the outcome variable is linear. This might not be true. The relationship could be polynomial or logarithmic.\nAdditionally, the data might contain some influential observations, such as outliers (or extreme values), that can affect the result of the regression.\nTherefore, the regression model must be closely diagnosed in order to detect potential problems and to check whether the assumptions made by the linear regression model are met or not. To do so, we generally examine the distribution of residuals errors, that can tell us more about our data.\n\n\n Workflow: Checks for Uncertainty\nLet us first look at the uncertainties in the estimates of slope and intercept. These are most easily read off from the broom::tidy-ed model:\n\n# housing_lm_tidy &lt;-  housing_lm %&gt;% broom::tidy()\nhousing_lm_tidy\n\n\n  \n\n\n\nPlotting this is simple too:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nhousing_lm_tidy %&gt;%\n  gf_col(estimate ~ term, fill = ~term, width = 0.25) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_errorbar(conf.low + conf.high ~ term,\n    width = 0.1,\n    title = \"Model Bar Plot for Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_theme(theme = theme_custom())\n##\nhousing_lm_tidy %&gt;%\n  gf_pointrange(estimate + conf.low + conf.high ~ term,\n    title = \"Model Point-Range Plot for Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_theme(theme = theme_custom())\n\n\n\n\n\n\n\n\n\n\nThe point-range plot helps to avoid what has been called “within-the-bar bias”. The estimate is just a value, which we might plot as a bar or as a point, with uncertainty error-bars.\nValues within the bar are not more likely!! This is the bias that the point-range plot avoids.\n\n Checks for Constant Variance/Heteroscedasticity\nLinear Modelling makes 4 fundamental assumptions:(“LINE”)\n\n\nLinear relationship between y and x\nObservations are independent.\nResiduals are normally distributed\nVariance of the y variable is equal at all values of x.\n\nWe can check these using checks and graphs: Here we plot the residuals against the independent/feature variable and see if there is a gross variation in their range\nhousing_lm_augment %&gt;%\n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth(method = \"loess\")\nhousing_lm_augment %&gt;%\n  gf_hline(yintercept = 0, colour = \"grey\", linewidth = 2) %&gt;%\n  gf_point(.resid ~ cmedv, title = \"Residuals vs Target Variable\")\nhousing_lm_augment %&gt;%\n  gf_dhistogram(~.resid, title = \"Histogram of Residuals\") %&gt;%\n  gf_fitdistr()\nhousing_lm_augment %&gt;%\n  gf_qq(~.resid, title = \"Q-Q Residuals\") %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Q-Q plot of residuals also has significant deviations from the normal quartiles. The residuals are not quite “like the night sky”, i.e. random enough. These point to the need for a richer model, with more predictors. The “trend line” of residuals vs predictors show a U-shaped pattern, indicating significant nonlinearity: there is a curved relationship in the graph. The solution can be a nonlinear transformation of the predictor variables, such as \\(\\sqrt(X)\\), \\(log(X)\\), or even \\(X^2\\). For instance, we might try a model for cmedv using \\(rm^2\\) instead of just rm as we have done. This will still be a linear model!\n\n\n\n\n\n\nTip\n\n\n\nBase R has a crisp command to plot these diagnostic graphs. But we will continue to use ggformula.\nplot(housing_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the ggplot extension packages named lindia also has a crisp command to plot these diagnostic graphs.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nlibrary(lindia)\ngg_diagnose(housing_lm,\n  mode = \"base_r\", # plots like those with base-r\n  theme = theme(\n    axis.title = element_text(size = 6, face = \"bold\"),\n    title = element_text(size = 8)\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nThe r-squared for a model lm(cmedv ~ rm^2) shows some improvement:\n\n\n[1] 0.5501221",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#extras",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#extras",
    "title": "Modelling with Linear Regression",
    "section": "Extras",
    "text": "Extras\n\n\n\n\n\n\nNoteMultiple Regression\n\n\n\nIt is also possible that there is more than one explanatory variable: this is multiple regression.\n\\[\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n\\tag{7}\\]\nwhere each of the \\(\\beta_i\\) are slopes defining the relationship between y and \\(x_i\\). Note that this is a vector dot-product, or inner-product, taken with a vector of input variables \\(x_i\\) and a vector of weights, \\(\\beta_i\\). Together, the RHS of that equation defines an n-dimensional hyperplane. The model is linear in the parameters \\(\\beta_i\\), e.g. these are OK:\n\\[\n\\color{black}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1 + \\pmb\\beta_2x_1^2 + \\epsilon_i\\\\\n& y_1 = \\pmb\\beta_0 + \\pmb\\gamma_1\\pmb\\delta_1x_1 + exp(\\pmb\\beta_2)x_2+ \\epsilon_i\\\\\n\\end{cases}\n}\n\\]\nbut not, for example, these:\n\\[\n\\color{red}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1^{\\beta_2} + \\epsilon_i\\\\\n& y_i = \\pmb\\beta_0 + exp(\\pmb\\beta_1x_1) + \\epsilon_i\\\\\n\\end{cases}\n}\n\\]\n\n\nThere are three ways5 to include more predictors:\n\n\nBackward Selection: We would typically start with a maximal model6 and progressively simplify the model by knocking off predictors that have the least impact on model accuracy.\n\nForward Selection: Start with no predictors and systematically add them one by one to increase the quality of the model\n\nMixed Selection: Wherein we start with no predictors and add them to gain improvement, or remove them at as their significance changes based on other predictors that have been added.\n\nThe first two are covered in the other tutorials above; Mixed Selection we will leave for a more advanced course. But for now we will first use just one predictor rm(Avg. no. of Rooms) to model housing prices.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#conclusions",
    "title": "Modelling with Linear Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have seen how starting from a basic EDA of the data, we have been able to choose a single Quantitative predictor variable to model a Quantitative target variable, using Linear Regression. As stated earlier, we may have wish to use more than one predictor variables, to build more sophisticated models with improved prediction capability. And there is more than one way of selecting these predictor variables, which we will examine in the Tutorials.\nSecondly, sometimes it may be necessary to mathematically transform the variables in the dataset to enable the construction of better models, something that was not needed here.\nWe may also encounter cases where the predictor variables seem to work together; one predictor may influence “how well” another predictor works, something called an interaction effect or a synergy effect. We might then have to modify our formula to include interaction terms that look like \\(predictor1 \\times predictor2\\).\nSo our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!\n\n\n\nOur Linear Regression WorkflowDataEDACheck RelationshipsBuild ModelTransform VariablesTry Multiple Regression and/or interaction effectsCheck Model DiagnosticsCheck R^2Interpret ModelApply ModelSimple orComplex Model DecisionIs the Model Possible?inspectggformulaglimpseskimcorrplotcorrgramggformula + purrrcor.testAll GoodInadequate11 Still Inadequate22Check R^2",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-references",
    "title": "Modelling with Linear Regression",
    "section": "\n References",
    "text": "References\n\n\nhttps://mlu-explain.github.io/linear-regression/\n\nThe Boston Housing Dataset, corrected version. StatLib @ CMU, lib.stat.cmu.edu/datasets/boston_corrected.txt\n\n\nhttps://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\n\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online\n\nMichael Crawley.(2013). The R Book,second edition. Chapter 11.\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. https://www.statlearning.com/\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\n\nhttp://r-statistics.co/Assumptions-of-Linear-Regression.html\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1. Also see http://www.dataanalysisbook.com/index.html\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167,https://doi:10.21105/joss.03167\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nbroom\n1.0.8\nRobinson, Hayes, and Couch (2025)\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ngeomtextpath\n0.1.5\nCameron and van den Brand (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggstatsplot\n0.13.1\nPatil (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nlindia\n0.10\nLee and Ventura (2023)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://doi.org/10.32614/CRAN.package.supernova.\n\n\nCameron, Allan, and Teun van den Brand. 2025. geomtextpath: Curved Text in “ggplot2”. https://doi.org/10.32614/CRAN.package.geomtextpath.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nLee, Yeuk Yu, and Samuel Ventura. 2023. lindia: Automated Linear Regression Diagnostic. https://doi.org/10.32614/CRAN.package.lindia.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2025. broom: Convert Statistical Objects into Tidy Tibbles. https://doi.org/10.32614/CRAN.package.broom.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#footnotes",
    "title": "Modelling with Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe model is linear in the parameters \\(\\beta_i\\), e.g. We can have this:↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nhttps://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. Linear Regression. Available Online↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(ggformula)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(infer)\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#setting-up-r-packages",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(ggformula)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(infer)\nlibrary(tidyverse)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#introduction",
    "title": "Modelling with Logistic Regression",
    "section": "\n Introduction",
    "text": "Introduction\nSometimes the dependent variable is Qualitative: an either/or categorization. for example, or the variable we want to predict might be won or lost the contest, has an ailment or not, voted or not in the last election, or graduated from college or not. There might even be more than two categories such as voted for Congress, BJP, or Independent; or never smoker, former smoker, or current smoker.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#the-logistic-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#the-logistic-regression-model",
    "title": "Modelling with Logistic Regression",
    "section": "\n The Logistic Regression Model",
    "text": "The Logistic Regression Model\nWe saw with the General Linear Model that it models the mean of a target Quantitative variable as a linear weighted sum of the predictor variables:\n\\[\n\\Large{y \\sim N(x_i^T * \\beta, ~~\\sigma^2)}\n\\tag{1}\\]\nThis model is considered to be general because of the dependence on potentially more than one explanatory variable, v.s. the simple linear model:1 \\(y = \\beta_0 + \\beta_1*x_1 + \\epsilon\\). The general linear model gives us model “shapes” that start from a simple straight line to a p-dimensional hyperplane.\nAlthough a very useful framework, there are some situations where general linear models are not appropriate:\n\nthe range of Y is restricted (e.g. binary, count)\nthe variance of Y depends on the mean (Taylor’s Law)2\n\n\nHow do we use the familiar linear model framework when the target/dependent variable is Categorical?\nLinear Models for Categorical Targets?\nRecall that we spoke of dummy-encoded  Qualitative **predictor** variables for our linear models and how we would dummy encode them using numerical values, such as 0 and 1, or +1 and -1. Could we try the same way for a target categorical variable?\n\\[\nY_i = \\beta_0 + \\beta_1*X_i + \\epsilon_i\\\\ \\nonumber\n\\] \\[\nwhere\\\\\\\n\\]\n\\[\n\\begin{align}\nY_i &= 0 ~ if ~~~\"No\"\\\\ \\nonumber\n    &= 1 ~ if ~~~ \"Yes\"  \\nonumber\n\\end{align}\n\\]\nSadly this seems to not work for categorical dependent variables using a simple linear model as before. Consider the Credit Card Default data from the package ISLR.\n\n\n\n  \n\n\n\nWe see balance and income are quantitative predictors; student is a qualitative predictor, and default is a qualitative target variable. If we naively use a linear model equation as model = lm(default ~ balance, data = Default) and plot it, then…\n\n\n\n\n\n\n\n\n\nFigure 1: Naive Linear Model\n\n\n\n\n\n\n…it is pretty much clear from Figure 1 that something is very odd. (no pun intended! See below!) If the only possible values for default are \\(No = 0\\) and \\(Yes = 1\\), how could we interpret predicted value of, say, \\(Y_i = 0.25\\) or \\(Y_i = 1.55\\), or perhaps \\(Y_i = -0.22\\)? Anything other than Yes/No is hard to interpret!\n\n\n\n Problems…and Solutions\nWhere do we go from here?\nLet us state what we might desire of our model:\n\n\nModel Equation: Despite this setback, we would still like our model to be as close as possible to the familiar linear model equation.\n\\[\nY_i = \\beta_0 + \\beta_1*X_i + \\epsilon_i\\\\ \\nonumber\n\\]\n\\[\nwhere\\\\\\\n\\] \\[\n\\begin{align}\nY_i &= 0 ~ if ~~~\"No\"\\\\ \\nonumber\n&= 1 ~ if ~~~ \"Yes\"  \\nonumber\n\\end{align}\n\\tag{2}\\]\n\nPredictors and Weights: We have quantitative predictors so we still want to use a linear-weighted sum for the RHS (i.e predictor side) of the model equation. What can we try to make this work? Especially for the LHS (i.e the target side)?\nMaking the LHS continuous: What can we try? In dummy encoding our target variable, we found a range of [0,1], which is the same range for a probability value! Could we try to use probability of the outcome as our target, even though we are interested in binary outcomes? This would still leave us with a range of \\([0,1]\\) for the target variable, as before.\n\n\n\n\n\n\n\nNoteBinomially distributed target variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we map our Categorical/Qualitative target variable into a Quantitative probability, we need immediately to look at the LINE assumptions in linear regression.\n\n\nIn linear regression, we assume a normally distributed target variable, i.e. the residuals/errors around the predicted value are normally distributed. With a categorical target variable with two levels \\(0\\) and \\(1\\) it would be impossible for the errors \\(e_i = Y_i - \\hat{Y_i}\\) to have a normal distribution, as assumed for the statistical tests to be valid. The errors are bounded by \\([0,1]\\)! One candidate for the error distribution in this case is the binomial distribution, whose mean and variance are p and np(1-p) respectively.\nNote immediately that the binomial variance moves with the mean! The LINE assumption of normality is clearly violated. And from the figure above, extreme probabilities (near 1 or 0) are more stable (i.e., have less error variance) than middle probabilities. So the model has “built-in” heteroscedasticity, which we need to counter with transformations such as the \\(log()\\) function. More on this very shortly!\n\n\n\n\nOdds?: How would one “extend” the range of a target variable from [0,1] to \\([-\\infty, \\infty]\\) ? One step would be to try the odds of the outcome, instead of trying to predict the outcomes directly (Yes or No), or their probabilities \\([0,1]\\).\n\n\n\n\n\n\n\nNoteOdds\n\n\n\nOdds of an event with probability p of occurrence is defined as \\(Odds = p/(1-p)\\). As can be seen, the odds are the ratio of two probabilities, that of the event and its complement. In the Default dataset just considered, the odds of default and the odds of non-default can be calculated as:\n\n\n\n  \n\n\n\n\\[\n\\begin{align}\np(Default) &= 333/(333 + 9667)\\\\ \\nonumber\n           &= 0.333\\\\ \\nonumber\n\\end{align}\n\\]\ntherefore:\n\\[\n\\begin{align}\nOdds~of~Default &=p(Default)/(1-p(Default))\\\\ \\nonumber\n            &= 0.333/(1-0.333)\\\\ \\nonumber\n            &= 0.5\\\\\n\\end{align}\n\\]\nand OddsNoDefault = \\(0.9667/(1-0.9667) = 29\\).\nNow, odds cover half of real number line, i.e. \\([0, \\infty]\\) ! Clearly, when the probability p of an event is \\(0\\), the odds are \\(0\\)…and when it nears \\(1\\), the odds tend to \\(\\infty\\). So we have transformed a simple probability that lies between \\([0,1]\\) to odds lying between \\([0, \\infty]\\). That’s one step towards making a linear model possible; we have “removed” one of the limits on our linear model’s prediction range by using Odds as our target variable.\n\n\n\n\nTransformation using log()?: We need one more leap of faith: how do we convert a \\([0, \\infty]\\) range to a \\([-\\infty, \\infty]\\)? Can we try a log transformation?\n\n\\[\nlog([0, \\infty]) ~ = ~ [-\\infty, \\infty]\n\\]\nThis extends the range of our Qualitative target to the same as with a Quantitative target!\nThere is an additional benefit if this log() transformation: the Error Distributions with Odds targets. See the plot below. Odds are a necessarily nonlinear function of probability; the slope of Odds ~ probability also depends upon the probability itself, as we saw with the probability curve earlier.\n\n\n\n\n\n\n\n\n\n(a) Odds\n\n\n\n\n\n\n\n\n\n(b) Log Odds\n\n\n\n\n\n\nFigure 2: Odds Plot\n\n\nTo understand this issue intuitively, consider what happens to, say, a 5% change in the odds ratio near 1.0. If the odds ratio is \\(1.0\\), then the probabilities p and 1-p are \\(0.5\\), and \\(0.5\\). A 20% increase in the odds ratio to \\(1.20\\) would correspond to probabilities of \\(0.545\\) and \\(0.455\\). However, if the original probabilities were \\(0.9\\) and \\(0.1\\) for an odds ratio \\(9\\), then a 20% increase (in odds ratio) to \\(10.8\\) would correspond to probabilities of \\(0.915\\) and \\(0.085\\), a much smaller change in the probabilities. The basic curve is non-linear and the log transformation flattens this out to provide a more linear relationship, which is what we desire.\nSo in our model, instead of modeling odds as the dependent variable, we will use \\(log(odds)\\), also known as the logit, defined as:\n\\[\n\\begin{align}\nlog(odds_i) &= log\\bigg[p_i/(1-p_i)\\bigg]\\\\ \\nonumber\n            &= logit(p_i)\\\\\n\\end{align}\n\\tag{3}\\]\nThis is our Logistic Regression Model, which uses a Quantitative Predictor variable to predict a Categorical target variable. We write the model as ( for the Default dataset ) :\n\\[\n\\Large{logit(default) = \\beta_0 + \\beta_1 * balance}\n\\tag{4}\\]\nThis means that:\n\\[\nlog(p(default)/(1-p(default))) = \\beta_0+\\beta_1 * balance\n\\] and therefore:\n\\[\n\\begin{align}\np(default) &= \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n&= \\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1 * balance)}}\n\\end{align}\n\\tag{5}\\]\nFrom the Equation 4 above it should be clear that a unit increase in balance should increase the odds of default by \\(\\beta_1\\) units. The RHS of Equation 5 is a sigmoid function of the weighted sum of predictors and is limited to the range [0,1].\n\n\n\n\n\n\n\n\n\n\n(a) naive linear regression model\n\n\n\n\n\n\n\n\n\n(b) logistic regression model\n\n\n\n\n\n\n\n\n\n(c) log odds gives linear models\n\n\n\n\n\n\nFigure 3: Model Plots\n\n\nIf we were to include income also as a predictor variable in the model, we might obtain something like:\n\n\\[\n\\begin{align}\np(default) &= \\frac{exp(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}{1 + exp(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}\\\\\n&= \\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}}\n\\end{align}\n\\tag{6}\\]\nThis model Equation 6 is plotted a little differently, since it includes three variables. We’ll see this shortly, with code. The thing to note is that the formula inside the exp() is a linear combination of the predictors!\n\n\nEstimation of Model Parameters: The parameters \\(\\beta_i\\) now need to be estimated. How might we do that? This last problem is that because we have made so many transformations to get to the logits that we want to model, the logic of minimizing the sum of squared errors(SSE) is no longer appropriate.\n\n\n\n\n\n\n\nNoteInfinite SSE!!\n\n\n\nThe probabilities for default are \\(0\\) and \\(1\\). At these values the log(odds) will map respectively to \\(-\\infty\\) and \\(\\infty\\) 🙀. So if we naively try to take residuals, we will find that they are all \\(\\infty\\) !! Hence the Sum of Squared Errors \\(SSE\\) cannot be computed and we need another way to assess the quality of our model.\n\n\nInstead, we will have to use maximum likelihood estimation(MLE) to estimate the models. The maximum likelihood method maximizes the probability of obtaining the data at hand against every choice of model parameters \\(\\beta_i\\). (And compare that method with the \\(X^2\\) (“chi-squared”) test and statistic instead of t and F to evaluate the model comparisons)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-breast-cancer-dataset",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-breast-cancer-dataset",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Breast Cancer Dataset",
    "text": "Workflow: Breast Cancer Dataset\nLet us proceed with the logistic regression workflow. We will use the well-known Wisconsin breast cancer dataset, readily available from Vincent Arel-Bundock’s website.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-read-the-data",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ncancer &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/dslabs/brca.csv\") %&gt;%\n  janitor::clean_names()\nglimpse(cancer)\n\nRows: 569\nColumns: 32\n$ rownames            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ x_radius_mean       &lt;dbl&gt; 13.540, 13.080, 9.504, 13.030, 8.196, 12.050, 13.4…\n$ x_texture_mean      &lt;dbl&gt; 14.36, 15.71, 12.44, 18.42, 16.84, 14.63, 22.30, 2…\n$ x_perimeter_mean    &lt;dbl&gt; 87.46, 85.63, 60.34, 82.61, 51.71, 78.04, 86.91, 7…\n$ x_area_mean         &lt;dbl&gt; 566.3, 520.0, 273.9, 523.8, 201.9, 449.3, 561.0, 4…\n$ x_smoothness_mean   &lt;dbl&gt; 0.09779, 0.10750, 0.10240, 0.08983, 0.08600, 0.103…\n$ x_compactness_mean  &lt;dbl&gt; 0.08129, 0.12700, 0.06492, 0.03766, 0.05943, 0.090…\n$ x_concavity_mean    &lt;dbl&gt; 0.066640, 0.045680, 0.029560, 0.025620, 0.015880, …\n$ x_concave_pts_mean  &lt;dbl&gt; 0.047810, 0.031100, 0.020760, 0.029230, 0.005917, …\n$ x_symmetry_mean     &lt;dbl&gt; 0.1885, 0.1967, 0.1815, 0.1467, 0.1769, 0.1675, 0.…\n$ x_fractal_dim_mean  &lt;dbl&gt; 0.05766, 0.06811, 0.06905, 0.05863, 0.06503, 0.060…\n$ x_radius_se         &lt;dbl&gt; 0.2699, 0.1852, 0.2773, 0.1839, 0.1563, 0.2636, 0.…\n$ x_texture_se        &lt;dbl&gt; 0.7886, 0.7477, 0.9768, 2.3420, 0.9567, 0.7294, 1.…\n$ x_perimeter_se      &lt;dbl&gt; 2.058, 1.383, 1.909, 1.170, 1.094, 1.848, 1.735, 2…\n$ x_area_se           &lt;dbl&gt; 23.560, 14.670, 15.700, 14.160, 8.205, 19.870, 20.…\n$ x_smoothness_se     &lt;dbl&gt; 0.008462, 0.004097, 0.009606, 0.004352, 0.008968, …\n$ x_compactness_se    &lt;dbl&gt; 0.014600, 0.018980, 0.014320, 0.004899, 0.016460, …\n$ x_concavity_se      &lt;dbl&gt; 0.023870, 0.016980, 0.019850, 0.013430, 0.015880, …\n$ x_concave_pts_se    &lt;dbl&gt; 0.013150, 0.006490, 0.014210, 0.011640, 0.005917, …\n$ x_symmetry_se       &lt;dbl&gt; 0.01980, 0.01678, 0.02027, 0.02671, 0.02574, 0.014…\n$ x_fractal_dim_se    &lt;dbl&gt; 0.002300, 0.002425, 0.002968, 0.001777, 0.002582, …\n$ x_radius_worst      &lt;dbl&gt; 15.110, 14.500, 10.230, 13.300, 8.964, 13.760, 15.…\n$ x_texture_worst     &lt;dbl&gt; 19.26, 20.49, 15.66, 22.81, 21.96, 20.70, 31.82, 2…\n$ x_perimeter_worst   &lt;dbl&gt; 99.70, 96.09, 65.13, 84.46, 57.26, 89.88, 99.00, 8…\n$ x_area_worst        &lt;dbl&gt; 711.2, 630.5, 314.9, 545.9, 242.2, 582.6, 698.8, 5…\n$ x_smoothness_worst  &lt;dbl&gt; 0.14400, 0.13120, 0.13240, 0.09701, 0.12970, 0.149…\n$ x_compactness_worst &lt;dbl&gt; 0.17730, 0.27760, 0.11480, 0.04619, 0.13570, 0.215…\n$ x_concavity_worst   &lt;dbl&gt; 0.239000, 0.189000, 0.088670, 0.048330, 0.068800, …\n$ x_concave_pts_worst &lt;dbl&gt; 0.12880, 0.07283, 0.06227, 0.05013, 0.02564, 0.065…\n$ x_symmetry_worst    &lt;dbl&gt; 0.2977, 0.3184, 0.2450, 0.1987, 0.3105, 0.2747, 0.…\n$ x_fractal_dim_worst &lt;dbl&gt; 0.07259, 0.08183, 0.07773, 0.06169, 0.07409, 0.083…\n$ y                   &lt;chr&gt; \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n\nskim(cancer)\n\n\nData summary\n\n\nName\ncancer\n\n\nNumber of rows\n569\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n31\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ny\n0\n1\n1\n1\n0\n2\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1\n285.00\n164.40\n1.00\n143.00\n285.00\n427.00\n569.00\n▇▇▇▇▇\n\n\nx_radius_mean\n0\n1\n14.13\n3.52\n6.98\n11.70\n13.37\n15.78\n28.11\n▂▇▃▁▁\n\n\nx_texture_mean\n0\n1\n19.29\n4.30\n9.71\n16.17\n18.84\n21.80\n39.28\n▃▇▃▁▁\n\n\nx_perimeter_mean\n0\n1\n91.97\n24.30\n43.79\n75.17\n86.24\n104.10\n188.50\n▃▇▃▁▁\n\n\nx_area_mean\n0\n1\n654.89\n351.91\n143.50\n420.30\n551.10\n782.70\n2501.00\n▇▃▂▁▁\n\n\nx_smoothness_mean\n0\n1\n0.10\n0.01\n0.05\n0.09\n0.10\n0.11\n0.16\n▁▇▇▁▁\n\n\nx_compactness_mean\n0\n1\n0.10\n0.05\n0.02\n0.06\n0.09\n0.13\n0.35\n▇▇▂▁▁\n\n\nx_concavity_mean\n0\n1\n0.09\n0.08\n0.00\n0.03\n0.06\n0.13\n0.43\n▇▃▂▁▁\n\n\nx_concave_pts_mean\n0\n1\n0.05\n0.04\n0.00\n0.02\n0.03\n0.07\n0.20\n▇▃▂▁▁\n\n\nx_symmetry_mean\n0\n1\n0.18\n0.03\n0.11\n0.16\n0.18\n0.20\n0.30\n▁▇▅▁▁\n\n\nx_fractal_dim_mean\n0\n1\n0.06\n0.01\n0.05\n0.06\n0.06\n0.07\n0.10\n▆▇▂▁▁\n\n\nx_radius_se\n0\n1\n0.41\n0.28\n0.11\n0.23\n0.32\n0.48\n2.87\n▇▁▁▁▁\n\n\nx_texture_se\n0\n1\n1.22\n0.55\n0.36\n0.83\n1.11\n1.47\n4.88\n▇▅▁▁▁\n\n\nx_perimeter_se\n0\n1\n2.87\n2.02\n0.76\n1.61\n2.29\n3.36\n21.98\n▇▁▁▁▁\n\n\nx_area_se\n0\n1\n40.34\n45.49\n6.80\n17.85\n24.53\n45.19\n542.20\n▇▁▁▁▁\n\n\nx_smoothness_se\n0\n1\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.03\n▇▃▁▁▁\n\n\nx_compactness_se\n0\n1\n0.03\n0.02\n0.00\n0.01\n0.02\n0.03\n0.14\n▇▃▁▁▁\n\n\nx_concavity_se\n0\n1\n0.03\n0.03\n0.00\n0.02\n0.03\n0.04\n0.40\n▇▁▁▁▁\n\n\nx_concave_pts_se\n0\n1\n0.01\n0.01\n0.00\n0.01\n0.01\n0.01\n0.05\n▇▇▁▁▁\n\n\nx_symmetry_se\n0\n1\n0.02\n0.01\n0.01\n0.02\n0.02\n0.02\n0.08\n▇▃▁▁▁\n\n\nx_fractal_dim_se\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n▇▁▁▁▁\n\n\nx_radius_worst\n0\n1\n16.27\n4.83\n7.93\n13.01\n14.97\n18.79\n36.04\n▆▇▃▁▁\n\n\nx_texture_worst\n0\n1\n25.68\n6.15\n12.02\n21.08\n25.41\n29.72\n49.54\n▃▇▆▁▁\n\n\nx_perimeter_worst\n0\n1\n107.26\n33.60\n50.41\n84.11\n97.66\n125.40\n251.20\n▇▇▃▁▁\n\n\nx_area_worst\n0\n1\n880.58\n569.36\n185.20\n515.30\n686.50\n1084.00\n4254.00\n▇▂▁▁▁\n\n\nx_smoothness_worst\n0\n1\n0.13\n0.02\n0.07\n0.12\n0.13\n0.15\n0.22\n▂▇▇▂▁\n\n\nx_compactness_worst\n0\n1\n0.25\n0.16\n0.03\n0.15\n0.21\n0.34\n1.06\n▇▅▁▁▁\n\n\nx_concavity_worst\n0\n1\n0.27\n0.21\n0.00\n0.11\n0.23\n0.38\n1.25\n▇▅▂▁▁\n\n\nx_concave_pts_worst\n0\n1\n0.11\n0.07\n0.00\n0.06\n0.10\n0.16\n0.29\n▅▇▅▃▁\n\n\nx_symmetry_worst\n0\n1\n0.29\n0.06\n0.16\n0.25\n0.28\n0.32\n0.66\n▅▇▁▁▁\n\n\nx_fractal_dim_worst\n0\n1\n0.08\n0.02\n0.06\n0.07\n0.08\n0.09\n0.21\n▇▃▁▁▁\n\n\n\n\n\nWe see that there are 31 Quantitative variables, all named as x_***, and one Qualitative variable,y, which is a two-level target. (B = Benign, M = Malignant). The dataset has 569 observations, and no missing data.\n\n Workflow: Data Munging\nLet us rename y as diagnosis and take two other Quantitative parameters as predictors, suitably naming them too. We will also create a binary-valued variable called diagnosis_malignant (Binary, Malignant = 1, Benign = 0) for use as a target in our logistic regression model.\n\nShow the Codecancer_modified &lt;- cancer %&gt;%\n  rename(\n    \"diagnosis\" = y,\n    \"radius_mean\" = x_radius_mean,\n    \"concave_points_mean\" = x_concave_pts_mean\n  ) %&gt;%\n  ## Convert diagnosis to factor\n  mutate(diagnosis = factor(\n    diagnosis,\n    levels = c(\"B\", \"M\"),\n    labels = c(\"B\", \"M\")\n  )) %&gt;%\n  ## New Variable\n  mutate(diagnosis_malignant = if_else(diagnosis == \"M\", 1, 0)) %&gt;%\n  select(radius_mean, concave_points_mean, diagnosis, diagnosis_malignant)\n\ncancer_modified\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow can we predict whether a cancerous tumour is Benign or Malignant, based on the variable radius_mean alone, and with both radius_mean and concave_points_mean?\n\n\n\n Workflow: EDA\nLet us use GGally to plot a set of combo-plots for our modified dataset:\n\nShow the Codetheme_set(new = theme_custom())\n#\ncancer_modified %&gt;%\n  select(diagnosis, radius_mean, concave_points_mean) %&gt;%\n  GGally::ggpairs(\n    mapping = aes(colour = diagnosis),\n    switch = \"both\",\n    # axis labels in more traditional locations(left and bottom)\n\n    progress = FALSE,\n    # no compute progress messages needed\n\n    # Choose the diagonal graphs (always single variable! Think!)\n    diag = list(continuous = \"densityDiag\", alpha = 0.3),\n    # choosing density\n\n    # Choose lower triangle graphs, two-variable graphs\n    lower = list(continuous = wrap(\"points\", alpha = 0.3)),\n    title = \"Cancer Pairs Plot #1\"\n  ) +\n  scale_color_brewer(\n    palette = \"Set1\",\n    aesthetics = c(\"color\", \"fill\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from GGally::ggpairs\n\n\n\n\nThe counts for “B” and “M” are not terribly unbalanced; and both the radius_mean and concave_pts_mean appear to have well-separated box plot distributions for “B” and “M”.\nGiven the visible separation of the box-plots for both variables radius_mean and concave_pts_mean, we can believe that these will be good choices as predictors.\nInterestingly, radius_mean and concave_pts_mean are also mutually well-correlated, with a \\(\\rho = 0.823\\); we may wish (later) to choose (a pair of) predictor variables that are less strongly correlated.\n\n\n\n\n Workflow: Model Building\n\n\n Model Code\n Workflow: Model Checking and Diagnostics\n Workflow: Checks for Uncertainty\n Logistic Regression Models as Hypothesis Tests\n\n\n\nLet us code two models, using one and then both the predictor variables:\n\nShow the Codecancer_fit_1 &lt;- glm(diagnosis_malignant ~ radius_mean,\n  data = cancer_modified,\n  family = binomial(link = \"logit\")\n)\n\ncancer_fit_1 %&gt;% broom::tidy()\n\n\n\n\n  \n\n\n\n\nTable 1: Simple Model\n\n\n\nThe equation for the simple model is:\n\\[\n\\begin{aligned}\n\\operatorname{diagnosis\\_malignant} &\\sim Bernoulli\\left(\\operatorname{prob}_{\\operatorname{diagnosis\\_malignant} = \\operatorname{1}}= \\hat{P}\\right) \\\\\n\\log\\left[ \\frac { \\hat{P} }{ 1 - \\hat{P} } \\right]\n&= -15.25 + 1.03(\\operatorname{radius\\_mean})\n\\end{aligned}\n\\tag{7}\\]\nIncreasing radius_mean by one unit changes the log odds by \\(\\hat{\\beta_1} = 1.033\\) or equivalently it multiplies the odds by \\(exp(\\hat{\\beta_1}) =  2.809\\). We can plot the model as shown below:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nqthresh &lt;- c(0.2, 0.5, 0.8)\nbeta01 &lt;- coef(cancer_fit_1)[1]\nbeta11 &lt;- coef(cancer_fit_1)[2]\ndecision_point &lt;- (log(qthresh / (1 - qthresh)) - beta01) / beta11\n##\ncancer_modified %&gt;%\n  gf_point(\n    diagnosis_malignant ~ radius_mean,\n    colour = ~diagnosis,\n    title = \"diagnosis ~ radius_mean\",\n    xlab = \"Average radius\",\n    ylab = \"Diagnosis (1=malignant)\", size = 3, show.legend = F\n  ) %&gt;%\n  # gf_fun(exp(1.033 * radius_mean - 15.25) / (1 + exp(1.033 * radius_mean - 15.25)) ~ radius_mean, xlim = c(1, 30), linewidth = 3, colour = \"red\") %&gt;%\n  gf_smooth(\n    method = glm,\n    method.args = list(family = \"binomial\"),\n    se = FALSE,\n    color = \"black\"\n  ) %&gt;%\n  gf_vline(xintercept = decision_point, linetype = \"dashed\") %&gt;%\n  gf_refine(annotate(\n    \"text\",\n    label = paste0(\"q = \", qthresh),\n    x = decision_point + 0.45,\n    y = 0.4,\n    angle = -90\n  ), scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_hline(yintercept = 0.5) %&gt;%\n  gf_theme(theme(plot.title.position = \"plot\")) %&gt;%\n  gf_refine(xlim(5, 30))\n\n\n\n\n\n\nFigure 4: Simple Model plot\n\n\n\n\nThe dotted lines show how the model can be used to classify the data in to two classes (“B” and “M”) depending upon the threshold probability \\(q\\).\nTaking both predictor variables, we obtain the model:\n\nShow the Codecancer_fit_2 &lt;- glm(diagnosis_malignant ~ radius_mean + concave_points_mean,\n  data = cancer_modified,\n  family = binomial(link = \"logit\")\n)\n\ncancer_fit_2 %&gt;% broom::tidy()\n\n\n\n\n  \n\n\n\n\nTable 2\n\n\n\nThe equation for the more complex model is:\n\\[\n\\begin{aligned}\n\\operatorname{diagnosis\\_malignant} &\\sim Bernoulli\\left(\\operatorname{prob}_{\\operatorname{diagnosis\\_malignant} = \\operatorname{1}}= \\hat{P}\\right) \\\\\n\\log\\left[ \\frac { \\hat{P} }{ 1 - \\hat{P} } \\right]\n&= -13.7 + 0.64(\\operatorname{radius\\_mean}) + 84.22(\\operatorname{concave\\_points\\_mean})\n\\end{aligned}\n\\tag{8}\\]\nIncreasing radius_mean by one unit changes the log odds by \\(\\hat{\\beta_1} = 0.6389\\) or equivalently it multiplies the odds by \\(exp(\\hat{\\beta_1}) =  1.894\\), provided concave_points_mean is held fixed.\nWe can plot the model as shown below: we create a scatter plot of the two predictor variables. The superimposed diagonal lines are lines for several constant values of threshold probability \\(q\\).\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nbeta02 &lt;- coef(cancer_fit_2)[1]\nbeta12 &lt;- coef(cancer_fit_2)[2]\nbeta22 &lt;- coef(cancer_fit_2)[3]\n##\ndecision_intercept &lt;- 1 / beta22 * (log(qthresh / (1 - qthresh)) - beta02)\ndecision_slope &lt;- -beta12 / beta22\n##\ncancer_modified %&gt;%\n  gf_point(concave_points_mean ~ radius_mean,\n    color = ~diagnosis, shape = ~diagnosis,\n    size = 3, alpha = 0.5\n  ) %&gt;%\n  gf_labs(\n    x = \"Average radius\",\n    y = \"Average concave\\nportions of the\\ncontours\",\n    color = \"Diagnosis\",\n    shape = \"Diagnosis\",\n    title = \"diagnosis ~ radius_mean + concave_points_mean\"\n  ) %&gt;%\n  gf_abline(\n    slope = decision_slope, intercept = decision_intercept,\n    linetype = \"dashed\"\n  ) %&gt;%\n  gf_refine(\n    scale_color_brewer(palette = \"Set1\"),\n    annotate(\"text\", label = paste0(\"q = \", qthresh), x = 10, y = c(0.08, 0.1, 0.115), angle = -17.155)\n  ) %&gt;%\n  gf_theme(theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\nFigure 5: Complex Model plot\n\n\n\n\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-logistic-regression-internals",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-logistic-regression-internals",
    "title": "Modelling with Logistic Regression",
    "section": "Workflow: Logistic Regression Internals",
    "text": "Workflow: Logistic Regression Internals\nAll that is very well, but what is happening under the hood of the glm command? Consider the diagnosis (target) variable and say the average_radius feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(diagnosis ~ average_radius, data = cancer_modified)\n\nStart with a sigmoid curve with some initial parameters \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\) that gives us some prediction of the probability of diagnosis for any given average_radius\n\nWe know the target labels for each data point ( i.e. “B” and “M”). We can calculate the likelihood of \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\), given the data.\nWe then change the values of \\(\\hat{\\beta_1}\\) and\\(\\hat{\\beta_0}\\) and calculate the likelihood again.\nThe set of parameters with the maximum likelihood(ML) for \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\) gives us our logistic regression model.\nUse that model henceforth as a model for prediction.\n\nHow does one find out the “ML” parameters? There is clearly a two step procedure:\n\nFind the likelihood of the data for the parameters \\(\\beta_1\\)\n\nMaximize the likelihood by varying them. In practice, the changes to the parameters (step 5) are made in accordance with a method such as the Newton-Raphson method that can rapidly find the ML values for the parameters.\n\nLet us visualize the variations and computations from step(5). For the sake of clarity:\n\nwe will take a small sample of the original dataset\nwe take several different values for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nUse these get a set of regression curves\nwhich we superimpose on the scatter plot of the sample\n\n\n\n\n\n\n\n\nFigure 6: Multiple Models\n\n\n\n\nIn Figure 6, we see three models: the “optimum” one in black, and two others in green and orange respectively.\nWe now project the actual points on to the regression curve, to obtain the predicted probability for each point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe predicted probability \\(p_i\\) for each datum(radius_mean) is for the tumour being Malignant. If the datum corresponds to a tumour that is Benign, we must take \\(1-p_i\\). Each datum point is assumed to be independent, so we can calculate the likelihood as a product of probabilities, as follows: In this way, we calculate the likelihood of the data, give the model parameters as:\n\\[\n\\large{\n\\begin{equation}\n\\begin{aligned}\nlikelihood &=  \\prod_{Malignant}^{}p_i ~ \\times ~ \\prod_{Benign}^{}(1 - p_i)\\\\\n&= \\prod_{}^{}p_i^{y_{Malignant = 1}} ~ \\times ~ (1-p_i)^{y_{Benign = 1}}\\\\\n&= \\prod_{}^{}(p_i)^{y_i} ~\\times~ (1-p_i)^{1-y_i}//\n~since~labels~y_i~are~binary~1~or~0//\n\\end{aligned}\n\\end{equation}\n}\n\\] Lastly, since this is a product of small numbers, it can lead to inaccuracies, so we take the log of the whole thing to make it into an addition, obtaining the log-likelihood (LL):\n\\[\n\\large{\n\\begin{equation}\n\\begin{aligned}\nlog~likelihood ~~ ll(\\beta_i) &= log\\prod_{}^{}(p_i)^{y_i} * (1-p_i)^{1-y_i}\\\\\n&= \\sum_{}^{} y_i * log (p_i) + (1-y_i) * log(1 - p_i)\\\\\n\\end{aligned}\n\\end{equation}\n}\n\\tag{9}\\]\nWe now need to find the (global) maximum of this quantity and determine the \\(\\beta_i\\). Flipping this problem around, we find the maximum likelihood by minimizing the slope/gradient of of the LL!! And, to minimize the slope of the LL, we use the Newton-Raphson method or equivalent. Phew!\n\n\n\n\n\n\nNoteThe Newton-Raphson Method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe black curve \\(y = fx)\\) is the function to be minimized, i.e. it is the gradient of the LL function.\nWe start with any arbitrary starting value of \\(x = x1, y1 = f(x1)\\) and calculate the tangent/slope/gradient equation \\(f'(x1)\\) at point \\((x1, y1) = (x1, f(x1))\\).\nThe tangent \\(f'(x1)\\) cuts the \\(x-axis\\) at \\(x2\\).(Grey line).\nRepeat.\nStop when the gradient becomes very small and \\(x_i\\) changes very in successive iterations.\n\n\n\nHow do we calculate the next value of x using the tangent?\n\nAt \\((x1,y1)\\), the tangent equation is: \\(y = y1 - slope1 * (x - x1)\\).\nThis equation applies at point \\((x2,0\\)), so \\(0 = y1 - slope1 *(x2 - x1)\\). (NOTE: Imagine that this is obtained by temporarily moving the y-axis to \\(x = x1\\) (dotted line), so \\(y1\\) in effect is the “c” in \\(y = mx + c\\))\nSolving for \\(x2\\), we get: \\(x2 = y1/slope1 - x1 = f(x1)/f'(x1)\\)\n\nSince \\(f(x)\\) is already the gradient of LL, we have: \\(x2 = x1 - ll'(x1)/ll''(x1)\\) !!\n\n\n\nTo be written up:\n\nFormula for gradient of LL\nConvergence of Newton- Raphson method for Maximum Likelihood\nHand Calculation of all steps (!!)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#conclusions",
    "title": "Modelling with Logistic Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\n\nLogistic Regression is a great ML algorithm for predicting Qualitative target variables.\nIt also works for multi-level/multi-valued Qual variables (multinomial logistic regression)\nThe internals of Logistic Regression are quite different compared to Linear Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#sec-references",
    "title": "Modelling with Logistic Regression",
    "section": "\n References",
    "text": "References\n\nJudd, Charles M. & McClelland, Gary H. & Ryan, Carey S. Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond. Routledge, Aug 2017. Chapter 14.\nEmi Tanaka.Logistic Regression https://emitanaka.org/iml/lectures/lecture-04A.html#/TOC. Course: ETC3250/5250, Monash University, Melbourne, Australia.\nGeeks for Geeks.Logistic Regression. https://www.geeksforgeeks.org/understanding-logistic-regression/\n\nGeeks for Geeks.Maximum Likelihood Estimation. https://www.geeksforgeeks.org/probability-density-estimation-maximum-likelihood-estimation/\n\nhttps://yury-zablotski.netlify.app/post/how-logistic-regression-works/\nhttps://uc-r.github.io/logistic_regression\nhttps://francisbach.com/self-concordant-analysis-for-logistic-regression/\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf\nhttps://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/\nP. Bingham, N.Q. Verlander, M.J. Cheal (2004). John Snow, William Farr and the 1849 outbreak of cholera that affected London: a reworking of the data highlights the importance of the water supply. Public Health Volume 118, Issue 6, September 2004, Pages 387-394. Read the PDF.\n\nhttps://peopleanalytics-regression-book.org/bin-log-reg.html\nMcGill University. Epidemiology https://www.medicine.mcgill.ca/epidemiology/joseph/courses/epib-621/logfit.pdf\n\nhttps://arunaddagatla.medium.com/maximum-likelihood-estimation-in-logistic-regression-f86ff1627b67\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nequatiomatic\n0.3.7\nAnderson, Heiss, and Sumners (2025)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\n\n\n\n\nAnderson, Daniel, Andrew Heiss, and Jay Sumners. 2025. equatiomatic: Transform Models into “LaTeX” Equations. https://doi.org/10.32614/CRAN.package.equatiomatic.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#footnotes",
    "title": "Modelling with Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf↩︎\nhttps://en.wikipedia.org/wiki/Taylor%27s_law↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#abstract",
    "href": "content/courses/Analytics/listing.html#abstract",
    "title": "Data Analytics for Managers and Creators",
    "section": "Abstract",
    "text": "Abstract\nThis Course takes Business Practitioners and Design Peasants on a journey of Data Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Design and Business context.\n\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving”\n— Libertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of business, design, and scientific activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will include in a full Data Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents (HTML / PDF/ Word) and/or on your own website.\nA diagram from a very popular textbook by Ismay and Kim may be relevant here:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#what-you-will-learn",
    "href": "content/courses/Analytics/listing.html#what-you-will-learn",
    "title": "Data Analytics for Managers and Creators",
    "section": "What you will learn",
    "text": "What you will learn\n\nData Basics: What does data look like and why should we care?\nUnderstand the R language, and appreciate how close it is t plain English, for the most part\nRapidly and intuitively creating Graphs and Data Visualizations using geometric metaphors to explore data for insights\nUse Statistical Tests, Procedures, Models, and Simulations and to answer Business and Design Questions\nUsing ML algorithms such Regression, Classification, and Clustering to develop Business Insights\nUse Linear Programming to make Business Decisions\nCreate crisp and readable Reports that can be shared in a Business Context",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#references",
    "href": "content/courses/Analytics/listing.html#references",
    "title": "Data Analytics for Managers and Creators",
    "section": "References",
    "text": "References\n\nVisualization and R language\n\nHadley Wickham, Mine Cetinkaya-Rundel, and Garett Grolemund. R for Data Science (2e). https://r4ds.hadley.nz. The most important reference for data visualization and analysis in R. Available free online.\nRobert Kabacoff. Modern Data Visualization with R. https://rkabacoff.github.io/datavis/. Available free online.\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art: An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361. 1.. Cole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253.\n\n\n\nAnalytics\n\nJames R Evans, Business Analytics: Methods, Models, and Decisions, Pearson Education, 2021.\nJudd, C.M., McClelland, G.H., & Ryan, C.S. (2017). Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, Third Edition (3rd ed.). Routledge. https://doi.org/10.4324/9781315744131\nThomas Maydon, The 4 Types of Data Analytics, https://www.kdnuggets.com/2017/07/4-types-data-analytics.html\nDimitris Bertsimas, Robert Freund, Data, Models, and Decisions: the Fundamentals of Management Science, Dynamic Ideas Press, 2004.\nCliff T. Ragsdale, Spreadsheet Modeling & Decision Analysis: A Practical Introduction to Management Science, South Western, Cengage Learning, Mason, OH, 2012.\nKeith McNulty. Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia https://peopleanalytics-regression-book.org. Available free online.\n\n\n\nStatistics\n\nMine Cetinkaya-Rundel, Johanna Hardin. Introduction to Modern Statistics. https://openintro-ims2.netlify.app. Available free online.\nDaniel T. Kaplan. Statistical Models (second edition). https://dtkaplan.github.io/SM2-bookdown/. Available free online.\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. https://dtkaplan.github.io/CompactInference/. Available free online.\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. https://www.mosaic-web.org/go/SM2-technique/. Available free online.\nJonas Kristoffer Lindeløv. Common statistical tests are linear models (or: how to teach stats). https://lindeloev.github.io/tests-as-linear/. Available free online.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#pedagogical-notes",
    "href": "content/courses/Analytics/listing.html#pedagogical-notes",
    "title": "Data Analytics for Managers and Creators",
    "section": "Pedagogical Notes",
    "text": "Pedagogical Notes\nWhy this course is what it is, and why it does what it does! Only if you are planning to be an educator yourself!!\n\nPRIMM\nThe method followed will be based on PRIMM:\n\nPREDICT: Inspect the code and guess at what the code might do, write predictions\nRUN: the code provided and check what happens\nINFER: what the parameters of the code do and write comments to explain. What bells and whistles can you see?\nMODIFY: the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\nMAKE: take an idea/concept of your own, and graph it.\n\n\n\n\nFrom https://primmportal.com, used without permission\n\n\nSo in this course, wherever you see “YOUR TURN”, please respond with questions of the data, explanations, more questions and if you are already confident, code chunks to create new calculations and graphs.\n\n\nAnd why teach R in this way?\nBecause we all know two things:\n\nPretty decent English\n\\(y = mx + c\\)\n\nLet us hear from Amelia McNamara:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#our-tools",
    "href": "content/courses/Analytics/listing.html#our-tools",
    "title": "Data Analytics for Managers and Creators",
    "section": "Our Tools",
    "text": "Our Tools\nThis is eventually meant to be a three many-in-one course, based on the following free and open source tools:\n\nR https://cran.r-project.org/ and RStudio https://posit.co/\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering,etc. RStudio is an integrated development environment (IDE) for R and Python.\nOrange Data Mining https://orangedatamining.com/\nOrange is also a FOSS visual point-and-click software for Data Mining and ML, developed at the University of Slovenia, in Ljubljana, Slovenia.\n\n\n\n\nRadiant – Business analytics using R and Shiny https://radiant-rstats.github.io/docs/index.html\nRadiant is a FOSS platform-independent browser-based interface for business analytics in R, developed at the University of San Diego. The application is based on the Shiny package and can be run using R, or in your browser with no installation required. The tool automatically installs a version of R and adds a Shiny-based GUI that removes the need to write R-code. Radiant can also be installed on top of an existing installation of R and invoked from within RStudio.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#learning-r-with-ai",
    "href": "content/courses/Analytics/listing.html#learning-r-with-ai",
    "title": "Data Analytics for Managers and Creators",
    "section": "Learning R with AI",
    "text": "Learning R with AI\nOf course.\n\nhttps://openai.com/index/improvements-to-data-analysis-in-chatgpt/\nhttps://rtutor.ai\nhttps://intro2r.library.duke.edu/ai.html\nhttps://chatlize.ai",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#business-analytics-courses-elsewhere",
    "href": "content/courses/Analytics/listing.html#business-analytics-courses-elsewhere",
    "title": "Data Analytics for Managers and Creators",
    "section": "Business Analytics Courses elsewhere",
    "text": "Business Analytics Courses elsewhere\n\nUniversity of San Diego, Rady School of Business. http://lab.rady.ucsd.edu/sawtooth/business_analytics_in_r/index.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#modules",
    "href": "content/courses/Analytics/listing.html#modules",
    "title": "Data Analytics for Managers and Creators",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html",
    "title": "\n Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#introduction-to-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#introduction-to-orange",
    "title": "\n Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#installing-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#installing-orange",
    "title": "\n Introduction to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#basic-usage-of-orange",
    "title": "\n Introduction to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#orange-workflows",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#orange-workflows",
    "title": "\n Introduction to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#widgets-and-channels",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#widgets-and-channels",
    "title": "\n Introduction to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#loading-data-into-orange",
    "title": "\n Introduction to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#simple-visuals-using-orange",
    "title": "\n Introduction to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#reference",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/index.html#reference",
    "title": "\n Introduction to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.(Download file)\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"hugeicons:orange\"></iconify-icon> Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/listing.html",
    "href": "content/courses/Analytics/Tools/listing.html",
    "title": "Tools and Software",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\n Introduction to R and RStudio\n\n\n10 min\n\n\n\n\n\n\n\n\n\n Introduction to Radiant\n\n\n2 min\n\n\n\n\n\n\n\n\n\n Introduction to Orange\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html",
    "title": "\n Satisfaction with AI Tools",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#setting-up-r-packages",
    "title": "\n Satisfaction with AI Tools",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#introduction",
    "title": "\n Satisfaction with AI Tools",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#read-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#inspect-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#data-dictionary",
    "title": "\n Satisfaction with AI Tools",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#research-question",
    "title": "\n Satisfaction with AI Tools",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#analysetransform-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#plot-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#tasks-and-discussion",
    "title": "\n Satisfaction with AI Tools",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html",
    "title": "Grain Transportation Cartels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#setting-up-r-packages",
    "title": "Grain Transportation Cartels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#introduction",
    "title": "Grain Transportation Cartels",
    "section": "Introduction",
    "text": "Introduction\nFrom: Robert H. Porter (1983). A Study of Cartel Stability: The Joint Executive Committee, 1880-1886. The Bell Journal of Economics, Vol. 14, No. 2 (Autumn, 1983), pp. 301-314:\nThe Joint Executive Committee (JEC) was a cartel (of railroad firms) which controlled eastbound freight shipments from Chicago to the Atlantic seaboard in the 1880’s. While different railroad firms in the JEC shipped grain to different port cities (for example, Baltimore and New York), most of the wheat handled by the cartel was subsequently exported overseas, and the rates charged by different firms (were) adjusted to compensate for differences in ocean shipping rates.\nPrices, rather than quantity, has typically been thought to be the strategic variable of firms in the rail-freight industry. Total demand was quite variable, and so the actual market share of any particular railroad firm would depend on both the prices charged by all the firms as well as unpredictable (random) forces. Price wars were not random, but precipitated by periods of slackened demand, which were presumably unpredictable, at least to some extent.\nOn the other hand, the predictable fluctuations in demand that resulted from the annual opening and closing of the Great Lakes (Superior / Michigan / Huron / Ontario / Erie ) to shipping (because they were frozen in winter), which determined the degree of outside competition, did not disrupt industry conduct. Rather, rates adjusted systematically with the lake navigation season.\nThis dataset is available on Vincent Arel-Bundock’s dataset repository, and is part of the R package AER (Applied Econometrics in R)."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#read-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Read the Data",
    "text": "Read the Data\n\ncartelstability &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/CartelStability.csv\")\ncartelstability\n\n\n  \n\n\nglimpse(cartelstability)\n\nRows: 328\nColumns: 6\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ price    &lt;dbl&gt; 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.35, 0…\n$ cartel   &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ quantity &lt;dbl&gt; 13632, 20035, 16319, 12603, 23079, 19652, 16211, 22914, 23710…\n$ season   &lt;chr&gt; \"Jan  1 - Jan 28\", \"Jan  1 - Jan 28\", \"Jan  1 - Jan 28\", \"Jan…\n$ ice      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#data-dictionary",
    "title": "Grain Transportation Cartels",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#research-question",
    "title": "Grain Transportation Cartels",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nHow do prices for per-tonne grain transport vary based on whether the cartel is working or not? Does this depend upon whether it is summer time or winter time? Why?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#inspectanalysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#inspectanalysetransform-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Inspect/Analyse/Transform the Data",
    "text": "Inspect/Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```\n\n\n\n\n  \n\n\n\nSome summarizing…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#plot-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#task-and-discussion",
    "title": "Grain Transportation Cartels",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nExplain what happens when it is stated “cartel is working” and “cartel is not working”.\nHow do prices for per-tonne grain transport vary based on whether the cartel is working or not? Does this depend upon whether it is summer time or winter time? Why?\nIs the cartel beneficial for customers of the JEC? What would be their behaviour based on whether the cartel was operational or not?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html",
    "title": "\n Antarctic Sea ice",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#setting-up-r-packages",
    "title": "\n Antarctic Sea ice",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#introduction",
    "title": "\n Antarctic Sea ice",
    "section": "Introduction",
    "text": "Introduction\nThe extent of Antarctic Sea Ice over time is monitored by the National Snow and Ice Data Center https://nsidc.org/.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#read-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Read the Data",
    "text": "Read the Data\n Download the Sea Ice data \n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R. NOTE: The sheet that contains our data of interest is titled “SH-Daily-Extent”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#inspect-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\n\n  \n\n\n\nAppreciate the structure of this data. You may even want to open it in Excel for a closer look. List any imperfections in your Data Dictionary. Why do these matter now? Why might they not have mattered earlier, up to now?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#data-dictionary",
    "title": "\n Antarctic Sea ice",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#analysetransform-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\nTry to figure what may be needed, based on the imperfections noted above, what you may attempt to clean the data. Refer to your “list of imperfections” in the data.\nThen look at the code below and execute line by line to get an idea.\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\n\nShow the Codeice %&gt;%\n  # Select columns\n  # Rename some while selecting !!\n  select(\"month\" = ...1, \"day\" = ...2, c(4:49)) %&gt;%\n  # Fill the month column! Yes!!\n  tidyr::fill(month) %&gt;%\n  # Make Wide Data into Long\n  pivot_longer(\n    cols = -c(month, day),\n    names_to = \"series\",\n    values_to = \"values\"\n  ) %&gt;%\n  # Regular Munging\n  mutate(\n    series = as.integer(series),\n    month = factor(month,\n      levels = month.name,\n      labels = month.name,\n      ordered = TRUE\n    ),\n    # Note munging for date!!\n    # Using the lubridate package, part of tidyverse\n    date = lubridate::make_date(\n      year = series,\n      month = month,\n      day = day\n    )\n  ) -&gt; ice_prepared\n\nice_prepared",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#research-question",
    "title": "\n Antarctic Sea ice",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look first at the graph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#plot-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#tasks-and-discussion",
    "title": "\n Antarctic Sea ice",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWhat might the red points represent?\nWhat is perhaps a befuddling aspect of this graph until you…Ohhh!!!!!!\nDraw a sketch of a similar chart for ice extents in the Arctic.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html",
    "title": "\n Heptathlon",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#setting-up-r-packages",
    "title": "\n Heptathlon",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#introduction",
    "title": "\n Heptathlon",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to scores of multiple athletes in the 7 events that make up the Heptathlon, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data",
    "title": "\n Heptathlon",
    "section": "Data",
    "text": "Data\n\nlibrary(HSAUR)\nheptathlon",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#download-the-modified-data",
    "title": "\n Heptathlon",
    "section": "Download the Modified data",
    "text": "Download the Modified data\nNot Applicable!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data-dictionary",
    "title": "\n Heptathlon",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#analyse-the-data",
    "title": "\n Heptathlon",
    "section": "Analyse the Data",
    "text": "Analyse the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#plot-the-data",
    "title": "\n Heptathlon",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#task-and-discussion",
    "title": "\n Heptathlon",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhich events in the 7-event heptathlon are most highly correlated with scores in hurdles?\nIf an athlete was a record holder in both high jump and hurdles, what would be your opinion about them? Justify based on the graph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html",
    "title": "\n School Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#setting-up-r-packages",
    "title": "\n School Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#introduction",
    "title": "\n School Scores",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#read-the-data",
    "title": "\n School Scores",
    "section": "Read the Data",
    "text": "Read the Data\n Download the School Scores Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#inspect-and-clean-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#inspect-and-clean-the-data",
    "title": "\n School Scores",
    "section": "Inspect and Clean the Data",
    "text": "Inspect and Clean the Data\nHint: Use the janitor package here to clean up the variable names. Try to use the big_camel case name format for variables.\n\n\nRows: 577\nColumns: 99\n$ Year                                              &lt;dbl&gt; 2005, 2005, 2005, 20…\n$ StateCode                                         &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"A…\n$ StateName                                         &lt;chr&gt; \"Alabama\", \"Alaska\",…\n$ TotalMath                                         &lt;dbl&gt; 559, 519, 530, 552, …\n$ TotalTestTakers                                   &lt;dbl&gt; 3985, 3996, 18184, 1…\n$ TotalVerbal                                       &lt;dbl&gt; 567, 523, 526, 563, …\n$ AcademicSubjectsArtsMusicAverageGpa               &lt;dbl&gt; 3.92, 3.76, 3.85, 3.…\n$ AcademicSubjectsArtsMusicAverageYears             &lt;dbl&gt; 2.2, 1.9, 2.1, 2.2, …\n$ AcademicSubjectsEnglishAverageGpa                 &lt;dbl&gt; 3.53, 3.35, 3.45, 3.…\n$ AcademicSubjectsEnglishAverageYears               &lt;dbl&gt; 3.9, 3.9, 3.9, 4.0, …\n$ AcademicSubjectsForeignLanguagesAverageGpa        &lt;dbl&gt; 3.54, 3.34, 3.41, 3.…\n$ AcademicSubjectsForeignLanguagesAverageYears      &lt;dbl&gt; 2.6, 2.1, 2.6, 2.6, …\n$ AcademicSubjectsMathematicsAverageGpa             &lt;dbl&gt; 3.41, 3.06, 3.25, 3.…\n$ AcademicSubjectsMathematicsAverageYears           &lt;dbl&gt; 4.0, 3.5, 3.9, 4.1, …\n$ AcademicSubjectsNaturalSciencesAverageGpa         &lt;dbl&gt; 3.52, 3.25, 3.43, 3.…\n$ AcademicSubjectsNaturalSciencesAverageYears       &lt;dbl&gt; 3.9, 3.2, 3.4, 3.7, …\n$ AcademicSubjectsSocialSciencesHistoryAverageGpa   &lt;dbl&gt; 3.59, 3.39, 3.55, 3.…\n$ AcademicSubjectsSocialSciencesHistoryAverageYears &lt;dbl&gt; 3.9, 3.4, 3.3, 3.6, …\n$ FamilyIncomeBetween20_40KMath                     &lt;dbl&gt; 513, 492, 498, 513, …\n$ FamilyIncomeBetween20_40KTestTakers               &lt;dbl&gt; 324, 401, 2121, 180,…\n$ FamilyIncomeBetween20_40KVerbal                   &lt;dbl&gt; 527, 500, 495, 526, …\n$ FamilyIncomeBetween40_60KMath                     &lt;dbl&gt; 539, 517, 520, 543, …\n$ FamilyIncomeBetween40_60KTestTakers               &lt;dbl&gt; 442, 539, 2270, 245,…\n$ FamilyIncomeBetween40_60KVerbal                   &lt;dbl&gt; 551, 522, 518, 555, …\n$ FamilyIncomeBetween60_80KMath                     &lt;dbl&gt; 550, 513, 524, 553, …\n$ FamilyIncomeBetween60_80KTestTakers               &lt;dbl&gt; 473, 603, 2372, 227,…\n$ FamilyIncomeBetween60_80KVerbal                   &lt;dbl&gt; 564, 519, 523, 570, …\n$ FamilyIncomeBetween80_100KMath                    &lt;dbl&gt; 566, 528, 534, 570, …\n$ FamilyIncomeBetween80_100KTestTakers              &lt;dbl&gt; 475, 444, 1866, 147,…\n$ FamilyIncomeBetween80_100KVerbal                  &lt;dbl&gt; 577, 534, 533, 580, …\n$ FamilyIncomeLessThan20KMath                       &lt;dbl&gt; 462, 464, 485, 489, …\n$ FamilyIncomeLessThan20KTestTakers                 &lt;dbl&gt; 175, 191, 891, 107, …\n$ FamilyIncomeLessThan20KVerbal                     &lt;dbl&gt; 474, 467, 474, 486, …\n$ FamilyIncomeMoreThan100KMath                      &lt;dbl&gt; 588, 541, 554, 572, …\n$ FamilyIncomeMoreThan100KTestTakers                &lt;dbl&gt; 980, 540, 3083, 314,…\n$ FamilyIncomeMoreThan100KVerbal                    &lt;dbl&gt; 590, 544, 546, 589, …\n$ GpaAMinusMath                                     &lt;dbl&gt; 569, 544, 541, 559, …\n$ GpaAMinusTestTakers                               &lt;dbl&gt; 724, 673, 3334, 298,…\n$ GpaAMinusVerbal                                   &lt;dbl&gt; 575, 546, 535, 572, …\n$ GpaAPlusMath                                      &lt;dbl&gt; 622, 600, 605, 629, …\n$ GpaAPlusTestTakers                                &lt;dbl&gt; 563, 173, 1684, 273,…\n$ GpaAPlusVerbal                                    &lt;dbl&gt; 623, 604, 593, 639, …\n$ GpaAMath                                          &lt;dbl&gt; 600, 580, 571, 579, …\n$ GpaATestTakers                                    &lt;dbl&gt; 1032, 671, 3854, 457…\n$ GpaAVerbal                                        &lt;dbl&gt; 608, 578, 563, 583, …\n$ GpaBMath                                          &lt;dbl&gt; 514, 492, 498, 492, …\n$ GpaBTestTakers                                    &lt;dbl&gt; 1253, 1622, 7193, 43…\n$ GpaBVerbal                                        &lt;dbl&gt; 525, 499, 499, 511, …\n$ GpaCMath                                          &lt;dbl&gt; 436, 466, 458, 419, …\n$ GpaCTestTakers                                    &lt;dbl&gt; 188, 418, 1184, 57, …\n$ GpaCVerbal                                        &lt;dbl&gt; 451, 472, 464, 436, …\n$ GpaDOrLowerMath                                   &lt;dbl&gt; 0, 424, 439, 0, 419,…\n$ GpaDOrLowerTestTakers                             &lt;dbl&gt; 0, 12, 16, 0, 240, 1…\n$ GpaDOrLowerVerbal                                 &lt;dbl&gt; 0, 466, 435, 0, 408,…\n$ GpaNoResponseMath                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GpaNoResponseTestTakers                           &lt;dbl&gt; 225, 427, 919, 78, 1…\n$ GpaNoResponseVerbal                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GenderFemaleMath                                  &lt;dbl&gt; 538, 505, 513, 536, …\n$ GenderFemaleTestTakers                            &lt;dbl&gt; 2072, 2161, 9806, 85…\n$ GenderFemaleVerbal                                &lt;dbl&gt; 561, 521, 522, 558, …\n$ GenderMaleMath                                    &lt;dbl&gt; 582, 535, 549, 570, …\n$ GenderMaleTestTakers                              &lt;dbl&gt; 1913, 1835, 8378, 74…\n$ GenderMaleVerbal                                  &lt;dbl&gt; 574, 526, 531, 570, …\n$ ScoreRangesBetween200To300MathFemales             &lt;dbl&gt; 22, 30, 119, 12, 297…\n$ ScoreRangesBetween200To300MathMales               &lt;dbl&gt; 10, 20, 72, 7, 1453,…\n$ ScoreRangesBetween200To300MathTotal               &lt;dbl&gt; 32, 50, 191, 19, 443…\n$ ScoreRangesBetween200To300VerbalFemales           &lt;dbl&gt; 14, 26, 115, 9, 3382…\n$ ScoreRangesBetween200To300VerbalMales             &lt;dbl&gt; 17, 26, 86, 3, 2433,…\n$ ScoreRangesBetween200To300VerbalTotal             &lt;dbl&gt; 31, 52, 201, 12, 581…\n$ ScoreRangesBetween300To400MathFemales             &lt;dbl&gt; 173, 233, 881, 68, 1…\n$ ScoreRangesBetween300To400MathMales               &lt;dbl&gt; 93, 153, 450, 31, 71…\n$ ScoreRangesBetween300To400MathTotal               &lt;dbl&gt; 266, 386, 1331, 99, …\n$ ScoreRangesBetween300To400VerbalFemales           &lt;dbl&gt; 123, 218, 739, 46, 1…\n$ ScoreRangesBetween300To400VerbalMales             &lt;dbl&gt; 84, 171, 613, 42, 10…\n$ ScoreRangesBetween300To400VerbalTotal             &lt;dbl&gt; 207, 389, 1352, 88, …\n$ ScoreRangesBetween400To500MathFemales             &lt;dbl&gt; 514, 696, 3215, 210,…\n$ ScoreRangesBetween400To500MathMales               &lt;dbl&gt; 293, 485, 1948, 137,…\n$ ScoreRangesBetween400To500MathTotal               &lt;dbl&gt; 807, 1181, 5163, 347…\n$ ScoreRangesBetween400To500VerbalFemales           &lt;dbl&gt; 430, 656, 3048, 183,…\n$ ScoreRangesBetween400To500VerbalMales             &lt;dbl&gt; 332, 552, 2398, 141,…\n$ ScoreRangesBetween400To500VerbalTotal             &lt;dbl&gt; 762, 1208, 5446, 324…\n$ ScoreRangesBetween500To600MathFemales             &lt;dbl&gt; 722, 813, 3576, 316,…\n$ ScoreRangesBetween500To600MathMales               &lt;dbl&gt; 614, 616, 3152, 244,…\n$ ScoreRangesBetween500To600MathTotal               &lt;dbl&gt; 1336, 1429, 6728, 56…\n$ ScoreRangesBetween500To600VerbalFemales           &lt;dbl&gt; 690, 729, 3661, 302,…\n$ ScoreRangesBetween500To600VerbalMales             &lt;dbl&gt; 617, 596, 3101, 236,…\n$ ScoreRangesBetween500To600VerbalTotal             &lt;dbl&gt; 1307, 1325, 6762, 53…\n$ ScoreRangesBetween600To700MathFemales             &lt;dbl&gt; 485, 342, 1688, 204,…\n$ ScoreRangesBetween600To700MathMales               &lt;dbl&gt; 611, 445, 2126, 239,…\n$ ScoreRangesBetween600To700MathTotal               &lt;dbl&gt; 1096, 787, 3814, 443…\n$ ScoreRangesBetween600To700VerbalFemales           &lt;dbl&gt; 596, 423, 1831, 242,…\n$ ScoreRangesBetween600To700VerbalMales             &lt;dbl&gt; 613, 375, 1679, 226,…\n$ ScoreRangesBetween600To700VerbalTotal             &lt;dbl&gt; 1209, 798, 3510, 468…\n$ ScoreRangesBetween700To800MathFemales             &lt;dbl&gt; 156, 47, 327, 49, 54…\n$ ScoreRangesBetween700To800MathMales               &lt;dbl&gt; 292, 116, 630, 83, 8…\n$ ScoreRangesBetween700To800MathTotal               &lt;dbl&gt; 448, 163, 957, 132, …\n$ ScoreRangesBetween700To800VerbalFemales           &lt;dbl&gt; 219, 109, 412, 77, 5…\n$ ScoreRangesBetween700To800VerbalMales             &lt;dbl&gt; 250, 115, 501, 93, 4…\n$ ScoreRangesBetween700To800VerbalTotal             &lt;dbl&gt; 469, 224, 913, 170, …",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#data-dictionary",
    "title": "\n School Scores",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#analyse-the-data",
    "title": "\n School Scores",
    "section": "Analyse the Data",
    "text": "Analyse the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-all-subjects",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-all-subjects",
    "title": "\n School Scores",
    "section": "Plot the Data: All Subjects",
    "text": "Plot the Data: All Subjects",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-maths-vs-family-income",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-maths-vs-family-income",
    "title": "\n School Scores",
    "section": "Plot the Data: Maths vs Family Income",
    "text": "Plot the Data: Maths vs Family Income",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#task-and-discussion",
    "title": "\n School Scores",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart #1?\nAnd Chart #2\nWrite a 2-line story based on each of the graphs, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html",
    "title": "\n Seattle Bicycle Zones",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#setting-up-r-packages",
    "title": "\n Seattle Bicycle Zones",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#introduction",
    "title": "\n Seattle Bicycle Zones",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#read-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#inspect-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#data-dictionary",
    "title": "\n Seattle Bicycle Zones",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#research-question",
    "title": "\n Seattle Bicycle Zones",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#analysetransform-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#plot-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#tasks-and-discussion",
    "title": "\n Seattle Bicycle Zones",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html",
    "title": "\n Coffee Flavours",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\nShow the Codeextrafont::loadfonts(quiet = TRUE)\nfont &lt;- \"Roboto Condensed\"\ntheme_set(new = theme_classic(base_size = 14))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  text = element_text(family = font),\n  # text elements\n  plot.title = element_text( # title\n    family = font, # set font family\n    size = 20, # set font size\n    face = \"bold\", # bold typeface\n    hjust = 0, # left align\n    # vjust = 2                #raise slightly\n    margin = margin(0, 0, 10, 0)\n  ),\n  plot.subtitle = element_text( # subtitle\n    family = font, # font family\n    size = 14, # font size\n    hjust = 0,\n    margin = margin(2, 0, 5, 0)\n  ),\n  plot.caption = element_text( # caption\n    family = font, # font family\n    size = 8, # font size\n    hjust = 1\n  ), # right align\n\n  axis.title = element_text( # axis titles\n    family = font, # font family\n    size = 10 # font size\n  ),\n  axis.text = element_text( # axis text\n    family = font, # axis family\n    size = 8\n  ) # font size\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#setting-up-r-packages",
    "title": "\n Coffee Flavours",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\nShow the Codeextrafont::loadfonts(quiet = TRUE)\nfont &lt;- \"Roboto Condensed\"\ntheme_set(new = theme_classic(base_size = 14))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  text = element_text(family = font),\n  # text elements\n  plot.title = element_text( # title\n    family = font, # set font family\n    size = 20, # set font size\n    face = \"bold\", # bold typeface\n    hjust = 0, # left align\n    # vjust = 2                #raise slightly\n    margin = margin(0, 0, 10, 0)\n  ),\n  plot.subtitle = element_text( # subtitle\n    family = font, # font family\n    size = 14, # font size\n    hjust = 0,\n    margin = margin(2, 0, 5, 0)\n  ),\n  plot.caption = element_text( # caption\n    family = font, # font family\n    size = 8, # font size\n    hjust = 1\n  ), # right align\n\n  axis.title = element_text( # axis titles\n    family = font, # font family\n    size = 10 # font size\n  ),\n  axis.text = element_text( # axis text\n    family = font, # axis family\n    size = 8\n  ) # font size\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#introduction",
    "title": "\n Coffee Flavours",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to scores various types of coffees on parameters such as aroma, flavour, after-taste etc.\n\n\n\n\n\n\nNoteBreadcrumbs\n\n\n\nSince there are some interesting pre-processing actions required of data, and some choices to be made as well, I will leave some breadcrumbs, and some intermediate results, for you to look at and figure out the analysis/EDA path that you might take! You can then vary these at will after getting a measure of confidence!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#read-the-data",
    "title": "\n Coffee Flavours",
    "section": "Read the Data",
    "text": "Read the Data\n\n\nRows: 1,339\nColumns: 43\n$ total_cup_points      &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75,…\n$ species               &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Ara…\n$ owner                 &lt;chr&gt; \"metad plc\", \"metad plc\", \"grounds for health ad…\n$ country_of_origin     &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\",…\n$ farm_name             &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas …\n$ lot_number            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ mill                  &lt;chr&gt; \"metad plc\", \"metad plc\", NA, \"wolensu\", \"metad …\n$ ico_number            &lt;chr&gt; \"2014/2015\", \"2014/2015\", NA, NA, \"2014/2015\", N…\n$ company               &lt;chr&gt; \"metad agricultural developmet plc\", \"metad agri…\n$ altitude              &lt;chr&gt; \"1950-2200\", \"1950-2200\", \"1600 - 1800 m\", \"1800…\n$ region                &lt;chr&gt; \"guji-hambela\", \"guji-hambela\", NA, \"oromia\", \"g…\n$ producer              &lt;chr&gt; \"METAD PLC\", \"METAD PLC\", NA, \"Yidnekachew Dabes…\n$ number_of_bags        &lt;dbl&gt; 300, 300, 5, 320, 300, 100, 100, 300, 300, 50, 3…\n$ bag_weight            &lt;chr&gt; \"60 kg\", \"60 kg\", \"1\", \"60 kg\", \"60 kg\", \"30 kg\"…\n$ in_country_partner    &lt;chr&gt; \"METAD Agricultural Development plc\", \"METAD Agr…\n$ harvest_year          &lt;chr&gt; \"2014\", \"2014\", NA, \"2014\", \"2014\", \"2013\", \"201…\n$ grading_date          &lt;chr&gt; \"April 4th, 2015\", \"April 4th, 2015\", \"May 31st,…\n$ owner_1               &lt;chr&gt; \"metad plc\", \"metad plc\", \"Grounds for Health Ad…\n$ variety               &lt;chr&gt; NA, \"Other\", \"Bourbon\", NA, \"Other\", NA, \"Other\"…\n$ processing_method     &lt;chr&gt; \"Washed / Wet\", \"Washed / Wet\", NA, \"Natural / D…\n$ aroma                 &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, …\n$ flavor                &lt;dbl&gt; 8.83, 8.67, 8.50, 8.58, 8.50, 8.42, 8.50, 8.33, …\n$ aftertaste            &lt;dbl&gt; 8.67, 8.50, 8.42, 8.42, 8.25, 8.42, 8.33, 8.50, …\n$ acidity               &lt;dbl&gt; 8.75, 8.58, 8.42, 8.42, 8.50, 8.50, 8.50, 8.42, …\n$ body                  &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, …\n$ balance               &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, …\n$ uniformity            &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00,…\n$ clean_cup             &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ sweetness             &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00,…\n$ cupper_points         &lt;dbl&gt; 8.75, 8.58, 9.25, 8.67, 8.58, 8.33, 8.50, 9.00, …\n$ moisture              &lt;dbl&gt; 0.12, 0.12, 0.00, 0.11, 0.12, 0.11, 0.11, 0.03, …\n$ category_one_defects  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ quakers               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ color                 &lt;chr&gt; \"Green\", \"Green\", NA, \"Green\", \"Green\", \"Bluish-…\n$ category_two_defects  &lt;dbl&gt; 0, 1, 0, 2, 2, 1, 0, 0, 0, 4, 1, 0, 0, 2, 2, 0, …\n$ expiration            &lt;chr&gt; \"April 3rd, 2016\", \"April 3rd, 2016\", \"May 31st,…\n$ certification_body    &lt;chr&gt; \"METAD Agricultural Development plc\", \"METAD Agr…\n$ certification_address &lt;chr&gt; \"309fcf77415a3661ae83e027f7e5f05dad786e44\", \"309…\n$ certification_contact &lt;chr&gt; \"19fef5a731de2db57d16da10287413f5f99bc2dd\", \"19f…\n$ unit_of_measurement   &lt;chr&gt; \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\"…\n$ altitude_low_meters   &lt;dbl&gt; 1950.0, 1950.0, 1600.0, 1800.0, 1950.0, NA, NA, …\n$ altitude_high_meters  &lt;dbl&gt; 2200.0, 2200.0, 1800.0, 2200.0, 2200.0, NA, NA, …\n$ altitude_mean_meters  &lt;dbl&gt; 2075.0, 2075.0, 1700.0, 2000.0, 2075.0, NA, NA, …\n\n\n Download Coffee data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#inspect-clean-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#inspect-clean-the-data",
    "title": "\n Coffee Flavours",
    "section": "Inspect, Clean the Data",
    "text": "Inspect, Clean the Data\nWhat are the non-numeric, or Qualitative variables here?\n\n\n\n  \n\n\n\nLook at the number of levels in those Qual variables!!Some are too many and some are so few… Suppose we count the data on the basis of a few?\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 1\n\n\n\nWhy did I choose these Qual factors to count with?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#data-dictionary",
    "title": "\n Coffee Flavours",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#research-question",
    "title": "\n Coffee Flavours",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNoteBreadcrumb 1\n\n\n\nAmong the country_of_origin with the 5 highest average total_cup_points, how do the average ratings vary in ranks on the other coffee parameters?\nWhy this somewhat long-winded question? Why all this averagestuff??\nWhy did I choose country_of_origin?Are there any other options?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#analysetransform-the-data",
    "title": "\n Coffee Flavours",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 3\n\n\n\nWe have too much coffee here! We need to compress this data!\nWhat??? Why? How? Where???\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 4\n\n\n\nWhere did all that coffee go??? Why are there only 5 rows in the data? Why the names of the columns take on a surname, ’_mean`??\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 5\n\n\n\nWhat just happened? How did we convert those mean numbers to ranks?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#plot-the-data",
    "title": "\n Coffee Flavours",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#discussion",
    "title": "\n Coffee Flavours",
    "section": "Discussion",
    "text": "Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html",
    "title": "\n Movie Profits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#setting-up-r-packages",
    "title": "\n Movie Profits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#introduction",
    "title": "\n Movie Profits",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to movies and genres, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data",
    "title": "\n Movie Profits",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#download-the-modified-data",
    "title": "\n Movie Profits",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Movie Profit Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data-dictionary",
    "title": "\n Movie Profits",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#plot-the-data",
    "title": "\n Movie Profits",
    "section": "Plot the Data",
    "text": "Plot the Data\n\n\nUsing R\nUsing Observable",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#task-and-discussion",
    "title": "\n Movie Profits",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nIdentify the type of plot\nWhat are the variables used to plot this graph?\nIf you were to invest in movie production ventures, which are the two best genres that you might decide to invest in?\nWhich R command might have been used to obtain the separate plots for each distributor?\nIf the original dataset had BUDGETS and PROFITS in separate columns, what preprocessing might have been done to achieve this plot?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(patchwork)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#setting-up-r-packages",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(patchwork)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#introduction",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Introduction",
    "text": "Introduction\nLegionnaires’ disease (LD) is a severe form of pneumonia (∼10–25% fatality rate) caused by inhalation of aerosols containing Legionella, a pathogenic gram-negative bacteria. These bacteria can grow, spread, and aerosolize through building water systems. A recent dramatic increase in LD incidence has been observed globally, with a 9-fold increase in the United States from 2000 to 2018,\nRecords were also maintained of atmospheric Sulphur Dioxide (SO2) and the acidity i.e. pH of the atmosphere around building water systems such as Cooling Towers (CT) and in Rainwater.\nThis data is from this paper: Yu F, Nair AA, Lauper U (2024), https://doi.org/10.6084/m9.figshare.25157852.v2",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#read-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#read-the-modified-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Read the Modified Data",
    "text": "Read the Modified Data\n\n\n\n Download LD prevalence data\n\n\n\n\n\n Download SO2 in Nassau data\n\n\n\n\n\n Download SO2 in mainland US data\n\n\n\n\n\n Download pH Cooling Tower\n\n\n\n\n\n Download pH Rainwater",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#inspect-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n```{r}\n#| label: inspect-skim-glimpse\n\n# Write in\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#data-dictionary",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.\n\n\nDescribe how you may plan to transform the data.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#research-question",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look first at the Charts below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#join-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#join-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Join the Data",
    "text": "Join the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\nHere is the plot-ready data:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#plot-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Plot the Data",
    "text": "Plot the Data\nTwo plots were generated by the researchers with this data. Can you reproduce these? Do these graphs prove/disprove any of your hypotheses? What might have been the Hypotheses that led the creating of these graphs?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#tasks-and-discussion",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown. Combine the multiple datasets into one if needed!\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat is a peculiar feature of these graphs?\n\n\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWhat data gathering / research activity might have been carried out to obtain the data graphed here? Provide some details.\nWrite a short story based on the chart, describing your inference/surprise.\nIs there a paradox in this case study? Hint: SO2 is caused by cars/busses running on fossil fuels.\nWhat Statistical Tests might you run to confirm what the charts are saying?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "",
    "text": "This is the YAML at the top of this Quarto document. Use order to sequence your blog posts, and df_print to tidily print your dataframes in the final HTML.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-r-packages",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)\n#\nlibrary(crosstable)\nlibrary(paletteer)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#introduction",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Introduction",
    "text": "Introduction\nAs a demonstration Data Analysis flow, I will take a dataset and show the various steps involved in the workflow: data inspection, cleaning, setting up a hypothesis, plotting a chart, and responding to the hypothesis.\n\n\n\n\n\n\nNote\n\n\n\nI will repeat the entire code in every chunk, so that the whole process is visible in one shot at the end. This is not something you should do as a practice.\n\n\nThis is a dataset pertaining to packaging of groceries, and the difficulty that elderly people face with opening or closing those packages. The study also included people who were experiencing hand pain due to ailments such as arthritis.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Data",
    "text": "Data\nThe data is available here: Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults. Figshare Dataset. https://doi.org/10.6084/m9.figshare.22637656.v1\nAnd, for you peasants, here too:\n\n\n Click me, peasants!\n\n\n Click me also, peasants!\n\n\n\nopening &lt;- opening %&gt;% janitor::clean_names()\nglimpse(opening)\ninspect(opening)\n\nRows: 17\nColumns: 17\n$ group          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;chr&gt; \"11\", \"9\", \"20\", \"7\", \"8\", \"62\", \"26\", \"25\", \"21\", \"5\",…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_suace     &lt;chr&gt; \"17\", \"11\", \"14\", \"6\", \"7\", \"18\", \"8\", \"38\", \"11\", \"25\"…\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\ncategorical variables:  \n        name     class levels  n missing\n1   time_jar character     14 17       0\n2 time_suace character     14 17       0\n                                   distribution\n1 7 (17.6%), 11 (11.8%), 12 (5.9%) ...         \n2 11 (11.8%), 6 (11.8%), 8 (11.8%) ...         \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3   max       mean         sd  n\n1           group numeric  1.0  1.0    1.0  2.0   2.0  1.4117647  0.5072997 17\n2             age numeric 66.0 70.0   73.0 78.0  91.0 74.5294118  6.6437720 17\n3             sex numeric  0.0  0.0    1.0  1.0   1.0  0.6470588  0.4925922 17\n4       hand_pain numeric  0.0  0.0    1.0  1.0   1.0  0.5294118  0.5144958 17\n5    hand_illness numeric  0.0  0.0    0.0  1.0   1.0  0.3529412  0.4925922 17\n6   hand_strength numeric  6.0 14.0   20.0 23.0  37.0 19.4529412  7.5407325 17\n7  pinch_strength numeric  1.7  4.5    5.5  6.5   9.5  5.4941176  2.0464819 17\n8   time_beverage numeric  2.0  4.0    6.0  9.0  21.0  7.3529412  5.1713293 17\n9      time_juice numeric  6.0  8.0   10.0 15.0  27.0 11.5294118  5.6802030 17\n10      time_milk numeric  2.0  6.0    8.0 21.0  52.0 16.2941176 15.3613342 17\n11  time_crackers numeric  6.0 19.0   31.0 47.0  61.0 32.5882353 17.2665096 17\n12    time_cheese numeric  9.0 14.0   25.0 35.0  80.0 28.0000000 17.8815547 17\n13 time_chickpeas numeric 16.0 27.0   30.0 62.0 128.0 45.8823529 30.6061316 17\n14    time_bottle numeric  4.0  6.0    6.0  8.0  18.0  7.2941176  3.3868257 17\n15      time_soup numeric  2.0  4.0    6.0 12.0  30.0 10.2352941  8.7644838 17\n   missing\n1        0\n2        0\n3        0\n4        0\n5        0\n6        0\n7        0\n8        0\n9        0\n10       0\n11       0\n12       0\n13       0\n14       0\n15       0\n\n\n\n## janitor is a good package to make clean names out of weird column names\n##\nclosing &lt;- closing %&gt;% janitor::clean_names()\nglimpse(closing)\ninspect(closing)\n\nRows: 17\nColumns: 17\n$ group          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;chr&gt; \"11\", \"9\", \"20\", \"7\", \"8\", \"62\", \"26\", \"25\", \"21\", \"5\",…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_suace     &lt;chr&gt; \"17\", \"11\", \"14\", \"6\", \"7\", \"18\", \"8\", \"38\", \"11\", \"25\"…\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\ncategorical variables:  \n        name     class levels  n missing\n1   time_jar character     14 17       0\n2 time_suace character     14 17       0\n                                   distribution\n1 7 (17.6%), 11 (11.8%), 12 (5.9%) ...         \n2 11 (11.8%), 6 (11.8%), 8 (11.8%) ...         \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3   max       mean         sd  n\n1           group numeric  1.0  1.0    1.0  2.0   2.0  1.4117647  0.5072997 17\n2             age numeric 66.0 70.0   73.0 78.0  91.0 74.5294118  6.6437720 17\n3             sex numeric  0.0  0.0    1.0  1.0   1.0  0.6470588  0.4925922 17\n4       hand_pain numeric  0.0  0.0    1.0  1.0   1.0  0.5294118  0.5144958 17\n5    hand_illness numeric  0.0  0.0    0.0  1.0   1.0  0.3529412  0.4925922 17\n6   hand_strength numeric  6.0 14.0   20.0 23.0  37.0 19.4529412  7.5407325 17\n7  pinch_strength numeric  1.7  4.5    5.5  6.5   9.5  5.4941176  2.0464819 17\n8   time_beverage numeric  2.0  4.0    6.0  9.0  21.0  7.3529412  5.1713293 17\n9      time_juice numeric  6.0  8.0   10.0 15.0  27.0 11.5294118  5.6802030 17\n10      time_milk numeric  2.0  6.0    8.0 21.0  52.0 16.2941176 15.3613342 17\n11  time_crackers numeric  6.0 19.0   31.0 47.0  61.0 32.5882353 17.2665096 17\n12    time_cheese numeric  9.0 14.0   25.0 35.0  80.0 28.0000000 17.8815547 17\n13 time_chickpeas numeric 16.0 27.0   30.0 62.0 128.0 45.8823529 30.6061316 17\n14    time_bottle numeric  4.0  6.0    6.0  8.0  18.0  7.2941176  3.3868257 17\n15      time_soup numeric  2.0  4.0    6.0 12.0  30.0 10.2352941  8.7644838 17\n   missing\n1        0\n2        0\n3        0\n4        0\n5        0\n6        0\n7        0\n8        0\n9        0\n10       0\n11       0\n12       0\n13       0\n14       0\n15       0",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data-dictionary",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Data Dictionary",
    "text": "Data Dictionary\nSeveral variables are wrongly encoded here, as can be seen. For instance group, and sex are encoded as &lt;dbl&gt; and need to be converted to factors before analysis. We will write our Data Dictionary based on this understanding, and then convert the variables appropriately. (The full workflow will be shown here for the opening dataset; it follows in identical fashion for the closing dataset.)\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\n\nhand_strength(dbl): Hand Strength, numerical\n\npinch_strength(dbl): Pinch Strength, numerical\n\ntime_jar (chr) Time to open a jar. Needs to be (dbl)\n\n\ntime_beverage(dbl)Time to open a beverage\n\ntime_suace (chr) Time to open sauce\n\ntime_juice (dbl) Time to open juice\n\ntime_milk (dbl) Time to open milk carton\n\ntime_crackers(dbl)Time to open crackers pack\n\ntime_cheese (dbl) Time to open cheese packet\n\ntime_chickpeas (dbl) Time to open chickpeas packet\n\ntime_bottle (dbl) Time to open a bottle\n\ntime_soup(dbl) Time to open a a can of soup\n\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\n\ngroup(dbl): Groups in the study. Two. Make into (fct).\n\nsex(dbl): sex of the participant. Make into (fct).\n\nhand_pain: Did they suffer from hand pain or not? Binary. Make into (fct).\n\nhand_illness: How different from hand_pain? Make into (fct).\n\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nSmall dataset of 17 rows. Several time values have been measured across the same set of subjects, resulting is what is called a “repeat measures” experiment. Subjects seem to be in two groups, and with or without hand-pain. Are these the two groups? What is the difference between hand_pain and hand_illness?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-transform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-transform-the-data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Analyse / Transform the Data",
    "text": "Analyse / Transform the Data\nWe need to first convert all the obvious Qual variables into, well, Qual factors! A few variables are also obviously Quant, and need to be transformed. We can also perform counts based on hand_pain and hand_illness to decide how to deal with them. And we will not modify the original data !!\n\nopening_modified &lt;- opening %&gt;%\n  # correct spelling mistake\n  rename(\"time_sauce\" = time_suace) %&gt;%\n  # If you want to do this fast!\n  mutate(across(contains(\"time\"), as.numeric)) %&gt;%\n  # Two \"NA\" entries exist\n\n  mutate(\n    hand_pain = as_factor(hand_pain),\n    hand_illness = as_factor(hand_illness),\n    group = as_factor(group),\n    sex = as_factor(sex)\n  )\n\nopening_modified\n\n\n  \n\n\nglimpse(opening_modified)\n\nRows: 17\nColumns: 17\n$ group          &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;fct&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;fct&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;dbl&gt; 11, 9, 20, 7, 8, 62, 26, 25, 21, 5, 7, 7, 13, 11, 12, 2…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_sauce     &lt;dbl&gt; 17, 11, 14, 6, 7, 18, 8, 38, 11, 25, 8, 6, 16, 29, 21, …\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\n\nAb theek hai. Haan, ab theek hai",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-the-data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "Analyse the Data",
    "text": "Analyse the Data\nLet us make some counts wrt Qual variables, and histograms of Quant variables and get used to our data.\n\nopening_modified %&gt;% count(sex)\n\n\n  \n\n\nopening_modified %&gt;% count(hand_pain)\n\n\n  \n\n\nopening_modified %&gt;% count(hand_pain, hand_illness)\n\n\n  \n\n\n\nReasonably balanced groups. Hand_pain and Hand_illness are not the same thing, as seen from the 4-fold counts above.\ntheme_set(theme_custom())\n##\nopening_modified %&gt;%\n  gf_histogram(~hand_strength, title = \"Hand Strength\")\nopening_modified %&gt;%\n  gf_histogram(~pinch_strength, title = \"Pinch Strength\")\nopening_modified %&gt;%\n  gf_histogram(~time_jar, \"Time to open a Jar\")\nopening_modified %&gt;%\n  gf_histogram(~time_bottle, title = \"Time to open a Bottle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistograms do not look symmetric, but then we have only 17 observations anyway. Elderly people can’t very well be expected to be normal, bless them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#research-questions",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#research-questions",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Research Questions",
    "text": "Research Questions\nWe can create more than one too, and even iteratively, after we have answered the first one and so on. Let us write two:\n\n\n\n\n\n\nNote\n\n\n\nQ1. Do opening times for groceries vary between people with hand_pain and those without?\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ1. Do opening times for groceries vary between people of different sex?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#more-data-transformation",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#more-data-transformation",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n More data transformation",
    "text": "More data transformation\nAs seen, this data is in untidy form: there are several numerical columns that have some “Qual” information embedded in their column names, such as the the kind of package that is being opened. We should transform the data into long form so that all the time numbers are stacked up in one column, and the types of packages are in another column, called grocery. For more info see https://www.garrickadenbuie.com/project/tidyexplain/.\n\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7), # Choose columns to stack (by negation)\n    names_to = \"operation\", # Name of stack column\n    values_to = \"times\" # Name of values column\n  )\n\n\n  \n\n\n\nOnce we do this, we realize that the word “time” in the column operation adds no value, since we want only the grocery involved.\n\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(\n    cols = operation,\n    delim = \"time_\",\n    # Rename \"operation\" column as \"grocery\", drop the silly column now containing only \"_time\"\n    names = c(NA, \"grocery\")\n  )\n\n\n  \n\n\n\nOK, looking better! Now if we plot times, we can colour or facet by grocery.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#first-plot",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#first-plot",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "First Plot",
    "text": "First Plot\n\ntheme_set(theme_custom())\n##\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  ## First Plot\n  gf_density_ridges(grocery ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  gf_labs(\n    title = \"Distribution of Opening and Closing Times\",\n    subtitle = \"For Various Grocery Packagings\"\n  )\n\n\n\n\n\n\n\nOk! Not bad! We now want to label to do two things:\n\nlabel facets 0 and 1 as “Pain” and “No Pain”.\nReorder the groceries so that they are in decreasing order of me(di)an(times).\nIn two steps!!\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  ## First Plot\n  gf_density_ridges(grocery ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  gf_labs(\n    title = \"Distribution of Opening and Closing Times\",\n    subtitle = \"For Various Grocery Packagings\"\n  )\n\n\n\n\n\n\n\nAnd to reorder the groceries in decreasing order of me(di)an (times):\n\n\n\n\n\n\nWarning\n\n\n\nThis does not seem to be happening at this time. Needs to be checked! Wonder what this chart is thinking…\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  ## First Plot modified\n\n  gf_density_ridges(\n    reorder(\n      grocery, # reorder the grocery var\n      times, # based on times variable\n      FUN = median\n    ) # taking the median times\n    ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  gf_labs(\n    title = \"Distribution of Opening and Closing Times\",\n    subtitle = \"For Various Grocery Packagings\"\n  )\n\n\n\n\n\n\n\nAlmost done! We need to relabel the y-axis name and also add some title and subtitles to our plot. And maybe add a point on each sub-plot to show the median opening times?\n\ntheme_set(theme_custom())\n##\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  group_by(grocery, hand_pain) %&gt;%\n  ## First Plot modified\n  gf_density_ridges(\n    reorder(\n      grocery, # reorder the grocery var\n      times, # based on times variable\n      FUN = median\n    ) # taking the median times\n    ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  ## Add the median points\n  gf_summary(\n    fun = \"median\", color = \"black\",\n    size = 1,\n    geom = \"point\"\n  ) %&gt;%\n  ## Facet by hand_pain\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  ## Add titles and labels\n  gf_labs(\n    title = \"Times take by Older People to Open Food Packages\",\n    x = \"Time in seconds\",\n    y = \"Type of Product\",\n    caption = \"Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults.\\n figshare Dataset.\\n https://doi.org/10.6084/m9.figshare.22637656.v1\"\n  )\n\n\n\n\n\n\n# %&gt;% gf_theme(theme_custom())\n\nClosing Times Analysis\nSince the as.numeric did not work for us in the analysis of opening data, I have found and used another function as_numeric from the sjlabelled package. Sigh.\n\ntheme_set(theme_custom())\n##\nclosing %&gt;%\n  mutate(across(starts_with(\"time\"), sjlabelled::as_numeric)) %&gt;%\n  pivot_longer(\n    cols = -c(1:7), names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"operation\")\n  ) %&gt;%\n  mutate(\n    operation = str_replace(string = operation, pattern = \"suace\", replacement = \"sauce\"),\n    hand_pain = factor(hand_pain,\n      levels = c(0, 1),\n      labels = c(\"No Hand Pain\", \"Hand Pain\")\n    )\n  ) %&gt;%\n  group_by(operation, hand_pain) %&gt;%\n  gf_density_ridges(reorder(operation, times, FUN = mean) ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_summary(fun = \"mean\", color = \"black\", size = 1, geom = \"point\") %&gt;%\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  gf_labs(\n    title = \"Times take by Older People to Close Food Packages\",\n    x = \"Time in seconds\", y = \"Type of Product\",\n    caption = \"Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults.\\n figshare Dataset.\\n https://doi.org/10.6084/m9.figshare.22637656.v1\"\n  )\n\n\n\n\n\n\n# %&gt;%\n#   gf_theme(theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#task-and-discussion",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nCreate the graph shown and discuss the following questions:\nWhat is the kind of plot used in the chart? A facetted ridge plot with medians marked using points\nWhat variables have been used in the chart?\n\nTime on X; Grocery Item on Y; Density on the ridges; Hand Pain for faceting\n\n\nQ1. Do opening times for groceries vary between people with hand_pain and those without?\n\nYes; the people with hand pain take longer to open the packages (meh, but all right!) While medians are not too different across the two groups, the distribution tails extend longer in the case of hand_pain = YES.\n\n\nWhy do that lines abruptly stop towards the right side of the upper half of the chart?\n\nBecause the extreme times are shorter across the board for closing, as compared to opening.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#references",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "References",
    "text": "References\n\nColour in R: https://r-for-artists.netlify.app/labs/04-graphics/04-colors\n\nThe paletteer package Over 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton?\n\nHere are the Qualitative Palettes (searchable):\n\n\n\n\n\n\nAnd the Quantitative/Continuous palettes (searchable):\n\n\n\n\n\n\nUse the commands:\n\nQual variable-&gt; colour/fill: scale_colour_paletteer_d(name = \"Legend Name\",                            palette = \"package::palette\",                           dynamic = TRUE/FALSE)\nQuant variable-&gt; colour/fill: scale_colour_paletteer_c(name = \"Legend Name\",                            palette = \"package::palette\",                           dynamic = TRUE/FALSE)\n\n\nIf you want those funky icons at the Section Headers, install this Quarto Extension, and then choosing the icons you want from https://iconify.design and using the iconify shortcode syntax shown below.\n\n\n\n{{&lt; iconify fluent-emoji exploding-head &gt;}}\n{{&lt; iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical &gt;}}\n{{&lt; iconify simple-icons:quarto &gt;}}",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html",
    "title": "\n Gender at the Work Place",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(scales)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#setting-up-r-packages",
    "title": "\n Gender at the Work Place",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(scales)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#introduction",
    "title": "\n Gender at the Work Place",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to gender and compensation at the workplace, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data",
    "title": "\n Gender at the Work Place",
    "section": "Data",
    "text": "Data\n\n\n[1] 2088   12",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#download-the-modified-data",
    "title": "\n Gender at the Work Place",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Job and Gender Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data-dictionary",
    "title": "\n Gender at the Work Place",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#plot-the-data",
    "title": "\n Gender at the Work Place",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#task-and-discussion",
    "title": "\n Gender at the Work Place",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nWhat kind of chart is used in the figure?\nWhat geometries have been used and to which variables have these geometries been mapped?\nBased on this graph, do you think gender plays a role in salaries? What is the trend you see?\nIf SALARY, NO_OF_WORKERS, GENDER, OCCUPATION were available in the original dataset, what pre-processing would have been necessary to obtain this plot?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n2009\n\n\nGentoo\nBiscoe\n43.3\n13.4\n209\n4400\nfemale\n2007\n\n\nChinstrap\nDream\n47.5\n16.8\n199\n3900\nfemale\n2008\n\n\nChinstrap\nDream\n46.0\n18.9\n195\n4150\nfemale\n2007\n\n\nChinstrap\nDream\n50.7\n19.7\n203\n4050\nmale\n2009\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n2009\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n2009\n\n\nChinstrap\nDream\n47.0\n17.3\n185\n3700\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.8\n15.7\n226\n5200\nmale\n2009\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\nGentoo\nBiscoe\n42.6\n13.7\n213\n4950\nfemale\n2008\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: &lt; 50% diameter narrowing\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\n\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\n\n\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\n\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/250-England-AngelaCarter/index.html",
    "href": "content/courses/ISTW/Modules/250-England-AngelaCarter/index.html",
    "title": " England - Angela Carter",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - Angela Carter"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html",
    "title": "\n USA - Carson McCullers",
    "section": "",
    "text": "We will read McCullers’ astonishing story, A Rock. A Tree. A Cloud. PDF\n\n\nWhat we might call weirdness in some people…wrongly!!\nHow to Know someone has Figured it All Out\nHow to Live\nBad Places and Bad Habits and Bad People…etc.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Carson McCullers"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#carson-mccullers",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#carson-mccullers",
    "title": "\n USA - Carson McCullers",
    "section": "",
    "text": "We will read McCullers’ astonishing story, A Rock. A Tree. A Cloud. PDF\n\n\nWhat we might call weirdness in some people…wrongly!!\nHow to Know someone has Figured it All Out\nHow to Live\nBad Places and Bad Habits and Bad People…etc.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Carson McCullers"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#notes-and-references",
    "title": "\n USA - Carson McCullers",
    "section": "Notes and References",
    "text": "Notes and References\nSongs for the Story !!\n\n\nAnd sadly, Renowned country singer Toby Keith dies at 62 after battle with stomach cancer.\nAnd so, maybe this one…\n\n\nAdditional Readings\n\nThe Arrested Development of Carson McCullers. https://www.newyorker.com/magazine/2024/03/04/carson-mccullers-a-life-mary-v-dearborn-book-review. “A writer lives differently from people who don’t write.” Of course.\n“Pretend it’s a Seed, OK?” https://youtu.be/wKbUVxjh0mg",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Carson McCullers"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#writing-prompts",
    "title": "\n USA - Carson McCullers",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\n“So long as you learn, it doesn’t matter who you learn from, does it?” ( Quote from “To Sir, With Love”)\nOn being scolded for being in Bad Company\nLife Lessons from Graffiti?\nOn seeing Artists and Designers everywhere…(bah!)\nA Strange Transformational Hobby…\nOn being a Gig Worker",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Carson McCullers"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#story",
    "href": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#story",
    "title": "\n Czechia - Milan Kundera",
    "section": "Story",
    "text": "Story\nWe will read this terrifying story by this great Writer. The Hitchhiking Game",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-czechia\"></iconify-icon> Czechia - Milan Kundera"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#themes",
    "href": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#themes",
    "title": "\n Czechia - Milan Kundera",
    "section": "Themes",
    "text": "Themes\n\nYoung Love\nGames and Teasing\nEgo and Pain\nRules in the Game\nConsent",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-czechia\"></iconify-icon> Czechia - Milan Kundera"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#notes-and-references",
    "title": "\n Czechia - Milan Kundera",
    "section": "Notes and References",
    "text": "Notes and References\nAdditional Material",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-czechia\"></iconify-icon> Czechia - Milan Kundera"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#song-for-the-story",
    "title": "\n Czechia - Milan Kundera",
    "section": "Song for the Story",
    "text": "Song for the Story\nSong: Chalo Ek Baar Phir Se\nAlbum: Gumrah\nYear: 1963\nArtist: Mahendra Kapoor\nDirector: B.R.Chopra\nStar Cast: Sunil Dutt, Mala Sinha, Ashok Kumar, Shashikala, Nirupa Roy, Nana Plasekar\nMusic Director: Ravi\nLyricist: Sahir Ludhianvi\n\n\n\nChalo ek baar phirse\n\n\n\n\n\nHindi (and quite some Urdu!) Lyrics\nEnglish Translation\n\n\n\nchalo ek baar phir se, ajnabii ban jaayein ham donon(2)\nCome, let us become strangers once again\n\n\n—–\n—–\n\n\nna main tumse koi ummiid rakhuun dil-navaazii kii\nI shall no longer maintain hopes of compassion from you\n\n\nna tum merii taraf dekho ghalat andaaz nazaron se\nNor shall you gaze at me with your deceptive glances.\n\n\nna mere dil ki dhaDkan laDkhaDaaye merii baaton mein\nMy heart shall no longer tremble when I speak,\n\n\n\nna zaahir ho tumhaari kashm-kash ka raaz nazaron se.\nNor shall your glances reveal the secret of your torment.\n\n\n—-\n—-\n\n\ntumhein bhii koii uljhan roktii hai pesh-qadmii se\nComplications prevent you from advancing further,\n\n\nmujhe bhii log kahte hain ki yeh jalve paraaye hain\nI too am told that I wear disguises.\n\n\nmere hamraah bhi rusvaayiaan hain mere maazii kii\nThe disgraces of my past are now my companions,\n\n\ntumhaare saath bhii guzrii huii raaton ke saaye hain\nwhile the shadows of bygone nights are with you too.\n\n\n—–\n—–\n\n\ntaarruf rog ho jaaye to usko bhuulnaa bahtar\nShould knowing one another become a disease, then it is best to forget it.\n\n\ntaalluq bojh ban jaaye to usko toDnaa achhaa\nShould a relationship become a burden, then it is best to end it.\n\n\nvoh afsaana jise anjaam tak laanaa na ho mumkin\nFor that tale which cannot culminate in a conclusion,\n\n\nuse ek khuubsuurat moD de kar chhoDna achhaa\nit is best to give it a beautiful turn and leave it be.\n\n\n—–\n—–\n\n\nchalo ek baar phir se, ajnabii ban jaayein ham donon\nCome, let us become strangers once again.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-czechia\"></iconify-icon> Czechia - Milan Kundera"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/100-Czechia-MilanKundera/index.html#writing-prompts",
    "title": "\n Czechia - Milan Kundera",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nBeing a conservatively brought up young person at a bar\nOn a deep desire to commit an act of violence and how you would resist it, or not\nOn finding hidden depths of personality on your best friend ( and how you found out)",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-czechia\"></iconify-icon> Czechia - Milan Kundera"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html",
    "title": "\n Ireland - William Trevor",
    "section": "",
    "text": "William Trevor",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#ireland-william-trevor",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#ireland-william-trevor",
    "title": "\n Ireland - William Trevor",
    "section": "",
    "text": "William Trevor",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#story",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#story",
    "title": "\n Ireland - William Trevor",
    "section": "Story",
    "text": "Story\nWe will read a sombre but enlightening story by this great Writer.\nA Choice of Butchers",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material",
    "title": "\n Ireland - William Trevor",
    "section": "Additional Material",
    "text": "Additional Material\nQuoted from the LitHub Website\n\nOne of his stories that has always spoken to me is “A Choice of Butchers.” Reading it again, I prepared myself in advance for the emotional impact it would have on me, only to experience its tragic force more acutely. He handles the intersection of childhood with the adult world so beautifully. To me this is a story both about the loss of innocence and, at the same time, the ongoing state of innocence that can betray us when we are young. I cherish the dreary details of the house, the oatmeal wallpaper, the way he describes domestic spaces and habits, and people’s physical traits. And that uncommon, perfect word at the end to describe the father: “rumbustiousness.” The story is layered and ambiguous, elegiac, brilliantly understated, impossible not to read in one sitting. It articulates the terrifying resentment, disappointment, and anger we can feel towards our parents, and the confusion and distress evoked by those very feelings.\n\n–Jhumpa Lahiri",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#themes",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#themes",
    "title": "\n Ireland - William Trevor",
    "section": "Themes",
    "text": "Themes\n\nFamilies\nAdults and Children\nParents are human too\nPetty Jealousy\nPersonal Folly\n“From the Mouths of Babes”",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material-1",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material-1",
    "title": "\n Ireland - William Trevor",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\n7 Writers share their favourite William Trevor story\nAnother good “Dad” story is this one by Grace Paley: A Conversation with My Father",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#song-for-the-story",
    "title": "\n Ireland - William Trevor",
    "section": "Song for the Story",
    "text": "Song for the Story\nBilly Currington’s Country hit from 2003, Walk a Little Straighter.\n\n“Walk a Little Straighter” was written by Billy Currington, making it a very personal composition. This song is a highly autobiographical track, inspired by Currington’s own childhood experiences. The song narrates the story of a young boy addressing his father’s alcohol addiction. “Walk a Little Straighter,” tells about the pain and hope a child feels towards his father who is struggling with alcoholism. Upon its release in 2003, “Walk a Little Straighter” became a significant success, reaching No. 8 on the US Billboard Hot Country Songs chart. It marked the beginning of Billy Currington’s successful career in country music.\n\n\n\n\nRead “Walk A Little Straighter” by Billy Currington on Genius",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#writing-prompts",
    "title": "\n Ireland - William Trevor",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nWhen Adults goof up\nWhat is “Good Parenting”?\nDo Adults apologize to young people?\n“ChhoTa mooh aur Badi Baat: Out of the Mouths of Babes”.\nCompare this story with Rudyard Kipling’s story, Tods’ Amendment @Kipling Society. See notes on the same web page.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-ireland\"></iconify-icon> Ireland - William Trevor"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#story",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#story",
    "title": "\n France - Guy de Maupassant",
    "section": "Story",
    "text": "Story\nWe will read Maupassant’s short short story, Boule de Suif\nA Youtube discussion on this story",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#themes",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#themes",
    "title": "\n France - Guy de Maupassant",
    "section": "Themes",
    "text": "Themes\n\nWomen and Objectification\nBeing Fat\nPrejudice\nFalse Gratitude?\nScapegoat Mechanism\nNetwork Effects: Reputations, Rumours, and Viral Trends\n“Convenience”",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#additional-material",
    "title": "\n France - Guy de Maupassant",
    "section": "Additional Material",
    "text": "Additional Material\n\nThe Collected Works of Guy de Maupassant https://www.gutenberg.org/files/21327/21327-h/21327-h.htm\n\n\nNotes and References\n\nRene Girard’s Mimetic Theory and the Scapegoat: https://violenceandreligion.com/mimetic-theory/\nFrear, G. L. (1992). René Girard on Mimesis, Scapegoats, and Ethics. The Annual of the Society of Christian Ethics, 12, 115–133. http://www.jstor.org/stable/23559770 1\nThe Network Effects Bible. https://www.nfx.com/post/network-effects-bible\nThe Beauty of Fat Women: Leonard Nimoy (“Mr Spock”) and his Full Body Project: https://www.theguardian.com/artanddesign/gallery/2015/mar/03/the-full-body-project-by-leonard-nimoy-in-pictures\n\n\nLeonard Nimoy, who died February 27,2015 at the age of 83, was beloved by fans for his distinctive portrayal of Mr. Spock on Star Trek. Those fans may not have known that Nimoy, through his work as a photographer, also championed women who did not conform to Hollywood’s ideal of physical perfection. [see also http://mashable.com/2015/02/26/body-positivity-get-involved] In 2007, Nimoy published The Full Body Project, a collection of photos featuring nude women of many shapes and sizes. Nimoy’s previous book of photographs captured images of nude women as well, though the models’ slim bodies hewed closely to the conventional standards of beauty. The inspiration for The Fully Body Project struck when a full-figured woman approached Nimoy and asked if he might photograph her and her friends.\n\nListen to Leonard Nimoy discuss this project on NPR:\n\n\n\nIt Ain’t Over until the Fat Lady Sings: https://knowyourphrase.com/aint-over-until-the-fat-lady-sings\nIs it all about Hips?: http://bollynatyam.com/books/",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#videossongs-for-the-story",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#videossongs-for-the-story",
    "title": "\n France - Guy de Maupassant",
    "section": "Videos/Songs for the Story",
    "text": "Videos/Songs for the Story\n\nFirst, A Song that is all about Network Effects !!!\n\nSong: Kuchh Toh Log Kahenge\nSinger: Kishore Kumar\nMusic: R. D. Burman\nFilm: Amar Prem (1972)\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\nLogon ka kaam hai kehna\nIt’s the job of people to say something\n\n\nChhodo bekaar ki baaton mein\nForget all these useless things\n\n\nKahin beet na jaaye raina\nOr else our night will end just around them\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n——\n——-\n\n\nKuch reet jagat ki aisi hai(2)\nSome traditions of the world are such that\n\n\nHar ek subah ki shaam huyi(2)\nEvery morning has had an evening (One thing leads to another)\n\n\nTu kaun hai, tera naam hai kya\nWho are you and what’s your name\n\n\nSita bhi yahan badnaam huyi\nEven Sita has been defamed here\n\n\nPhir kyun sansaar ki baaton se\nThen why with these conversations of the world\n\n\nBheeg gaye tere naina\nHave your eyes become bedewed\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n———-\n————\n\n\nHumko joh taane dete hai(2)\nThose who taunt us saying that\n\n\nHum khoye hai in rang-raliyon mein(2)\nWe’re lost in this debauchery\n\n\nHumne unko bhi chup chupke\nSecretly I’ve also seen them\n\n\nAate dekha in galiyon mein\nComing in these streets\n\n\nYeh sach hai jhoothi baat nahi\nThis is the truth and not a lie\n\n\nTum bolo yeh sach hai na\nYou tell me, isn’t this the truth?\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n…\n…\n\n\n\n\n\nA Woman From Berlin\n\nBetween April 20th and June 22nd of 1945 the anonymous author of A Woman in Berlin wrote about life within the falling city as it was sacked by the Russian Army. Fending off the boredom and deprivation of hiding, the author records her experiences, observations and meditations in this stark and vivid diary. Accounts of the bombing, the rapes, the rationing of food and the overwhelming terror of death are rendered in the dispassionate, though determinedly optimistic prose of a woman fighting for survival amidst the horror and inhumanity of war. This diary was first published in America in 1954 in an English translation and in Britain in 1955. A German language edition was published five years later in Geneva and was met with tremendous controversy. In 2003, over forty years later, it was republished in Germany to critical acclaim - and more controversy. This diary has been unavailable since the 1960s and is now newly translated into English. A Woman in Berlin is an astonishing and deeply affecting account.\nIt has also been made into a movie. https://www.theguardian.com/film/2009/nov/26/anonyma-a-woman-in-berlin.\nHere is the trailer:",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#writing-prompts",
    "title": "\n France - Guy de Maupassant",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nWhen You Did Nothing to Prevent Something\nActs of Commission vs Acts of Omission: Which Regret is easier?\nMy Big Fat Friend",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#footnotes",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#footnotes",
    "title": "\n France - Guy de Maupassant",
    "section": "Footnotes",
    "text": "Footnotes\n\nYou should be able to log in to JSTOR from campus.↩︎",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:france\"></iconify-icon> France - Guy de Maupassant"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html",
    "title": "\n Spain-Merce Rodoreda",
    "section": "",
    "text": "1997 photo by Vilallonga of a portrait of Mercè Rodoreda Rodoreda (via Wikimedia Commons CC BY-SA 2.5)\n\n\nMercé Rodoreda i Gurguí was born on 10 October 1908 in Barcelona. She is the most translated author from Catalan into any other language, and her novel La plaça del Diamant (In Diamond Square) is one of the most celebrated novels of the Spanish Civil War.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#merce-rodoreda",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#merce-rodoreda",
    "title": "\n Spain-Merce Rodoreda",
    "section": "",
    "text": "1997 photo by Vilallonga of a portrait of Mercè Rodoreda Rodoreda (via Wikimedia Commons CC BY-SA 2.5)\n\n\nMercé Rodoreda i Gurguí was born on 10 October 1908 in Barcelona. She is the most translated author from Catalan into any other language, and her novel La plaça del Diamant (In Diamond Square) is one of the most celebrated novels of the Spanish Civil War.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#story",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#story",
    "title": "\n Spain-Merce Rodoreda",
    "section": "Story",
    "text": "Story\nWe will read Rodoreda’s story Rain.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#themes",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#themes",
    "title": "\n Spain-Merce Rodoreda",
    "section": "Themes",
    "text": "Themes\n\nIndependent Single Women\nMaking Decisions\nIs Marriage crumbling as an Institution?\nLuxury Beliefs (after Rob Henderson)",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#additional-material",
    "title": "\n Spain-Merce Rodoreda",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\n\n#RivetingReviews: An Authentic, Brilliant, Catalan Life: A Profile Of Mercè Rodoreda by West Camel. 20 June, 2023. https://www.eurolitnetwork.com/rivetingreviews-an-authentic-brilliant-catalan-life-a-profile-of-merce-rodoreda-by-west-camel/\n\nHenderson, Ron. 12 June 2022. Luxury Beliefs are Status Symbols: The struggle for distinction. https://www.robkhenderson.com/p/status-symbols-and-the-struggle-for?utm_campaign=post&utm_medium=web Accessed 07 Jan 2024. \n\nSong for the Story\nArtistes: Dragon ( pop group from NZ)\nTitle: Rain\nDate: July 1983\nChart Position: US(88); Aus(2)\nLyrics: https://www.lyrics.com/lyric/1406915/Rain",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#writing-prompts",
    "title": "\n Spain-Merce Rodoreda",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Ghosting\nOn Being Indecisive\n“Is Marriage Crumbling”, and other Luxury Beliefs.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:spain\"></iconify-icon> Spain-Merce Rodoreda"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/240-USA-GracePaley/index.html",
    "href": "content/courses/ISTW/Modules/240-USA-GracePaley/index.html",
    "title": " USA - Grace Paley",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Grace Paley"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html",
    "href": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html",
    "title": "\n USA - Lucia Berlin",
    "section": "",
    "text": "https://www.theguardian.com/books/booksblog/2016/may/18/a-brief-survey-of-the-short-story-lucia-berlin",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Lucia Berlin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html#lucia-berlin",
    "href": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html#lucia-berlin",
    "title": "\n USA - Lucia Berlin",
    "section": "",
    "text": "https://www.theguardian.com/books/booksblog/2016/may/18/a-brief-survey-of-the-short-story-lucia-berlin",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Lucia Berlin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/290-Israel-EtgarKeret/index.html",
    "href": "content/courses/ISTW/Modules/290-Israel-EtgarKeret/index.html",
    "title": " Israel - Etgar Keret",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-israel\"></iconify-icon> Israel - Etgar Keret"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#story",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#story",
    "title": "\n USA - Raymond Carver",
    "section": "Story",
    "text": "Story\nWe will read Carver’s luminous story, Cathedral. PDF",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Raymond Carver"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#themes",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#themes",
    "title": "\n USA - Raymond Carver",
    "section": "Themes",
    "text": "Themes\n\nBlindness\nJealousy\nIrony / Miscomprehension\nCheerfulness of the Blind",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Raymond Carver"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#additional-material",
    "title": "\n USA - Raymond Carver",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nRaymond Carver on How to Write\n\nShort Film on this story: \n    \n\nH.G. Wells’ story, The Country of the Blind\n\n\n“The Country of the Blind” is a short story written by H. G. Wells. It was first published in the April 1904 issue of The Strand Magazine and included in a 1911 collection of Wells’s short stories, The Country of the Blind and Other Stories. It is one of Wells’s best known short stories, and features prominently in literature dealing with blindness.-Wikipedia\n\n 4. Sparsh, starring Naseeruddin Shah and Shabana Azmi.\n\n\nhttps://www.youtube.com/watch?v=8yB4IwsnyQ8\n\nSparsh (English: Touch) is a 1980 Indian Hindi feature film directed by Sai Paranjpye. It stars Naseeruddin Shah and Shabana Azmi playing the characters of a visually impaired principal and a sighted teacher in a school for the blind, where they fall in love though soon their complexes tag along and they struggle to get past them to reconnect with the “touch” of love.\nThe film remains most memorable for the subtle acting of its leads, plus the handling of the issue of relationships with the visually handicapped, revealing the emotional and perception divide between the worlds of the “blind” and the “sighted”, epitomized by the characters. The film won the National Film Award for Best Feature Film in Hindi. However, the film’s release was delayed by almost 4 years.\n\nShorter Summary:\nhttps://www.dailymotion.com/video/x3nv3l5\n\n\nAnd Then there was Light is the autobiography of Jacques Lusseyran, the blind hero of the French Resistance during WWII.\n\n\nhttps://angelusnews.com/news/us-world/jacques-lusseyran-blind-hero-of-the-french-resistance/\n\n“And There Was Light” is the strange and beautiful autobiography of Jacques Lusseyran, “blind hero of the French Resistance.”\nBorn in Paris in 1924, Lusseyran lost his sight at the age of 8 in a schoolroom incident. Even at that age, he was groping toward the transcendent.\nTrying to navigate his way around a world he could no longer see, he came to learn that inanimate things are alive, and of the sympathetic current that runs between the branches of a tree in springtime, and that if you press the little stone you’ve secreted in your pocket, it will press back.\nHe wrote, “The seeing commit a strange error. They believe that we know the world only through our eyes. For my part, I discovered that the universe consists of pressure, that every object and every living being reveals itself to us at first by a kind of quiet yet unmistakable pressure that indicates its intention and its form. I even experienced the following wonderful fact: A voice, the voice of a person, permits him to appear in a picture. When the voice of a man reaches me, I immediately perceive his figure, his rhythm, and most of his intentions.\n\nSong(s) for the Story !!\n\nMahendra Kapoor from the movie Sambandh(1969):\n\n\n\n\n\nDeep Purple, When a Blind Man Cries",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Raymond Carver"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#writing-prompts",
    "title": "\n USA - Raymond Carver",
    "section": "Writing Prompts",
    "text": "Writing Prompts",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Raymond Carver"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html",
    "title": "\n Italy - Primo Levi",
    "section": "",
    "text": "Levi, a 23-year old chemist, was arrested in December 1943 and transported to Auschwitz in February 1944. There he remained until the camp was liberated on 27 January 1945. He arrived back home in Turin in October, unrecognisable to the concierge who had seen him only a couple of years earlier.\n\n\nHydrogen from Primo Levi’s collection The Periodic Table https://archive.org/download/ThePeriodicTable-PrimoLevi/periodic-primo.pdf\n\n\nEpiphany\n\nCuriosity, the Desire to Know, and Perseverance\n\nRule Breaking\n\n“Other Worldliness”\n\nMetaphors: Describing an idea using vocabulary from another domain\nTRIZ, aka Teoriya Resheniya Izobretatelskhikh Zadatch",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Primo Levi"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#italy-primo-levi",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#italy-primo-levi",
    "title": "\n Italy - Primo Levi",
    "section": "",
    "text": "Levi, a 23-year old chemist, was arrested in December 1943 and transported to Auschwitz in February 1944. There he remained until the camp was liberated on 27 January 1945. He arrived back home in Turin in October, unrecognisable to the concierge who had seen him only a couple of years earlier.\n\n\nHydrogen from Primo Levi’s collection The Periodic Table https://archive.org/download/ThePeriodicTable-PrimoLevi/periodic-primo.pdf\n\n\nEpiphany\n\nCuriosity, the Desire to Know, and Perseverance\n\nRule Breaking\n\n“Other Worldliness”\n\nMetaphors: Describing an idea using vocabulary from another domain\nTRIZ, aka Teoriya Resheniya Izobretatelskhikh Zadatch",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Primo Levi"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#additional-material",
    "title": "\n Italy - Primo Levi",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nThe last story in The Periodic Table, titled Carbon is also a fantastic metaphoric journey of a single Carbon atom. Read it!! It has been described as the most accessible piece of science writing!\n\nMore Holocaust Reading:\n\nTadeuscz Borowski’s “Postal Indiscretions” is a series of letters written from a concentration camp.\n\nHis short story, This Way for the Gas, Ladies and Gentlemen is also a harrowing read. Weblink to PDF\n\nThe World of Tadeuscz Borowski’s Auschwitz https://www.nybooks.com/daily/2021/09/12/the-world-of-tadeusz-borowskis-auschwitz/\n\n\n\nhttps://www.theguardian.com/books/2017/apr/22/primo-levi-auschwitz-if-this-is-a-man-memoir-70-years\nA Student Reflection on Primo Levi’s Hydrogen: https://ocw.mit.edu/courses/literature/21l-325-small-wonders-staying-alive-spring-2007/assignments/periodic2.pdf\nExtract from “The key to the highest truths”: Primo Levi and the beauty of chemistry\n\n\nNonetheless, reading Levi’s writing over lockdown, I was reminded that he also witnessed chemistry’s most detestable side at Auschwitz, as part of the Chemical Kommando transporting magnesium chloride, and at the IG-Farben laboratory. Despite this, Levi never lost sight of the beauty of chemistry: for me, found in the sublimation of brilliant emerald-green crystals of nickelocene; in the jagged, imperfect trace of an action potential on the electromyograph; in the faint rainbow of lines emitted by potassium under a sodium discharge lamp. If Levi were to observe us in these practical classes, complete with our rash deductions, amateurish mistakes and shattered glassware, I like to think he would be pleased.\n\nSong for the Story !!\nSong: Flying Sorcery Artiste: Al Stewart\nAlastair(“Al”) Ian Stewart (born 5 September 1945) is a Scottish singer-songwriter and folk-rock musician who rose to prominence as part of the British folk revival in the 1960s and 1970s. He developed a unique style of combining folk-rock songs with delicately woven tales of characters and events from history.\n\n\n\nRead “Flying Sorcery” by Al Stewart on Genius",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Primo Levi"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#writing-prompts",
    "title": "\n Italy - Primo Levi",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nMy Genius Friend\nWhat I learnt by Breaking Rules\nDescribing a Scientific Concept using everyday objects",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Primo Levi"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/180-India-RuthPrawerJhabvala/index.html#india-ruth-prawer-jhabvala",
    "href": "content/courses/ISTW/Modules/180-India-RuthPrawerJhabvala/index.html#india-ruth-prawer-jhabvala",
    "title": "\n India - Ruth Prawer Jhabvala",
    "section": "India : Ruth Prawer Jhabvala",
    "text": "India : Ruth Prawer Jhabvala\nFrom: https://www.britannica.com/biography/Ruth-Prawer-Jhabvala\n\nRuth Prawer Jhabvala, original name Ruth Prawer, (born May 7, 1927, Cologne, Germany—died April 3, 2013, New York, New York, U.S.), novelist and screenwriter, well known for her witty and insightful portrayals of contemporary Indian lives and, especially, for her 46 years as a pivotal member of Ismail Merchant and James Ivory’s filmmaking team.\n\n\nJhabvala’s family was Jewish, and in 1939 they emigrated from Germany to England; she was made a naturalized British citizen in 1948. After receiving an M.A. in English (1951) from Queen Mary College, London, she married an Indian architect and moved to India, where she lived for the next 24 years. After 1975 she lived in New York City, becoming a U.S. citizen in 1986.\n\nStory\nWe will read this beautiful, beautifully written story, with a completely cringe-worthy character in it.\nThe Interview\nThemes\n\nJoint Family\n“Victim Mentality”\nIncest\nNotes and References\n\nThe Difficult Genius of Ruth Prawer Jhabvala\nAdditional Material\n\nThe Great Indian Family: New Roles, Old Responsibilities, https://www.amazon.in/Gitanjali-Prasad/e/B001HPLWHW/ref=dp_byline_cont_pop_book_1\nMukul Kesavan, The ugly Indian man: Of hygiene, hair and horrible habits, https://www.telegraphindia.com/opinion/the-ugly-indian-man-of-hygiene-hair-and-horrible-habits/cid/1026680#\nSong for the Story\nSong: Yeh Jeevan Hai Is Jeevan Ka Yahi Hai\nMovie: Piya Ka Ghar\nYear: 1972\nSinger: Kishore Kumar Music: Laxmikant Pyarelal\nLyrics: Anand Bakshi\nCast: Anil Dhawan, Jaya Bhaduri\nDirector: Basu Chatterjee\n\n\nWriting Prompts\n\nCritical Reflection on the Story!\n\nSarcastic Piece on the Indian Male\nNav-Vadhu describing the first week after an arranged marriage\n\nAn analysis of a regional movie that has the Joint Family as a Theme\nThe Joys of Living in a Joint Family",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:india\"></iconify-icon> India - Ruth Prawer Jhabvala"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html",
    "title": "\n England - V S Pritchett",
    "section": "",
    "text": "V S Pritchett\n\n\n\n\n\n“V.S. Pritchett (1900–1997)was the best short story writer in English during the twentieth century. There should be no argument about this. (emphasis mine)”\n— David Miller, That Glimpse of Truth: The 100 Finest Short Stories Ever Written, Head of Zeus, 2014. ISBN:9781784080037\n\n\nExtracted from https://www.theguardian.com/books/booksblog/2008/feb/22/vspritchett\n\nVictor Sawdon Pritchett, or VSP, as he preferred to be known (he loathed his Christian name), exemplifies the gap that can yawn between reputation and readership. Hugely productive throughout his 97-year life as a short story writer, essayist, biographer, autobiographer and novelist, he is little read just 11 years after his death. In his short fiction Pritchett is one of the English writers who most clearly exhibits the mark of Chekhov’s influence. The significance the Russian placed on the commonplace thing and apparently incidental aside is there, as is the deceptively simple expression of complex emotional processes. But the chief Chekhovian element which Pritchett makes his own is the way he subsumes himself within the story. To borrow from drama, Pritchett should be seen, not as a director with a signature style, but as an actor with the ability to lose himself entirely in whatever role he is playing. His stories situate the reader in direct relation to their characters, with little or no authorial filter between them.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#v.-s.-pritchett",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#v.-s.-pritchett",
    "title": "\n England - V S Pritchett",
    "section": "",
    "text": "V S Pritchett\n\n\n\n\n\n“V.S. Pritchett (1900–1997)was the best short story writer in English during the twentieth century. There should be no argument about this. (emphasis mine)”\n— David Miller, That Glimpse of Truth: The 100 Finest Short Stories Ever Written, Head of Zeus, 2014. ISBN:9781784080037\n\n\nExtracted from https://www.theguardian.com/books/booksblog/2008/feb/22/vspritchett\n\nVictor Sawdon Pritchett, or VSP, as he preferred to be known (he loathed his Christian name), exemplifies the gap that can yawn between reputation and readership. Hugely productive throughout his 97-year life as a short story writer, essayist, biographer, autobiographer and novelist, he is little read just 11 years after his death. In his short fiction Pritchett is one of the English writers who most clearly exhibits the mark of Chekhov’s influence. The significance the Russian placed on the commonplace thing and apparently incidental aside is there, as is the deceptively simple expression of complex emotional processes. But the chief Chekhovian element which Pritchett makes his own is the way he subsumes himself within the story. To borrow from drama, Pritchett should be seen, not as a director with a signature style, but as an actor with the ability to lose himself entirely in whatever role he is playing. His stories situate the reader in direct relation to their characters, with little or no authorial filter between them.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#story",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#story",
    "title": "\n England - V S Pritchett",
    "section": "Story",
    "text": "Story\nWe will read his story A Family Man.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#themes",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#themes",
    "title": "\n England - V S Pritchett",
    "section": "Themes",
    "text": "Themes\n\nLies and Lying\nAffairs\n“Collateral” damage to other people\n“Satyam bruyat, Priyam Bruyaat..\n\nसत्यं ब्रूयात् प्रियं ब्रूयात् न ब्रूयात् सत्यमप्रियम्\nप्रियं च नानृतं ब्रूयात् एष धर्मः सनातनः ॥",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#additional-material",
    "title": "\n England - V S Pritchett",
    "section": "Additional Material",
    "text": "Additional Material\n\nAdultery makes for good fiction!\n\nFord Madox Ford, The Good Soldier\n\nJane Smiley, The Age of Grief\n\nKate Chopin, The Awakening",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#notes-and-references",
    "title": "\n England - V S Pritchett",
    "section": "Notes and References",
    "text": "Notes and References\n\nhttps://truthultimate.com/satyam-bruyat-priyam-bruyat/",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#songs-for-the-story",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#songs-for-the-story",
    "title": "\n England - V S Pritchett",
    "section": "Songs for the Story",
    "text": "Songs for the Story\n\nWhitney Houston (1985) : Saving All my Love for You\n\n\n\n\nFleetwood Mac (1987) : Tell Me Lies",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#writing-prompts",
    "title": "\n England - V S Pritchett",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nLying for the sake of goodness",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-kingdom\"></iconify-icon> England - V S Pritchett"
    ]
  },
  {
    "objectID": "content/courses/ISTW/listing.html",
    "href": "content/courses/ISTW/listing.html",
    "title": "Literary Jukebox: In Short, the World",
    "section": "",
    "text": "“I read closely, word by word, sentence by sentence, pondering each deceptively minor decision that the writer had made. And though it’s impossible to recall every source of inspiration and instruction, I can remember the novels and stories that seemed to me revelations: wells of beauty and pleasure that were also textbooks, private lessons in the art of fiction.”\n— Francine Prose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Italy - Dino Buzzati\n\n8 min\n\n\n\n\n\n\n\n\n\n France - Guy de Maupassant\n\n10 min\n\n\n\n\n\n\n\n\n\nJapan - Hisaye Yamamoto\n\n8 min\n\n\n\n\n\n\n\n\n\n Peru - Ventura Garcia Calderon\n\n4 min\n\n\n\n\n\n\n\n\n\n Russia - Maxim Gorky\n\n10 min\n\n\n\n\n\n\n\n\n\n Egypt - Alifa Rifaat\n\n8 min\n\n\n\n\n\n\n\n\n\n Brazil - Clarice Lispector\n\n2 min\n\n\n\n\n\n\n\n\n\n England - V S Pritchett\n\n2 min\n\n\n\n\n\n\n\n\n\n Russia - Ivan Bunin\n\n9 min\n\n\n\n\n\n\n\n\n\n Czechia - Milan Kundera\n\n8 min\n\n\n\n\n\n\n\n\n\n Sweden - Lars Gustaffsson\n\n3 min\n\n\n\n\n\n\n\n\n\n Canada - John Cheever\n\n2 min\n\n\n\n\n\n\n\n\n\n Ireland - William Trevor\n\n3 min\n\n\n\n\n\n\n\n\n\n USA - Raymond Carver\n\n3 min\n\n\n\n\n\n\n\n\n\n Italy - Primo Levi\n\n3 min\n\n\n\n\n\n\n\n\n\n India - Ruth Prawer Jhabvala\n\n2 min\n\n\n\n\n\n\n\n\n\n USA - Carson McCullers\n\n1 min\n\n\n\n\n\n\n\n\n\n Zimbabwe - Petina Gappah\n\n3 min\n\n\n\n\n\n\n\n\n\n India - Bharati Mukherjee\n\n1 min\n\n\n\n\n\n\n\n\n\n USA - Lucia Berlin\n\n1 min\n\n\n\n\n\n\n\n\n\n USA - Grace Paley\n\n1 min\n\n\n\n\n\n\n\n\n\n England - Angela Carter\n\n1 min\n\n\n\n\n\n\n\n\n\n Spain-Merce Rodoreda\n\n1 min\n\n\n\n\n\n\n\n\n\n USA - Kurt Vonnegut\n\n1 min\n\n\n\n\n\n\n\n\n\n Israel - Etgar Keret\n\n1 min\n\n\n\n\n\n\n\n\n\n Israel - Ruth Calderon\n\n1 min\n\n\n\n\nNo matching items\n\n  \n\n Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World"
    ]
  },
  {
    "objectID": "content/courses/Tech4Ed/Modules/40-UsingManim/index.html",
    "href": "content/courses/Tech4Ed/Modules/40-UsingManim/index.html",
    "title": "🧭 Using Manim",
    "section": "",
    "text": "Manim Community https://www.manim.community"
  },
  {
    "objectID": "content/courses/Tech4Ed/Modules/40-UsingManim/index.html#references",
    "href": "content/courses/Tech4Ed/Modules/40-UsingManim/index.html#references",
    "title": "🧭 Using Manim",
    "section": "",
    "text": "Manim Community https://www.manim.community"
  },
  {
    "objectID": "content/courses/Tech4Ed/Modules/10-UsingIdyll/index.html",
    "href": "content/courses/Tech4Ed/Modules/10-UsingIdyll/index.html",
    "title": "🧭 Using Idyll",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education",
      "🧭 Using Idyll"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#setting-up-r-packages",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n# Data manipulation and data sources\noptions(htmltools.preserve.raw = FALSE, echo = TRUE)\nlibrary(tidygraph)\nlibrary(igraph)\nlibrary(palmerpenguins)\nlibrary(igraphdata)\n\n\n# To render htmlwidgets as iframe widgets\n# https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/\n# library(widgetframe)\n\n# htmlwidget related libraries\nlibrary(htmlwidgets)\n\n\n# Widget Libraries\nlibrary(leaflet)\nlibrary(plotly)\nlibrary(DT)\nlibrary(echarts4r)\nlibrary(echarts4r.assets)\nlibrary(canvasXpress)\nlibrary(rgl)\nlibrary(networkD3)\nlibrary(threejs)\nlibrary(slickR)\nlibrary(crosstalk)\n# Linkable widgets in crosstalk - github repo only\n# devtools::install_github(\"kent37/summarywidget\")\nlibrary(summarywidget)\nlibrary(tidyverse)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Introduction",
    "text": "Introduction\nThere are very many great JavaScript libraries for creating eye-popping and even interactive charts! And these are now available in R, and can be invoked using R code! So we can “use JavaScript” in R, as it were, without knowing JavaScript! And create something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike the Dormouse, no complaints!\nWe will explore a few them, as an alternative to ggplot !!\nThis may be too much of a good thing, or a much of muchness but then, we can always use more then one way of telling our stories!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nhtmlwidgets usage",
    "text": "htmlwidgets usage\nhtmlwidgets creates, well, widgets, that can visualize data in many ways. HTML widgets work just like R plots except they produce interactive web visualizations. These can be used in RMarkdown, in flexdashboards, and in shiny apps.\nAll the possible widgets ( 127 of them on CRAN ) are listed here: https://gallery.htmlwidgets.org/\nSome packages that offer widgets for use in htmlwidgets:\n\n\nnetworkD3:\n\n\nForce directed networks with simpleNetwork and forceNetwork\n\nSankey diagrams with sankeyNetwork\n\nRadial networks with radialNetwork\n\nDendro networks with dendroNetwork\n\n\nUsing networkD3\n\n\nlibrary(networkD3)\ndata(\"karate\")\n\n# Make separate data frames for edges and nodes\n# networkD3 needs indices starting from 1\nkarate_edges &lt;- karate %&gt;%\n  as_tbl_graph() %&gt;%\n  tidygraph::activate(edges) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::mutate(source = from - 1, target = to - 1) %&gt;%\n  rename(value = weight) %&gt;%\n  select(source, target, value)\nkarate_edges\n\n\n  \n\n\nkarate_nodes &lt;- karate %&gt;%\n  as_tbl_graph() %&gt;%\n  tidygraph::activate(nodes) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::mutate(group = as.character(Faction)) %&gt;%\n  select(name, label, group, color)\nkarate_nodes\n\n\n  \n\n\nsimpleNetwork(karate_edges,\n  charge = -50,\n  fontSize = 12, zoom = FALSE,\n  fontFamily = \"serif\"\n)\n\n\n\n\n\n\n\nforceNetwork(\n  Links = karate_edges,\n  Nodes = karate_nodes,\n  Value = \"value\",\n  # width of edges, dbl\n  NodeID = \"name\",\n  # chr\n  Group = \"group\",\n  # Node group, chr\n  # Nodesize = \"label\" # chr !!!\n  # linkColour = \"value\"\n)\n\n\n\n\n\n\nCreating a Sankey Diagram.\n\n# Code is not working need to fix\n# No nodes showing up...\nUCB_graph &lt;-\n  UCBAdmissions %&gt;%\n  as.data.frame() %&gt;%\n  # select(Gender, Admit, Dept, Freq) %&gt;%\n  as_tbl_graph()\nUCB_graph\n\nUCB_nodes &lt;- UCB_graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\nUCB_nodes\n\nUCB_links &lt;- UCB_graph %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  dplyr::mutate(from = from - 1, to = to - 1)\nUCB_links\n\nsankeyNetwork(\n  Links = UCB_links,\n  Nodes = UCB_nodes,\n  Source = \"from\",\n  Target = \"to\",\n  Value = \"Freq\",\n  LinkGroup = \"Dept\",\n  fontSize = 20,\n  fontFamily = \"Arial\"\n)\n\nsimpleNetwork, forceNetwork and sankeyNetwork use a similar node and link data structure, organized as two data frames (not tibbles)\nchordNetwork uses an association matrix type of matrix or a data frame organized in the same way, where entry (n,m) represents the strength of the link from group n to group m. Matrix needs to be square !! “Column names” and “Row names” need to be the same if the data is a data.frame.\n\ndata &lt;- matrix(rpois(n = 16, lambda = 50),\n  nrow = 4, ncol = 4\n)\nchordNetwork(data,\n  labels = c(\"A\", \"B\", \"C\", \"D\")\n)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs-wip",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs-wip",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using threejs ( WIP )",
    "text": "Using threejs ( WIP )\n\nhttps://bwlewis.github.io/rthreejs/\nhttps://bwlewis.github.io/rthreejs/crosstalk.html\n\n\ngraphjs usage\n\ndata(\"LeMis\") # igraph object\nLeMis\n\nIGRAPH b7852d9 U--- 77 254 -- \n+ attr: label (v/c), color (v/c), size (v/n)\n+ edges from b7852d9:\n [1]  1-- 2  1-- 3  1-- 4  3-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--10\n[11] 11--12  4--12  3--12  1--12 12--13 12--14 12--15 12--16 17--18 17--19\n[21] 18--19 17--20 18--20 19--20 17--21 18--21 19--21 20--21 17--22 18--22\n[31] 19--22 20--22 21--22 17--23 18--23 19--23 20--23 21--23 22--23 17--24\n[41] 18--24 19--24 20--24 21--24 22--24 23--24 13--24 12--24 24--25 12--25\n[51] 25--26 24--26 12--26 25--27 12--27 17--27 26--27 12--28 24--28 26--28\n[61] 25--28 27--28 12--29 28--29 24--30 28--30 12--30 24--31 31--32 12--32\n[71] 24--32 28--32 12--33 12--34 28--34 12--35 30--35 12--36 35--36 30--36\n+ ... omitted several edges\n\n# V(LeMis)$label\n# V(LeMis)$color\ngraphjs(LeMis,\n  layout = list( # animates between a list of layouts\n    # layouts need to be 3D layouts\n    # Or each can be a 3 column matrix with n(rows) = n(vertices)\n    layout_randomly(LeMis, dim = 3),\n    layout_on_sphere(LeMis),\n    layout_with_drl(LeMis, dim = 3), # note! somewhat slow...\n    layout_with_fr(LeMis, dim = 3, niter = 30)\n  ),\n  main = list(\"random layout\", \"sphere layout\", \"drl layout\", \"fr layout\"),\n  fpl = 300\n)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nglobejs usage",
    "text": "globejs usage\nPlot points, arcs and images on a globe in 3D using three.js. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n\nglobejs(lat = lat, long = long)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n\n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)\n\n\n\n\n\n\n\n# Plotting arcs on the globe\n# Requires 4 column data frame for start and end lat and long\n\narcs &lt;- data.frame(\n  start_lat = runif(10, min = -20, max = 20),\n  start_lon = runif(10, min = -20, max = 20),\n  end_lat = runif(10, min = -20, max = 20),\n  end_lon = runif(10, min = -20, max = 20) + 60\n)\narcs\n\n\n  \n\n\nglobejs(\n  arcs = arcs,\n  arcsColor = \"gold\",\n  arcsLwd = 4,\n  arcsHeight = 0.6,\n  arcsOpacity = 1,\n  rotationlat = 0,\n  rotationlong = -2.2,\n  bg = \"lightblue\", atmosphere = FALSE,\n  pointsize = 2\n)\n\n\n\n\n\n\nthreejs contains a dataset called flights obtained from Callum Prentice’s FlightStream page: http://callumprentice.github.io/apps/flight_stream/index.html\n\nthreejs::flights() %&gt;% head()\n\n\n  \n\n\nfrequent_destinations &lt;-\n  threejs::flights() %&gt;%\n  dplyr::group_by(dest_lat, dest_long) %&gt;%\n  count(sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  slice_max(n = 10, order_by = n)\nfrequent_destinations\n\n\n  \n\n\nfrequent_flights &lt;- threejs::flights() %&gt;%\n  semi_join(frequent_destinations,\n    by = c(\"dest_lat\" = \"dest_lat\", \"dest_long\" = \"dest_long\")\n  ) %&gt;%\n  unique()\n\nfrequent_flights %&gt;%\n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\n\norigin_lat\norigin_long\ndest_lat\ndest_long\n\n\n\n1\n40.657633\n17.947033\n47.464722\n8.549167\n\n\n2\n44.828335\n-0.715556\n47.464722\n8.549167\n\n\n3\n51.382669\n-2.719089\n47.464722\n8.549167\n\n\n4\n34.875117\n33.624850\n47.464722\n8.549167\n\n\n5\n27.931886\n-15.386586\n47.464722\n8.549167\n\n\n6\n25.557111\n34.583711\n47.464722\n8.549167\n\n\n7\n40.783200\n-91.125500\n41.978603\n-87.904842\n\n\n8\n39.834564\n-88.865689\n41.978603\n-87.904842\n\n\n9\n13.681108\n100.747283\n1.350189\n103.994433\n\n\n10\n-12.408333\n130.872660\n1.350189\n103.994433\n\n\n11\n19.934856\n110.458961\n1.350189\n103.994433\n\n\n12\n30.229503\n120.434453\n1.350189\n103.994433\n\n\n13\n22.308919\n113.914603\n1.350189\n103.994433\n\n\n14\n8.113200\n98.316872\n1.350189\n103.994433\n\n\n15\n34.434722\n135.244167\n25.077731\n121.232822\n\n\n16\n2.745578\n101.709917\n1.350189\n103.994433\n\n\n17\n14.508647\n121.019581\n1.350189\n103.994433\n\n\n18\n5.297139\n100.276864\n1.350189\n103.994433\n\n\n19\n-31.940278\n115.966944\n1.350189\n103.994433\n\n\n20\n11.546556\n104.844139\n1.350189\n103.994433\n\n\n21\n13.410666\n103.812840\n1.350189\n103.994433\n\n\n22\n16.907305\n96.133222\n1.350189\n103.994433\n\n\n23\n10.818797\n106.651856\n1.350189\n103.994433\n\n\n24\n1.350189\n103.994433\n25.077731\n121.232822\n\n\n25\n51.132767\n13.767161\n47.464722\n8.549167\n\n\n26\n42.760277\n10.239445\n47.464722\n8.549167\n\n\n27\n46.991067\n15.439628\n47.464722\n8.549167\n\n\n28\n47.793304\n13.004333\n47.464722\n8.549167\n\n\n29\n21.539400\n109.294000\n34.447119\n108.751592\n\n\n30\n29.719217\n106.641678\n40.080111\n116.584556\n\n\n31\n29.719217\n106.641678\n31.143378\n121.805214\n\n\n32\n29.719217\n106.641678\n34.447119\n108.751592\n\n\n33\n30.578528\n103.947086\n40.080111\n116.584556\n\n\n34\n30.578528\n103.947086\n31.143378\n121.805214\n\n\n35\n30.578528\n103.947086\n34.447119\n108.751592\n\n\n36\n25.935064\n119.663272\n34.447119\n108.751592\n\n\n37\n34.490900\n102.371900\n34.447119\n108.751592\n\n\n38\n45.623403\n126.250328\n34.447119\n108.751592\n\n\n39\n24.992364\n102.743536\n25.077731\n121.232822\n\n\n40\n24.992364\n102.743536\n34.447119\n108.751592\n\n\n41\n29.297778\n90.911944\n34.447119\n108.751592\n\n\n42\n35.046100\n118.412000\n34.447119\n108.751592\n\n\n43\n36.857214\n117.215992\n34.447119\n108.751592\n\n\n44\n43.907106\n87.474244\n34.447119\n108.751592\n\n\n45\n38.269200\n109.731000\n34.447119\n108.751592\n\n\n46\n36.898731\n30.800461\n47.464722\n8.549167\n\n\n47\n27.178317\n33.799436\n47.464722\n8.549167\n\n\n48\n42.572778\n21.035833\n47.464722\n8.549167\n\n\n50\n41.961622\n21.621381\n47.464722\n8.549167\n\n\n51\n50.865917\n7.142744\n51.477500\n-0.461389\n\n\n52\n50.865917\n7.142744\n48.110278\n16.569722\n\n\n53\n50.865917\n7.142744\n47.464722\n8.549167\n\n\n54\n52.461056\n9.685078\n48.110278\n16.569722\n\n\n55\n53.630389\n9.988228\n51.477500\n-0.461389\n\n\n56\n53.630389\n9.988228\n48.110278\n16.569722\n\n\n57\n53.630389\n9.988228\n47.464722\n8.549167\n\n\n58\n48.689878\n9.221964\n48.110278\n16.569722\n\n\n59\n52.559686\n13.287711\n48.110278\n16.569722\n\n\n60\n10.307542\n123.979439\n1.350189\n103.994433\n\n\n61\n15.185833\n120.560278\n1.350189\n103.994433\n\n\n62\n10.713044\n122.545297\n1.350189\n103.994433\n\n\n63\n14.508647\n121.019581\n40.080111\n116.584556\n\n\n64\n14.508647\n121.019581\n31.143378\n121.805214\n\n\n66\n14.508647\n121.019581\n25.077731\n121.232822\n\n\n67\n12.994414\n80.180517\n1.350189\n103.994433\n\n\n68\n40.560000\n109.997000\n34.447119\n108.751592\n\n\n69\n31.780019\n117.298436\n31.143378\n121.805214\n\n\n70\n24.992364\n102.743536\n40.080111\n116.584556\n\n\n71\n24.992364\n102.743536\n31.143378\n121.805214\n\n\n73\n26.538522\n106.800703\n1.350189\n103.994433\n\n\n74\n26.883333\n100.233330\n34.447119\n108.751592\n\n\n75\n24.401100\n98.531700\n40.080111\n116.584556\n\n\n77\n5.937208\n116.051181\n31.143378\n121.805214\n\n\n78\n13.681108\n100.747283\n31.143378\n121.805214\n\n\n79\n43.541200\n125.120100\n31.143378\n121.805214\n\n\n80\n33.511306\n126.493028\n31.143378\n121.805214\n\n\n82\n18.766847\n98.962644\n31.143378\n121.805214\n\n\n83\n38.965667\n121.538600\n31.143378\n121.805214\n\n\n84\n29.102800\n110.443000\n31.143378\n121.805214\n\n\n85\n22.308919\n113.914603\n31.143378\n121.805214\n\n\n86\n8.113200\n98.316872\n31.143378\n121.805214\n\n\n87\n45.623403\n126.250328\n31.143378\n121.805214\n\n\n88\n33.149700\n130.302000\n31.143378\n121.805214\n\n\n89\n36.181083\n140.415444\n31.143378\n121.805214\n\n\n90\n22.577094\n120.350006\n31.143378\n121.805214\n\n\n91\n34.434722\n135.244167\n31.143378\n121.805214\n\n\n93\n25.218106\n110.039197\n31.143378\n121.805214\n\n\n94\n36.117000\n103.617000\n31.143378\n121.805214\n\n\n95\n22.149556\n113.591558\n31.143378\n121.805214\n\n\n96\n31.143378\n121.805214\n1.350189\n103.994433\n\n\n97\n31.143378\n121.805214\n25.077731\n121.232822\n\n\n98\n31.197875\n121.336319\n34.447119\n108.751592\n\n\n99\n41.382400\n123.290100\n34.447119\n108.751592\n\n\n100\n38.280686\n114.697300\n25.077731\n121.232822\n\n\n102\n22.006400\n113.376000\n31.143378\n121.805214\n\n\n103\n19.088686\n72.867919\n51.477500\n-0.461389\n\n\n104\n19.088686\n72.867919\n1.350189\n103.994433\n\n\n105\n50.901389\n4.484444\n43.677223\n-79.630556\n\n\n106\n28.566500\n77.103088\n51.477500\n-0.461389\n\n\n107\n28.566500\n77.103088\n1.350189\n103.994433\n\n\n109\n37.936358\n23.944467\n51.477500\n-0.461389\n\n\n110\n37.936358\n23.944467\n48.110278\n16.569722\n\n\n111\n35.339719\n25.180297\n48.110278\n16.569722\n\n\n112\n41.669167\n44.954722\n48.110278\n16.569722\n\n\n113\n35.040222\n-106.609194\n41.978603\n-87.904842\n\n\n114\n57.201944\n-2.197778\n51.477500\n-0.461389\n\n\n115\n5.605186\n-0.166786\n51.477500\n-0.461389\n\n\n116\n36.674900\n-4.499106\n51.477500\n-0.461389\n\n\n117\n42.557100\n-92.400300\n41.978603\n-87.904842\n\n\n118\n31.722556\n35.993214\n41.978603\n-87.904842\n\n\n119\n52.308613\n4.763889\n51.477500\n-0.461389\n\n\n120\n59.651944\n17.918611\n51.477500\n-0.461389\n\n\n121\n43.991922\n-76.021739\n41.978603\n-87.904842\n\n\n123\n33.636719\n-84.428067\n51.477500\n-0.461389\n\n\n124\n33.636719\n-84.428067\n41.978603\n-87.904842\n\n\n125\n24.432972\n54.651138\n51.477500\n-0.461389\n\n\n126\n24.432972\n54.651138\n41.978603\n-87.904842\n\n\n127\n30.194528\n-97.669889\n51.477500\n-0.461389\n\n\n128\n30.194528\n-97.669889\n41.978603\n-87.904842\n\n\n129\n42.234875\n-85.552058\n41.978603\n-87.904842\n\n\n130\n26.270834\n50.633610\n51.477500\n-0.461389\n\n\n131\n41.297078\n2.078464\n51.477500\n-0.461389\n\n\n132\n41.938889\n-72.683222\n41.978603\n-87.904842\n\n\n133\n60.293386\n5.218142\n51.477500\n-0.461389\n\n\n134\n54.618056\n-5.872500\n51.477500\n-0.461389\n\n\n135\n44.535444\n11.288667\n51.477500\n-0.461389\n\n\n136\n12.949986\n77.668206\n51.477500\n-0.461389\n\n\n137\n40.477111\n-88.915917\n41.978603\n-87.904842\n\n\n138\n36.124472\n-86.678194\n41.978603\n-87.904842\n\n\n140\n42.364347\n-71.005181\n51.477500\n-0.461389\n\n\n141\n42.364347\n-71.005181\n41.978603\n-87.904842\n\n\n142\n50.901389\n4.484444\n51.477500\n-0.461389\n\n\n143\n47.590000\n7.529167\n51.477500\n-0.461389\n\n\n144\n47.436933\n19.255592\n51.477500\n-0.461389\n\n\n145\n42.940525\n-78.732167\n41.978603\n-87.904842\n\n\n146\n39.175361\n-76.668333\n51.477500\n-0.461389\n\n\n147\n39.175361\n-76.668333\n41.978603\n-87.904842\n\n\n148\n49.012779\n2.550000\n51.477500\n-0.461389\n\n\n149\n49.012779\n2.550000\n41.978603\n-87.904842\n\n\n150\n35.035278\n-85.203808\n41.978603\n-87.904842\n\n\n151\n38.138639\n-78.452861\n41.978603\n-87.904842\n\n\n152\n41.884694\n-91.710806\n41.978603\n-87.904842\n\n\n153\n41.411689\n-81.849794\n41.978603\n-87.904842\n\n\n154\n35.214000\n-80.943139\n51.477500\n-0.461389\n\n\n155\n35.214000\n-80.943139\n41.978603\n-87.904842\n\n\n156\n35.214000\n-80.943139\n43.677223\n-79.630556\n\n\n157\n39.997972\n-82.891889\n41.978603\n-87.904842\n\n\n158\n40.039250\n-88.278056\n41.978603\n-87.904842\n\n\n159\n38.818094\n-92.219631\n41.978603\n-87.904842\n\n\n160\n55.617917\n12.655972\n51.477500\n-0.461389\n\n\n161\n-33.964806\n18.601667\n51.477500\n-0.461389\n\n\n162\n21.036528\n-86.877083\n41.978603\n-87.904842\n\n\n163\n39.048836\n-84.667822\n41.978603\n-87.904842\n\n\n164\n44.772726\n-89.646635\n41.978603\n-87.904842\n\n\n165\n39.902375\n-84.219375\n41.978603\n-87.904842\n\n\n166\n42.402000\n-90.709472\n41.978603\n-87.904842\n\n\n167\n38.852083\n-77.037722\n41.978603\n-87.904842\n\n\n168\n38.852083\n-77.037722\n43.677223\n-79.630556\n\n\n170\n39.861656\n-104.673178\n51.477500\n-0.461389\n\n\n171\n39.861656\n-104.673178\n41.978603\n-87.904842\n\n\n172\n32.896828\n-97.037997\n51.477500\n-0.461389\n\n\n173\n32.896828\n-97.037997\n41.978603\n-87.904842\n\n\n174\n32.896828\n-97.037997\n43.677223\n-79.630556\n\n\n175\n25.261125\n51.565056\n41.978603\n-87.904842\n\n\n176\n41.533972\n-93.663083\n41.978603\n-87.904842\n\n\n177\n42.212444\n-83.353389\n41.978603\n-87.904842\n\n\n178\n53.421333\n-6.270075\n51.477500\n-0.461389\n\n\n179\n53.421333\n-6.270075\n41.978603\n-87.904842\n\n\n180\n51.289453\n6.766775\n51.477500\n-0.461389\n\n\n181\n51.289453\n6.766775\n41.978603\n-87.904842\n\n\n182\n25.252778\n55.364444\n51.477500\n-0.461389\n\n\n183\n0.042386\n32.443503\n51.477500\n-0.461389\n\n\n184\n55.950000\n-3.372500\n51.477500\n-0.461389\n\n\n185\n31.807250\n-106.377583\n41.978603\n-87.904842\n\n\n186\n38.036997\n-87.532364\n41.978603\n-87.904842\n\n\n187\n40.692500\n-74.168667\n51.477500\n-0.461389\n\n\n188\n40.692500\n-74.168667\n41.978603\n-87.904842\n\n\n189\n46.920650\n-96.815764\n41.978603\n-87.904842\n\n\n190\n41.804475\n12.250797\n51.477500\n-0.461389\n\n\n191\n41.804475\n12.250797\n41.978603\n-87.904842\n\n\n192\n26.072583\n-80.152750\n41.978603\n-87.904842\n\n\n193\n42.965424\n-83.743629\n41.978603\n-87.904842\n\n\n194\n50.026421\n8.543125\n51.477500\n-0.461389\n\n\n195\n43.582014\n-96.741914\n41.978603\n-87.904842\n\n\n196\n40.978472\n-85.195139\n41.978603\n-87.904842\n\n\n197\n36.151219\n-5.349664\n51.477500\n-0.461389\n\n\n198\n55.871944\n-4.433056\n51.477500\n-0.461389\n\n\n199\n57.662836\n12.279819\n51.477500\n-0.461389\n\n\n200\n44.485072\n-88.129589\n41.978603\n-87.904842\n\n\n201\n42.880833\n-85.522806\n41.978603\n-87.904842\n\n\n202\n46.238064\n6.108950\n51.477500\n-0.461389\n\n\n203\n52.461056\n9.685078\n51.477500\n-0.461389\n\n\n205\n60.317222\n24.963333\n51.477500\n-0.461389\n\n\n206\n22.308919\n113.914603\n41.978603\n-87.904842\n\n\n207\n41.066959\n-73.707575\n41.978603\n-87.904842\n\n\n208\n34.637194\n-86.775056\n41.978603\n-87.904842\n\n\n209\n17.453117\n78.467586\n51.477500\n-0.461389\n\n\n210\n38.944533\n-77.455811\n51.477500\n-0.461389\n\n\n211\n29.984433\n-95.341442\n51.477500\n-0.461389\n\n\n212\n29.984433\n-95.341442\n41.978603\n-87.904842\n\n\n213\n37.649944\n-97.433056\n41.978603\n-87.904842\n\n\n214\n39.717331\n-86.294383\n41.978603\n-87.904842\n\n\n215\n40.976922\n28.814606\n51.477500\n-0.461389\n\n\n216\n30.494056\n-81.687861\n41.978603\n-87.904842\n\n\n217\n40.639751\n-73.778925\n51.477500\n-0.461389\n\n\n218\n40.639751\n-73.778925\n41.978603\n-87.904842\n\n\n219\n40.639751\n-73.778925\n43.677223\n-79.630556\n\n\n220\n40.639751\n-73.778925\n47.464722\n8.549167\n\n\n221\n-26.139166\n28.246000\n51.477500\n-0.461389\n\n\n222\n2.745578\n101.709917\n51.477500\n-0.461389\n\n\n224\n29.226567\n47.968928\n51.477500\n-0.461389\n\n\n225\n36.080056\n-115.152250\n51.477500\n-0.461389\n\n\n226\n36.080056\n-115.152250\n41.978603\n-87.904842\n\n\n227\n36.080056\n-115.152250\n43.677223\n-79.630556\n\n\n228\n33.942536\n-118.408075\n51.477500\n-0.461389\n\n\n229\n33.942536\n-118.408075\n41.978603\n-87.904842\n\n\n230\n33.942536\n-118.408075\n31.143378\n121.805214\n\n\n231\n33.942536\n-118.408075\n43.677223\n-79.630556\n\n\n232\n53.865897\n-1.660569\n51.477500\n-0.461389\n\n\n233\n34.875117\n33.624850\n51.477500\n-0.461389\n\n\n234\n38.036500\n-84.605889\n41.978603\n-87.904842\n\n\n235\n40.777245\n-73.872608\n41.978603\n-87.904842\n\n\n236\n40.777245\n-73.872608\n43.677223\n-79.630556\n\n\n237\n51.477500\n-0.461389\n41.978603\n-87.904842\n\n\n238\n51.477500\n-0.461389\n48.110278\n16.569722\n\n\n239\n51.477500\n-0.461389\n43.677223\n-79.630556\n\n\n240\n51.477500\n-0.461389\n47.464722\n8.549167\n\n\n241\n34.729444\n-92.224306\n41.978603\n-87.904842\n\n\n242\n43.878986\n-91.256711\n41.978603\n-87.904842\n\n\n243\n40.493556\n-3.566764\n41.978603\n-87.904842\n\n\n244\n53.353744\n-2.274950\n41.978603\n-87.904842\n\n\n245\n18.503717\n-77.913358\n41.978603\n-87.904842\n\n\n246\n39.297606\n-94.713905\n41.978603\n-87.904842\n\n\n247\n28.429394\n-81.308994\n41.978603\n-87.904842\n\n\n248\n28.429394\n-81.308994\n43.677223\n-79.630556\n\n\n249\n40.193494\n-76.763403\n41.978603\n-87.904842\n\n\n250\n35.042417\n-89.976667\n41.978603\n-87.904842\n\n\n251\n19.436303\n-99.072097\n41.978603\n-87.904842\n\n\n252\n39.140972\n-96.670833\n41.978603\n-87.904842\n\n\n253\n25.793250\n-80.290556\n41.978603\n-87.904842\n\n\n254\n25.793250\n-80.290556\n43.677223\n-79.630556\n\n\n255\n42.947222\n-87.896583\n41.978603\n-87.904842\n\n\n256\n41.448528\n-90.507539\n41.978603\n-87.904842\n\n\n257\n46.353611\n-87.395278\n41.978603\n-87.904842\n\n\n258\n43.139858\n-89.337514\n41.978603\n-87.904842\n\n\n259\n44.881956\n-93.221767\n41.978603\n-87.904842\n\n\n260\n29.993389\n-90.258028\n41.978603\n-87.904842\n\n\n261\n35.764722\n140.386389\n41.978603\n-87.904842\n\n\n262\n35.764722\n140.386389\n1.350189\n103.994433\n\n\n263\n35.764722\n140.386389\n25.077731\n121.232822\n\n\n264\n35.393089\n-97.600733\n41.978603\n-87.904842\n\n\n265\n41.303167\n-95.894069\n41.978603\n-87.904842\n\n\n266\n41.978603\n-87.904842\n40.080111\n116.584556\n\n\n267\n41.978603\n-87.904842\n31.143378\n121.805214\n\n\n268\n41.978603\n-87.904842\n43.677223\n-79.630556\n\n\n269\n39.871944\n-75.241139\n43.677223\n-79.630556\n\n\n270\n39.871944\n-75.241139\n47.464722\n8.549167\n\n\n271\n28.945464\n-13.605225\n47.464722\n8.549167\n\n\n272\n36.674900\n-4.499106\n48.110278\n16.569722\n\n\n273\n38.282169\n-0.558156\n47.464722\n8.549167\n\n\n274\n31.722556\n35.993214\n48.110278\n16.569722\n\n\n275\n59.651944\n17.918611\n48.110278\n16.569722\n\n\n276\n24.432972\n54.651138\n40.080111\n116.584556\n\n\n277\n24.432972\n54.651138\n1.350189\n103.994433\n\n\n278\n41.297078\n2.078464\n48.110278\n16.569722\n\n\n280\n-27.384167\n153.117500\n1.350189\n103.994433\n\n\n282\n49.012779\n2.550000\n48.110278\n16.569722\n\n\n283\n39.601944\n19.911667\n48.110278\n16.569722\n\n\n284\n35.531747\n24.149678\n48.110278\n16.569722\n\n\n285\n55.617917\n12.655972\n48.110278\n16.569722\n\n\n286\n37.466781\n15.066400\n47.464722\n8.549167\n\n\n288\n51.289453\n6.766775\n48.110278\n16.569722\n\n\n289\n51.289453\n6.766775\n47.464722\n8.549167\n\n\n290\n41.804475\n12.250797\n48.110278\n16.569722\n\n\n291\n43.809953\n11.205100\n48.110278\n16.569722\n\n\n292\n32.697889\n-16.774453\n48.110278\n16.569722\n\n\n293\n32.697889\n-16.774453\n47.464722\n8.549167\n\n\n295\n50.026421\n8.543125\n48.110278\n16.569722\n\n\n296\n28.452717\n-13.863761\n48.110278\n16.569722\n\n\n297\n28.452717\n-13.863761\n47.464722\n8.549167\n\n\n298\n54.913250\n8.340472\n47.464722\n8.549167\n\n\n303\n60.317222\n24.963333\n48.110278\n16.569722\n\n\n304\n60.317222\n24.963333\n47.464722\n8.549167\n\n\n306\n35.339719\n25.180297\n47.464722\n8.549167\n\n\n307\n38.872858\n1.373117\n47.464722\n8.549167\n\n\n308\n37.435128\n25.348103\n48.110278\n16.569722\n\n\n309\n36.399169\n25.479333\n48.110278\n16.569722\n\n\n310\n37.068319\n22.025525\n48.110278\n16.569722\n\n\n311\n34.875117\n33.624850\n48.110278\n16.569722\n\n\n312\n51.505278\n0.055278\n47.464722\n8.549167\n\n\n315\n27.931886\n-15.386586\n48.110278\n16.569722\n\n\n317\n40.493556\n-3.566764\n48.110278\n16.569722\n\n\n318\n40.493556\n-3.566764\n47.464722\n8.549167\n\n\n319\n35.857497\n14.477500\n48.110278\n16.569722\n\n\n320\n48.353783\n11.786086\n48.110278\n16.569722\n\n\n321\n45.630606\n8.728111\n48.110278\n16.569722\n\n\n322\n40.886033\n14.290781\n47.464722\n8.549167\n\n\n323\n43.658411\n7.215872\n48.110278\n16.569722\n\n\n324\n49.498700\n11.066897\n48.110278\n16.569722\n\n\n325\n40.898661\n9.517628\n48.110278\n16.569722\n\n\n326\n40.898661\n9.517628\n47.464722\n8.549167\n\n\n327\n39.553610\n2.727778\n48.110278\n16.569722\n\n\n328\n39.553610\n2.727778\n47.464722\n8.549167\n\n\n329\n38.925467\n20.765311\n48.110278\n16.569722\n\n\n330\n36.405419\n28.086192\n48.110278\n16.569722\n\n\n331\n40.519725\n22.970950\n48.110278\n16.569722\n\n\n332\n38.905394\n16.242269\n47.464722\n8.549167\n\n\n333\n47.793304\n13.004333\n48.110278\n16.569722\n\n\n334\n28.044475\n-16.572489\n48.110278\n16.569722\n\n\n335\n28.044475\n-16.572489\n47.464722\n8.549167\n\n\n337\n52.559686\n13.287711\n47.464722\n8.549167\n\n\n338\n48.110278\n16.569722\n47.464722\n8.549167\n\n\n339\n17.136749\n-61.792667\n43.677223\n-79.630556\n\n\n340\n33.636719\n-84.428067\n43.677223\n-79.630556\n\n\n341\n12.501389\n-70.015221\n43.677223\n-79.630556\n\n\n342\n24.432972\n54.651138\n43.677223\n-79.630556\n\n\n343\n19.267000\n-69.742000\n43.677223\n-79.630556\n\n\n344\n41.297078\n2.078464\n43.677223\n-79.630556\n\n\n345\n32.364042\n-64.678703\n43.677223\n-79.630556\n\n\n346\n41.938889\n-72.683222\n43.677223\n-79.630556\n\n\n347\n13.074603\n-59.492456\n43.677223\n-79.630556\n\n\n348\n36.124472\n-86.678194\n43.677223\n-79.630556\n\n\n349\n4.701594\n-74.146947\n43.677223\n-79.630556\n\n\n350\n42.364347\n-71.005181\n43.677223\n-79.630556\n\n\n352\n39.175361\n-76.668333\n43.677223\n-79.630556\n\n\n353\n22.513200\n-78.511000\n43.677223\n-79.630556\n\n\n354\n49.012779\n2.550000\n43.677223\n-79.630556\n\n\n355\n41.411689\n-81.849794\n43.677223\n-79.630556\n\n\n357\n39.997972\n-82.891889\n43.677223\n-79.630556\n\n\n358\n55.617917\n12.655972\n43.677223\n-79.630556\n\n\n359\n21.036528\n-86.877083\n43.677223\n-79.630556\n\n\n360\n39.048836\n-84.667822\n43.677223\n-79.630556\n\n\n362\n39.861656\n-104.673178\n43.677223\n-79.630556\n\n\n364\n42.212444\n-83.353389\n43.677223\n-79.630556\n\n\n365\n53.421333\n-6.270075\n43.677223\n-79.630556\n\n\n366\n40.692500\n-74.168667\n43.677223\n-79.630556\n\n\n367\n41.804475\n12.250797\n43.677223\n-79.630556\n\n\n368\n26.072583\n-80.152750\n43.677223\n-79.630556\n\n\n369\n50.026421\n8.543125\n43.677223\n-79.630556\n\n\n370\n19.292778\n-81.357750\n43.677223\n-79.630556\n\n\n371\n23.562631\n-75.877958\n43.677223\n-79.630556\n\n\n372\n12.004247\n-61.786192\n43.677223\n-79.630556\n\n\n373\n-23.432075\n-46.469511\n43.677223\n-79.630556\n\n\n374\n22.989153\n-82.409086\n43.677223\n-79.630556\n\n\n375\n22.308919\n113.914603\n43.677223\n-79.630556\n\n\n376\n20.785589\n-76.315108\n43.677223\n-79.630556\n\n\n377\n38.944533\n-77.455811\n43.677223\n-79.630556\n\n\n378\n29.984433\n-95.341442\n43.677223\n-79.630556\n\n\n379\n39.717331\n-86.294383\n43.677223\n-79.630556\n\n\n380\n40.976922\n28.814606\n43.677223\n-79.630556\n\n\n382\n17.935667\n-76.787500\n43.677223\n-79.630556\n\n\n387\n-12.021889\n-77.114319\n43.677223\n-79.630556\n\n\n388\n10.593289\n-85.544408\n43.677223\n-79.630556\n\n\n389\n18.503717\n-77.913358\n43.677223\n-79.630556\n\n\n390\n39.297606\n-94.713905\n43.677223\n-79.630556\n\n\n392\n40.193494\n-76.763403\n43.677223\n-79.630556\n\n\n393\n19.436303\n-99.072097\n43.677223\n-79.630556\n\n\n395\n42.947222\n-87.896583\n43.677223\n-79.630556\n\n\n396\n44.881956\n-93.221767\n43.677223\n-79.630556\n\n\n398\n29.993389\n-90.258028\n43.677223\n-79.630556\n\n\n399\n48.353783\n11.786086\n43.677223\n-79.630556\n\n\n400\n25.038958\n-77.466231\n43.677223\n-79.630556\n\n\n401\n35.764722\n140.386389\n43.677223\n-79.630556\n\n\n403\n40.080111\n116.584556\n43.677223\n-79.630556\n\n\n405\n33.434278\n-112.011583\n43.677223\n-79.630556\n\n\n406\n40.491467\n-80.232872\n43.677223\n-79.630556\n\n\n407\n21.773625\n-72.265886\n43.677223\n-79.630556\n\n\n408\n19.757900\n-70.570033\n43.677223\n-79.630556\n\n\n409\n18.567367\n-68.363431\n43.677223\n-79.630556\n\n\n410\n31.143378\n121.805214\n43.677223\n-79.630556\n\n\n411\n35.877639\n-78.787472\n43.677223\n-79.630556\n\n\n412\n43.118866\n-77.672389\n43.677223\n-79.630556\n\n\n413\n26.536167\n-81.755167\n43.677223\n-79.630556\n\n\n414\n32.733556\n-117.189667\n43.677223\n-79.630556\n\n\n415\n-33.392975\n-70.785803\n43.677223\n-79.630556\n\n\n416\n47.449000\n-122.309306\n43.677223\n-79.630556\n\n\n417\n37.618972\n-122.374889\n43.677223\n-79.630556\n\n\n418\n9.993861\n-84.208806\n43.677223\n-79.630556\n\n\n419\n22.492192\n-79.943611\n43.677223\n-79.630556\n\n\n420\n38.748697\n-90.370028\n43.677223\n-79.630556\n\n\n421\n43.111187\n-76.106311\n43.677223\n-79.630556\n\n\n422\n32.011389\n34.886667\n43.677223\n-79.630556\n\n\n423\n27.975472\n-82.533250\n43.677223\n-79.630556\n\n\n424\n13.733194\n-60.952597\n43.677223\n-79.630556\n\n\n425\n48.110278\n16.569722\n43.677223\n-79.630556\n\n\n426\n23.034445\n-81.435278\n43.677223\n-79.630556\n\n\n427\n52.165750\n20.967122\n43.677223\n-79.630556\n\n\n428\n46.485001\n-84.509445\n43.677223\n-79.630556\n\n\n429\n49.210833\n-57.391388\n43.677223\n-79.630556\n\n\n430\n53.309723\n-113.579722\n43.677223\n-79.630556\n\n\n431\n45.868889\n-66.537222\n43.677223\n-79.630556\n\n\n432\n44.225277\n-76.596944\n43.677223\n-79.630556\n\n\n433\n44.880833\n-63.508610\n43.677223\n-79.630556\n\n\n434\n56.653333\n-111.221944\n43.677223\n-79.630556\n\n\n435\n45.322500\n-75.669167\n43.677223\n-79.630556\n\n\n436\n46.791111\n-71.393333\n43.677223\n-79.630556\n\n\n437\n42.275556\n-82.955556\n43.677223\n-79.630556\n\n\n438\n46.112221\n-64.678611\n43.677223\n-79.630556\n\n\n439\n50.431944\n-104.665833\n43.677223\n-79.630556\n\n\n440\n48.371944\n-89.323889\n43.677223\n-79.630556\n\n\n441\n46.161388\n-60.047779\n43.677223\n-79.630556\n\n\n442\n46.625000\n-80.798889\n43.677223\n-79.630556\n\n\n443\n45.316111\n-65.890278\n43.677223\n-79.630556\n\n\n444\n48.569721\n-81.376667\n43.677223\n-79.630556\n\n\n445\n45.470556\n-73.740833\n43.677223\n-79.630556\n\n\n446\n45.470556\n-73.740833\n47.464722\n8.549167\n\n\n447\n49.193889\n-123.184444\n43.677223\n-79.630556\n\n\n448\n49.910036\n-97.239886\n43.677223\n-79.630556\n\n\n449\n52.170834\n-106.699722\n43.677223\n-79.630556\n\n\n450\n43.033056\n-81.151111\n43.677223\n-79.630556\n\n\n451\n46.363611\n-79.422778\n43.677223\n-79.630556\n\n\n452\n51.113888\n-114.020278\n43.677223\n-79.630556\n\n\n453\n46.290001\n-63.121111\n43.677223\n-79.630556\n\n\n454\n48.646944\n-123.425833\n43.677223\n-79.630556\n\n\n455\n47.618610\n-52.751945\n43.677223\n-79.630556\n\n\n456\n43.677223\n-79.630556\n47.464722\n8.549167\n\n\n457\n34.519672\n113.840889\n25.077731\n121.232822\n\n\n458\n28.189158\n113.219633\n25.077731\n121.232822\n\n\n459\n26.883333\n100.233330\n25.077731\n121.232822\n\n\n460\n29.826683\n121.461906\n25.077731\n121.232822\n\n\n461\n31.742042\n118.862025\n25.077731\n121.232822\n\n\n462\n41.382400\n123.290100\n25.077731\n121.232822\n\n\n463\n52.308613\n4.763889\n25.077731\n121.232822\n\n\n468\n23.392436\n113.298786\n1.350189\n103.994433\n\n\n471\n49.012779\n2.550000\n40.080111\n116.584556\n\n\n472\n49.012779\n2.550000\n31.143378\n121.805214\n\n\n473\n49.012779\n2.550000\n1.350189\n103.994433\n\n\n476\n49.012779\n2.550000\n47.464722\n8.549167\n\n\n477\n42.212444\n-83.353389\n51.477500\n-0.461389\n\n\n479\n45.726387\n5.090833\n48.110278\n16.569722\n\n\n481\n36.691014\n3.215408\n51.477500\n-0.461389\n\n\n482\n36.691014\n3.215408\n40.080111\n116.584556\n\n\n483\n36.691014\n3.215408\n48.110278\n16.569722\n\n\n486\n19.088686\n72.867919\n47.464722\n8.549167\n\n\n487\n22.654739\n88.446722\n1.350189\n103.994433\n\n\n489\n28.566500\n77.103088\n41.978603\n-87.904842\n\n\n490\n28.566500\n77.103088\n31.143378\n121.805214\n\n\n492\n28.566500\n77.103088\n48.110278\n16.569722\n\n\n493\n28.566500\n77.103088\n47.464722\n8.549167\n\n\n495\n37.469075\n126.450517\n1.350189\n103.994433\n\n\n500\n5.937208\n116.051181\n1.350189\n103.994433\n\n\n501\n5.937208\n116.051181\n25.077731\n121.232822\n\n\n502\n6.166850\n102.293014\n1.350189\n103.994433\n\n\n503\n1.484697\n110.346933\n1.350189\n103.994433\n\n\n505\n6.329728\n99.728667\n1.350189\n103.994433\n\n\n506\n4.322014\n113.986806\n1.350189\n103.994433\n\n\n508\n20.521800\n-103.311167\n41.978603\n-87.904842\n\n\n510\n37.466781\n15.066400\n48.110278\n16.569722\n\n\n511\n61.174361\n-149.996361\n41.978603\n-87.904842\n\n\n514\n33.367467\n-7.589967\n51.477500\n-0.461389\n\n\n515\n33.367467\n-7.589967\n47.464722\n8.549167\n\n\n518\n13.440947\n-89.055728\n43.677223\n-79.630556\n\n\n535\n60.317222\n24.963333\n40.080111\n116.584556\n\n\n536\n60.317222\n24.963333\n31.143378\n121.805214\n\n\n537\n60.317222\n24.963333\n1.350189\n103.994433\n\n\n539\n60.317222\n24.963333\n34.447119\n108.751592\n\n\n551\n41.248055\n-8.681389\n47.464722\n8.549167\n\n\n557\n28.566500\n77.103088\n25.077731\n121.232822\n\n\n560\n41.804475\n12.250797\n31.143378\n121.805214\n\n\n563\n41.804475\n12.250797\n47.464722\n8.549167\n\n\n565\n45.445103\n9.276739\n48.110278\n16.569722\n\n\n566\n53.882469\n28.030731\n48.110278\n16.569722\n\n\n569\n29.719217\n106.641678\n25.077731\n121.232822\n\n\n570\n38.965667\n121.538600\n25.077731\n121.232822\n\n\n571\n25.935064\n119.663272\n25.077731\n121.232822\n\n\n575\n22.639258\n113.810664\n25.077731\n121.232822\n\n\n576\n36.266108\n120.374436\n25.077731\n121.232822\n\n\n577\n25.077731\n121.232822\n34.447119\n108.751592\n\n\n578\n9.006792\n7.263172\n51.477500\n-0.461389\n\n\n582\n43.352072\n77.040508\n51.477500\n-0.461389\n\n\n583\n31.722556\n35.993214\n51.477500\n-0.461389\n\n\n593\n33.820931\n35.488389\n51.477500\n-0.461389\n\n\n596\n13.681108\n100.747283\n51.477500\n-0.461389\n\n\n597\n55.740322\n9.151778\n47.464722\n8.549167\n\n\n606\n30.121944\n31.405556\n51.477500\n-0.461389\n\n\n611\n30.578528\n103.947086\n51.477500\n-0.461389\n\n\n615\n55.408611\n37.906111\n51.477500\n-0.461389\n\n\n624\n-34.822222\n-58.535833\n51.477500\n-0.461389\n\n\n625\n37.014425\n-7.965911\n51.477500\n-0.461389\n\n\n628\n8.616444\n-13.195489\n51.477500\n-0.461389\n\n\n631\n-22.808903\n-43.243647\n51.477500\n-0.461389\n\n\n634\n-23.432075\n-46.469511\n51.477500\n-0.461389\n\n\n636\n40.467500\n50.046667\n51.477500\n-0.461389\n\n\n640\n22.308919\n113.914603\n51.477500\n-0.461389\n\n\n641\n35.552258\n139.779694\n51.477500\n-0.461389\n\n\n645\n38.872858\n1.373117\n51.477500\n-0.461389\n\n\n646\n37.469075\n126.450517\n51.477500\n-0.461389\n\n\n648\n21.679564\n39.156536\n51.477500\n-0.461389\n\n\n650\n37.435128\n25.348103\n51.477500\n-0.461389\n\n\n652\n36.399169\n25.479333\n51.477500\n-0.461389\n\n\n653\n50.345000\n30.894722\n51.477500\n-0.461389\n\n\n655\n-8.858375\n13.231178\n51.477500\n-0.461389\n\n\n661\n59.800292\n30.262503\n51.477500\n-0.461389\n\n\n663\n51.477500\n-0.461389\n40.080111\n116.584556\n\n\n664\n51.477500\n-0.461389\n31.143378\n121.805214\n\n\n665\n51.477500\n-0.461389\n1.350189\n103.994433\n\n\n672\n23.843333\n90.397781\n51.477500\n-0.461389\n\n\n673\n23.843333\n90.397781\n1.350189\n103.994433\n\n\n674\n4.944200\n114.928353\n31.143378\n121.805214\n\n\n675\n4.944200\n114.928353\n1.350189\n103.994433\n\n\n676\n24.550560\n55.103174\n51.477500\n-0.461389\n\n\n677\n28.189158\n113.219633\n34.447119\n108.751592\n\n\n678\n22.608267\n108.172442\n34.447119\n108.751592\n\n\n679\n39.124353\n117.346183\n34.447119\n108.751592\n\n\n681\n43.670833\n142.447500\n25.077731\n121.232822\n\n\n683\n13.681108\n100.747283\n25.077731\n121.232822\n\n\n684\n13.681108\n100.747283\n48.110278\n16.569722\n\n\n685\n-27.384167\n153.117500\n25.077731\n121.232822\n\n\n686\n23.392436\n113.298786\n25.077731\n121.232822\n\n\n687\n49.012779\n2.550000\n25.077731\n121.232822\n\n\n688\n-6.125567\n106.655897\n25.077731\n121.232822\n\n\n690\n42.775200\n141.692283\n25.077731\n121.232822\n\n\n691\n30.578528\n103.947086\n25.077731\n121.232822\n\n\n692\n-8.748169\n115.167172\n25.077731\n121.232822\n\n\n693\n33.585942\n130.450686\n25.077731\n121.232822\n\n\n694\n13.483450\n144.795983\n25.077731\n121.232822\n\n\n695\n21.221192\n105.807178\n25.077731\n121.232822\n\n\n696\n40.851422\n111.824103\n25.077731\n121.232822\n\n\n697\n30.229503\n120.434453\n25.077731\n121.232822\n\n\n698\n41.770000\n140.821944\n25.077731\n121.232822\n\n\n699\n22.308919\n113.914603\n25.077731\n121.232822\n\n\n700\n45.623403\n126.250328\n25.077731\n121.232822\n\n\n701\n37.469075\n126.450517\n25.077731\n121.232822\n\n\n702\n40.639751\n-73.778925\n25.077731\n121.232822\n\n\n705\n36.394611\n136.406544\n25.077731\n121.232822\n\n\n706\n2.745578\n101.709917\n25.077731\n121.232822\n\n\n707\n25.218106\n110.039197\n25.077731\n121.232822\n\n\n708\n33.942536\n-118.408075\n25.077731\n121.232822\n\n\n709\n22.149556\n113.591558\n25.077731\n121.232822\n\n\n712\n34.756944\n133.855278\n25.077731\n121.232822\n\n\n713\n40.080111\n116.584556\n25.077731\n121.232822\n\n\n714\n11.546556\n104.844139\n25.077731\n121.232822\n\n\n716\n38.139722\n140.916944\n25.077731\n121.232822\n\n\n717\n47.449000\n-122.309306\n25.077731\n121.232822\n\n\n718\n37.618972\n-122.374889\n25.077731\n121.232822\n\n\n719\n10.818797\n106.651856\n25.077731\n121.232822\n\n\n721\n-7.379831\n112.786858\n25.077731\n121.232822\n\n\n722\n36.857214\n117.215992\n25.077731\n121.232822\n\n\n723\n25.077731\n121.232822\n43.677223\n-79.630556\n\n\n724\n56.923611\n23.971111\n48.110278\n16.569722\n\n\n725\n56.923611\n23.971111\n47.464722\n8.549167\n\n\n726\n6.498553\n-58.254119\n43.677223\n-79.630556\n\n\n729\n10.595369\n-61.337242\n43.677223\n-79.630556\n\n\n730\n35.179528\n128.938222\n25.077731\n121.232822\n\n\n731\n35.179528\n128.938222\n34.447119\n108.751592\n\n\n732\n8.977889\n38.799319\n40.080111\n116.584556\n\n\n733\n-37.008056\n174.791667\n31.143378\n121.805214\n\n\n734\n59.651944\n17.918611\n40.080111\n116.584556\n\n\n735\n40.560000\n109.997000\n40.080111\n116.584556\n\n\n737\n21.539400\n109.294000\n40.080111\n116.584556\n\n\n738\n13.681108\n100.747283\n40.080111\n116.584556\n\n\n739\n30.121944\n31.405556\n40.080111\n116.584556\n\n\n740\n23.392436\n113.298786\n40.080111\n116.584556\n\n\n741\n23.392436\n113.298786\n31.143378\n121.805214\n\n\n742\n23.392436\n113.298786\n34.447119\n108.751592\n\n\n745\n34.519672\n113.840889\n40.080111\n116.584556\n\n\n746\n43.541200\n125.120100\n40.080111\n116.584556\n\n\n747\n41.538100\n120.435000\n40.080111\n116.584556\n\n\n748\n42.235000\n118.908000\n40.080111\n116.584556\n\n\n749\n36.716600\n127.499119\n40.080111\n116.584556\n\n\n754\n18.766847\n98.962644\n40.080111\n116.584556\n\n\n755\n55.617917\n12.655972\n40.080111\n116.584556\n\n\n756\n28.189158\n113.219633\n40.080111\n116.584556\n\n\n757\n42.775200\n141.692283\n40.080111\n116.584556\n\n\n760\n30.578528\n103.947086\n1.350189\n103.994433\n\n\n763\n31.941667\n119.711667\n40.080111\n116.584556\n\n\n764\n40.060300\n113.482000\n40.080111\n116.584556\n\n\n765\n31.300000\n107.500000\n40.080111\n116.584556\n\n\n766\n40.025500\n124.286600\n40.080111\n116.584556\n\n\n767\n28.566500\n77.103088\n40.080111\n116.584556\n\n\n768\n38.965667\n121.538600\n40.080111\n116.584556\n\n\n770\n38.965667\n121.538600\n34.447119\n108.751592\n\n\n771\n39.850000\n110.033000\n40.080111\n116.584556\n\n\n772\n51.289453\n6.766775\n40.080111\n116.584556\n\n\n773\n25.252778\n55.364444\n40.080111\n116.584556\n\n\n774\n29.102800\n110.443000\n40.080111\n116.584556\n\n\n775\n40.692500\n-74.168667\n40.080111\n116.584556\n\n\n776\n40.692500\n-74.168667\n31.143378\n121.805214\n\n\n777\n41.804475\n12.250797\n40.080111\n116.584556\n\n\n778\n39.224061\n125.670150\n40.080111\n116.584556\n\n\n779\n25.935064\n119.663272\n40.080111\n116.584556\n\n\n780\n50.026421\n8.543125\n40.080111\n116.584556\n\n\n781\n50.026421\n8.543125\n31.143378\n121.805214\n\n\n782\n32.900000\n115.816667\n40.080111\n116.584556\n\n\n783\n33.585942\n130.450686\n31.143378\n121.805214\n\n\n784\n37.558311\n126.790586\n40.080111\n116.584556\n\n\n785\n46.238064\n6.108950\n40.080111\n116.584556\n\n\n786\n32.391100\n105.702000\n40.080111\n116.584556\n\n\n787\n19.934856\n110.458961\n40.080111\n116.584556\n\n\n790\n40.851422\n111.824103\n40.080111\n116.584556\n\n\n791\n40.851422\n111.824103\n31.143378\n121.805214\n\n\n793\n31.780019\n117.298436\n40.080111\n116.584556\n\n\n794\n30.229503\n120.434453\n40.080111\n116.584556\n\n\n796\n30.229503\n120.434453\n34.447119\n108.751592\n\n\n797\n22.308919\n113.914603\n40.080111\n116.584556\n\n\n798\n8.113200\n98.316872\n40.080111\n116.584556\n\n\n799\n49.204997\n119.825000\n40.080111\n116.584556\n\n\n800\n46.083000\n122.017000\n40.080111\n116.584556\n\n\n801\n42.841400\n93.669200\n40.080111\n116.584556\n\n\n802\n35.552258\n139.779694\n40.080111\n116.584556\n\n\n803\n21.318681\n-157.922428\n40.080111\n116.584556\n\n\n804\n45.623403\n126.250328\n40.080111\n116.584556\n\n\n806\n28.562200\n121.429000\n40.080111\n116.584556\n\n\n807\n38.944533\n-77.455811\n40.080111\n116.584556\n\n\n808\n29.984433\n-95.341442\n40.080111\n116.584556\n\n\n809\n37.469075\n126.450517\n40.080111\n116.584556\n\n\n810\n38.481944\n106.009167\n40.080111\n116.584556\n\n\n811\n38.481944\n106.009167\n31.143378\n121.805214\n\n\n812\n38.481944\n106.009167\n34.447119\n108.751592\n\n\n813\n40.976922\n28.814606\n40.080111\n116.584556\n\n\n814\n40.976922\n28.814606\n31.143378\n121.805214\n\n\n815\n29.338600\n117.176000\n40.080111\n116.584556\n\n\n816\n29.338600\n117.176000\n34.447119\n108.751592\n\n\n817\n40.639751\n-73.778925\n40.080111\n116.584556\n\n\n818\n26.899700\n114.737500\n40.080111\n116.584556\n\n\n819\n29.733000\n115.983000\n40.080111\n116.584556\n\n\n820\n24.796400\n118.590000\n31.143378\n121.805214\n\n\n821\n46.843394\n130.465389\n40.080111\n116.584556\n\n\n822\n32.857000\n103.683000\n31.143378\n121.805214\n\n\n823\n28.865000\n115.900000\n40.080111\n116.584556\n\n\n824\n28.865000\n115.900000\n31.143378\n121.805214\n\n\n825\n34.434722\n135.244167\n40.080111\n116.584556\n\n\n829\n25.825800\n114.912000\n40.080111\n116.584556\n\n\n830\n26.538522\n106.800703\n40.080111\n116.584556\n\n\n831\n26.538522\n106.800703\n31.143378\n121.805214\n\n\n832\n26.538522\n106.800703\n34.447119\n108.751592\n\n\n833\n25.218106\n110.039197\n40.080111\n116.584556\n\n\n835\n25.218106\n110.039197\n34.447119\n108.751592\n\n\n836\n33.942536\n-118.408075\n40.080111\n116.584556\n\n\n838\n51.148056\n-0.190278\n40.080111\n116.584556\n\n\n841\n36.117000\n103.617000\n40.080111\n116.584556\n\n\n842\n36.117000\n103.617000\n34.447119\n108.751592\n\n\n843\n26.883333\n100.233330\n40.080111\n116.584556\n\n\n844\n29.297778\n90.911944\n40.080111\n116.584556\n\n\n845\n24.207500\n109.391000\n40.080111\n116.584556\n\n\n846\n40.493556\n-3.566764\n40.080111\n116.584556\n\n\n847\n44.523889\n129.568889\n40.080111\n116.584556\n\n\n848\n-37.673333\n144.843333\n31.143378\n121.805214\n\n\n849\n22.149556\n113.591558\n40.080111\n116.584556\n\n\n851\n31.428100\n104.741000\n40.080111\n116.584556\n\n\n853\n48.353783\n11.786086\n40.080111\n116.584556\n\n\n854\n48.353783\n11.786086\n31.143378\n121.805214\n\n\n855\n34.991389\n126.382778\n40.080111\n116.584556\n\n\n856\n45.630606\n8.728111\n40.080111\n116.584556\n\n\n857\n45.630606\n8.728111\n31.143378\n121.805214\n\n\n858\n47.239628\n123.918131\n40.080111\n116.584556\n\n\n859\n29.826683\n121.461906\n40.080111\n116.584556\n\n\n860\n34.858414\n136.805408\n31.143378\n121.805214\n\n\n861\n31.742042\n118.862025\n40.080111\n116.584556\n\n\n862\n31.742042\n118.862025\n34.447119\n108.751592\n\n\n863\n22.608267\n108.172442\n40.080111\n116.584556\n\n\n865\n35.764722\n140.386389\n40.080111\n116.584556\n\n\n866\n35.764722\n140.386389\n31.143378\n121.805214\n\n\n867\n32.070800\n120.976000\n40.080111\n116.584556\n\n\n868\n26.195814\n127.645869\n40.080111\n116.584556\n\n\n871\n40.080111\n116.584556\n31.143378\n121.805214\n\n\n872\n40.080111\n116.584556\n1.350189\n103.994433\n\n\n874\n40.080111\n116.584556\n48.110278\n16.569722\n\n\n875\n40.080111\n116.584556\n34.447119\n108.751592\n\n\n878\n31.143378\n121.805214\n34.447119\n108.751592\n\n\n881\n22.639258\n113.810664\n34.447119\n108.751592\n\n\n882\n36.266108\n120.374436\n34.447119\n108.751592\n\n\n886\n27.912200\n120.852000\n34.447119\n108.751592\n\n\n887\n31.494400\n120.429000\n34.447119\n108.751592\n\n\n898\n50.026421\n8.543125\n25.077731\n121.232822\n\n\n899\n34.796111\n138.189444\n25.077731\n121.232822\n\n\n902\n19.934856\n110.458961\n25.077731\n121.232822\n\n\n904\n34.436111\n132.919444\n25.077731\n121.232822\n\n\n906\n21.318681\n-157.922428\n25.077731\n121.232822\n\n\n908\n24.344525\n124.186983\n25.077731\n121.232822\n\n\n909\n22.577094\n120.350006\n40.080111\n116.584556\n\n\n911\n22.577094\n120.350006\n1.350189\n103.994433\n\n\n912\n28.865000\n115.900000\n25.077731\n121.232822\n\n\n914\n31.877222\n131.448611\n25.077731\n121.232822\n\n\n915\n31.803397\n130.719408\n25.077731\n121.232822\n\n\n919\n34.858414\n136.805408\n25.077731\n121.232822\n\n\n921\n26.195814\n127.645869\n25.077731\n121.232822\n\n\n923\n5.297139\n100.276864\n25.077731\n121.232822\n\n\n927\n16.907305\n96.133222\n25.077731\n121.232822\n\n\n928\n7.367303\n134.544278\n25.077731\n121.232822\n\n\n932\n-33.946111\n151.177222\n25.077731\n121.232822\n\n\n933\n18.302897\n109.412272\n25.077731\n121.232822\n\n\n935\n34.214167\n134.015556\n25.077731\n121.232822\n\n\n937\n36.648333\n137.187500\n25.077731\n121.232822\n\n\n938\n25.077731\n121.232822\n48.110278\n16.569722\n\n\n940\n9.071364\n-79.383453\n43.677223\n-79.630556\n\n\n946\n21.420428\n-77.847433\n43.677223\n-79.630556\n\n\n950\n7.180756\n79.884117\n1.350189\n103.994433\n\n\n958\n22.308919\n113.914603\n34.447119\n108.751592\n\n\n966\n52.308613\n4.763889\n40.080111\n116.584556\n\n\n967\n52.308613\n4.763889\n31.143378\n121.805214\n\n\n968\n23.392436\n113.298786\n51.477500\n-0.461389\n\n\n974\n28.918900\n111.640000\n40.080111\n116.584556\n\n\n976\n34.519672\n113.840889\n31.143378\n121.805214\n\n\n980\n43.541200\n125.120100\n25.077731\n121.232822\n\n\n981\n36.247500\n113.126000\n40.080111\n116.584556\n\n\n985\n28.189158\n113.219633\n31.143378\n121.805214\n\n\n990\n40.025500\n124.286600\n31.143378\n121.805214\n\n\n995\n40.094000\n94.481800\n34.447119\n108.751592\n\n\n996\n42.212444\n-83.353389\n40.080111\n116.584556\n\n\n997\n42.212444\n-83.353389\n31.143378\n121.805214\n\n\n999\n29.102800\n110.443000\n25.077731\n121.232822\n\n\n1001\n25.935064\n119.663272\n1.350189\n103.994433\n\n\n1005\n19.934856\n110.458961\n31.143378\n121.805214\n\n\n1006\n21.221192\n105.807178\n40.080111\n116.584556\n\n\n1007\n31.780019\n117.298436\n34.447119\n108.751592\n\n\n1015\n29.934200\n122.362000\n40.080111\n116.584556\n\n\n1017\n37.469075\n126.450517\n31.143378\n121.805214\n\n\n1020\n33.616653\n73.099233\n40.080111\n116.584556\n\n\n1021\n24.796400\n118.590000\n40.080111\n116.584556\n\n\n1022\n46.843394\n130.465389\n31.143378\n121.805214\n\n\n1023\n28.865000\n115.900000\n34.447119\n108.751592\n\n\n1027\n2.745578\n101.709917\n31.143378\n121.805214\n\n\n1030\n26.538522\n106.800703\n25.077731\n121.232822\n\n\n1037\n44.523889\n129.568889\n31.143378\n121.805214\n\n\n1038\n30.754000\n106.062000\n40.080111\n116.584556\n\n\n1039\n42.088056\n127.548889\n40.080111\n116.584556\n\n\n1040\n47.239628\n123.918131\n31.143378\n121.805214\n\n\n1045\n22.608267\n108.172442\n25.077731\n121.232822\n\n\n1054\n18.302897\n109.412272\n34.447119\n108.751592\n\n\n1060\n37.746897\n112.628428\n34.447119\n108.751592\n\n\n1064\n30.783758\n114.208100\n34.447119\n108.751592\n\n\n1065\n30.836100\n108.406000\n34.447119\n108.751592\n\n\n1067\n2.745578\n101.709917\n40.080111\n116.584556\n\n\n1071\n11.679431\n122.376294\n1.350189\n103.994433\n\n\n1072\n52.308613\n4.763889\n41.978603\n-87.904842\n\n\n1073\n52.308613\n4.763889\n43.677223\n-79.630556\n\n\n1077\n33.636719\n-84.428067\n47.464722\n8.549167\n\n\n1094\n37.469075\n126.450517\n41.978603\n-87.904842\n\n\n1097\n40.639751\n-73.778925\n31.143378\n121.805214\n\n\n1115\n-8.858375\n13.231178\n40.080111\n116.584556\n\n\n1116\n60.193917\n11.100361\n48.110278\n16.569722\n\n\n1121\n53.421333\n-6.270075\n48.110278\n16.569722\n\n\n1123\n53.421333\n-6.270075\n47.464722\n8.549167\n\n\n1128\n25.252778\n55.364444\n31.143378\n121.805214\n\n\n1129\n25.252778\n55.364444\n1.350189\n103.994433\n\n\n1130\n25.252778\n55.364444\n25.077731\n121.232822\n\n\n1131\n25.252778\n55.364444\n48.110278\n16.569722\n\n\n1132\n25.252778\n55.364444\n43.677223\n-79.630556\n\n\n1133\n25.252778\n55.364444\n47.464722\n8.549167\n\n\n1134\n-37.673333\n144.843333\n1.350189\n103.994433\n\n\n1136\n8.977889\n38.799319\n51.477500\n-0.461389\n\n\n1138\n8.977889\n38.799319\n31.143378\n121.805214\n\n\n1139\n-34.945000\n138.530556\n1.350189\n103.994433\n\n\n1140\n-37.008056\n174.791667\n1.350189\n103.994433\n\n\n1143\n-43.489358\n172.532225\n1.350189\n103.994433\n\n\n1146\n50.026421\n8.543125\n47.464722\n8.549167\n\n\n1157\n24.432972\n54.651138\n31.143378\n121.805214\n\n\n1160\n44.818444\n20.309139\n51.477500\n-0.461389\n\n\n1165\n43.809953\n11.205100\n47.464722\n8.549167\n\n\n1166\n46.238064\n6.108950\n47.464722\n8.549167\n\n\n1167\n51.432447\n12.241633\n47.464722\n8.549167\n\n\n1168\n45.200761\n7.649631\n47.464722\n8.549167\n\n\n1170\n42.695194\n23.406167\n48.110278\n16.569722\n\n\n1171\n42.695194\n23.406167\n47.464722\n8.549167\n\n\n1172\n13.912583\n100.606750\n1.350189\n103.994433\n\n\n1173\n13.912583\n100.606750\n34.447119\n108.751592\n\n\n1175\n8.095969\n98.988764\n1.350189\n103.994433\n\n\n1176\n31.780019\n117.298436\n25.077731\n121.232822\n\n\n1177\n38.481944\n106.009167\n25.077731\n121.232822\n\n\n1180\n63.985000\n-22.605556\n51.477500\n-0.461389\n\n\n1181\n63.985000\n-22.605556\n43.677223\n-79.630556\n\n\n1182\n63.985000\n-22.605556\n47.464722\n8.549167\n\n\n1189\n39.850000\n110.033000\n34.447119\n108.751592\n\n\n1198\n41.101400\n121.062000\n31.143378\n121.805214\n\n\n1203\n31.428100\n104.741000\n31.143378\n121.805214\n\n\n1204\n35.179528\n128.938222\n31.143378\n121.805214\n\n\n1208\n4.567972\n101.092194\n1.350189\n103.994433\n\n\n1209\n3.775389\n103.209056\n1.350189\n103.994433\n\n\n1210\n35.417000\n116.533000\n34.447119\n108.751592\n\n\n1211\n-6.900625\n107.576294\n1.350189\n103.994433\n\n\n1213\n-1.268272\n116.894478\n1.350189\n103.994433\n\n\n1214\n-6.125567\n106.655897\n40.080111\n116.584556\n\n\n1215\n-6.125567\n106.655897\n31.143378\n121.805214\n\n\n1216\n-6.125567\n106.655897\n1.350189\n103.994433\n\n\n1219\n-8.748169\n115.167172\n1.350189\n103.994433\n\n\n1221\n40.976922\n28.814606\n1.350189\n103.994433\n\n\n1222\n-7.788181\n110.431758\n1.350189\n103.994433\n\n\n1223\n-8.757322\n116.276675\n1.350189\n103.994433\n\n\n1225\n1.549447\n124.925878\n1.350189\n103.994433\n\n\n1226\n0.460786\n101.444539\n1.350189\n103.994433\n\n\n1227\n-2.898250\n104.699903\n1.350189\n103.994433\n\n\n1231\n33.511306\n126.493028\n25.077731\n121.232822\n\n\n1232\n18.766847\n98.962644\n25.077731\n121.232822\n\n\n1239\n43.041000\n144.193000\n25.077731\n121.232822\n\n\n1244\n13.410666\n103.812840\n25.077731\n121.232822\n\n\n1247\n30.582200\n117.050200\n40.080111\n116.584556\n\n\n1249\n28.918900\n111.640000\n34.447119\n108.751592\n\n\n1252\n40.060300\n113.482000\n34.447119\n108.751592\n\n\n1254\n32.900000\n115.816667\n34.447119\n108.751592\n\n\n1255\n40.851422\n111.824103\n34.447119\n108.751592\n\n\n1257\n33.777200\n119.147800\n34.447119\n108.751592\n\n\n1258\n42.841400\n93.669200\n34.447119\n108.751592\n\n\n1259\n35.799700\n107.603000\n34.447119\n108.751592\n\n\n1264\n28.852200\n105.393000\n34.447119\n108.751592\n\n\n1265\n30.754000\n106.062000\n34.447119\n108.751592\n\n\n1267\n40.926389\n107.738889\n34.447119\n108.751592\n\n\n1274\n36.898731\n30.800461\n48.110278\n16.569722\n\n\n1280\n55.408611\n37.906111\n48.110278\n16.569722\n\n\n1287\n27.178317\n33.799436\n48.110278\n16.569722\n\n\n1302\n25.557111\n34.583711\n48.110278\n16.569722\n\n\n1306\n32.011389\n34.886667\n48.110278\n16.569722\n\n\n1308\n40.560000\n109.997000\n31.143378\n121.805214\n\n\n1309\n21.539400\n109.294000\n31.143378\n121.805214\n\n\n1318\n37.271600\n118.281900\n31.143378\n121.805214\n\n\n1319\n25.935064\n119.663272\n31.143378\n121.805214\n\n\n1329\n26.195814\n127.645869\n31.143378\n121.805214\n\n\n1334\n43.352072\n77.040508\n40.080111\n116.584556\n\n\n1339\n50.901389\n4.484444\n40.080111\n116.584556\n\n\n1354\n37.271600\n118.281900\n40.080111\n116.584556\n\n\n1355\n36.636900\n109.554000\n40.080111\n116.584556\n\n\n1374\n48.528044\n135.188361\n40.080111\n116.584556\n\n\n1375\n56.180000\n92.475000\n40.080111\n116.584556\n\n\n1381\n59.800292\n30.262503\n40.080111\n116.584556\n\n\n1384\n36.117000\n103.617000\n25.077731\n121.232822\n\n\n1387\n4.191833\n73.529128\n40.080111\n116.584556\n\n\n1392\n49.566667\n117.329444\n40.080111\n116.584556\n\n\n1395\n55.012622\n82.650656\n40.080111\n116.584556\n\n\n1408\n39.794444\n106.799444\n34.447119\n108.751592\n\n\n1410\n51.956944\n4.437222\n48.110278\n16.569722\n\n\n1424\n41.297078\n2.078464\n47.464722\n8.549167\n\n\n1426\n43.301097\n-2.910608\n51.477500\n-0.461389\n\n\n1450\n43.302061\n-8.377256\n51.477500\n-0.461389\n\n\n1467\n35.416111\n51.152222\n51.477500\n-0.461389\n\n\n1468\n35.416111\n51.152222\n40.080111\n116.584556\n\n\n1469\n35.416111\n51.152222\n48.110278\n16.569722\n\n\n1471\n40.467500\n50.046667\n40.080111\n116.584556\n\n\n1472\n40.467500\n50.046667\n48.110278\n16.569722\n\n\n1473\n5.765280\n103.007000\n1.350189\n103.994433\n\n\n1474\n44.941444\n17.297501\n47.464722\n8.549167\n\n\n1476\n29.102800\n110.443000\n34.447119\n108.751592\n\n\n1478\n19.934856\n110.458961\n34.447119\n108.751592\n\n\n1484\n45.306110\n130.996670\n40.080111\n116.584556\n\n\n1490\n38.280686\n114.697300\n34.447119\n108.751592\n\n\n1495\n29.733300\n118.256000\n34.447119\n108.751592\n\n\n1500\n42.775200\n141.692283\n31.143378\n121.805214\n\n\n1501\n34.796111\n138.189444\n31.143378\n121.805214\n\n\n1504\n34.436111\n132.919444\n31.143378\n121.805214\n\n\n1508\n35.552258\n139.779694\n1.350189\n103.994433\n\n\n1509\n37.571100\n139.064600\n31.143378\n121.805214\n\n\n1513\n36.394611\n136.406544\n31.143378\n121.805214\n\n\n1514\n31.803397\n130.719408\n31.143378\n121.805214\n\n\n1515\n33.827222\n132.699722\n31.143378\n121.805214\n\n\n1518\n32.916944\n129.913611\n31.143378\n121.805214\n\n\n1525\n34.756944\n133.855278\n31.143378\n121.805214\n\n\n1527\n46.223686\n14.457611\n48.110278\n16.569722\n\n\n1528\n46.223686\n14.457611\n47.464722\n8.549167\n\n\n1536\n44.818444\n20.309139\n48.110278\n16.569722\n\n\n1537\n44.818444\n20.309139\n47.464722\n8.549167\n\n\n1539\n54.377569\n18.466222\n47.464722\n8.549167\n\n\n1540\n50.077731\n19.784836\n47.464722\n8.549167\n\n\n1541\n51.102683\n16.885836\n47.464722\n8.549167\n\n\n1542\n11.546556\n104.844139\n31.143378\n121.805214\n\n\n1550\n51.022222\n71.466944\n48.110278\n16.569722\n\n\n1551\n33.511306\n126.493028\n40.080111\n116.584556\n\n\n1559\n37.469075\n126.450517\n48.110278\n16.569722\n\n\n1560\n37.469075\n126.450517\n34.447119\n108.751592\n\n\n1561\n37.469075\n126.450517\n43.677223\n-79.630556\n\n\n1568\n52.308613\n4.763889\n1.350189\n103.994433\n\n\n1570\n52.308613\n4.763889\n48.110278\n16.569722\n\n\n1572\n52.308613\n4.763889\n47.464722\n8.549167\n\n\n1588\n35.857497\n14.477500\n47.464722\n8.549167\n\n\n1593\n19.292778\n-81.357750\n41.978603\n-87.904842\n\n\n1597\n49.626575\n6.211517\n48.110278\n16.569722\n\n\n1612\n50.026421\n8.543125\n41.978603\n-87.904842\n\n\n1615\n50.026421\n8.543125\n1.350189\n103.994433\n\n\n1619\n52.461056\n9.685078\n47.464722\n8.549167\n\n\n1625\n51.432447\n12.241633\n48.110278\n16.569722\n\n\n1628\n48.353783\n11.786086\n41.978603\n-87.904842\n\n\n1633\n48.353783\n11.786086\n47.464722\n8.549167\n\n\n1634\n-1.319167\n36.927500\n47.464722\n8.549167\n\n\n1635\n49.498700\n11.066897\n47.464722\n8.549167\n\n\n1637\n48.689878\n9.221964\n47.464722\n8.549167\n\n\n1643\n50.077731\n19.784836\n48.110278\n16.569722\n\n\n1645\n52.165750\n20.967122\n47.464722\n8.549167\n\n\n1646\n53.353744\n-2.274950\n48.110278\n16.569722\n\n\n1648\n4.191833\n73.529128\n31.143378\n121.805214\n\n\n1650\n36.674900\n-4.499106\n47.464722\n8.549167\n\n\n1652\n59.651944\n17.918611\n47.464722\n8.549167\n\n\n1653\n37.936358\n23.944467\n47.464722\n8.549167\n\n\n1657\n52.453856\n-1.748028\n47.464722\n8.549167\n\n\n1658\n13.681108\n100.747283\n47.464722\n8.549167\n\n\n1660\n42.364347\n-71.005181\n47.464722\n8.549167\n\n\n1661\n50.901389\n4.484444\n47.464722\n8.549167\n\n\n1662\n47.590000\n7.529167\n48.110278\n16.569722\n\n\n1663\n47.436933\n19.255592\n47.464722\n8.549167\n\n\n1664\n30.121944\n31.405556\n47.464722\n8.549167\n\n\n1667\n55.617917\n12.655972\n47.464722\n8.549167\n\n\n1669\n42.561353\n18.268244\n47.464722\n8.549167\n\n\n1671\n55.408611\n37.906111\n47.464722\n8.549167\n\n\n1675\n40.692500\n-74.168667\n47.464722\n8.549167\n\n\n1681\n-23.432075\n-46.469511\n47.464722\n8.549167\n\n\n1683\n46.238064\n6.108950\n48.110278\n16.569722\n\n\n1687\n22.989153\n-82.409086\n47.464722\n8.549167\n\n\n1689\n22.308919\n113.914603\n47.464722\n8.549167\n\n\n1691\n38.944533\n-77.455811\n47.464722\n8.549167\n\n\n1692\n40.976922\n28.814606\n47.464722\n8.549167\n\n\n1694\n-26.139166\n28.246000\n47.464722\n8.549167\n\n\n1695\n50.345000\n30.894722\n47.464722\n8.549167\n\n\n1696\n36.080056\n-115.152250\n47.464722\n8.549167\n\n\n1697\n33.942536\n-118.408075\n47.464722\n8.549167\n\n\n1700\n59.800292\n30.262503\n47.464722\n8.549167\n\n\n1704\n46.004275\n8.910578\n47.464722\n8.549167\n\n\n1705\n49.626575\n6.211517\n47.464722\n8.549167\n\n\n1706\n45.726387\n5.090833\n47.464722\n8.549167\n\n\n1708\n53.353744\n-2.274950\n47.464722\n8.549167\n\n\n1709\n25.793250\n-80.290556\n47.464722\n8.549167\n\n\n1712\n45.630606\n8.728111\n47.464722\n8.549167\n\n\n1714\n43.658411\n7.215872\n47.464722\n8.549167\n\n\n1715\n35.764722\n140.386389\n47.464722\n8.549167\n\n\n1718\n41.978603\n-87.904842\n47.464722\n8.549167\n\n\n1719\n60.193917\n11.100361\n47.464722\n8.549167\n\n\n1720\n44.572161\n26.102178\n47.464722\n8.549167\n\n\n1721\n40.080111\n116.584556\n47.464722\n8.549167\n\n\n1723\n50.100833\n14.260000\n47.464722\n8.549167\n\n\n1725\n18.567367\n-68.363431\n47.464722\n8.549167\n\n\n1726\n31.143378\n121.805214\n47.464722\n8.549167\n\n\n1727\n31.606886\n-8.036300\n47.464722\n8.549167\n\n\n1729\n37.618972\n-122.374889\n47.464722\n8.549167\n\n\n1730\n1.350189\n103.994433\n47.464722\n8.549167\n\n\n1732\n43.538944\n16.297964\n47.464722\n8.549167\n\n\n1735\n32.011389\n34.886667\n47.464722\n8.549167\n\n\n1736\n27.975472\n-82.533250\n47.464722\n8.549167\n\n\n1738\n45.505278\n12.351944\n47.464722\n8.549167\n\n\n1740\n39.489314\n-0.481625\n47.464722\n8.549167\n\n\n1745\n45.742931\n16.068778\n47.464722\n8.549167\n\n\n1766\n24.796400\n118.590000\n25.077731\n121.232822\n\n\n1794\n12.949986\n77.668206\n1.350189\n103.994433\n\n\n1798\n11.030031\n77.043383\n1.350189\n103.994433\n\n\n1799\n29.719217\n106.641678\n1.350189\n103.994433\n\n\n1800\n18.766847\n98.962644\n1.350189\n103.994433\n\n\n1801\n10.155556\n76.391389\n1.350189\n103.994433\n\n\n1802\n28.189158\n113.219633\n1.350189\n103.994433\n\n\n1804\n16.043917\n108.199370\n1.350189\n103.994433\n\n\n1805\n-8.546553\n125.524719\n1.350189\n103.994433\n\n\n1807\n7.125522\n125.645778\n1.350189\n103.994433\n\n\n1808\n21.221192\n105.807178\n1.350189\n103.994433\n\n\n1810\n17.453117\n78.467586\n1.350189\n103.994433\n\n\n1813\n24.992364\n102.743536\n1.350189\n103.994433\n\n\n1814\n27.696583\n85.359100\n1.350189\n103.994433\n\n\n1828\n-20.430235\n57.683600\n40.080111\n116.584556\n\n\n1829\n-20.430235\n57.683600\n31.143378\n121.805214\n\n\n1834\n30.121944\n31.405556\n48.110278\n16.569722\n\n\n1835\n30.121944\n31.405556\n43.677223\n-79.630556\n\n\n1838\n25.088200\n104.958700\n31.143378\n121.805214\n\n\n1852\n43.541200\n125.120100\n34.447119\n108.751592\n\n\n1864\n31.941667\n119.711667\n34.447119\n108.751592\n\n\n1865\n40.060300\n113.482000\n31.143378\n121.805214\n\n\n1866\n31.300000\n107.500000\n31.143378\n121.805214\n\n\n1872\n-8.748169\n115.167172\n31.143378\n121.805214\n\n\n1884\n34.633000\n98.867000\n34.447119\n108.751592\n\n\n1886\n21.221192\n105.807178\n31.143378\n121.805214\n\n\n1887\n36.524000\n114.430000\n31.143378\n121.805214\n\n\n1895\n33.777200\n119.147800\n40.080111\n116.584556\n\n\n1900\n42.841400\n93.669200\n31.143378\n121.805214\n\n\n1902\n21.318681\n-157.922428\n31.143378\n121.805214\n\n\n1906\n29.934200\n122.362000\n31.143378\n121.805214\n\n\n1914\n39.856900\n98.341400\n34.447119\n108.751592\n\n\n1915\n26.899700\n114.737500\n34.447119\n108.751592\n\n\n1916\n29.515000\n108.830000\n40.080111\n116.584556\n\n\n1917\n35.417000\n116.533000\n40.080111\n116.584556\n\n\n1921\n32.857000\n103.683000\n34.447119\n108.751592\n\n\n1922\n39.542922\n76.019956\n34.447119\n108.751592\n\n\n1948\n34.410000\n112.280000\n40.080111\n116.584556\n\n\n1949\n34.550000\n119.250000\n40.080111\n116.584556\n\n\n1950\n35.046100\n118.412000\n40.080111\n116.584556\n\n\n1951\n35.046100\n118.412000\n31.143378\n121.805214\n\n\n1952\n24.207500\n109.391000\n31.143378\n121.805214\n\n\n1953\n28.852200\n105.393000\n40.080111\n116.584556\n\n\n1954\n28.852200\n105.393000\n31.143378\n121.805214\n\n\n1960\n34.991389\n126.382778\n31.143378\n121.805214\n\n\n1962\n30.754000\n106.062000\n31.143378\n121.805214\n\n\n1965\n29.826683\n121.461906\n34.447119\n108.751592\n\n\n1969\n31.742042\n118.862025\n31.143378\n121.805214\n\n\n1972\n22.608267\n108.172442\n1.350189\n103.994433\n\n\n2010\n33.585942\n130.450686\n1.350189\n103.994433\n\n\n2018\n34.434722\n135.244167\n1.350189\n103.994433\n\n\n2023\n34.858414\n136.805408\n1.350189\n103.994433\n\n\n2029\n35.764722\n140.386389\n48.110278\n16.569722\n\n\n2036\n39.457583\n-74.577167\n41.978603\n-87.904842\n\n\n2047\n33.679750\n-78.928333\n41.978603\n-87.904842\n\n\n2048\n37.721278\n-122.220722\n41.978603\n-87.904842\n\n\n2050\n31.428100\n104.741000\n34.447119\n108.751592\n\n\n2074\n44.535444\n11.288667\n48.110278\n16.569722\n\n\n2075\n50.901389\n4.484444\n48.110278\n16.569722\n\n\n2077\n47.436933\n19.255592\n48.110278\n16.569722\n\n\n2082\n46.785167\n23.686167\n48.110278\n16.569722\n\n\n2085\n42.561353\n18.268244\n48.110278\n16.569722\n\n\n2087\n36.713056\n28.792500\n48.110278\n16.569722\n\n\n2089\n48.357222\n35.100556\n48.110278\n16.569722\n\n\n2092\n36.237611\n43.963158\n48.110278\n16.569722\n\n\n2093\n40.147275\n44.395881\n48.110278\n16.569722\n\n\n2098\n46.991067\n15.439628\n48.110278\n16.569722\n\n\n2105\n49.924786\n36.289986\n48.110278\n16.569722\n\n\n2106\n38.944533\n-77.455811\n48.110278\n16.569722\n\n\n2107\n47.178492\n27.620631\n48.110278\n16.569722\n\n\n2109\n47.260219\n11.343964\n48.110278\n16.569722\n\n\n2110\n40.976922\n28.814606\n48.110278\n16.569722\n\n\n2111\n40.639751\n-73.778925\n48.110278\n16.569722\n\n\n2113\n50.345000\n30.894722\n48.110278\n16.569722\n\n\n2114\n46.927744\n28.930978\n48.110278\n16.569722\n\n\n2115\n46.642514\n14.337739\n48.110278\n16.569722\n\n\n2117\n45.034689\n39.170539\n48.110278\n16.569722\n\n\n2118\n48.663055\n21.241112\n48.110278\n16.569722\n\n\n2120\n59.800292\n30.262503\n48.110278\n16.569722\n\n\n2123\n38.781311\n-9.135919\n48.110278\n16.569722\n\n\n2125\n48.233219\n14.187511\n48.110278\n16.569722\n\n\n2128\n49.812500\n23.956111\n48.110278\n16.569722\n\n\n2134\n40.886033\n14.290781\n48.110278\n16.569722\n\n\n2137\n46.426767\n30.676464\n48.110278\n16.569722\n\n\n2139\n41.978603\n-87.904842\n48.110278\n16.569722\n\n\n2141\n44.572161\n26.102178\n48.110278\n16.569722\n\n\n2144\n38.175958\n13.091019\n48.110278\n16.569722\n\n\n2145\n50.100833\n14.260000\n48.110278\n16.569722\n\n\n2146\n42.572778\n21.035833\n48.110278\n16.569722\n\n\n2149\n47.258208\n39.818089\n48.110278\n16.569722\n\n\n2150\n45.785597\n24.091342\n48.110278\n16.569722\n\n\n2151\n43.824583\n18.331467\n48.110278\n16.569722\n\n\n2153\n41.961622\n21.621381\n48.110278\n16.569722\n\n\n2155\n43.538944\n16.297964\n48.110278\n16.569722\n\n\n2157\n38.905394\n16.242269\n48.110278\n16.569722\n\n\n2161\n42.359392\n19.251894\n48.110278\n16.569722\n\n\n2162\n41.414742\n19.720561\n48.110278\n16.569722\n\n\n2166\n43.232072\n27.825106\n48.110278\n16.569722\n\n\n2167\n45.505278\n12.351944\n48.110278\n16.569722\n\n\n2187\n-26.139166\n28.246000\n1.350189\n103.994433\n\n\n2191\n40.128082\n32.995083\n48.110278\n16.569722\n\n\n2192\n40.898553\n29.309219\n48.110278\n16.569722\n\n\n2193\n40.898553\n29.309219\n47.464722\n8.549167\n\n\n2194\n47.485033\n9.560775\n48.110278\n16.569722\n\n\n2197\n33.616653\n73.099233\n51.477500\n-0.461389\n\n\n2199\n33.616653\n73.099233\n43.677223\n-79.630556\n\n\n2200\n24.906547\n67.160797\n51.477500\n-0.461389\n\n\n2201\n24.906547\n67.160797\n43.677223\n-79.630556\n\n\n2202\n31.521564\n74.403594\n51.477500\n-0.461389\n\n\n2203\n31.521564\n74.403594\n43.677223\n-79.630556\n\n\n2206\n11.679431\n122.376294\n25.077731\n121.232822\n\n\n2217\n-9.443383\n147.220050\n1.350189\n103.994433\n\n\n2232\n25.261125\n51.565056\n51.477500\n-0.461389\n\n\n2234\n25.261125\n51.565056\n40.080111\n116.584556\n\n\n2235\n25.261125\n51.565056\n31.143378\n121.805214\n\n\n2236\n25.261125\n51.565056\n1.350189\n103.994433\n\n\n2237\n25.261125\n51.565056\n48.110278\n16.569722\n\n\n2238\n25.261125\n51.565056\n47.464722\n8.549167\n\n\n2251\n31.722556\n35.993214\n47.464722\n8.549167\n\n\n2262\n38.781311\n-9.135919\n43.677223\n-79.630556\n\n\n2263\n41.248055\n-8.681389\n43.677223\n-79.630556\n\n\n2264\n37.741184\n-25.697870\n43.677223\n-79.630556\n\n\n2266\n52.268028\n104.388975\n40.080111\n116.584556\n\n\n2283\n27.701900\n118.001000\n34.447119\n108.751592\n\n\n2284\n49.207947\n-2.195508\n47.464722\n8.549167\n\n\n2286\n59.651944\n17.918611\n41.978603\n-87.904842\n\n\n2291\n55.617917\n12.655972\n41.978603\n-87.904842\n\n\n2293\n55.617917\n12.655972\n31.143378\n121.805214\n\n\n2294\n55.617917\n12.655972\n1.350189\n103.994433\n\n\n2300\n50.901389\n4.484444\n41.978603\n-87.904842\n\n\n2306\n23.077242\n72.634650\n1.350189\n103.994433\n\n\n2308\n41.297078\n2.078464\n1.350189\n103.994433\n\n\n2334\n55.408611\n37.906111\n1.350189\n103.994433\n\n\n2339\n41.804475\n12.250797\n1.350189\n103.994433\n\n\n2363\n4.191833\n73.529128\n1.350189\n103.994433\n\n\n2365\n48.353783\n11.786086\n1.350189\n103.994433\n\n\n2366\n45.630606\n8.728111\n1.350189\n103.994433\n\n\n2378\n24.957640\n46.698776\n1.350189\n103.994433\n\n\n2382\n53.047500\n8.786667\n48.110278\n16.569722\n\n\n2385\n55.972642\n37.414589\n48.110278\n16.569722\n\n\n2386\n55.972642\n37.414589\n43.677223\n-79.630556\n\n\n2387\n55.972642\n37.414589\n47.464722\n8.549167\n\n\n2389\n21.679564\n39.156536\n1.350189\n103.994433\n\n\n2390\n21.679564\n39.156536\n43.677223\n-79.630556\n\n\n2392\n46.914100\n7.497153\n48.110278\n16.569722\n\n\n2393\n37.986814\n58.360967\n51.477500\n-0.461389\n\n\n2408\n40.976922\n28.814606\n41.978603\n-87.904842\n\n\n2436\n6.933206\n100.392975\n1.350189\n103.994433\n\n\n2445\n26.883333\n100.233330\n1.350189\n103.994433\n\n\n2447\n22.149556\n113.591558\n1.350189\n103.994433\n\n\n2450\n29.826683\n121.461906\n1.350189\n103.994433\n\n\n2458\n37.936358\n23.944467\n43.677223\n-79.630556\n\n\n2465\n55.871944\n-4.433056\n43.677223\n-79.630556\n\n\n2467\n51.148056\n-0.190278\n43.677223\n-79.630556\n\n\n2469\n53.353744\n-2.274950\n43.677223\n-79.630556\n\n\n2474\n45.505278\n12.351944\n43.677223\n-79.630556\n\n\n2477\n33.875031\n10.775461\n47.464722\n8.549167\n\n\n2478\n36.075833\n10.438611\n47.464722\n8.549167\n\n\n2479\n36.851033\n10.227217\n48.110278\n16.569722\n\n\n2480\n36.851033\n10.227217\n47.464722\n8.549167\n\n\n2485\n31.742042\n118.862025\n1.350189\n103.994433\n\n\n2487\n-28.164444\n153.504722\n1.350189\n103.994433\n\n\n2490\n51.148056\n-0.190278\n48.110278\n16.569722\n\n\n2491\n51.148056\n-0.190278\n47.464722\n8.549167\n\n\n2492\n51.874722\n-0.368333\n47.464722\n8.549167\n\n\n2493\n40.652083\n-75.440806\n41.978603\n-87.904842\n\n\n2496\n42.748267\n-73.801692\n41.978603\n-87.904842\n\n\n2501\n44.257526\n-88.507576\n41.978603\n-87.904842\n\n\n2503\n35.436194\n-82.541806\n41.978603\n-87.904842\n\n\n2504\n41.338478\n-75.723403\n41.978603\n-87.904842\n\n\n2507\n33.562942\n-86.753550\n41.978603\n-87.904842\n\n\n2509\n43.564361\n-116.222861\n41.978603\n-87.904842\n\n\n2514\n44.471861\n-73.153278\n41.978603\n-87.904842\n\n\n2517\n45.777643\n-111.160151\n41.978603\n-87.904842\n\n\n2518\n33.938833\n-81.119528\n41.978603\n-87.904842\n\n\n2519\n40.916083\n-81.442194\n41.978603\n-87.904842\n\n\n2521\n32.898647\n-80.040528\n41.978603\n-87.904842\n\n\n2528\n47.168400\n-88.489100\n41.978603\n-87.904842\n\n\n2529\n38.805805\n-104.700778\n41.978603\n-87.904842\n\n\n2539\n46.842091\n-92.193649\n41.978603\n-87.904842\n\n\n2546\n44.865800\n-91.484300\n41.978603\n-87.904842\n\n\n2547\n42.159889\n-76.891611\n41.978603\n-87.904842\n\n\n2557\n38.950944\n-95.663611\n41.978603\n-87.904842\n\n\n2563\n-23.432075\n-46.469511\n41.978603\n-87.904842\n\n\n2564\n36.097750\n-79.937306\n41.978603\n-87.904842\n\n\n2565\n34.895556\n-82.218889\n41.978603\n-87.904842\n\n\n2568\n21.318681\n-157.922428\n41.978603\n-87.904842\n\n\n2573\n38.944533\n-77.455811\n41.978603\n-87.904842\n\n\n2584\n32.311167\n-90.075889\n41.978603\n-87.904842\n\n\n2589\n42.778700\n-84.587357\n41.978603\n-87.904842\n\n\n2603\n40.850971\n-96.759250\n41.978603\n-87.904842\n\n\n2604\n43.532913\n-84.079647\n41.978603\n-87.904842\n\n\n2610\n42.932556\n-71.435667\n41.978603\n-87.904842\n\n\n2614\n43.169500\n-86.238200\n41.978603\n-87.904842\n\n\n2616\n30.691231\n-88.242814\n41.978603\n-87.904842\n\n\n2622\n25.778489\n-100.106878\n41.978603\n-87.904842\n\n\n2625\n25.038958\n-77.466231\n41.978603\n-87.904842\n\n\n2629\n20.898650\n-156.430458\n41.978603\n-87.904842\n\n\n2644\n7.180756\n79.884117\n51.477500\n-0.461389\n\n\n2645\n7.180756\n79.884117\n40.080111\n116.584556\n\n\n2646\n7.180756\n79.884117\n31.143378\n121.805214\n\n\n2648\n6.284467\n81.124128\n40.080111\n116.584556\n\n\n2649\n6.284467\n81.124128\n31.143378\n121.805214\n\n\n2650\n23.593278\n58.284444\n47.464722\n8.549167\n\n\n2659\n55.591531\n37.261486\n43.677223\n-79.630556\n\n\n2783\n16.043917\n108.199370\n31.143378\n121.805214\n\n\n2817\n53.047500\n8.786667\n47.464722\n8.549167\n\n\n2818\n52.134642\n7.684831\n47.464722\n8.549167\n\n\n2819\n35.416111\n51.152222\n31.143378\n121.805214\n\n\n2843\n33.679750\n-78.928333\n43.677223\n-79.630556\n\n\n2851\n18.439417\n-66.001833\n43.677223\n-79.630556\n\n\n2853\n18.040953\n-63.108900\n43.677223\n-79.630556\n\n\n2860\n49.956112\n-119.377778\n43.677223\n-79.630556\n\n\n2877\n38.292392\n27.156953\n48.110278\n16.569722\n\n\n2878\n38.292392\n27.156953\n47.464722\n8.549167\n\n\n2885\n42.359392\n19.251894\n47.464722\n8.549167\n\n\n\n\n\n\n# Get a nice image of the globe from NASA\nearth &lt;- \"http://eoimages.gsfc.nasa.gov/images/imagerecords/73000/73909/world.topo.bathy.200412.3x5400x2700.jpg\"\n\nglobejs(\n  img = earth,\n  lat = frequent_flights$dest_lat,\n  long = frequent_flights$dest_long,\n  arcs = frequent_flights,\n  # value = frequent_destinations$n,\n  color = \"red\",\n  # bodycolor = \"#aaaaff\",\n  arcsHeight = 0.3,\n  arcsLwd = 2,\n  arcsColor = \"#ffff00\",\n  arcsOpacity = 0.35,\n  atmosphere = FALSE,\n  # color=\"#00aaff\",\n  pointsize = 2,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  # lightcolor = \"#aaeeff\",\n  # emissive = \"#0000ee\",\n  # bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using scatterplot3js and friends",
    "text": "Using scatterplot3js and friends\n3D scatter plots with points and lines can be achieved using scatterplot3js, points3D, and lines3D.\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(\n  x = penguins$bill_length_mm,\n  y = penguins$flipper_length_mm,\n  z = penguins$body_mass_g\n)\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(\n  x = penguins$bill_length_mm,\n  y = penguins$flipper_length_mm,\n  z = penguins$body_mass_g,\n  cex.symbols = 0.2\n) # Smaller Points\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\n\nscatterplot3js(\n  x = penguins$bill_length_mm,\n  y = penguins$flipper_length_mm,\n  z = penguins$body_mass_g,\n  cex.symbols = 0.2\n) # Smaller Points"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "References",
    "text": "References\n\nBring the best of JavaScript data visualization to R, https://www.htmlwidgets.org/\nUsing htmlwidgets in Rmarkdown, https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/\nKarambelkar et al, htmlwidgets and knitr , https://cran.r-project.org/web/packages/widgetframe/vignettes/widgetframe_and_knitr.html\nThe patchwork package to combine plots. https://patchwork.data-imaginist.com/\nThe threejs package: three.js widgets for R https://bwlewis.github.io/rthreejs/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction",
    "text": "Introduction\nTime Series data are important in data visualization where events have a temporal dimension, such as with finance, transportation, music, telecommunications for example.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n##########################################\n# Install core TimeSeries Packages\n# library(ctv)\n# ctv::install.views(\"TimeSeries\", coreOnly = TRUE)\n# To update core TimeSeries packages\n# ctv::update.views(\"TimeSeries\")\n# Time Series Core Packages\n##########################################\nlibrary(tsibble)\nlibrary(feasts) # Feature Extraction and Statistics for Time Series\nlibrary(fable) # Forecasting Models for Tidy Time Series\nlibrary(tseries) # Time Series Analysis and Computational Finance\nlibrary(forecast)\nlibrary(zoo)\n##########################################\nlibrary(tsibbledata) # Time Series Demo Datasets\n## New package from Mitchell Ohara-Wild in June 2025\nlibrary(ggtime)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction-to-time-series-data-formats",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction-to-time-series-data-formats",
    "title": "Lab-12: Time is a Him!!",
    "section": "\n Introduction to Time Series: Data Formats",
    "text": "Introduction to Time Series: Data Formats\nThere are multiple formats for time series data.\n\nThe base ts format: The stats::ts() function will convert a numeric vector into an R time series object. The format is ts(vector, start=, end=, frequency=) where start and end are the times of the first and last observation and frequency is the number of observations per unit time (1=annual, 4=quarterly, 12=monthly, etc.). Used by established packages like forecast\n\nTibble format: the simplest is of course the standard tibble / dataframe, with a time variable to indicate that the other variables vary with time. Used by more recent packages such as timetk & modeltime\n\nThe modern tsibble (time series tibble) format: this is a new format for time series analysis, and is used by the tidyverts set of packages (fable, feasts and others).\nThere is also a tsbox package from ROpenScience that allows easy inter-conversion between these ( and other! ) formats!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Creating time series",
    "text": "Creating time series\nIn this first example, we will use simple ts data, and then do another with a tibble dataset, and then a third example with an tsibble formatted dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nplot(AirPassengers)\n\n\n\n\n\n\n\nLet us take data that is “time oriented” but not in ts format, and convert it to ts: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency) where  - data represents the data vector - start represents the first observation in time series\n- end represents the last observation in time series\n- frequency represents number of observations per unit time. For example, frequency=1 for monthly data.\nWe will pick simple numerical vector data ( i.e. not a timeseries ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n  \n\n\nChickWeight_ts &lt;- ts(ChickWeight$weight, frequency = 2)\nplot(ChickWeight_ts)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionThe ts format\n\n\n\nThe ts format is not recommended for new analysis since it does not permit inclusion of multiple time series in one dataset, nor other categorical variables for grouping etc.\n\n\n\ntibble format data\nSome “time-oriented” datasets are available in tibble form. Let us try to plot one, the walmart_sales_weekly dataset from the timetk package:\n\ndata(walmart_sales_weekly, package = \"timetk\")\nwalmart_sales_weekly\n\n\n  \n\n\n\nThis dataset is a tibble with a Date column. The Dept column is clearly a categorical column that allows us to distinguish separate time series, i.e. one for each value of Dept. We will convert that to a factor( it is an double precision number ) and then plot the data using this column on the Date on the \\(x\\)-axis:\n\nwalmart_sales_weekly %&gt;%\n  # convert Dept number to a **categorical factor**\n  mutate(Dept = forcats::as_factor(Dept)) %&gt;%\n  gf_point(Weekly_Sales ~ Date,\n    group = ~Dept,\n    colour = ~Dept, data = .\n  ) %&gt;%\n  gf_line() %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nFor more analysis and forecasting etc., it is useful to convert this tibble into a tsibble:\n\nwalmart_tsibble &lt;- as_tsibble(walmart_sales_weekly,\n  index = Date,\n  key = c(id, Dept)\n)\nwalmart_tsibble\n\n\n  \n\n\n\nThe 7D states the data is weekly. There is a Date column and all the other numerical variables are time-varying quantities. The categorical variables such as id, and Dept allow us to identify separate time series in the data, and these have 7 combinations hence are 7 time series in this data, as indicated.\n\n\nLet us plot Weekly_Sales, colouring the time series by Dept:\n\nwalmart_tsibble %&gt;%\n  gf_line(Weekly_Sales ~ Date,\n    colour = ~ as_factor(Dept), data = .\n  ) %&gt;%\n  gf_point() %&gt;%\n  gf_theme(theme_minimal()) %&gt;%\n  labs(title = \"Weekly Sales by Dept at Walmart\")\n\n[[1]]\n\n\n\n$title\n[1] \"Weekly Sales by Dept at Walmart\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\n\n\n\nFigure 1: Walmart Time Series\n\n\n\n\n\nWe can also do a quick autoplot that seems to offer less control and is also not interactive.\n\nwalmart_tsibble %&gt;%\n  dplyr::group_by(Dept) %&gt;%\n  autoplot(Weekly_Sales) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\ntsibble format data\nIn the packages tsibbledata and fpp3 we have a good choice of tsibble format data. Let us pick one:\n\nhh_budget\n\n\n  \n\n\n\nThere are 4 keys ( id variables ) here, one for each country. Six other quantitative columns are the individual series. Let us plot (some of) the timeseries:\nggplot2::theme_set(theme_classic())\nhh_budget %&gt;%\n  gf_path(Debt ~ Year,\n    colour = ~Country,\n    title = \"Debt over Time\"\n  )\n##\nhh_budget %&gt;%\n  gf_path(Savings ~ Year,\n    colour = ~Country,\n    title = \"Savings over Time\"\n  )\n##\nhh_budget %&gt;%\n  gf_path(Expenditure ~ Year,\n    colour = ~Country,\n    title = \"Expenditure over Time\"\n  )\n##\nhh_budget %&gt;%\n  gf_path(Wealth ~ Year,\n    colour = ~Country,\n    title = \"Wealth over Time\"\n  )"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "title": "Lab-12: Time is a Him!!",
    "section": "One more example",
    "text": "One more example\nOften we have data in table form, that is time-oriented, with a date like column, and we need to convert it into a tsibble for analysis:\n\nprison &lt;- readr::read_csv(\"https://OTexts.com/fpp3/extrafiles/prison_population.csv\")\nglimpse(prison)\n\nRows: 3,072\nColumns: 6\n$ Date       &lt;date&gt; 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01…\n$ State      &lt;chr&gt; \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"NS…\n$ Gender     &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Ma…\n$ Legal      &lt;chr&gt; \"Remanded\", \"Remanded\", \"Sentenced\", \"Sentenced\", \"Remanded…\n$ Indigenous &lt;chr&gt; \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\",…\n$ Count      &lt;dbl&gt; 0, 2, 0, 5, 7, 58, 5, 101, 51, 131, 145, 323, 355, 1617, 12…\n\n\nWe have a Date column for the time index, and we have unique key variables like State, Gender, Legal and Indigenous. Count is the value that is variable over time. It also appears that the data is quarterly, since mosaic::inspect reports the max_diff in the Date column as \\(92\\). .Run mosaic::inspect(prison) in your Console\n\nprison_tsibble &lt;- prison %&gt;%\n  mutate(quarter = yearquarter(Date)) %&gt;%\n  select(-Date) %&gt;% # Remove the Date column now that we have quarters\n  as_tsibble(index = quarter, key = c(State, Gender, Legal, Indigenous))\n\nprison_tsibble\n\n\n  \n\n\n\n(Here, ATSI stands for Aboriginal or Torres Strait Islander.). We have \\(64\\) time series here, organized quarterly.\nLet us examine the key variables:\n\nprison_tsibble %&gt;% distinct(Indigenous)\n\n\n  \n\n\nprison_tsibble %&gt;% distinct(State)\n\n\n  \n\n\n\nSo we can plot the time series, faceted / coloured by State:\n\nprison_tsibble %&gt;%\n  tsibble::index_by() %&gt;%\n  group_by(Indigenous, State) %&gt;%\n  # filter(State == \"NSW\") %&gt;%\n  summarise(Total = sum(Count)) %&gt;%\n  gf_point(Total ~ quarter,\n    colour = ~Indigenous,\n    shape = ~Indigenous\n  ) %&gt;%\n  gf_line() %&gt;%\n  # Note that the y-axes are all. different!!\n  gf_facet_wrap(vars(State), scale = \"free_y\") %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nHmm…looks like New South Wales(NSW) as something different going on compared to the rest of the states in Aus. Because of the large cities there…"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Decomposing Time Series",
    "text": "Decomposing Time Series\nWe can decompose the Weekly_Sales into components representing trends, seasonal events that repeat, and irregular noise. Since each Dept could have a different set of trends, we will do this first for one Dept, say Dept #95:\n\nwalmart_decomposed_season &lt;- walmart_tsibble %&gt;%\n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  #\n  fabletools::model(\n    season = STL(Weekly_Sales ~ season(window = \"periodic\"))\n  )\nwalmart_decomposed_season %&gt;% fabletools::components()\n\n\n  \n\n\n###\nwalmart_decomposed_ets &lt;- walmart_tsibble %&gt;%\n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  #\n  fabletools::model(\n    ets = ETS(box_cox(Weekly_Sales, 0.3))\n  )\n###\nwalmart_decomposed_ets %&gt;% fabletools::components()\n\n\n  \n\n\n###\nwalmart_decomposed_arima &lt;- walmart_tsibble %&gt;%\n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  fabletools::model(arima = ARIMA(log(Weekly_Sales)))\nwalmart_decomposed_arima %&gt;% broom::tidy()\n\n\n  \n\n\n\n\nwalmart_decomposed_season %&gt;%\n  components() %&gt;%\n  autoplot() +\n  labs(title = \"Seasonal Variations in Weekly Sales, Dept #95\")\n\n\n\n\n\n\nwalmart_decomposed_ets %&gt;%\n  components() %&gt;%\n  autoplot() +\n  labs(title = \"ETS Variations in Weekly Sales, Dept #95\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "title": "Lab-12: Time is a Him!!",
    "section": "Conclusion",
    "text": "Conclusion\nTBW"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "title": "Lab-12: Time is a Him!!",
    "section": "References",
    "text": "References\n\nForecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#where-does-data-come-from",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#where-does-data-come-from",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#why-visualize",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#why-visualize",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\n\n\n\nRape Capital\n\n\n\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#what-are-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#what-are-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\n\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\n\n\n\nCommon Geometric Aesthetics in Charts\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nPoints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to I nter-Quartile Range, w hisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "",
    "text": "Alice asks the Catterpillar the Way"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#slides-and-tutorials",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n R Tutorial\n Slides \n Leaflet Tutorial\n MapviewTutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#introduction",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#introduction",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Introduction",
    "text": "Introduction\nChoropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?\n\n\nBubble Map\nWhat information could this map below represent?\n\n\nWhat is there to not like about maps!!! Let us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata, osmplotr and rnaturalearth.\nWe will learn to make static and interactive maps and to show off different kinds of data on them, data that have an inherently “spatial” spread or significance! sf + ggplot and tmap give us great static maps. Interactive maps we will make with leaflet and mapview; tmap is also capable of creating interactive maps.\nTrade Routes? Populations? Street Crime hotspots? Theatre and Food districts and popular Restaurants? Literary Paris, London and Barcelona?\nAll possible !!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "References",
    "text": "References\n\nOSM Basic Maps Vignette\nNikita Voevodin, R, Not the Best Practices\nNico Hahn, Making Maps with R\nEmine Fidan, Interactive Maps in R\nLovelace et al, Geocomputation in R\nhttps://github.com/CityOfNewYork/nyc-planimetrics/blob/main/Capture_Rules.md\nKate Vavra-Musser. R Spatial Notebook series. https://vavramusser.github.io/r-spatial/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "title": "Lab-3: Drink Me!",
    "section": "Setting up Quarto",
    "text": "Setting up Quarto\nQuarto is installed along with RStudio. We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "title": "Lab-3: Drink Me!",
    "section": "Practice",
    "text": "Practice\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.Switch to Visual mode, if it is not already there.\nUse the visual mode tool bar.\nTry to create Sections, code chunks, embedding images and tables.\nHit the Render button to see how the documents is converted into an html document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "title": "Lab-3: Drink Me!",
    "section": "References",
    "text": "References\n\nhttps://www.markdowntutorial.com\nhttps://quarto.org/docs/authoring/markdown-basics.html\nhttps://rmarkdown.rstudio.com/index.html\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "title": "Lab-3: Drink Me!",
    "section": "Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "title": "Lab-3: Drink Me!",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nhttps://rmarkdown.rstudio.com/lesson-1.html\nDesirée De Leon, Alison Hill: rstudio4edu: A Handbook for Teaching and Learning with R and RStudio, https://rstudio4edu.github.io/rstudio4edu-book/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Course, R for Artists and Managers. The material is based on A Layered Grammar of Graphics by Hadley Wickham.\nThe intent of this Tutorial is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#setting-up-r-packages",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n## packages\nlibrary(tidyverse) ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)\nlibrary(paletteer) ## scico  and many other colour palettes palettes(http://www.fabiocrameri.ch/colourmaps.php) in R\nlibrary(ggtext) ## add improved text rendering to ggplot2\nlibrary(ggforce) ## add missing functionality to ggplot2\nlibrary(ggdist) ## add uncertainty visualizations to ggplot2\nlibrary(ggformula) ## Formula interface to ggplot\nlibrary(magick) ## load images into R\nlibrary(patchwork) ## combine outputs from ggplot2\nlibrary(palmerpenguins)\n\nlibrary(showtext) ## add google fonts to plots\n\nknitr::opts_chunk$set(\n  error = TRUE,\n  comment = NA,\n  warning = FALSE,\n  errors = FALSE,\n  message = FALSE,\n  tidy = FALSE,\n  cache = FALSE,\n  echo = TRUE,\n  warning = FALSE,\n  # from the vignette for the showtext package\n  fig.showtext = TRUE,\n  fig.retina = 1,\n  fig.width = 9,\n  fig.height = 8,\n  fig.path = \"06a-figs/\"\n)\n\nPlot Fonts and Theme\n\nShow the Code```{r}\n#| label: plot-theme\n#| code-fold: true\n#| messages: false\n#| warning: false\n\nlibrary(sysfonts)\nlibrary(showtext)\nfont_add(family = \"Alegreya\", regular = \"../../../gfonts/fonts/Alegreya/Alegreya-Regular.ttf\")\nfont_add(family = \"Roboto Condensed\", regular = \"../../../gfonts/fonts/RobotoCondensed-Regular.ttf\")\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n```\n\n\n\nShow the Code## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using Google Fonts",
    "text": "Using Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nShow the Codesysfonts::font_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n# set the google fonts as default\nshowtext::showtext_auto()\n\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nTipggformula and ggplot worlds\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\npenguins"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;- gf_point(bill_depth_mm ~ bill_length_mm,\n  data = penguins,\n  alpha = 0.6, size = 3.5\n)\ngf\n\n\n\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(alpha = .6, size = 3.5)\ngg"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\n\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ngf1 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n\n    # colour by continuous variable\n    color = ~body_mass_g,\n    alpha = .6, size = 3.5\n  ) %&gt;%\n  ## custom axes scaling\n  gf_refine(\n    scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n    scale_y_continuous(\n      breaks = seq(12.5, 22.5, by = 2.5),\n      limits = c(12.5, 22.5)\n    ),\n\n    ## custom colors from the scico package\n    ## using the paletteer super package\n    paletteer::scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1),\n\n    ## custom labels\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)\",\n      subtitle = \"A scatter plot of bill depth versus bill length.\",\n      caption = \"Data: Gorman, Williams & Fraser (2014) PLoS ONE\",\n      x = \"Bill Length (mm)\",\n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\" # Neat Way of naming a scale and legend\n    )\n  )\n\ngf1\n\n\n\n\n\n\n\n\n\n\ngg1 &lt;- penguins %&gt;%\n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm),\n    alpha = .6,\n    size = 3.5\n  ) +\n  geom_point(aes(colour = body_mass_g)) +\n\n\n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n\n  ## custom colors from the scico package\n  paletteer::scale_color_paletteer_c(`\"scico::bamako\"`,\n    direction = -1\n  ) +\n\n  ## custom labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) PLoS ONE\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )\ngg1"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using element_markdown()\n",
    "text": "Using element_markdown()\n\n\n\nUsing ggformula\nUsing ggplot\n\n\n\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext\n\ngf2 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~body_mass_g,\n    alpha = 0.6, size = 3.5\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n    scale_y_continuous(\n      breaks = seq(12.5, 22.5, by = 2.5),\n      limits = c(12.5, 22.5)\n    ),\n\n    ## custom colors from the scico package\n    paletteer::scale_color_paletteer_c(\"scico::bamako\",\n      direction = -1\n    ),\n\n    ## custom labels using element_markdown()\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = \"A scatter plot of bill depth versus bill length.\",\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"**Bill Length** (mm)\",\n      y = \"**Bill Depth** (mm)\",\n      color = \"Body mass (g)\"\n    )\n  ) %&gt;%\n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\ngf2\n\n\n\n\n\n\n\n\n\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  paletteer::scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1) +\n\n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "\nelement_markdown() in combination with HTML",
    "text": "element_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## use HTML syntax to change text color\n##\ngf2 %&gt;%\n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;')\n\n\n\n\n\n\n## use HTML syntax to change font and text size\ngf2 %&gt;%\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\n\n\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n\n\n\n\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Adding images to ggplot",
    "text": "Adding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;%\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\n\n\n\n## use HTML syntax to add images to text elements\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Annotations with geom_richtext() and geom_textbox()\n",
    "text": "Annotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n# Create a label tibble\n# Three rich text labels,\n# so three sets of locations x and y, and angle of rotation\nlabels &lt;- tibble(\n  x = c(34, 56, 54),\n  y = c(20, 18.5, 14.5),\n  angle = c(12, 20, 335),\n  species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n  lab = c(\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n  )\n)\nlabels\n\n\n  \n\n\ngf_rich &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~species,\n    alpha = 0.6, size = 3.5, data = penguins\n  ) +\n\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\n\ngg_rich &lt;- ggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_rich"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Formatted Text boxes on plots",
    "text": "Formatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ngf_box &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~species,\n    alpha = 0.6, size = 3.5, data = penguins\n  ) +\n\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n\n\n  # Now for the Plot Titles and Labels, as before\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\n\n\nNote again the use of the plus sign with ggformula. This works!\n\n\n\ngg_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n\n\n  # Now for the Plot Titles and Labels, as before\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using geom_texbox() for formatted text boxes with word wrapping",
    "text": "Using geom_texbox() for formatted text boxes with word wrapping\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y,\n      label = label\n    ),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\",\n    colour = \"black\",\n    # This is ESSENTIAL !!!\n    # It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n    # So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    # box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggforce}\n",
    "text": "Using {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n\nUsing ggformula and ggforce\nUsing ggplot and ggforce\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~body_mass_g,\n    alpha = .6,\n    size = 3.5\n  ) +\n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n\n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n\n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    color = \"black\",\n    # This is good to include\n    # Else ellipses get coloured too\n\n    alpha = .15,\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g),\n    alpha = .6,\n    size = 3.5\n  ) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n\n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  # rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n\n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    alpha = .15,\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\nlibrary(concaveman)\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "ggplot tricks",
    "text": "ggplot tricks\n\ngg0 &lt;-\n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    alpha = .15,\n    show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n  scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )\ngg0\n\n\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\ngg1b &lt;- gg1 + theme(plot.caption.position = \"panel\")\ngg1b\n\n\n\n\n\n\n\nLegend Design\n\ngg1b + theme(legend.position = \"top\")\n\n\n\n\n\n\n# ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b +\n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(\n    title.position = \"top\",\n    title.hjust = .5,\n    barwidth = unit(20, \"lines\"),\n    barheight = unit(.5, \"lines\")\n  ))\n\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"../images/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;- gg2 +\n  annotation_custom(img, ymin = 18, ymax = 28, xmin = 58, xmax = 65) +\n  labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg5"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {patchwork}\n",
    "text": "Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;%\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;%\n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) +\n  geom_violin() +\n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg5 | (gg6 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ngg5 / (gg6 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n)) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "dplyr Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#slides-and-tutorials",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "dplyr Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": " Introduction",
    "text": "Introduction\nWe meet the most important idea in R: tidy data. Once data is tidy, there is a great deal of insight to be obtained from it, by way of tables, graphs and explorations!\nWe will get hands on with dplyr, the R-package that belongs in the tidyverse and is a terrific toolbox to clean, transform, reorder, and summarize your data. And we will be ready to ask Questions of our data and embark on analyzing it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "Readings",
    "text": "Readings\n\nR4DS dplyr chapter\nModernDive dplyr chapter\nRStudio dplyr Cheatsheet"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "title": "Lab-2: Down the R-abbit Hole…",
    "section": "Introduction",
    "text": "Introduction\nWelcome!\nLet’s start our journey to the Garden of Data Visualization, with this terrific presentation by the great ( and sadly late..) Hans Rosling.\nThe best stats you’ve ever seen by Hans Rosling:\n\nWe will run some boiler-plate R code today! Nothing ( almost! ) to code! We will get used to the tools and words of the trade: R, RStudio, installation, packages, libraries…."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nDiagrammeR(\"\n        sequenceDiagram\n\n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry...\n        Arvind -&gt;&gt; Anamika: Ok write it today\n\n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry...\n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n\n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\ngrViz(\"digraph{\n\n      graph[rankdir = LR]\n\n      node[shape = rectangle, style = filled]\n\n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n\n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n\n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n\n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n\n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n\n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n\n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n\n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n\n      }\")\n\n\n\n\n\n\nmermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nDiagrammeR(\n  \"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;\nstyle B fill:#87AB51;\nstyle C fill:#3C8937;\nstyle D fill:#23772C;\nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\ngrViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\n//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\n//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\n[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\n[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\n#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\n#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\n[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\n[&lt;table&gt; table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\n#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\n# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\n[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\n#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html",
    "title": "Using Constructor Objects in p5.js",
    "section": "",
    "text": "p5.js allows us to use Javascript features such as Objects. An Object in p5.js is a form of data + a set of things you can do with that data, what are called methods. Think of an Object as a spanner in your toolkit, which can tell you the gauge/size of mechanical components and also do specific things with these.\nWhen would we need to use these?\nSome libraries in p5.js, such as p5.sound create Oscillator objects, which can generate sounds with different frequencies. And these have methods associated with them, such as turning them on/off, modifying their amplitudes and frequencies (“modulations”).\nWe can also create our own Objects which can include some methods of their own.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#introduction",
    "title": "Using Constructor Objects in p5.js",
    "section": "",
    "text": "p5.js allows us to use Javascript features such as Objects. An Object in p5.js is a form of data + a set of things you can do with that data, what are called methods. Think of an Object as a spanner in your toolkit, which can tell you the gauge/size of mechanical components and also do specific things with these.\nWhen would we need to use these?\nSome libraries in p5.js, such as p5.sound create Oscillator objects, which can generate sounds with different frequencies. And these have methods associated with them, such as turning them on/off, modifying their amplitudes and frequencies (“modulations”).\nWe can also create our own Objects which can include some methods of their own.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#examples-of-objects",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#examples-of-objects",
    "title": "Using Constructor Objects in p5.js",
    "section": "Examples of Objects",
    "text": "Examples of Objects\nLet us see an example of a built-in Object type offered by p5.js. Then we will look at the way in which we can create Objects of our own.\n\nObjects in the p5.sound library\n\n\nCreating our own Obects",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#references",
    "title": "Using Constructor Objects in p5.js",
    "section": "References",
    "text": "References\n\nDan Shiffman, of course:\n\n\n\nhttps://www.cs.cmu.edu/~tcortina/15104-f20/lectures/24-MoreSound.pdf",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html",
    "title": "Tools and Installation",
    "section": "",
    "text": "We will predomnantly use p5.js in this course, since most of use have had some exposure to it and it is of course easy to acquire skill in p5.js rapidly. (Some R code may also be demonstrated.)",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#introduction",
    "title": "Tools and Installation",
    "section": "",
    "text": "We will predomnantly use p5.js in this course, since most of use have had some exposure to it and it is of course easy to acquire skill in p5.js rapidly. (Some R code may also be demonstrated.)",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#the-p5.js-web-editor",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#the-p5.js-web-editor",
    "title": "Tools and Installation",
    "section": "The p5.js web editor",
    "text": "The p5.js web editor\nThe best way to use p5.js is to use its web editor:\np5js web editor",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-p5.js-if-you-have-to",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-p5.js-if-you-have-to",
    "title": "Tools and Installation",
    "section": "Installing p5.js (if you have to)",
    "text": "Installing p5.js (if you have to)\nOne may also install p5.js, for situations when one is not online, e.g. for stand-alone projects and public installations that do not have internet access, like a Cat that has swallowed a 8Raspberry Pi that makes cat calls at passersby at the metro, if you want to go that far. We will then also have to install a web-server so that we can see our code outputs locally.\n\nHead off to https://p5js.org/download/ and download the Complete Library. This is a .zip file that you should extract to a folder named p5 within your ~/Documents folder.\nYour p5 folder will look like this:\n\n\n\n\n\n\nFigure 1: p5 folder",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-visual-studio-code",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-visual-studio-code",
    "title": "Tools and Installation",
    "section": "Installing Visual Studio Code",
    "text": "Installing Visual Studio Code\n\nHead off to https://code.visualstudio.com/download and choose the appropriate file for your OS.\n\n\n\n\n\n\nNoteMac people\n\n\n\nCheck whether you have Apple silicon ( M1/M2/M3…) and choose appropriately. The universal version of the software also seems worth trying.\n\n\nOpen VS Code and click on View -&gt; Explorer. Navigate to your p5 folder and open it. This will allow you to edit all files there and keep track of other files within this “project” folder.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-a-local-webserver",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-a-local-webserver",
    "title": "Tools and Installation",
    "section": "Installing a local WebServer",
    "text": "Installing a local WebServer\nWhen we edit p5.js code within VS Code, we will want to see the resultant HTML rendering, since p5.js puts out an HTML file everytime. We need to bind this output to the browser using a VSCode Extension called, Live Server.\n\nOpen the VS Code extension manager (CTRL-SHIFT-X / CMD-SHIFT-X)\nSearch for and install the Live Server extension.\n\nAdd a p5.js project folder to your VS Code Workspace. You have already done this.\nWith your project’s index.html or sketch.js file open, start the Live Server using the “Go Live” button in the status bar, or by using ALT-L ALT-O.\nYour sketch should now open in your default browser at location: 127.0.0.1:5500. Browser window usually pops up.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#references",
    "title": "Tools and Installation",
    "section": "References",
    "text": "References\n\nThe Coding Train set of video tutorials https://www.youtube.com/@TheCodingTrain\nDan Shiffman. The Nature of Code book. https://natureofcode.com\nCodeAcademy. p5.js short cheatsheet. https://www.codecademy.com/learn/learn-p5js/modules/p5js-introduction-to-creative-coding/cheatsheet",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/listing.html",
    "title": "Tools and Tech",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nTools and Installation\n\n\n2 min\n\n\n\n\n\n\nAdding Libraries to p5.js\n\n\n1 min\n\n\n\n\n\n\nUsing Constructor Objects in p5.js\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html",
    "title": "\n Affine Transformation Fractals",
    "section": "",
    "text": "This is a mathematically created fern! It uses, (gasp!) repeated matrix multiplication and addition!\nWe’ll see.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#inspiration",
    "title": "\n Affine Transformation Fractals",
    "section": "",
    "text": "This is a mathematically created fern! It uses, (gasp!) repeated matrix multiplication and addition!\nWe’ll see.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#introduction",
    "title": "\n Affine Transformation Fractals",
    "section": "Introduction",
    "text": "Introduction\nThe self-similarity of fractals suggests that we could create new fractals from a basic shape using the following procedure:\n\nStart with a basic shape, e.g. a rectangle\nDefine a set of transformations: scaling / mirroring / translation / combination (say n scaled+rotated replicates)\nRun these transformations on the basic shape\nFeed the output back to the input ( Classic IFR )\nWait for the pattern to emerge.\n\nSee the figure below to get an idea of this process.\n\n\n\n\n\nFigure 1: Emerging Fractal\n\n\nWell, this works, provided the transformations include significant amounts of scaling (i.e. reduction in size). You can imagine that if the basic shape does not shrink fast enough, the pattern converges very slowly and would remain chunky even after a large number of iterations.\nSecondly, the number of operations quickly becomes exponentially high, as each stage creates n-fold computation increase. Indeed, if we run \\(d\\) iterations, then the computations scale as \\(n^d\\), which can very quickly become out of hand!!\nSo what to do? Just like with the DeepSeek-R1 algorithm that simplified a great deal of AI algorithms, we have recourse to what is called the Barnsley Algorithm. NOTE: especially note the terrific pictures on this stackexchange page!\nFirst let us understand what are Affine Transformations and then build our fractals.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#what-is-an-affine-transformation",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#what-is-an-affine-transformation",
    "title": "\n Affine Transformation Fractals",
    "section": "What is an Affine Transformation?",
    "text": "What is an Affine Transformation?\nAffine Transformations are defined as a transformations of a space that are:\n\nlinear (no nonlinear functions of an x-coordinate, say \\(e^x\\))\nreversible.\n\nAffine transformations can be represented by matrices which multiply the coordinates of a shape in space. Multiple transformations can be understood a series of matrix multiplications, and can indeed be collapsed into a SINGLE matrix multiplication of the coordinates of the shape.\nSee this webpage at Mathigon to get an idea of rigid transformations of shape.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#some-examples-of-affine-transformations",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#some-examples-of-affine-transformations",
    "title": "\n Affine Transformation Fractals",
    "section": "Some Examples of Affine Transformations",
    "text": "Some Examples of Affine Transformations\n\nHere are some short videos of affine transformations:\n\n\n\n\n\n\n\n\nFigure 2: Scaling Along X\n\n\n\n\n\n\n\n\n\nFigure 3: Scaling Along Y\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Shearing Along X\n\n\n\n\n\n\n\n\n\nFigure 5: Shearing Along Y\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Translation Along X\n\n\n\n\n\n\n\n\n\nFigure 7: Translation Along Y",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#designing-with-affine-transformations",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#designing-with-affine-transformations",
    "title": "\n Affine Transformation Fractals",
    "section": "Designing with Affine Transformations",
    "text": "Designing with Affine Transformations\nSo how do we use these Affine Transformations? Let us paraphrase what Gary William Flake says in his book The Computational Beauty of Nature:\nIf \\(p\\) is a point in space, and its affine transformation(s) is \\(L(p\\)), then:\n\nIf \\(p\\) is on the final fractal, then so is \\(L(p)\\);\nIf \\(p\\) is not part of the final fractal, then \\(L(p)\\) will be atleast closer to the final fractal than \\(p\\) itself.\n\nThese ideas give us our final algorithm for designing a fractal with affine transformations.\n\nStart with any point \\(p\\)\n\nPick a (set of) Affine transformations \\(L_i(p)\\) that allow us to imagine the final shape\nTake the affine transformation \\(L_i(p)\\) of point \\(p\\). Choose \\(i\\) at random!\nUse an IFR: pipe the result back into the input\nMake a large number of iterations\nPlot all intermediate points that come out of the IFR\n\nWith this approach, the points rapidly land up on the fractal which builds up over multiple iterations. We can start anywhere in space and it will still converge.\nThe additional feature of the Barnsley algorithm is the randomness: since most fractals use not one but several affine transformations to create a multiplicity of forms, at each iteration we can randomly choose between them!\nThe block diagram of the Barnsley Algorithm looks like this:\n\n\n\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\nHow to understand this sketch? Here is Dan Shiffman again!\n\n\n\nIn the code below, the Affine transformations \\(Af_i\\) are of the form\n\\[\nAF_i = A_i * X + B_i, ~ i = 1...4\n\\tag{1}\\]\nwith four options each for matrix \\(A\\) and matrix \\(B\\), and \\(X = (x,y)\\), the current point coordinates (seed input, then output feedback for recursion). There are 50000 iterations performed and at each interation, a random A and a random B are picked to provide the Affine Transformation for that iteration.\nThe starting “seed point” is simply \\(X = (0,0)\\).\nThe probabilities with which each affine transformation is chosen are not all equal; these can be tweaked to see the effect on the fractal.\nThe four options for the \\(A_i\\) matrices are:\nShow the CodeA &lt;- vector(mode = \"list\", length = 4)\n# Four Affine translation Matrices\nA[[1]] &lt;- matrix(c(0, 0, 0, 0.18), nrow = 2)\nA[[2]] &lt;- matrix(c(0.85, -0.04, 0.04, 0.85), nrow = 2)\nA[[3]] &lt;- matrix(c(0.2, 0.23, -0.26, 0.22), nrow = 2)\nA[[4]] &lt;- matrix(c(-0.15, 0.36, 0.28, 0.24), nrow = 2)\nas_sym(A[[1]])\nas_sym(A[[2]])\nas_sym(A[[3]])\nas_sym(A[[4]])\n\n\n\n\nc: ⎡0   0  ⎤\n   ⎢       ⎥\n   ⎣0  0.18⎦\nc: ⎡0.85   0.04⎤\n   ⎢           ⎥\n   ⎣-0.04  0.85⎦\nc: ⎡0.2   -0.26⎤\n   ⎢           ⎥\n   ⎣0.23  0.22 ⎦\nc: ⎡-0.15  0.28⎤\n   ⎢           ⎥\n   ⎣0.36   0.24⎦\n\n\n\n\n\n$$\n\\[\\begin{bmatrix}\n0.00 & 0.00 \\\\\n0.00 & 0.18 \\\\\n\\end{bmatrix}\\]\n$$ {#eq-A1}\n$$\n\\[\\begin{bmatrix}\n0.85 &  0.04 \\\\\n-0.04 &  0.85 \\\\\n\\end{bmatrix}\\]\n$$\n$$\n\\[\\begin{bmatrix}\n0.20 & -0.26 \\\\\n0.23 &  0.22 \\\\\n\\end{bmatrix}\\]\n$$\n$$\n\\[\\begin{bmatrix}\n-0.15 &  0.28 \\\\\n0.36 &  0.24 \\\\\n\\end{bmatrix}\\]\n$$\n\n\n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{V}\n\\tag{2}\\]\n\n\n$$\n\\[\\begin{bmatrix}\n0.00 & 0.00 \\\\\n0.00 & 0.18 \\\\\n\\end{bmatrix}\\]\n$$ {#eq-A1}\n\nAnd the four options for the \\(B_i\\) matrices are:\n\nShow the Code# Four Simple translation Matrices\nb &lt;- vector(mode = \"list\", length = 4)\nb[[1]] &lt;- matrix(c(0, 0))\nb[[2]] &lt;- matrix(c(0, 1.6))\nb[[3]] &lt;- matrix(c(0, 1.6))\nb[[4]] &lt;- matrix(c(0, 0.54))\nas_sym(b[[1]])\nas_sym(b[[2]])\nas_sym(b[[3]])\nas_sym(b[[4]])\n\nc: [0  0]ᵀ\nc: [0  1.6]ᵀ\nc: [0  1.6]ᵀ\nc: [0  0.54]ᵀ\n\n\nBy randomly choosing any of the \\(16\\) resulting transformations, with different but fixed probablilities, we compute and render the Barnsley fern:\n\nShow the Code# Iteratively build the fern\ntheme_set(theme_custom())\n#\nn &lt;- 50000\nx &lt;- numeric(n)\ny &lt;- numeric(n)\nx[1] &lt;- 0\ny[1] &lt;- 0 # Starting point (0,0). Can be anything!\n\nfor (i in 1:(n - 1)) {\n  # Randomly sample the 4 + 4 translations based on a probability\n  # Change these to try different kinds of ferns\n  trans &lt;- sample(1:4, prob = c(.02, .9, .09, .08), size = 1)\n\n  # Translate **current** xy based on the selected translation\n  # Apply one of 16 possible affine transformations\n  xy &lt;- A[[trans]] %*% c(x[i], y[i]) + b[[trans]]\n  x[i + 1] &lt;- xy[1] # Save x component\n  y[i + 1] &lt;- xy[2] # Save y component\n}\n# Plot this baby\n# plot(y,x,col= \"pink\",cex=0.1)\ngf_point(y ~ x,\n  colour = \"lightgreen\", size = 0.02,\n  title = \"Barnsley Fern\"\n)\n\n\n\n\n\n\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[1]])\nas_sym(X)\nas_sym(A[[1]]) %*% as_sym(X)\n\nc: ⎡0   0  ⎤\n   ⎢       ⎥\n   ⎣0  0.18⎦\nc: [5  5]ᵀ\nc: [0  0.9]ᵀ\n\nVertical movement with Shrinkage\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[2]])\nas_sym(X)\nas_sym(A[[2]]) %*% as_sym(X)\n\nc: ⎡0.85   0.04⎤\n   ⎢           ⎥\n   ⎣-0.04  0.85⎦\nc: [5  5]ᵀ\nc: [4.45  4.05]ᵀ\n\nModest Shrinkage of Both X and Y, X more than Y\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[3]])\nas_sym(X)\nas_sym(A[[3]]) %*% as_sym(X)\n\nc: ⎡0.2   -0.26⎤\n   ⎢           ⎥\n   ⎣0.23  0.22 ⎦\nc: [5  5]ᵀ\nc: [-0.3  2.25]ᵀ\n\nLarge Shrinkage of Both X and Y, Y more than X\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[4]])\nas_sym(X)\nas_sym(A[[4]]) %*% as_sym(X)\n\nc: ⎡-0.15  0.28⎤\n   ⎢           ⎥\n   ⎣0.36   0.24⎦\nc: [5  5]ᵀ\nc: [0.65  3.0]ᵀ\n\nShrinkage of Both X and Y, X more than Y",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#wait-but-why",
    "title": "\n Affine Transformation Fractals",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nOK, so why did this become a fern??\nIf we look at the list of affine transformations, we see that there are essentially 4 movements possible: https://en.wikipedia.org/wiki/Barnsley_fern\n\na simple vertical y-axis movement, with shrinkage\na gentle rotation with very little shrinkage\na rotation to the right with shrinkage\na rotation to the left with shrinkage\n\nThe second transformation is the one most commonly used!! The others are relatively rarely used! So the points slowly slope to the right and do now get squashed up close to the start: they retain sufficient size in (x,y) coordinates for the fern to slowly spread to the right.\nSo we can design the affine transformations based on an intuition of how we might draw the fractal by hand, say larger strokes to the right, smaller to the left etc, and and decide on the frequency of strokes based on how often these strokes might be used in drawing.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#references",
    "title": "\n Affine Transformation Fractals",
    "section": "\n References",
    "text": "References\n\nRyan Bradley-Evans. (Oct 7, 2020). Barnsley’s Fern Fractal in R. https://astro-ryan.medium.com/barnsleys-fern-fractal-in-r-e52a357e23db\n\nAffine Transformations @ The Algorithm Archive. https://www.algorithm-archive.org/contents/affine_transformations/affine_transformations.html\n\nIterated Function systems @ The Algorithm Archivehttps://www.algorithm-archive.org/contents/IFS/IFS.html\n\np5.js Tutorial: Coordinates and Transformations. https://p5js.org/tutorials/coordinates-and-transformations/\n\nThe Coding Train: Algorithmic Botany. https://thecodingtrain.com/tracks/algorithmic-botany\n\nBarnsley Fern @ Wikipedia https://en.wikipedia.org/wiki/Barnsley_fern\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncaracas\n2.1.1\nAndersen and Højsgaard (2023)\n\n\nmatlib\n1.0.0\nFriendly, Fox, and Chalmers (2024)\n\n\n\n\n\n\nAndersen, Mikkel Meyer, and Søren Højsgaard. 2023. caracas: Computer Algebra. https://doi.org/10.32614/CRAN.package.caracas.\n\n\nFriendly, Michael, John Fox, and Phil Chalmers. 2024. matlib: Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics. https://doi.org/10.32614/CRAN.package.matlib.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/listing.html",
    "title": "Geometry",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n  Circles\n\n\n25 min\n\n\n\n\n\n\n Complex Numbers\n\n\n6 min\n\n\n\n\n\n\nFractals\n\n\n10 min\n\n\n\n\n\n\n  Affine Transformation Fractals\n\n\n16 min\n\n\n\n\n\n\nL-Systems\n\n\n17 min\n\n\n\n\n\n\nKolams and Lusona\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html",
    "title": "L-Systems",
    "section": "",
    "text": "Trees\n“I think that I shall never see\nA poem lovely as a tree.\nA tree whose hungry mouth is prest\nAgainst the earth’s sweet flowing breast;\nA tree that looks at God all day,\nAnd lifts her leafy arms to pray;\nA tree that may in Summer wear\nA nest of robins in her hair;\nUpon whose bosom snow has lain;\nWho intimately lives with rain.\nPoems are made by fools like me,\nBut only God can make a tree.”\n— Joyce Kilmer, 1915",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#inspiration",
    "title": "L-Systems",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\nJapanese Daisugi",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#introduction",
    "title": "L-Systems",
    "section": "Introduction",
    "text": "Introduction\nTrees are fractal in nature, meaning that patterns created by the large structures, such as the main branches, repeat themselves in smaller structures, such as smaller branches…. a universal growth pattern first observed by Leonardo da Vinci 500 years ago: a simple yet startling relationship that always holds between the size of a tree’s trunk and sizes of its branches.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#an-introduction-to-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#an-introduction-to-l-systems",
    "title": "L-Systems",
    "section": "An Introduction to L-Systems",
    "text": "An Introduction to L-Systems\nFrom Job Talle’s Website:\nLindenmayer systems were originally conceived by Hungarian biologist Aristid Lindenmayer while studying algae growth. He developed L-systems as a way to describe the growth process of algae and simple plants. The result was a type of language in which the recursive and self similar properties of organism growth can be expressed. Indeed, L-systems can be used to generate natural patterns, but well known mathematical patterns can also be written as an L-system. In this article, I will explain different flavors of L-systems, and I will demonstrate them by rendering 2D Lindenmayer systems and 3D Lindenmayer systems using turtle graphics.\nThe language is very simple. It consists of symbols (the alphabet) and production rules. The first state of the sentence is called the axiom. The production rules can be applied repeatedly on this axiom to evolve or grow the system. A simple example would be a system with the axiom \\(AA\\), and the rule \\(A→ABA\\).\nYou can see how a self expanding sentence can be analogous to cell division in plants and other biological processes.\n\\[\naxiom: AA\n\\] \\[\nProduction ~ Rule: A --&gt; ABA\n\\]\n\\[\nIterations:\\\\\\\n1. A\\\\\\\n\\]\n\\[\n2. ~\\color{magenta}{A}~\\color{Black}B~\\color{magenta}{A}\\\\\\\n\\] \\[\n3.~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}\\\\\\\n\\] \\[\n4.~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}~~\\color{Black}B~~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}\n\\]\nL-system Structures thus develop through a process of string rewriting. A string of letters is transformed into a new string of letters using simple rules called productions. The process is repeated indefinitely, each time using the string that was just produced as the source for the next string. Because the strings tend to grow with each rewrite, an L-system can become arbitrarily complex, but always guided by a simple process dictated by a fixed set of simple rules. In this respect, L-systems are a manifestation of Complexity Phenomena.\nAll right, but how does this become a tree??\nTwo things need to be done:\n\nEach symbol in the language needs to be mapped to a left branch or a right branch. (with turn angle)\nAt each application of the production rules, branch size is scaled down by a number.\n\n\n\n\n\n\nFigure 1\n\n\nImage Courtesy Christophe Eloy | University of Provence.\nIn the Figure 1, the RHS shows a typical figure generated by the L-system language. This figure shows both the LHS and RHS turns, and the fairly rapid reduction in the size of the branches.\nLet us see how we can create “Algorithmic Trees”.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#creating-trees-using-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#creating-trees-using-l-systems",
    "title": "L-Systems",
    "section": "Creating Trees using L-systems",
    "text": "Creating Trees using L-systems\n\n\nUsing p5.js\nUsing R\nFractal Grower\n\n\n\n\n\n\n\n\n\nWe will use the package LindenmayerR to create our tree.\n\nShow the Codelibrary(tidyverse)\n###\nlibrary(LindenmayeR)\nlibrary(LearnGeom)\nlibrary(TurtleGraphics)\noptions(max.print = 20)\n\n\n\n# Lindemayer tree\n\n## Dictionary of Symbols\ndictionary &lt;- data.frame(\n  symbol = c(\"F\", \"f\", \"+\", \"-\", \"[\", \"]\"), # symbol column\n  action = c(\"F\", \"f\", \"+\", \"-\", \"[\", \"]\"), # action column\n  stringsAsFactors = FALSE\n)\n\n## Axioms to start and morph\ntree_morph_rules &lt;- data.frame(\n  inp = c(\"F\"), # starting axiom\n  out = c(\"F[+F][-F]\"), # Morphing Rule\n  stringsAsFactors = FALSE\n)\n\n## Build the Tree with commands\nLtree &lt;- Lsys(\n  init = \"F\",\n  rules = tree_morph_rules,\n  n = 2,\n  verbose = 0, # No progress messages please...\n  retAll = FALSE # One Vector output at the end only\n)\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[1]]\n  start end    insert\n1     1   1 F[+F][-F]\n\n[[1]]\n     start end\n[1,]     1   1\n[2,]     4   4\n[3,]     8   8\n\n[[1]]\n  start end    insert\n1     1   1 F[+F][-F]\n2     4   4 F[+F][-F]\n3     8   8 F[+F][-F]\n\n## Now draw the tree\ndrawLsys(\n  string = Ltree,\n  drules = dictionary,\n  stepSize = 10, shrinkFactor = 1.2, # integers shrink!\n  ang = 12,\n  st = c(50, 10, 90) # Root Position x, y, angle from bottom-left\n)\n\n\n\n\n\n\n\nA more complex, and more botanical-looking, tree or seaweed:\n\n## Define Axiom and Mutation Rules\nfractal_tree_rules &lt;- data.frame(\n  inp = c(\"X\", \"F\"),\n  out = c(\"F-[[X]+X]+F[+FX]-X\", \"FF\"),\n  stringsAsFactors = FALSE\n)\n\n## Create the Algorithmic Tree\nfractal_tree &lt;- Lsys(\n  init = \"X\", # Axiom\n  rules = fractal_tree_rules,\n  n = 4,\n  verbose = 0,\n  retAll = FALSE\n)\n\n## Now draw the tree\ndrawLsys(\n  string = fractal_tree,\n  drules = dictionary,\n  stepSize = 2, # Shrink by half each time\n  ang = runif(n = 1, min = 3, max = 30),\n  st = c(50, 5, 90), # Origin of tree\n  gp = gpar(col = \"chocolate4\", fill = \"honeydew\")\n)\n\ngrid.text(\"Fractal Seaweed (n = 4)\", 0.25, 0.25)\n\n\n\n\n\n\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[1]]\n  start end             insert\n1     1   1 F-[[X]+X]+F[+FX]-X\n\n[[1]]\n     start end\n[1,]     5   5\n[2,]     8   8\n[3,]    15  15\n[4,]    18  18\n\n[[2]]\n     start end\n[1,]     1   1\n[2,]    11  11\n[3,]    14  14\n\n[[1]]\n  start end             insert\n1     5   5 F-[[X]+X]+F[+FX]-X\n2     8   8 F-[[X]+X]+F[+FX]-X\n3    15  15 F-[[X]+X]+F[+FX]-X\n4    18  18 F-[[X]+X]+F[+FX]-X\n\n[[2]]\n  start end insert\n1     1   1     FF\n2    11  11     FF\n3    14  14     FF\n\n[[1]]\n      start end\n [1,]    10  10\n [2,]    13  13\n [3,]    20  20\n [4,]    23  23\n [5,]    30  30\n [6,]    33  33\n [7,]    40  40\n [8,]    43  43\n [9,]    56  56\n[10,]    59  59\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n[[2]]\n      start end\n [1,]     1   1\n [2,]     2   2\n [3,]     6   6\n [4,]    16  16\n [5,]    19  19\n [6,]    26  26\n [7,]    36  36\n [8,]    39  39\n [9,]    46  46\n[10,]    47  47\n [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]\n\n[[1]]\n  start end             insert\n1    10  10 F-[[X]+X]+F[+FX]-X\n2    13  13 F-[[X]+X]+F[+FX]-X\n3    20  20 F-[[X]+X]+F[+FX]-X\n4    23  23 F-[[X]+X]+F[+FX]-X\n5    30  30 F-[[X]+X]+F[+FX]-X\n6    33  33 F-[[X]+X]+F[+FX]-X\n [ reached 'max' / getOption(\"max.print\") -- omitted 10 rows ]\n\n[[2]]\n  start end insert\n1     1   1     FF\n2     2   2     FF\n3     6   6     FF\n4    16  16     FF\n5    19  19     FF\n6    26  26     FF\n [ reached 'max' / getOption(\"max.print\") -- omitted 12 rows ]\n\n[[1]]\n      start end\n [1,]    17  17\n [2,]    20  20\n [3,]    27  27\n [4,]    30  30\n [5,]    37  37\n [6,]    40  40\n [7,]    47  47\n [8,]    50  50\n [9,]    63  63\n[10,]    66  66\n [ reached 'max' / getOption(\"max.print\") -- omitted 54 rows ]\n\n[[2]]\n      start end\n [1,]     1   1\n [2,]     2   2\n [3,]     3   3\n [4,]     4   4\n [5,]     8   8\n [6,]     9   9\n [7,]    13  13\n [8,]    23  23\n [9,]    26  26\n[10,]    33  33\n [ reached 'max' / getOption(\"max.print\") -- omitted 74 rows ]\n\n[[1]]\n  start end             insert\n1    17  17 F-[[X]+X]+F[+FX]-X\n2    20  20 F-[[X]+X]+F[+FX]-X\n3    27  27 F-[[X]+X]+F[+FX]-X\n4    30  30 F-[[X]+X]+F[+FX]-X\n5    37  37 F-[[X]+X]+F[+FX]-X\n6    40  40 F-[[X]+X]+F[+FX]-X\n [ reached 'max' / getOption(\"max.print\") -- omitted 58 rows ]\n\n[[2]]\n  start end insert\n1     1   1     FF\n2     2   2     FF\n3     3   3     FF\n4     4   4     FF\n5     8   8     FF\n6     9   9     FF\n [ reached 'max' / getOption(\"max.print\") -- omitted 78 rows ]\n\n\n\n\nHead off to https://www.cs.unm.edu/~joel/PaperFoldingFractal/paper.html. Download the .jar file and save it say in your Documents folder. Open and play. Instructions are on the website. Make note of how the scaling factor works here.\nSome suggestions!! Note the alphabet!!! \n\n\n\n\n\n\n\nCol1\nCol2\n\n\n\n\n\nPythagorean Tree:\n\n\nAxiom: [++!++!++!]xy\n\nProduction Rules:\n\nx =\n\n[-[!++!++!++!]!xy\n\ny =\n\n\n\n[+![!++!++!++!]!xy\n-   startAngle: 0\n-   turnAngle: 45.0\n-   growth : 1.408\n\n\n |\n\n\n\nB. Krishna’s Anklets:\n\n\nAxiom: ++af-h-f+h\n-   f\n\n\n## h\nf++h++f-h-f++h++f-h-f++h\n\n\n\n\nProduction Rules:\n\nf = f - h - f ++ h ++ f - h - f\nstartAngle: 0\nturnAngle: 45.0\ngrowth:1.0",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#design-principles-for-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#design-principles-for-l-systems",
    "title": "L-Systems",
    "section": "Design Principles for L-Systems",
    "text": "Design Principles for L-Systems\n\nPick a set of symbols. i.e. our alphabet (say two or three letters of the alphabet)\nMap these to specific movements in the growth of the tree\nDecide on an axiom. It can include one or more of the symbols.\nDecide on a (set of) production rules. These should generate all the symbols in our alphabet. (Why?)\nDecide on a scaling factor\n\nApply the production rules multiple times starting with the axiom, develop an extensive string using this recursion\nPlot the resulting string, scaling the individual recursions by the scaling factor.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#wait-but-why",
    "title": "L-Systems",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBy making use of “just sufficient randomness” in a few parameters, it is possible to generate very organic-looking trees\nThese tree-like layouts can show up in a surprising number of places, such as transport networks, residential layouts.\nThe multiple iterations generated from a few simple rules embody all the complexity of a language.\nTrees become a great metaphor for a diverse set of things and ideas.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#references",
    "title": "L-Systems",
    "section": "\n References",
    "text": "References\n\nJob Talle. Lindenmayer Systems https://jobtalle.com/lindenmayer_systems.html\n\nC. J Jennings. L-systems. https://cgjennings.ca/articles/l-systems/\n\nJoel Castellanos. Fractal Grower: Java Software for Growing Lindenmayer Substitution Fractals (L-systems). https://www.cs.unm.edu/~joel/PaperFoldingFractal/paper.html\n\nPaul Bourke. L systems User Notes. https://paulbourke.net/fractals/lsys/\n\nPrzemysław Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, 1990.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html",
    "title": "  Dancing With Newton",
    "section": "",
    "text": "What do we recall from our school-time encounter with Newtonian Physics?\nWe have several aspects to movement, e.g.:\n\nposition\nvelocity\nacceleration\nrotation\noscillation\nfriction\n\nAnd here are some more terms from the spacecraft/rocket science domain:\n\nroll\npitch\nyaw\njerk\n\nAnything else?\nWe need to think of a way in which these aspects can be modelled and coded to suit our creative needs. Let us define these terms once again, so that we know what we are talking about!\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere an object is, say in {x,y} coordinates.\n\n\nIn p5.js we can create the coordinates of an object separately, as x and y, OR create a p5.Vector object that can manipulate both coordinates together.\nWhy would we manipulate coordinates? Because we want things to move! Movement is manipulation of position.\n\n\n\n\n\n\n\n\n\nNoteVelocity\n\n\n\nRate of change of distance travelled. So \\(velocity \\sim distance/time\\). We can have movement in the x or y direction independently, or both movements can be simultaneous ( but not necessarily equal!) using the p5.Vector objects.\n\n\nHow do we model this in code? p5.js has a FrameRate (60 frames/second. Check this!!); this is the rate at which the code computes and refreshes the display. We can take individual x and y positions and add something to them each frame, causing the object to move. If the object is a p5.Vector object, we manipulate these coordinates using Vector arithmetic, which we will peep into shortly."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html#introduction",
    "title": "  Dancing With Newton",
    "section": "",
    "text": "What do we recall from our school-time encounter with Newtonian Physics?\nWe have several aspects to movement, e.g.:\n\nposition\nvelocity\nacceleration\nrotation\noscillation\nfriction\n\nAnd here are some more terms from the spacecraft/rocket science domain:\n\nroll\npitch\nyaw\njerk\n\nAnything else?\nWe need to think of a way in which these aspects can be modelled and coded to suit our creative needs. Let us define these terms once again, so that we know what we are talking about!\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere an object is, say in {x,y} coordinates.\n\n\nIn p5.js we can create the coordinates of an object separately, as x and y, OR create a p5.Vector object that can manipulate both coordinates together.\nWhy would we manipulate coordinates? Because we want things to move! Movement is manipulation of position.\n\n\n\n\n\n\n\n\n\nNoteVelocity\n\n\n\nRate of change of distance travelled. So \\(velocity \\sim distance/time\\). We can have movement in the x or y direction independently, or both movements can be simultaneous ( but not necessarily equal!) using the p5.Vector objects.\n\n\nHow do we model this in code? p5.js has a FrameRate (60 frames/second. Check this!!); this is the rate at which the code computes and refreshes the display. We can take individual x and y positions and add something to them each frame, causing the object to move. If the object is a p5.Vector object, we manipulate these coordinates using Vector arithmetic, which we will peep into shortly."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/20-Vectors/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/20-Vectors/index.html#references",
    "title": "\n Vectors",
    "section": "References",
    "text": "References\n\nhttps://www.math.hkust.edu.hk/~machas/vector-calculus-for-engineers.pdf",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Vectors"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "",
    "text": "In this hopefully short and crisp tour, we will look at elementary operations on matrices: transpose, addition/subtraction and multiplication, and finally the inverse.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#introduction",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "",
    "text": "In this hopefully short and crisp tour, we will look at elementary operations on matrices: transpose, addition/subtraction and multiplication, and finally the inverse.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#what-is-a-matrix",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#what-is-a-matrix",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "What is a Matrix?",
    "text": "What is a Matrix?\nAn rectangular array of numbers, arranged with rows and columns. A few examples are:\n\n\nEquation 1 is a 3 x 3 matrix (3 rows and 3 columns).\n\n\\[\n\\begin{bmatrix}\n2,3,3\\\\\n3,1,2\\\\\n1,2,4\\\\\n\\end{bmatrix}\n\\tag{1}\\]\n\nEquation 2 is a 3 x 4 matrix (3 rows and 4 columns).\n\n\\[\n\\begin{bmatrix}\n2,3,3,4\\\\\n3,1,2,3\\\\\n1,2,4,1\\\\\n\\end{bmatrix}\n\\tag{2}\\]\n\nEquation 3 is a 1 x 4 matrix (1 rows and 4 columns).\n\n\\[\n\\begin{bmatrix}\n2 ~ 3 ~ 4 ~ 2\\\\\n\\end{bmatrix}\n\\tag{3}\\]\n\nEquation 2 is a 3 x 1 matrix (3 rows and 1 columns).\n\n\\[\n\\begin{bmatrix}\n2\\\\\n3\\\\\n1\\\\\n\\end{bmatrix}\n\\tag{4}\\]",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#transpose",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#transpose",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Transpose",
    "text": "Transpose\nA matrix transpose is accomplished by writing rows as columns and vice versa. Hence an \\(r~ x ~ c\\) matrix becomes transposed into a \\(c~ x ~ r\\) matrix, as shown below:\n\n\n\n\nc: ⎡a  c  1⎤\n   ⎢       ⎥\n   ⎣b  d  2⎦\n\n\n\n\n\nc: ⎡a  b⎤\n   ⎢    ⎥\n   ⎢c  d⎥\n   ⎢    ⎥\n   ⎣1  2⎦",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#addition-and-subtraction",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#addition-and-subtraction",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Addition and Subtraction",
    "text": "Addition and Subtraction\nMatrices can be added or subtracted when they are of the same dimensions, rows and columns. Matrix elements are added or subtracted in element-wise fashion.\nAddition\n\n\n\n\nc: ⎡a  0⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡a  c⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡2⋅a  c⎤\n   ⎢      ⎥\n   ⎣2⋅b  2⎦\n\n\n\n\nSubtraction\n\n\n\n\nc: ⎡a  0⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡a  c⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡0  -c⎤\n   ⎢     ⎥\n   ⎣0  0 ⎦",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#multiplication",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#multiplication",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Multiplication",
    "text": "Multiplication\nMatrix multiplication can be visualized in many ways!! According to Gilbert Strang’s famous book on Linear Algebra, there are no less than 4 very useful ways. Let us understand perhaps the easiest one, Linear combination of Columns.\nSuppose there are three students who bought two kinds of food and drink:\n\nStudent A: 2 samosas and 1 chai\nStudent B: 3 samosas and 0 chai\nStudent C: 0 samosa and 2 chai-s (Just before Arvind’s R class.)\n\nSamosas cost 20 and chai costs 10. How much do they pay?\n\n\n\\[\n\\left[\\begin{matrix}\\color{red}{2} & \\color{blue}{1}\\\\\\\\\\color{red}{3} & \\color{blue}{0}\\\\\\\\\\color{red}{0} & \\color{blue}{2}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}\\color{red}{20}\\\\\\\\\\color{blue}{10}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}50\\\\\\\\60\\\\\\\\20\\end{matrix}\\right]\n\\]\n\n\n\n\n\\[A\\]\n\n\\[B\\]\n\n\\[C\\]\n\n\nWe see that the first number \\(20\\) in B multiplies the entire first column in A, and the second number \\(10\\) multiplies the entire second column in A. These two multiplied columns are added to get the matrix C.\nSuppose they had two options for shops? And the prices were different?\n\nSamosas cost 20 and chai costs 10 at Shop#1. (as before)\nSamosas cost 15 and chai costs 15 at Shop#2.\n\n\n\n\\[\n\\left[\\begin{matrix}\\color{red}{2} & \\color{blue}{1}\\\\\\\\\\color{red}{3} & \\color{blue}{0}\\\\\\\\\\color{red}{0} & \\color{blue}{2}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}20 & 15\\\\\\\\10 & 15\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}50 & 45\\\\\\\\60 & 45\\\\\\\\20 & 30\\end{matrix}\\right]\n\\]\n\n\n\n\n\\[A2\\]\n\n\\[B2\\]\n\n\\[C2\\]\n\n\nShop#2 is cheaper. Drag peasant#3 and go there, peasants#1 and #2.\nHow did this multiplication happen? Same story as with one shop/prices, repeated to handle the second shop. Matrix B2 now has two columns for prices, for Shop#1 and Shop#2. Multiplication takes each column in B2 and does the same weighted-column-addition with A2. Hence two columns of answers in C2.\n\n\n\n\n\n\nImportantMatrix Multiplication in 4 ways\n\n\n\nAs shown in Gilbert Strang’s book, there are 4 ways of thinking about matrix multiplication:\n\nOur method of weighting columns in A using columns B and adding\nWeighting rows in B using rows in A and adding\nMultiplying individual values from rows in A and columns in B and adding up, for each location in C. ( This is the common textbook method)\nMultiplying rows in A and columns in B. ( A vectorized version of the previous method).\n\nFor now, this one method should suffice.\nReferences\n\nGilbert Strang. Linear Algebra and its Applications. Thomson/Brooks-Cole.\nGlenn Henshaw.(Sep 28, 2019) Three Ways to Understand Matrix Multiplication. https://ghenshaw-work.medium.com/3-ways-to-understand-matrix-multiplication-fe8a007d7b26",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#references",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "References",
    "text": "References\n\nGilbert Strang. Linear Algebra and its Applications. Thomson/Brooks-Cole.\nGlenn Henshaw.(Sep 28, 2019) Three Ways to Understand Matrix Multiplication. https://ghenshaw-work.medium.com/3-ways-to-understand-matrix-multiplication-fe8a007d7b26",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/listing.html",
    "title": "Math Concepts",
    "section": "",
    "text": "No matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Maths Basics"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#introduction",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#activities",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#activities",
    "title": "Working with Networks",
    "section": "Activities",
    "text": "Activities\nActivity-1: Secret Santa Game\nLet us play this in the vanilla way: Paper chits with names in a bin and drawing them in turn. What can go wrong with this?\nShould we use this instead? https://www.drawnames.com.sg/secret-santa-generator\nDiscussion: Nodes, Links, Link Directionality, Connected and Disconnected Networks\nActivity-2: Barabasi Cocktail Party Game\nThis is a game “invented” by Alberto-Laszlo Barabasi, a Network Science pioneer and expert, who has written a wonderful, and wonderfully acessible, book on Network Science, available online http://networksciencebook.com/\nDiscussion: Network Mechanisms, Information Flow, Giant Component,Emergence\nActivity-3: Indian Surnames Game\n\nHow many common India surnames do we know? Let us write them on the board.\nEach of us will now look at each surname and recollect how many people they know with that surname.\nWrite down the score for each surname.\nLet’s plot this on (yet another ) network!!\nTry also to create a map using this website: The Memory Underground https://memoryunderground.com\n\n\nDiscussion: Node Degree, Giant Component?  Small Worlds?  Multi-Link network, Link Values or Costs\nActivity-4: Way-Spotting Game\nNow that we have an idea of nodes, links and costs, let us get an experience of some more network science ideas:\n\nMake groups of 3.\nHead over to http://www.wayspotting.com/index.html\n\nPlay!! Make a note of your route each time ( your “traversal” of the network)\nNote if you can see the following:\n\n\n\nFrequently Used Nodes\nFrequently used Links\n\n\n\nSee here for more info: https://medium.com/@ran_katzir/teaching-network-science-using-board-games-f78489a3b3bd\n\n\nDiscussion: Network Traversal, Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-5: Hi, I am Kevin Bacon, SMI Foundation 2022 Batch\nLet us find a Keven Bacon in SMI Foundation Studies Programme!!\n\nCollect friends Data from across college/class, import and plot, analyze and comment\nUse this online tool at DataBasic.io https://databasic.io to Connect the Dots, OR\nEven more fun at at GraphCommons https://graphcommons.com/graphs/new\n\n\nDiscussion: Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-6: Can you Introduce me to Chandler, again?\n\nTake your favourite Literary Work / TV Serial / Movie and create a Network Database for it.\nVisualize it either with or without tech tools From Teach Engineering, this Activity Sheet https://www.teachengineering.org/activities/view/uno_graphtheory_lesson01_activity2\n\nCan also use Graph Comicshttps://aviz.fr/~bbach/graphcomics/\n\n\nDiscussion: Networks are everywhere, Cannot \"unsee\" them, You are a node and you are a link..are you?\nActivity-7: Why are all Metro Maps at 45 degrees?\n\nhttps://artsandculture.google.com/asset/the-tate-gallery-by-tube-david-booth/PAG-Gx_SV2jNiw?hl=en\nhttps://search.r-project.org/CRAN/refmans/ggraph/html/layout_tbl_graph_metro.html\nHenry Beck’s London Underground Map. https://artsandculture.google.com/asset/underground-map-henry-c-beck-london-transport-and-waterlow-sons-ltd/fAHJweSexswKxw?hl=en and\n\nhttps://g.co/arts/UNfBm6QL5kukYm4r9"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#references",
    "title": "Working with Networks",
    "section": "References",
    "text": "References\n\nDmitry Zinoniev, Network Science Intro Slides, https://www.slideshare.net/DmitryZinoviev/workshop-20212296\nMark Newman, The Physics of Networks,Read the PDF\nA Network oriented short story. Frigyes Karinthy, “Chains”. Read PDF\nWho told you about Srishti? Where? Read Mark Granovetter, The Strength of Weak Ties. https://www.cs.cmu.edu/~jure/pub/papers/granovetter73ties.pdf\nPlamen Ch. Ivanov.(30 June 2021) The New Field of Network Physiology: Building the Human Physiolome. https://doi.org/10.3389/fnetp.2021.711778"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/listing.html",
    "title": "Systems",
    "section": "",
    "text": "No matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html",
    "title": "The Multilayer Perceptron",
    "section": "",
    "text": "This was our bare bones Perceptron, or neuron as we will refer to it henceforth:\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\]\n\n\n\nFor the multi-layer perceptron, two changes were made:\n\nChanging the hard-threshold activation into a more soft sigmoid activation\naddition of (one or more ) hidden layers.\n\nLet us discuss these changes in detail.\n\n\nWe said earlier that the weighting and adding is a linear operation.\nWhile this is great, simple linear translations of data are not capable of generating what we might call learning or generalization ability.\nThe outout of the perceptron is a “learning decision” that is made by deciding if the combined output is greater or smaller than a threshold.\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 1: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nThe hard-threshold used in the Perceptron allowed us to make certain decisions based on linear combinations of the input data. But what is the dataset possesses classes that are not separable in a linear way? What if different categories of points are intertwined with a curved boundary between classes?\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 2: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nSo how do we implement this nonlinear Activation Block?\n\nOne of the popular functions used in the Activation Block is a function based on the exponential function \\(e^x\\).\nWhy? Because this function retains is identity when differentiated! This is a very convenient property!\n\n\n\nSigmoid Activation\n\n\n\n\n\n\n\nNoteRemembering Logistic Regression\n\n\n\nRecall your study of Logistic Regression. There, the Sigmoid function was used to model the odds of the (Qualitative) target variable against the (Quantitative) predictor.\n\n\n\n\n\n\n\n\nNoteBut Why Sigmoid?\n\n\n\nBecause the Sigmoid function is differentiable. And linear in the mid ranges. Oh, and remember the Chain Rule?\n\\[\n\\begin{align}\n\\frac{df(x)}{dx}\n&= \\frac{d}{dx} * \\frac{1}{1 + e^{-x}} \\\\\\\n&= -(1 + e^{-x})^{-2} * \\frac{d}{dx}(1 + e^{-x})~~\\text{(Using Chain Rule)}\\\\\n&= -(1 + e^{-x})^{-2} * (-e^{-x})\\\\\n&=  \\frac{e^{-x}}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{1}{1 + e^{-x}} * \\Bigg({\\frac{1 + e^{-x}}{1 + e^{-x}}} - \\frac{1}{1 + e^{-x}}\\Bigg)\\\\\\\n&\\text{ and therefore}\\\\\\\n\\Large{\\frac{df(x)}{dx}} &= \\Large{f(x) * (1 - f(x))}\\\\\n\\end{align}\n\\]\n\n\n\nThe MLP adds several layers of perceptrons, in layers, as shown below:\n\n\n\n\n\nHere, i1, i2, and i3 are input neurons: they are simply inputs and are drawn as circles in the literature.\nThe h1, h2, h3 are neuron in the so-called hidden layer; hidden because they are not inputs!\nThe neurons o1, o2, and o3 are output neurons.\nThe signals/information flows from left to right in the diagram. And we have shown every neuron connected to everyone in the next layer downstream.\n\nHow do we mathematically, and concisely, express the operation of the MLP? Let us setup a notation for the MLP weights.\n\n\n\\(l\\) : layer index;\n\n\\(j\\), \\(k\\) : neuron index in two adjacent layers\n\n\\(W^l_{jk}\\) (i.e. \\(W^{layer}_{{source}~{destn}}\\)) : weight from \\(j\\)th neuron / \\((l−1)\\)th layer to \\(k\\)th neuron / \\(l\\)th layer;\n\n\\(b^l_k\\) : bias of the \\(k\\)th neuron in the \\(l\\)th layer.\n\n\\(a^l_k\\) : activation (output) of \\(k\\)th neuron / \\(l\\)th layer.\n\n\n\n\n We can write the outputs of the layer-2 as:\n\\[\n\\begin{align}\n(k = 1): ~ a_{12} = sigmoid~(~\\color{red}{W^2_{11}*a_{11}} + \\color{skyblue}{W^2_{21}*a_{21}} + \\color{forestgreen}{W^2_{31}*a_{31}} ~ + b_{12})\\\\\n(k = 2): ~ a_{22} = sigmoid~(~W^2_{12}*a_{11} + W^2_{22}*a_{21} + W^2_{32}*a_{31}~ + b_{22} )\\\\\n(k = 3): ~ a_{32} = sigmoid~(~W^2_{13}*a_{11} + W^2_{23}*a_{21} + W^2_{33}*a_{31}~ + b_{32})\\\\\n\\end{align}\n\\]\nIn (dreaded?) matrix notation :\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nIn compact notation we write, in general:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\]\n\\[\na^l_j=σ(\\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)\n\\tag{1}\\]",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#what-is-a-multilayer-perceptron",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#what-is-a-multilayer-perceptron",
    "title": "The Multilayer Perceptron",
    "section": "",
    "text": "This was our bare bones Perceptron, or neuron as we will refer to it henceforth:\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\]\n\n\n\nFor the multi-layer perceptron, two changes were made:\n\nChanging the hard-threshold activation into a more soft sigmoid activation\naddition of (one or more ) hidden layers.\n\nLet us discuss these changes in detail.\n\n\nWe said earlier that the weighting and adding is a linear operation.\nWhile this is great, simple linear translations of data are not capable of generating what we might call learning or generalization ability.\nThe outout of the perceptron is a “learning decision” that is made by deciding if the combined output is greater or smaller than a threshold.\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 1: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nThe hard-threshold used in the Perceptron allowed us to make certain decisions based on linear combinations of the input data. But what is the dataset possesses classes that are not separable in a linear way? What if different categories of points are intertwined with a curved boundary between classes?\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 2: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nSo how do we implement this nonlinear Activation Block?\n\nOne of the popular functions used in the Activation Block is a function based on the exponential function \\(e^x\\).\nWhy? Because this function retains is identity when differentiated! This is a very convenient property!\n\n\n\nSigmoid Activation\n\n\n\n\n\n\n\nNoteRemembering Logistic Regression\n\n\n\nRecall your study of Logistic Regression. There, the Sigmoid function was used to model the odds of the (Qualitative) target variable against the (Quantitative) predictor.\n\n\n\n\n\n\n\n\nNoteBut Why Sigmoid?\n\n\n\nBecause the Sigmoid function is differentiable. And linear in the mid ranges. Oh, and remember the Chain Rule?\n\\[\n\\begin{align}\n\\frac{df(x)}{dx}\n&= \\frac{d}{dx} * \\frac{1}{1 + e^{-x}} \\\\\\\n&= -(1 + e^{-x})^{-2} * \\frac{d}{dx}(1 + e^{-x})~~\\text{(Using Chain Rule)}\\\\\n&= -(1 + e^{-x})^{-2} * (-e^{-x})\\\\\n&=  \\frac{e^{-x}}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{1}{1 + e^{-x}} * \\Bigg({\\frac{1 + e^{-x}}{1 + e^{-x}}} - \\frac{1}{1 + e^{-x}}\\Bigg)\\\\\\\n&\\text{ and therefore}\\\\\\\n\\Large{\\frac{df(x)}{dx}} &= \\Large{f(x) * (1 - f(x))}\\\\\n\\end{align}\n\\]\n\n\n\nThe MLP adds several layers of perceptrons, in layers, as shown below:\n\n\n\n\n\nHere, i1, i2, and i3 are input neurons: they are simply inputs and are drawn as circles in the literature.\nThe h1, h2, h3 are neuron in the so-called hidden layer; hidden because they are not inputs!\nThe neurons o1, o2, and o3 are output neurons.\nThe signals/information flows from left to right in the diagram. And we have shown every neuron connected to everyone in the next layer downstream.\n\nHow do we mathematically, and concisely, express the operation of the MLP? Let us setup a notation for the MLP weights.\n\n\n\\(l\\) : layer index;\n\n\\(j\\), \\(k\\) : neuron index in two adjacent layers\n\n\\(W^l_{jk}\\) (i.e. \\(W^{layer}_{{source}~{destn}}\\)) : weight from \\(j\\)th neuron / \\((l−1)\\)th layer to \\(k\\)th neuron / \\(l\\)th layer;\n\n\\(b^l_k\\) : bias of the \\(k\\)th neuron in the \\(l\\)th layer.\n\n\\(a^l_k\\) : activation (output) of \\(k\\)th neuron / \\(l\\)th layer.\n\n\n\n\n We can write the outputs of the layer-2 as:\n\\[\n\\begin{align}\n(k = 1): ~ a_{12} = sigmoid~(~\\color{red}{W^2_{11}*a_{11}} + \\color{skyblue}{W^2_{21}*a_{21}} + \\color{forestgreen}{W^2_{31}*a_{31}} ~ + b_{12})\\\\\n(k = 2): ~ a_{22} = sigmoid~(~W^2_{12}*a_{11} + W^2_{22}*a_{21} + W^2_{32}*a_{31}~ + b_{22} )\\\\\n(k = 3): ~ a_{32} = sigmoid~(~W^2_{13}*a_{11} + W^2_{23}*a_{21} + W^2_{33}*a_{31}~ + b_{32})\\\\\n\\end{align}\n\\]\nIn (dreaded?) matrix notation :\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nIn compact notation we write, in general:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\]\n\\[\na^l_j=σ(\\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)\n\\tag{1}\\]",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#wait-but-why",
    "title": "The Multilayer Perceptron",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nThe “vanilla” perceptron was big advance in AI and learning. However, it was realized that this can only make classification decisions with data that are linearly separable.\nIncluding a differentiable non-linearity in the activation block allows us to deform the coordinate space in which the data points are mapped.\nThis deformation may permit unique views of the data wherein the categories of data are separable by an n-dimensional plane.\nThis idea is also used in a machine learning algorithm called Support Vector Machines.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#mlps-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#mlps-in-code",
    "title": "The Multilayer Perceptron",
    "section": "MLPs in Code",
    "text": "MLPs in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#references",
    "title": "The Multilayer Perceptron",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\n3D MatMul Visualizerhttps://bhosmer.github.io/mm/ref.html",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html",
    "title": "MLPs and Backpropagation",
    "section": "",
    "text": "We saw how each layer works:\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\tag{1}\\]\nand:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\tag{2}\\]\nSee how the connections between neurons are marked by weights: these multiply the signal from the previous neuron. The multiplied/weighted products are added up in the neuron, and the sum is given to the activation block therein.\nSo learning?\nThe only controllable variables in a neural network are these weights! So learning involves adapting these weights so that they can perform a useful function.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#what-is-the-learning-process",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#what-is-the-learning-process",
    "title": "MLPs and Backpropagation",
    "section": "What is the Learning Process?",
    "text": "What is the Learning Process?\nThe process of adapting the weights of a neural network can be described in the following steps:\n\n\nTraining Set: Training is over several known input-output pairs (“training data”)\n\nTraining Epoch: For each input, the signals propagate forward until we have an output\n\nError Calculation: Output is compared with desired output, to calculate error\n\n\nBackpropagation: Each neuron (and its weights) need to be told what is their share of the error! Errors therefore need to be sent backward from the output to input, unravelling them from layer \\(l\\) to layer \\(l-1\\). (like apportioning blame !!).\n\nError-to-Cost: How does error at any given neuron relate to the idea of an overall Cost function? Is the Cost function also apportioned in the same way?\n\nDifferentiate: Evaluate the effect of each weight/bias on the (apportioned) error overall Cost. (Slope!!)\n\nGradient Descent: Adapt the weights/biases with a small step in the opposite direction to the slope.\n\nThere.\nWhat is the Output Error?\nIf \\(d(k)\\) are the desired outputs of the NN (over an entire training set), and \\(y(k)\\) are the outputs of the output layer, then we calculate the error at the outputs of the NN as:\n\\[\ne(k) = a(k) - d(k)\n\\tag{3}\\]\nThis error is calculated at each output for each training epoch/sample/batch. (More about the batch-mode in a bit.)\nWhat is the Cost Function?\nWe define the cost or objective function as the squared error averaged over all neurons:\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\\\\n&= \\frac{1}{2n}\\sum^{n~neurons}_{k=1}(a_i - d_i)^2\n\\end{align}\n\\tag{4}\\]\nThe \\(a_i\\)s are the outputs of \\(n\\) neurons and \\(d_i\\) are the desired outputs for each of the training samples.\nThe Cost Function is of course dependent upon the Weights and the biases, and is to be minimized by adapting these. Using the sum of squared errors, along with the linear operations in the NN guarantees that the Cost Function (usually) has one global, minimum.\nWhat is Backpropagation of Error?\nAs we stated earlier, error is calculated at the output. In order to adapt all weights, we need to send error proportionately back along the network, towards the input. This proportional error will give us a basis to adapt the individual weights anywhere in the network.\nWhat does “proportional” mean here? Consider the diagram below:\n\n\n\n\n\n\\[\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}}\\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{21}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}} \\\\\ne_{31}~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{magenta}{\\pmb{h_1}}} \\\\\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{21}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{31} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{31}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}}  \\\\\n\\end{align}\n\\]\n\n\nThese are the contributions of the error \\(e_{12}\\) to each of the previous neurons.\nAnother way of looking at this:\n\n\n\n\n\n\\[\n\\begin{align}\ne_{11} =~ e_{12} * \\frac{W_{11}}{Sum~of~weights~to~{\\color{orange}{\\pmb {h_1}}}}\\\\\n+ ~ e_{22} * \\frac{W_{21}}{Sum~of~Weights~to~\\color{pink}{\\pmb{h_2}}} \\\\\n+ ~ e_{32} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{teal}{\\pmb{h_3}}}  \\\\\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\ne_{11} = ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{orange}{W_{11} + W_{21} + W_{31}}}}\\\\\n+ ~e_{22} * \\frac{W_{12}}{\\pmb{\\color{pink}{W_{12} + W_{22} + W_{32}}}} \\\\\n+ ~e_{32} * \\frac{W_{13}}{\\pmb{\\color{teal}{W_{13} + W_{23} + W_{33}}}}  \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\ne_{21} = similar~expression!!\\\\\n\\\ne_{31} = similar~expression!!\\\\\n\\end{align}\n\\]\n\n\nEquation corrected by Gayatri Jadhav, April 2025\nThis is the total error at \\(e_{11}\\) from all the three output errors. So:\n\nWe have taken each output error, \\(e_{*2}\\) and parcelled it back to the preceding neurons in proportion to the connecting Weight. This makes intuitive sense; we are making those neurons put their money where their mouth is. As Nassim Nicholas Taleb says, people (and neurons!) need to pay for their opinions, especially when things go wrong!\nThe accumulated error at each neuron in layer \\(l-1\\) is the weighted sum of back-propagated error contributions from all layer \\(l\\) neurons to which we are connected.\nSo we can compactly write the relationships above as:\n\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} =\n\\Bigg(\n\\begin{bmatrix}\n\\frac{W_{11}}{D_{11}} & \\frac{W_{12}}{D_{12}} & \\frac{W_{13}}{D_{13}}\\\\\n\\frac{W_{21}}{D_{21}} & \\frac{W_{22}}{D_{22}} & \\frac{W_{23}}{D_{23}}\\\\\n\\frac{W_{31}}{D_{31}} & \\frac{W_{32}}{D_{32}} & \\frac{W_{33}}{D_{33}}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nThe denominators make things look complicated! But if we are able to simply ignore them for a moment, then we see a very interesting thing:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nThis new approximate matrix is the tranpose of our original Weight matrix from Equation 1! The rows there have become columns here!! That makes intuitive sense: in the forward information direction, we were accounting for information from the point of view of the destinations; in the reverse error backpropagation direction, we are accounting for information from the point of view of the sources.\nWriting this equation in a compact way:\n\\[\n\\Large{e^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}}\n\\tag{5}\\]\nThis is our equation for backpropagation of error.\nWhy is ignoring all those individual denominators justified? Let us park that question until we have understood the one last step in NN training, the Gradient Descent.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#here-comes-the-rain-maths-again",
    "title": "MLPs and Backpropagation",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.ka. 3Blue1Brown.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#backpropagation-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#backpropagation-in-code",
    "title": "MLPs and Backpropagation",
    "section": "Backpropagation in Code",
    "text": "Backpropagation in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#references",
    "title": "MLPs and Backpropagation",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#what-is-a-perceptron",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#what-is-a-perceptron",
    "title": "The Perceptron",
    "section": "What is a Perceptron?",
    "text": "What is a Perceptron?\nThe perceptron was invented by Frank Rosenblatt is considered one of the foundational pieces of neural network structures. The output is viewed as a decision from the neuron and is usually propagated as an input to other neurons inside the neural network.\n\n\nPerceptron\n\nMath Intuition\n\nWe can imagine this as a set of inputs that averaged in weighted fashion.\n\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\tag{1}\\]\n\nSince the inputs are added with linear weighting, this effectively acts like a linear transformation of the input data.\n\nA linear equation of this sort is the general equation of an n-dimensional plane.\nIf we imagine the input as representing the n-coordinates in a plane, then the multiplications scale/stretch/compress the plane, like a rubber sheet. (But do not fold it.)\nIf there were only 2 inputs, we could mentally picture this with a handkerchief.\n\n\nMore metaphorically, it seems like the neuron is consulting each of the inputs, asking for their opinion, and then making a decision by attaching different amounts of significance to each opinion.\nThe Structure should remind you of Linear Regression !\n\nSo how does it work? Consider the interactive diagram below:\n\n\n\nThe coordinate axes are as shown X, Y, and Z\nThe grey and yellow points are the data we wish to classify into two categories, unsurprisingly “yellow” and grey”.\nThe Weight vector line is a vector of all the weights in the Perceptron.\nNow, as per the point-normal form of an n-dimensional plane, the multiplication of the input data with the weight vector is like taking a vector dot product ( aka inner product) ! And: every point on the plane has a dot product of ZERO. See the purple vector which is normal to the Weight vector: its dot product with the Weight vector is zero.\nData points that are off this “normal plane” in either direction (above and below) will have dot-products which will be positive or negative depending upon the direction!\nHence we can use the dot-product POLARITY to decide if a point is above or below the plane defined by the Weight vector. Which is what is done in the threshold-based activation!\nThe bias \\(b\\) defines the POSITION of the plane; and the Weights define the direction. Together, they classify the points based on the Equation 1.\nTry to move the slider to get an intuition of how the plane moves with the bias. Clearly, the bias is very influential in deciding the POLARITY of the dot-products. When it aligns with the purple vector (\\(dot product = 0\\)), it works best.\nWhy “Linear”?\nWhy are (almost) all operations linear operations in a NN?\n\nWe said that the weighted sums are a linear operation, but why is this so?\nWe wish to be able to set-up analytic functions for performance of the NN, and be able to differentiate them to be able to optimize them.\nNon-linear blocks, such as threshold blocks/signum-function based slicers are not differentiable and we are unable to set up such analysis.\nNote the title of this reference.\nWhy is there a Bias input?\n\nWe want the weighted sum of the inputs to mean something significant, before we accept it.\nThe bias is subtracted from the weighted sum of inputs, and the bias input could also (notionally) have a weight.\nThe bias is like a threshold which the weighted sum has to exceed; if it does, the neuron is said to fire.\n\nSo with all that vocabulary, we might want to watch this longish video by the great Dan Shiffman:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#perceptrons-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#perceptrons-in-code",
    "title": "The Perceptron",
    "section": "Perceptrons in Code",
    "text": "Perceptrons in Code\n\n\n R\n p5.js\n\n\n\nLet us try a simple single layer NN in R. We will use the R package neuralnet.\n\nShow the Code# Load the package\n# library(neuralnet)\n\n# Use iris\n# Create Training and Testing Datasets\ndf_train &lt;- iris %&gt;% slice_sample(n = 100)\ndf_test &lt;- iris %&gt;% anti_join(df_train)\nhead(iris)\n\n\n  \n\n\nShow the Code# Create a simle Neural Net\nnn &lt;- neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = df_train,\n  hidden = 0,\n  # act.fct = \"logistic\", # Sigmoid\n  linear.output = TRUE\n) # TRUE to ignore activation function\n\n# str(nn)\n\n# Plot\nplot(nn)\n\n# Predictions\n# Predict &lt;- compute(nn, df_test)\n# Predict\n# cat(\"Predicted values:\\n\")\n# print(Predict$net.result)\n#\n# probability &lt;- Predict$net.result\n# pred &lt;- ifelse(probability &gt; 0.5, 1, 0)\n# cat(\"Result in binary values:\\n\")\n# pred %&gt;% as_tibble()\n\n\n\n\n\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#references",
    "title": "The Perceptron",
    "section": "References",
    "text": "References\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nNeural Network Playground. https://playground.tensorflow.org/\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nNeural Networks Visual with vcubingx\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. http://neuralnetworksanddeeplearning.com/index.html\n\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland.https://www.sscardapane.it/alice-book/\n\nUsing R for DL\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nMaths\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\nneuralnet\n1.44.2\n@neuralnet",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html",
    "title": "Large Language Models",
    "section": "",
    "text": "Frank Rosenblatt’s Perceptron\nDeep Learning Networks\n\n\nInput Layers\nOutput Layers\nHidden Layers\nActivation\n\n\nAdaptation and Training\n\n\nBackpropagation\nError Functions and Surfaces\n\n\nWorking\n\n\n“Repeated Weighted Averaging with Thresholding”\nHow does that end up “learning”? Is there an intuitive explanation?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#inspiration",
    "title": "Large Language Models",
    "section": "",
    "text": "Frank Rosenblatt’s Perceptron\nDeep Learning Networks\n\n\nInput Layers\nOutput Layers\nHidden Layers\nActivation\n\n\nAdaptation and Training\n\n\nBackpropagation\nError Functions and Surfaces\n\n\nWorking\n\n\n“Repeated Weighted Averaging with Thresholding”\nHow does that end up “learning”? Is there an intuitive explanation?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#neural-nets-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#neural-nets-in-code",
    "title": "Large Language Models",
    "section": "Neural Nets in Code",
    "text": "Neural Nets in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#references",
    "title": "Large Language Models",
    "section": "References",
    "text": "References\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\nNeural Network Playground. https://playground.tensorflow.org/\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\nNeural Networks Visual with vcubingx\n\n\nPart 1. https://youtu.be/UOvPeC8WOt8\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. http://neuralnetworksanddeeplearning.com/index.html\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland. https://www.sscardapane.it/alice-book/\nUsing R for DL\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\nMaths\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/30-Agents/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/30-Agents/index.html",
    "title": "Working with Agents",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n\n\nMonte Carlo Simulations\n\nBayesian Thinking"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#introduction",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n\n\nMonte Carlo Simulations\n\nBayesian Thinking"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#references",
    "title": "Working with Chance",
    "section": "References",
    "text": "References\n\nhttps://californiaglobe.com/fr/the-story-of-arnold-schwarzeneggers-infamous-hidden-expletive-veto/\n\nPhilip B. Stark (2010). Null and Vetoed: “Chance Coincidence”?, Chance, November 2010, v23(4), 43–46.). https://www.stat.berkeley.edu/~stark/Preprints/acrosticVeto09.htm\nLo Bello, A. (1991). Ask Marilyn: The Mathematical Controversy in Parade Magazine. The Mathematical Gazette, 75(473), 275–277. https://doi.org/10.2307/3619484.PDF."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html#introduction",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html",
    "title": "\n Making Noise Predictably",
    "section": "",
    "text": "TRON the movie",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-noise-based-outcomes-will-we-see-today",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-noise-based-outcomes-will-we-see-today",
    "title": "\n Making Noise Predictably",
    "section": "\n What noise based outcomes will we see today?",
    "text": "What noise based outcomes will we see today?\nWe will understand the basics of procedural noise generation: generating random noise-like numbers that allow us to model and create very realistic-looking textures, such as wood, fire, marble, terrain, mountains, and clouds.\nLet us quickly see this intro to Perlin Noise:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#inspiration",
    "title": "\n Making Noise Predictably",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\nArch\n\n\n\n\n\nCat Fur Up Close\n\n\n\n\n\nFigure 1: Perlin Noise based Textures",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-is-perlin-noise",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-is-perlin-noise",
    "title": "\n Making Noise Predictably",
    "section": "\n What is Perlin Noise?",
    "text": "What is Perlin Noise?\nOk, this is going to be a long explanation!!!\nA. Inner Product Computation\nLet us start by dividing up 2D space ( for now!!) into square-shaped cells. At each vertex we randomly place a unit gradient vector labelled \\(r_{i}\\) that points in a random direction. See the figure below:\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA perhaps more evocative picture may be this representation of an event from the Mahabharata: \n\n\nWe wish to calculate the Perlin Noise amount at any point of interest inside the cell.\n\nWe draw difference vectors to the point from each of the 4 vertices.\nWe compute the vector dot product with each of the \\(r_{i}\\) and the above difference vectors. ( 4 dot products )\nThese are shown in the text print at the side of the figure.\n\n\n\n\n\n\n\nNoteDot Products are Scalars with Polarity\n\n\n\nNote how the 4 dot products change as you move the mouse/touchpad. This changes the 4 gradient vectors and hence the scalar dot products change in amplitude and polarity.\nIn a typical Perlin Noise implementation, the gradient vectors are fixed after an initial setup. So each gradient vector generates a range of dot-product values as the point of interest moves within the cell.\n\n\nB. Interpolation of Dot Product values\nWith the 4 scalar dot products, we are now ready to compute the Perlin Noise value at the point of interest. There are several ways of doing this:\n\nSimply take the average\nTake a weighted average, with fixed weights.\nUse a weighting/interpolating function: The closer a point of interest is to one or other of the cell vertices, the higher is the contribution of the corresponding dot-product.\n\nThe third approach is the one embedded within (all?) Perlin Noise implementations. The interpolating function is:\n\\[\nf(t) = 6t^5-15t^4+10t^3\n\\tag{1}\\]\nand looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpolation Function \\(f(t)\\) has smooth ends\n\n\n\nBoth \\(\\frac{df(t)}{dt}\\) and \\(\\frac{d^2f(t)}{dt^2}\\) are continuous at the ends of the range of the function (t = 0 and t = 1).\n\\[\n\\begin{array}{lcl}f'(t) & = & \\ \\frac{d}{dt}[6*t^5 - 15*t^4 + 10*t^3]\\\\\n& = & 30 * (t^4 - 2 * t^3 + t^2)\\\\\n& = & 0  \\ \\text{@ t = 0 and t = 1}\n\\end{array}\n\\]\n\\[\n\\begin{array}{lcl}f''(t) & = & \\ \\frac{d^2}{dt^2}[6*t^5 - 15*t^4 + 10*t^3]\\\\\n& = &60 * (2 * t^3 - 3 * t^2 + t)\\\\\n& = & 0  \\ \\text{@ t = 0 and t = 1}\n\\end{array}\n\\] This ensures that there are not sudden changes in the noise function near about the vertices.\n\n\nD. Fractal Overlay and Combining\nNow that we have one grid full of a layer of noise generated by weighted dot-products, we can appreciate one more thing: we can overlay the space with several layers of such noise values. Why would this be a good idea?\nThis multiple layer overlay creates a very natural-looking fractal-ness in the resulting noise function. Most natural looking shapes like landscapes, mountains, vegetables, flames…all have this self-similar structure where when one zooms in, the magnified function looks pretty much like the un-zoomed version!!\nSo how we create and merge overlays? We create several more-closely-spaced grids overlaid on the first one, and generate noise in the same way. These layers of noise-s are scaled by a factor (Usually \\(\\Large{\\frac{1}{2^n}}\\)), where \\(n\\) is the “order” of the layer. Each new finely-spaced layer generates similar-looking noise functions, which are combined with smaller and smaller weights to achieve that final polished fractal look of Perlin Noise.\nWe will explore this fractality with code. For now, here is Ken Perlin’s own explanation from 1999:\n\n\n“The outline of my algorithm to create noise is very simple. Given an input point P, look at each of the surrounding grid points. In two dimensions there will be four surrounding grid points; in three dimensions there will be eight. In n dimensions, a point will have 2n surrounding grid points.For each surrounding grid point Q, choose a pseudo-random gradient vector G. It is very important that for any particular grid point you always choose the same gradient vector. Compute the inner product G . (P-Q). This will give the value at P of the linear function with gradient G which is zero at grid point Q. Now you have 2n of these values. Interpolate between them down to your point, using an S-shaped cross-fade curve (eg: 3t2-2t3) to weight the interpolant in each dimension. This step will require computing n S-curves, followed by 2n-1 linear interpolations.”\n— Ken Perlin",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#creating-textures-and-waveforms-with-perlin-noise",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#creating-textures-and-waveforms-with-perlin-noise",
    "title": "\n Making Noise Predictably",
    "section": "\n Creating Textures and Waveforms with Perlin Noise",
    "text": "Creating Textures and Waveforms with Perlin Noise\n\n\nUsing p5.js\nUsing R\n\n\n\nHere is a landscape generated using Perlin Noise:\n\n\n\n\n\n\nThe ambient package allows us to create a variety of noise patterns, including Perlin Noise. The commands are: gen_perlin() and noise_perlin(), whose arguments are:\n\n\ndim: The dimensions (height, width, (and depth)) of the noise to be generated. The length determines the dimensionality of the noise.\n\nfrequency: Determines the granularity of the features in the noise.\n\ninterpolator:How should values between sampled points be calculated? Either ‘linear’, ‘hermite’, or ‘quintic’ (default), ranging from lowest to highest quality.\n\nfractal: The fractal type to use. Either ‘none’, ‘fbm’ (default), ‘billow’, or ‘rigid-multi’. It is suggested that you experiment with the different types to get a feel for how they behaves.\n\noctaves: The number of noise layers used to create the fractal noise. Ignored if fractal = ‘none’. Defaults to 3.\n\nlacunarity: The frequency multiplier between successive noise layers when building fractal noise. Ignored if fractal = ‘none’. Defaults to 2.\n\ngain: The relative strength between successive noise layers when building fractal noise. Ignored if fractal = ‘none’. Defaults to 0.5.\n\npertubation: The perturbation to use. Either ‘none’ (default), ‘normal’, or ‘fractal’. Defines the displacement (warping) of the noise, with ‘normal’ giving a smooth warping and ‘fractal’ giving a more erratic warping.\n\npertubation_amplitude: The maximal perturbation distance from the origin. Ignored if pertubation = ‘none’. Defaults to 1.\n\nx, y, z: Coordinates to get noise value from\n\nseed: The seed to use for the noise. If NULL a random seed will be used:\n\nnoise2 &lt;- noise_perlin(\n  dim = c(400, 400), # height/width\n  frequency = 0.01, # Lower = less granular, more organic\n  interpolator = \"quintic\", #' linear', 'hermite', or 'quintic'\n  fractal = \"fbm\", # Try \"billow\" , \"rigid-multi\"\n  octaves = 5,\n  lacunarity = 2,\n  gain = 0.8, # Default = 0.5 giving 1/2^n scaling\n  pertubation = \"none\", # Note the incorrect spelling\n  pertubation_amplitude = 1 # Note the incorrect spelling\n)\n## generates a matrix\nnoise2 %&gt;% as_tibble()\n# Plot the matrix\nplot(as.raster(normalise(noise2)))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n# Using the generator\ngrid &lt;- long_grid(seq(1, 10, length.out = 1000), seq(1, 10, length.out = 1000))\ngrid$noise &lt;- gen_perlin(grid$x, grid$y,\n  octaves = 5,\n  frequency = 1.2\n)\nplot(grid, noise)\n##\ngrid %&gt;%\n  gf_point(y ~ x,\n    colour = ~noise,\n    size = 0.01, show.legend = F\n  ) %&gt;%\n  gf_refine(\n    scale_color_gradient(\n      low = \"orangered\",\n      high = \"black\"\n    ),\n    coord_fixed()\n  ) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems the ambient package cannot generate 1D-Perlin noise, so we cannot generate say time waveforms based on this idea. p5.js of course can do 1D.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#videos",
    "title": "\n Making Noise Predictably",
    "section": "Videos",
    "text": "Videos\nOf course there are videos by Dan Shiffman on the topic of Perlin Noise:\n\nPerlin Noise in p5.js. https://www.youtube.com/watch?v=Qf4dIN99e2w&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=3&pp=iAQB\n\nPerlin Noise in 2D. https://www.youtube.com/watch?v=ikwNrFvnL3g&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=6&pp=iAQB\n\nPerlin Noise in Detail. https://www.youtube.com/watch?v=D1BBj2VaBl4&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=7&pp=iAQB\n\nGraphing Perlin Noise. https://www.youtube.com/watch?v=y7sgcFhk6ZM&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=5&pp=iAQB",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#wait-but-why",
    "title": "\n Making Noise Predictably",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nGenerating random waveforms and textures is an important part of Creative Computational projects.\nThese ideas are used in landscape, texture and sound generation.\n“Normal” random noise is too jagged to have the natural look that we would like\nPerlin Noise creates both a smoothness and a fine-grain random structure in an arbitrary number of dimensions.\nThis ends up looking more “organic” and “natural”. Think cats.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#references",
    "title": "\n Making Noise Predictably",
    "section": "\n References",
    "text": "References\n\nKen Perlin.(1999). Making Noise. https://web.archive.org/web/20151221035155/http://www.noisemachine.com/talk1/index.html. Based on a talk presented at GDCHardCore on Dec 9, 1999.\nhttps://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/perlin-noise-part-2/perlin-noise.html\nhttps://www.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-noise/a/perlin-noise\nhttps://adrianb.io/2014/08/09/perlinnoise.html\nhttps://www.arendpeter.com/Perlin_Noise.html\nhttps://mzucker.github.io/html/perlin-noise-math-faq.html\nhttps://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/\nGorilla Sun Blog. Perlin Noise. https://www.gorillasun.de/tag/perlin-noise/\n\nThe ambient package in R: https://ambient.data-imaginist.com/\n\n\nTextbooks\n\nPatricio Gonzalez Vivo & Jen Lowe. The Book of Shaders. https://thebookofshaders.com/11/\n\n\n\n\n R Package Citations\nResources\n\nhttps://opengameart.org/content/700-noise-textures\nhttps://github.com/sighack/perlin-noise-fields\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#introduction",
    "href": "content/courses/MathModelsDesign/listing.html#introduction",
    "title": "Math Models for Creative Coders",
    "section": "Introduction",
    "text": "Introduction\nWe will study several Mathematical Models and apply them to Art and Design. The algorithms will be examined and then coded in p5.js / q5.js / p5play.js; however other open source tools (FOSS) and Javascript libraries may also be introduced as and when needed and as and when I learn and get excited about them.\nThis course stands on “three legs”: Maths, Code + Tech, and Artifacts, as shown below. As this course grows and hopefully becomes more un-popular ;-D, more and more of these modules below will become reality!\n\nMath Experiments with Code\n\n\nMaths\nCode + Tech\nArtifacts\n\n\n\n\nIterated FunctionsComplex NumbersRandom NumbersVector AlgebraPerlin Noise\np5.js / q5.js / p5play.jsShapesStack, Push and Pull, TranslateVectorsRecursionConstructors and OOP\nFractalsKolamsAngolan Sona PatternsL-Systems SymmetriesTextures\n\n\nLinear Systems  Impulse ResponsesConvolutionElectrical System Theory (poles; zeroes; resonance…)?DSP basics?AM/FM/PM Modulation?Fourier SeriesBessel Functions? Waves; Echoes ?Non-linearities; Saturation; Hysteresis ?\nAdding External Physics and Sound Librariesp5.soundCamera, Sound and MicHandphone based InteractionsOSP Protocol based interactions with external hardware over WiFi / BT\nSoundVideoTextCrowd-Sourced acts (jam session; flash-techno mob…)?Mouse - Touchpad Orchestra?\n\n\nNeural Net Basics Matrix AlgebraTrainingBackpropagationGradient DescentClassification, Regression, Clustering\nml5.js libraries\nMulti-Layer PerceptronsConvolutional Neural NetworksPose, Gesture, and Face Detection\n\n\nComplexity and Emergent Phenomena?\nInterfacing p5.js with Makey-Makey / Arduino / Raspberry Pi?Sensors?Working with other materials like sand, water, paper, wood, cloth, balloons ?“Action at a Distance” using say quantum entanglement IFTTT?\nFriendship Networks? Games?Public Space InstallationsSimulationsRemote Education with Gesture tracking and live Coding?“Literary” Events with Tech?\n\n\nI’ll\nBe\nBack",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#references",
    "href": "content/courses/MathModelsDesign/listing.html#references",
    "title": "Math Models for Creative Coders",
    "section": "References",
    "text": "References\n\nCourse Abstract written by Arnold Schwarzenegger.\n\n\nGeneral\n\nBret Victor. Learnable Programming. https://worrydream.com/LearnableProgramming/\nMichael Nielsen.(February 2016). Toward an exploratory medium for mathematics. https://cognitivemedium.com/emm/emm.html\nRune Madsen. Programming Design Systems. https://programmingdesignsystems.com. A free digital book that teaches a practical introduction to the new foundations of graphic design.\nColah’s ( not Kolha’s 🦊 ) Blog. https://colah.github.io/https://colah.github.io/\n\n\n\np5.js, p5play.js, q5.js, and Processing\n\nGetting Started with p5.js https://p5js.org/tutorials/setting-up-your-environment/\nCoding Rainbow: Basic-est set of videos on p5.js &lt;https://youtube.com/playlist?list=PLglp04UYZK_PrN6xWo_nJ-8kzyXDyFUwi&si=BWSDVX9-Bt85KXdO&gt;\nCoding Train with Dan Shiffman:\n\nWebsite: https://thecodingtrain.com/\nGithub: https://github.com/CodingTrain/website-archive\n\np5.js Wiki. https://github.com/processing/p5.js/wiki\np5.js at CodeAcademy https://www.codecademy.com/learn/learn-p5js\np5.js at HappyCoding https://happycoding.io/tutorials/p5js/\nOpenProcessing https://openprocessing.org\nhttps://www.codecademy.com/content-items/5e5c0d2a7b20535fbe8aed05e739e027\np5play Game Engine https://p5play.org/ p5play is for creating interactive art and games with the Box2D physics engine.\nq5.js Home https://q5js.org/home/\n\n\n\nR-language-related Resources\n\nThomas Lin Pedersen:\n\nWebsite: https://www.data-imaginist.com/art\nWebsite: https://ambient.data-imaginist.com/index.html (R package ambient)\nGithub:\n\nAntonio Sánchez Chinchón:\n\nWebsite: https://fronkonstin.com\nGithub: https://github.com/aschinchon/abstractions\n\nDanielle Navarro’s Generative Art:\n\nWebsite: https://art-from-code.netlify.app\nGithub: https://github.com/arvindvenkatadri/art-from-code (forked by me)\nWebsite: https://art-from-code.netlify.app\n\nClaus Wilke:\n\nWebsite: https://clauswilke.com/art/\n\nGenerative Art by Katharina Brunner:\n\nGithub:https://github.com/cutterkom/generativeart\nWebpage:https://katharinabrunner.de/generativeart/\n\nWilliam Chase:\n\nWebsite: https://www.williamrchase.com/\nGithub: https://github.com/will-r-chase\nBlog Posts: https://www.williamrchase.com/writing/\n\nhttps://buttondown.email/willchase/archive/the-generative-art-dataviz-spectrum/\nhttps://www.williamrchase.com/post/strange-attractors-12-months-of-art-february/\nhttps://www.williamrchase.com/writing/2019-09-30-flow-fields-12-months-of-art-september/\nhttps://www.williamrchase.com/writing/2019-08-30-12-months-of-art-august/\nPoisson Disc sampling https://www.williamrchase.com/writing/2019-07-29-textues-and-geometric-shapes-12-months-of-art-july/ and the poissoned R package by @coolbutuseless\n\n\nMarcus Volz:\n\nWebsite: https://marcusvolz.com\nGithub: https://github.com/marcusvolz\n\nhttps://generative.substack.com/p/generative-art-and-r\nGenerative Art. https://paulvanderlaken.com/2020/05/02/generative-art-computer-design-painting/\nR-tistry with ggplot: https://www.bigbookofr.com/art.html#thinking-outside-the-grid---a-bare-bones-intro-to-rtistry-concepts-in-r-using-ggplot\nhttps://www.rforscience.com/scientific-computing.html\n\n\n\nDeep Learning and AI\n\nPractical Deep Learning for Coders. https://course.fast.ai/\nMichael Nielsen. http://neuralnetworksanddeeplearning.com/index.html\nIan Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning Book. https://www.deeplearningbook.org/\nDive into Deep Learning. Interactive deep learning book with code, math, and discussions. Implemented with PyTorch, NumPy/MXNet, JAX, and TensorFlowUsing Other Tools. https://www.d2l.ai/index.html\nSimon Scardapane. Alice’s Adventures in Differentiable Wonderland. https://www.sscardapane.it/assets/alice/Alice_book_volume_1.pdf\nFrançois Fleuret. The Little Book of Deep Learning. https://fleuret.org/public/lbdl.pdf\nFrank Rosenblatt. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. https://gwern.net/doc/ai/nn/1962-rosenblatt-principlesofneurodynamics.pdf\nWarren S. McCulloch, Walter Pitts (1943). A Logical Calculus of the Ideas of Immanent in Nervous Activity. BULLETIN OF MATHEMATICAL BIOPHYSICS VOLUME 5, 1943. &lt;https://www.aemea.org/math/McCulloch_Pitts_1943.pdfhttps://www.aemea.org/math/McCulloch_Pitts_1943.pdf\n\n\n\nOther Tools\n\nhttps://generatecoll.medium.com/how-i-used-excel-to-create-abstract-album-artwork-fee740d4414f\nRandom Digital Beauty. https://anaselk.com/p/generative-r/\nMaking Explanations (tools): https://explorabl.es/tools/\nUsing p5 in R. Yeah. https://alistaire.rbind.io/blog/p5-in-r/\ncreateCanvas Podcast. https://soundcloud.com/processingfoundation\nhttps://processingfoundation.org/education\nhttps://nannou.cc\nhttps://openframeworks.cc/\nhttps://libcinder.org\nSophia Crespo: (makes speculative biological creatures using neural networks)\n\nWebsite: https://sofiacrespo.com/\nWebsite: https://entangledothers.studio/\nGithub:\n\nR for Scientific Visualization. https://www.rforscience.com/visualisation.html\nR for Scientific Computing. https://www.rforscience.com/scientific-computing.html",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#other-interesting-websites-and-works",
    "href": "content/courses/MathModelsDesign/listing.html#other-interesting-websites-and-works",
    "title": "Math Models for Creative Coders",
    "section": "Other Interesting Websites and Works",
    "text": "Other Interesting Websites and Works\n\nThe Book of Shaders by Patricio Gonzalez Vivo and Jen Lowehttps://thebookofshaders.com/\nScott Murray’s D3 Art page: https://scottmurray.org/\nInigo Quilez, Digital Artist: https://iquilezles.org/ (ShaderToy, GraphToy, and MadeThisThing)\nReddit Generative Art Forum. https://www.reddit.com/r/generative/\nAI tools for Journalists. https://journaliststoolbox.ai/ A vast list of tools for every purpose you can think of.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#creative-coding-courses-elsewhere",
    "href": "content/courses/MathModelsDesign/listing.html#creative-coding-courses-elsewhere",
    "title": "Math Models for Creative Coders",
    "section": "Creative Coding Courses Elsewhere",
    "text": "Creative Coding Courses Elsewhere\n\nAllison Parrish. https://creative-coding.decontextualize.com\nMatthew Bardin. https://pdm.lsupathways.org\nTim Cortina at Carnegie Mellon Univ2. https://www.cs.cmu.edu/~tcortina/15104-f20/lectures/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#learning-modules",
    "href": "content/courses/MathModelsDesign/listing.html#learning-modules",
    "title": "Math Models for Creative Coders",
    "section": "Learning Modules",
    "text": "Learning Modules",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#introduction",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Introduction",
    "text": "Introduction\nIn the previous Module on Problems and Contradictions we understood how to identify an Administrative Contradiction in everyday situations. This is the first and most important step in the TRIZ Problem Solving Method.\nAnother important idea in TRIZ is that of Available Resources. Let us appreciate this idea with the help of two games.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-1",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-1",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Available Resources: Game #1",
    "text": "Available Resources: Game #1\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor. Your resources are:\n\nCandle\nMatchbox\nThumbtack",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-2",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-2",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Available Resources: Game #2",
    "text": "Available Resources: Game #2\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#discussion",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#references",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "References",
    "text": "References\n\nhttps://www.wikiwand.com/en/Candle_problem\nResources Game PDF\nhttps://thedecisionlab.com/biases/functional-fixedness\nhttps://www.interaction-design.org/literature/article/the-laws-of-figure-ground-praegnanz-closure-and-common-fate-gestalt-principles-3\nStan Kaplan, An Introduction to TRIZ (PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achieve it”, Springer, 2012.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#parallel-thinking",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#parallel-thinking",
    "title": "The Art of Parallel Thinking",
    "section": "Parallel Thinking",
    "text": "Parallel Thinking\nEdward de Bono invented the concept and phrase, “Parallel Thinking” in his book with the same title. Let us understand what this is:\n\n\n\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#propositions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#propositions",
    "title": "The Art of Parallel Thinking",
    "section": "Propositions",
    "text": "Propositions\nNow that we have an idea of about Parallel Thinking, let us get into it! Let us consider a couple of ideas for schemes that have been “around” for some time:\n\nMilitary Conscription for 2 years\nChildren Manage the House for one week\nYoung People must be assigned one Elderly Person to live with\nAll Cars must be removed from the City Centre\n\nCan we apply Parallel Thinking to these and come up with a vote, or even a way to achieve these, if we can agree with the Propositions?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#some-of-the-de-bono-thinking-tools",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#some-of-the-de-bono-thinking-tools",
    "title": "The Art of Parallel Thinking",
    "section": "Some of the de Bono Thinking Tools:",
    "text": "Some of the de Bono Thinking Tools:\n\n6 Thinking Hats Quick Reference (PDF Link)\n6H Technique - PMI: Plus Minus Interesting : https://www.debono.com/de-bono-thinking-lessons-1/1.-PMI-lesson-plan\nAlso see:https://sites.google.com/site/qepcafe/modules/expand/plus-minus-interesting-pmi\n6H Technique - APC: Alternatives, Possibilities, Choices: https://www.debono.com/de-bono-thinking-lessons-1/6.-APC-lesson-plan\n6H Technique - CAF: Consider All Factors: https://www.debono.com/de-bono-thinking-lessons-1/2.-CAF-lesson-plan\n6H Technique - OPV: Other Peoples’ Voices: https://www.debono.com/de-bono-thinking-lessons-1/7.-OPV-lesson-plan\n6H Technique - CnS: Consequence and Sequel : https://www.debono.com/de-bono-thinking-lessons-1/3.-C%26S-lesson-plan",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#references",
    "title": "The Art of Parallel Thinking",
    "section": "References",
    "text": "References\n\nReading in Parallel Thinking (Web Link)\nEdward de Bono, Six Thinking Hats (Goodreads)\nQuality Enhancement Program (QEP cafe) https://sites.google.com/site/qepcafe/#training-modules ( Link appears to be dead ;-() )\nReverse Brainstorming https://sites.google.com/site/qepcafe/modules/expand/reverse-brainstorming ( Link appears to be dead ;-() )\nhttps://www.mindtools.com/a32qxsh/reverse-brainstorming",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#introduction",
    "title": "I am Water",
    "section": "Introduction",
    "text": "Introduction\nThere is some stuff lying on the floor of the classroom.\n\nLego\nWhere’s Waldo? Books \nMagic Eye Books \nDominos \nMagnets + Links \nParquetry Blocks \nImaginarium Train Set + Thomas the Tank Engine \n\nPlay!! Yes, you.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-1",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-1",
    "title": "I am Water",
    "section": "Discussion #1",
    "text": "Discussion #1\n\nDid you get bored?\nDid you lose sense of time at any point?\nDid anything become too difficult at any time?\nWhat did you do at such times?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#what-is-flow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#what-is-flow",
    "title": "I am Water",
    "section": "What is Flow?",
    "text": "What is Flow?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will understand Flow from:\n\nThe book Flow and the Psychology of Happiness, pp; 72-76; by Mihaly Csikszentmihalyi. First, let us learn to say his name!!\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \nHere is the book.\n\nThis video: \n\n\n\n\nThis TED Talk : Flow, the Secret to Happiness \n\n\n 3. This is Water: David Foster Wallace’s famous Talk: \n  4. Let us quickly read and discuss this extract from the Play and Playground Encyclopedia https://www.pgpedia.com/m/man-play-and-games",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-2",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-2",
    "title": "I am Water",
    "section": "Discussion #2",
    "text": "Discussion #2\n\nIs the idea of Flow elusive?\nDo only some people get it, and then only at some times?\nWas David Foster Wallace’s talk somewhat self-deluding?\nThen what is this Empathy stuff we talk about when we speak of Design Thinking?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#pop-quix-want-still-more-flow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#pop-quix-want-still-more-flow",
    "title": "I am Water",
    "section": "Pop Quix: Want still more Flow?",
    "text": "Pop Quix: Want still more Flow?\nNo, that is not a bad spelling….why not?\nGo find me in this picture !!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#references",
    "title": "I am Water",
    "section": "References",
    "text": "References\n\nScott Eberle, “The Aspects of Play” https://www.journalofplay.org/sites/www.journalofplay.org/files/pdf-articles/6-2-article-elements-of-play.pdf\n“Platform Creativity: Domain, Field, and Person”. https://medium.com/call4/domain-8a22b6b486f4\n8 Ways To Create Flow According to Mihaly Csikszentmihalyi. https://positivepsychology.com/mihaly-csikszentmihalyi-father-of-flow/\nIan Bogost, “Play at Anything”.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#introduction",
    "title": "Birds of Different Feathers",
    "section": "Introduction",
    "text": "Introduction\nSocrates said, “Gnothi seauton”(Γνῶθι σαυτόν), know thyself. Let us do that, as we create some novel situations for ourselves in a Game.\n\nIcebreaker Game!\nLook at the slide full of numbers on the screen. I would like you to:\n\nSilently work through the slide until you have found all the numbers\n\nRaise your hand when you have done so.\n\n\n\nA Personality Game\nI have divided you up into 4 groups. Each group will be given a specific task. You will need to:\n\nDiscuss how to complete the Task\n\nComplete the Task and document your work\n\nAlso document the main aspects of Discussion in your group. Who said/ suggested what; what were the points of agreement; what were the contentious points etc.\n\nPresent all of this : 7 + 5 minutes / team",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#references",
    "title": "Birds of Different Feathers",
    "section": "References",
    "text": "References\n\nHere is the MBTI Test site: https://www.16personalities.com/free-personality-test\nIce-breaker Activity Presentation (Go to Webpage)\nMBTI Game: PDF",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#why-does-this-matter-to-us",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#why-does-this-matter-to-us",
    "title": "Birds of Different Feathers",
    "section": "Why does this Matter to Us",
    "text": "Why does this Matter to Us\nFrom the two exercises, we see that:\n- reality really matters upon the viewer: and that there are many possible points of view.\n- We all have our “default modes” for behaviour\n- As creators we need to be able straddle, if possible, the spectrum of the 4 preference pairs, in order to comprehend reality better.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#introduction",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Introduction",
    "text": "Introduction\nLet us take our first step into the world of TRIZ. What did you think of immediately when you saw the first picture on this page?\nIn TRIZ, the fundamental way of looking at an Inventive Design Problem is to discover and propose Contradictions. These are rendered in as simple and stark a language as possible…the starker the better!\nOnce we have our Contradiction (and there can be more than one in a given Design Situation !) we can use TRIZ Principles to solve them WITHOUT COMPROMISE.\nWhat sort of Contradictions do we see in these familiar objects? What is good and what is not so good? Could that be the source of a problem to solve?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-everyday-objects-for-us-to-contemplate",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-everyday-objects-for-us-to-contemplate",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Some Everyday Objects for us to Contemplate",
    "text": "Some Everyday Objects for us to Contemplate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContemplate and note down for each Object:\n\nWhat is the Object meant to do? What is its Main Purpose?\nWhat are the other Accompanying Objects that it works with?\nWhat is One Aspect, or Parameter, or Knob for each of the Objects that you have listed, that makes the Object useful?\n\n\n\n\n\n\n\n\n\n\nWhat are the current Settings/Values for each Knob?\nChange the Setting of Each Knob to its natural opposite extreme. What Happens? Is it a good thing? When?\nYou will see that in many cases, each Knob creates a Certain Outcome at one Setting and another Outcome at the Opposite Setting.\nAre both Outcomes desirable? Do you want “eat your cake and have it too?”\nThis could be the source of your PROBLEM / CONTRADICTION !!\n\nIn TRIZ, this way of expressing a Problem as a simple Contradiction is referred to as stating an ADMINISTRATIVE CONTRADICTION (AC). (Oh those Russians…).\nLater we will use our experience with making metaphors to convert the AC into a more TRIZ-like Contradiction, using TRIZ language.\nLet us now consider some examples first and then get some practice at setting up simple AC:",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-example-contradictions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-example-contradictions",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Some Example Contradictions…",
    "text": "Some Example Contradictions…\nWe see them everywhere….if one is observant!! Let’s see a few:\nC1: Contradiction-1\nHow is this for a Contradiction? A shelf in a supermarket carries this placard for a shelf of FRESH JUICE:\n\n\n\n\n\n\n\n\n\n\nC2: Contradiction-2\nHere is another: \n\nC3: Contradiction-3\nSnails want to stay safe, and there is safety on the ground, but the ground is too hot. On the Ground, a Snail can be by itself, above the ground, they become visible to predators.\n\n\n\n\nSo…they need to group together.\nC4: Contradiction-4\nMost people’s healthcare is tied to their job….Therefore, every corona-time layoff creates yet another person without health insurance in this country. (The Corona Pandemic needs healthcare and therefore health Insurance.)\n\nThis outbreak is highlighting, with extreme clarity, every major contradiction of this society and its decaying social order. https://t.co/EtSUY3pPMi\n— Revolutionary Left Radio (@RevLeftRadio)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradictory-situations",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradictory-situations",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Contradictory Situations",
    "text": "Contradictory Situations\nWe don’t contemplate only objects at all times; indeed, as designers/artists/creators, we want to be able to make objects. What we more commonly contemplate is a situation.\nHow does one figure Parameters/Aspects/Knobs on situations?\nWe use what is called an Ishikawa Fishbone Diagram. There are many versions of this diagram depending upon the DOMAIN it is applied in; it should be considered more as a process for thinking. You should search for other forms of this diagram and quickly learn to apply them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDeciding which are the Knobs that matter, and how they effect the outcome is not always this simple. It would typically involve research using structured experimentation to determine the effects of turning the knobs and deciding which ones matter. This experimentation lies within a DOMAIN called [Design of Experiments].",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#documenting-the-administrative-contradiction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#documenting-the-administrative-contradiction",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Documenting the Administrative Contradiction",
    "text": "Documenting the Administrative Contradiction\nYou can document your analysis of the Situation using the following questionnaire 5W+H format:\n\nWhat does the problem seem to be?\n\nWho has the problem?\n\n(Think Knobs!!)\n\n\nWhen does the problem occur? All the time? Under certain circumstances?\nWhere does the problem occur?\n\nWhy does the problem occur?\n\n“Ask why 5 times” – W. Edwards Deming\n\n\nHow does the problem occur?\n\n\nNOTE: This process should remind you of our exercise on the Guilford and Wallach-Kogan Divergent Thinking Game, except that this is of course more structured method, whereas that was simple brainstorming.\n\nWith this method you should be able to state an Administrative Contradiction in the following (loose!) sentence structure — Items in &lt; &gt; come from the Ishikawa and your 5W + H questions):\nWhen we, as &lt; WHO / MANPOWER &gt;, attempt to perform &lt; HOW / METHOD &gt; during &lt;WHEN&gt; on &lt; WHERE / MACHINERY / KNOB&gt;, we improve &lt;EFFECT&gt;, but lose out on &lt;negative EFFECT&gt;.\nLet’s see this method in action. The Domain of work may not be ours, but the context should be clear enough!\n\n  Let us now apply these ideas to examine the following challenges below.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradiction-challenges",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradiction-challenges",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Contradiction Challenges",
    "text": "Contradiction Challenges\n\nChallenge #1: Web and PDF\nChallenge #2: Web and PDF\nChallenge #3: Web and PDF\nChallenge #4: Web and PDF",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#additional-readings",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#additional-readings",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Additional Readings",
    "text": "Additional Readings\nDo glance through these articles and try to form a “contradiction mentality” about things around you:\n\nEllen Domb (1997). How to Help TRIZ Beginners Succeed. https://www.metodolog.ru/triz-journal/archives/1997/04/a/index.html\nhttps://www.bbc.com/worklife/article/20201109-why-the-paradox-mindset-is-the-key-to-success\nhttps://hbr.org/2008/06/the-contradictions-that-drive-toyotas-success\nOpen Source TRIZ: Contradiction Challenges! https://www.opensourcetriz.com/images/1_OpenSourceTRIZ_Pictures/1.1_Teaching_Materials/01_Contradictions_Exercises_Actual.pdf\nhttps://taproot.com/whats-wrong-with-5-whys-complete-article/&lt;/u",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#introduction",
    "title": "I Think, Therefore I am",
    "section": "Introduction",
    "text": "Introduction\nOften times we swiftly come to conclusions or decisions, without being aware that\na) we are making a big mistake based on the information that is available,\nb) we are looking for our fondest beliefs to be true, or worse\nc) we are being manipulated !\nHence, one of the important points of view we need to retain is to be aware of our Cognitive Biases. Being aware of this may be the first step to check ourselves and therefore arrive a more nuanced “appreciation of the situation”, that would then allow us to formulate our Innovative Problems better.\nLet us now engage in some simple activities that show off! to us, our Cognitive Biases!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#some-simple-cognitive-bias-games",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#some-simple-cognitive-bias-games",
    "title": "I Think, Therefore I am",
    "section": "Some Simple Cognitive Bias Games",
    "text": "Some Simple Cognitive Bias Games",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-1-cognitive-miserliness",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-1-cognitive-miserliness",
    "title": "I Think, Therefore I am",
    "section": "Bias #1: Cognitive Miserliness",
    "text": "Bias #1: Cognitive Miserliness\nGame\nLet us start with a set of relatively easy games. Here PDF are some questions for you. Please write or call out the answers as you see them.\nInterpretation\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-2-halo-effect",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-2-halo-effect",
    "title": "I Think, Therefore I am",
    "section": "Bias #2 Halo Effect",
    "text": "Bias #2 Halo Effect\nGame\nLook at the set of words given here PDF. We have a set of words arranged in a series; each word-set describing a person. Please look at these and then write a brief description of that person.\nLet us now compare what different people wrote about the persons described the respective word-sets.\nInterpretation\nWhat we saw was that the word-set are the same but in reverse order. The SEQUENCE in which we encounter them colours our opinion of the person whom they describe. So first impressions do seem to be last impressions! This is a Cognitive Bias the Halo Effect at work. We formulate decisions based on first impressions. What can we do to over come this bias?\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-3-priming",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-3-priming",
    "title": "I Think, Therefore I am",
    "section": "Bias #3 Priming",
    "text": "Bias #3 Priming\nGame\nYou will be given a small questionnaire PDF, with a single multiple-choice question. Please enter your answer at the appropriate location.\nIntepretation\nThe Marvels of Priming\nAs is common in science, the first big breakthrough in our understanding of the mechanism of association was an improvement in a method of measurement. Until a few decades ago, the only way to study associations was to ask many people questions such as, “What is the first word that comes to your mind when you hear the word DAY?” The researchers tallied the frequency of responses, such as “night,” “sunny,” or “long.” In the 1980s, psychologists discovered that exposure to a word causes immediate and measurable changes in the ease with which many related words can be evoked. If you have recently seen or heard the word EAT, you are temporarily more likely to complete the word fragment SO_P as SOUP than as SOAP. The opposite would happen, of course, if you had just seen WASH. We call this a priming effect and say that the idea of EAT primes the idea of SOUP, and that WASH primes SOAP.\nPriming effects take many forms. If the idea of EAT is currently on your mind (whether or not you are conscious of it), you will be quicker than usual to recognize the word SOUP when it is spoken in a whisper or presented in a blurry font. And of course you are primed not only for the idea of soup but also for a multitude of food-related ideas, including fork, hungry, fat, diet, and cookie. If for your most recent meal you sat at a wobbly restaurant table, you will be primed for wobbly as well. Furthermore, the primed ideas have some ability to prime other ideas, although more weakly. Like ripples on a pond, activation spreads through a small part of the vast network of associated ideas. The mapping of these ripples is now one of the most exciting pursuits in psychological research.\nThe Ideomotor Effect Another major advance in our understanding of memory was the discovery that priming is not restricted to concepts and words. You cannot know this from conscious experience, of course, but you must accept the alien idea that your actions and your emotions can be primed by events of which you are not even aware.\nIn an experiment that became an instant classic, the psychologist John Bargh and his collaborators asked students at New York University — most aged eighteen to twentytwo — to assemble four-word sentences from a set of five words (for example, “finds he it yellow instantly”). For one group of students, half the scrambled sentences contained words associated with the elderly, such as Florida, forgetful, bald, gray, or wrinkle. When they had completed that task, the young participants were sent out to do another experiment in an office down the hall. That short walk was what the experiment was about. The researchers unobtrusively measured the time it took people to get from one end of the corridor to the other. As Bargh had predicted, the young people who had fashioned a sentence from words with an elderly theme walked down the hallway significantly more slowly than the others.\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-4",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-4",
    "title": "I Think, Therefore I am",
    "section": "Bias #4",
    "text": "Bias #4\nGame\nIntepretation\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#conclusions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#conclusions",
    "title": "I Think, Therefore I am",
    "section": "Conclusions",
    "text": "Conclusions\nSo what should we do to try to overcome these ( and other ) Cognitive Biases?\n\nBe aware: Awareness is the first step towards overcoming errors in judgement. Keeping in mind the potentially harmful consequences of first impressions is helpful when meeting new people.\n\n\nSlow down: The second step is to deliberately slow down your judgement and any subsequent decisions. For example, never make a recruitment choice straight after the interview.\n\n\nBe systematic: Finally, try to engage your analytical reasoning skills by taking a systematic approach. This sounds trickier than it is. In the context of interviewing, you could prepare a list of essential criteria and force yourself to consider each one carefully before making a choice.\n\nThis helps us to not arrive at flawed ideas to pursue but ones that we are reasonably sure we can justify to….our Field, as conceptualized by Mihaly Csikszentmihalyi.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#references",
    "title": "I Think, Therefore I am",
    "section": "References",
    "text": "References\n\nKahneman, Daniel. “Thinking Fast and Slow”\nBenson, Nigel C.; Ginsburg, Joannah; Grand, Voula; Lazyan, Merrin & Weeks, Marcus; Collin, Catherine, “The Psychology Handbook: Big Ideas Simply Explained”\nPashler, Harold. “Encyclopedia of the Mind”\nHolyoak, Keith J.& Morrison, Robert G., “The Cambridge Handbook of Thinking and Reasoning”\nQuality Enhancement Program (QEP cafe) https://sites.google.com/site/qepcafe/#training-modules",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Think, Therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#metaphors-for-our-contradictions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#metaphors-for-our-contradictions",
    "title": "TRIZ - A Contradictory Language",
    "section": "Metaphors for our Contradictions",
    "text": "Metaphors for our Contradictions\nRecall that you considered several familiar Objects in the earlier module on Problems and Contradictions. And also considered a few sample Situations to find Administrative Contradictions in them.\nWe used the method of:\n\nFinding Knobs in Objects and Situations using the Ishikawa Diagram\nTurning the Knobs both ways\nFinding what improves or worsens in each case\nDeciding upon an Administrative Contradiction\n\n\nNow that we know how problems can be stated as simple Administrative Contradictions (AC), we need to take the next step and make what TRIZ calls Technical Contradictions (TC) and Physical Contradictions (PC). To do this, we will use a set of metaphoric phrases that are an integral part of (classical) TRIZ. These metaphoric phrases are simple enough and provide rich troves for imaginative problem solving.\nBefore we study this TRIZ Contradiction Language however, we need to obtain a quick understanding of its history.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#the-history-of-triz",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#the-history-of-triz",
    "title": "TRIZ - A Contradictory Language",
    "section": "The History of TRIZ",
    "text": "The History of TRIZ\n\n\n\n\n\n\n\n\nThe creator of TRIZ, Genrich Altshuller, was born in Russia in 1926, made his first invention at age 14 (9th Grade), and was later educated as a mechanical engineer. At the time he started working on TRIZ, in 1946, he was employed in the patent department of the Soviet navy, assisting inventors in filing their patents, in Baku, Azerbaijan.\nWhile there he became intrigued by the question of how an invention happens:\n\nIs it a matter of luck? The result of a mental “light bulb” turning on, as in the comics? Or can inventions be seen as the result of systematic patterns of inventive thinking?\n\nAltshuller adopted an empirical approach to answering this question. He studied thousands of patents, looking for commonalities, repetitive patterns, and principles of inventive thought. As he found them he codified and documented them, as shown below. His results, when eventually published, attracted many enthusiasts who continued and expanded the work over the years, reviewing what is now estimated to be more than two million patents worldwide.\n\n\n\n\n\n\n\n\nLevel 1: Apparent Solution. This level requires no real invention; it consists of minor adaptations of existing concepts. Simple improvement of a technical system. They require knowledge available within an industry relevant to that system.\nLevel 2: Improvement. This level makes small improvements to existing approaches. Inventions include the resolution of a technical contradiction(TC). They require knowledge from different areas within an industry relevant to the system.\nLevel 3: Invention Inside the Paradigm. This level uses methods from other fields and improves previous approaches. This is an invention containing a resolution of a physical contradiction(PC). It requires knowledge from other industries.\nLevels #2 & #3 solve contradictions and therefore are innovative by definition.\nLevel 4. Invention Outside the Paradigm. This level involves a new design that is based on modifications of existing principles but in a manner not previously used. This level involves development a new technology. It is developed by using breakthrough solutions that requires knowledge from different fields of science. This fourth level also improves upon a technical system, but without solving an existing technical problem. Instead, it improves the function by replacing the original technology with a new technology. For example a mechanical system is replaced with a chemical system to perform the function.\nLevel 5: Discovery. This is the highest invention level. It consists of completely new concepts using new principles, and involves the discovery of new phenomena. The new phenomenon is discovered that allows pushing the existing technology to a higher level.\nAltshuller concluded from his research that a large number of patents (77%) belong only to Levels #1 and #2. The practical utilization of TRIZ methodology can help inventors elevate their innovative solutions to Levels #3 and #4. As a result of this work, hundreds of technical papers and many books on TRIZ have been published, including 14 books by Altshuller himself.\nTRIZ is actively used in Companies such as Boeing, Bridgestone, Eastman Kodak, Ford Motor Company, Harley-Davidson Motor Company, Hewlett-Packard, Illinois Tool Works, Inficon, Ingersoll Rand, Kimberly-Clark, L.G. Electronics, Lucent Technologies, Michelin, National Semiconductor, NASA, Philips, Rolls-Royce, Samsung, Siemens, Western Digital, and Xerox among others.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#speaking-triz",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#speaking-triz",
    "title": "TRIZ - A Contradictory Language",
    "section": "Speaking TRIZ",
    "text": "Speaking TRIZ\nAltshuller made two more discoveries (at least!) based on his study of Soviet Patents:\n\nThat when the the problems solved in the patents were expressed as Contradictions, there were only a few standard phrases using which these Contradictions could be constructed. These were an astonishingly small 48 in number!! Every problem could be described as a contradiction using some pair of these 48 parameters. We will call these the 48 TRIZ Parameters1.\nWhen the solutions in the Patents were examined, they could all be expressed simply as one of 40 abstract nouns or phrases. Just…40! We will call these the 40 TRIZ Inventive Principles\n\nSome examples:\n\nTRIZ Parameters: Weight of an stationary Object, Loss of Substance, and Temperature.\n\nTRIZ Inventive Principles: Asymmetry, Parameter Change and The Other Way Around.\n\nSo the task is:\n\nTake your Administrative Contradiction (AC)\n\nConvert “each side” (i.e terms) of the AC into a TRIZ Technical Contradiction by using a pair of the 48 TRIZ Parameters.\n\nMake sure that you are able to justify to yourself the metaphorical connection between the terms of AC and the chosen pair of TRIZ Parameters. For example, “Loss of substance” could mean “loss of money”!! Write this down, for both sides of your AC.\n\nWe may be able to also create a Physical Contradiction at this time, when we are examining the Ishikawa knobs.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#some-contradiction-examples",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#some-contradiction-examples",
    "title": "TRIZ - A Contradictory Language",
    "section": "Some Contradiction Examples",
    "text": "Some Contradiction Examples\nTC Examples:\n\nIncreasing the power of an engine (positive improvement) requires an increase in the size of the engine (negative effect). So, an inventor considers increasing the power partially in order to reduce the negative effect (compromise solution).\n\nTo increase the speed of an airplane, a new and more powerful engine is installed. This increases the weight of the airplane so the wings can no longer support it during takeoff. Increasing the wing size produces more drag, slowing the airplane down.\n\nPC Examples:\n\nLanding gear must be present on an airplane in order to land and takeoff. It should not be present during flight because of an increase in air drag. The physical contradiction is that the landing gear must be both present and absent. This contradiction is resolved by separating the requirements in time — make the landing gear retractable.\nFor high water diving, water must be “hard” to support the diver and “soft” so as not to injure the diver. The physical contradiction: The water must be hard and soft at the same time. This contradiction is resolved by separating the requirements in space: Saturate the water with air bubbles — the pool contains both air and water.\n… an inherent contradiction in takeout pizza products. The customer wants the pizza to be hot, but hot pizza gives off steam. As a result, the cardboard box lid absorbs the steam, softens, and collapses down on the pizza. When the customer lifts the lid of the pizza box after arriving home, some of the pizza, which has stuck to the lid, attaches to the raised lid and the customer is not pleased because a significant amount of the cheese that he paid for is not edible. We want the pizza to be hot for one reason (an enjoyable pizza) and we want it to be cold for another (preventing the lid from becoming soggy and collapsing due to steam and water absorption). The little plastic tripod that is in the center of many takeout pizzas was the subject of the talk I heard. Such a simple invention! It resolves the contradiction of the pizza being hot and cold at the same time. (Jack Hipple, The Ideal Result)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#thinking-problem-solving-and-language",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#thinking-problem-solving-and-language",
    "title": "TRIZ - A Contradictory Language",
    "section": "Thinking, Problem Solving, and Language",
    "text": "Thinking, Problem Solving, and Language\nHere is Albert Einstein speaking on The Language of Science. The transcript is here.PDF.\nOne quote from this short reading seems very apposite here:\n\n“The super-national character of scientific concepts and scientific language is due to the fact that they have been set up by the best brains of all countries and all times. In solitude, and yet in cooperative effort as regards the final effect, they created the spiritual tools for the technical revolutions which have transformed the life of mankind in the last centuries. Their system of concepts has served as a guide in the bewildering chaos of perceptions so that we learned to grasp general truths from particular observations.”",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#references",
    "title": "TRIZ - A Contradictory Language",
    "section": "References",
    "text": "References\n\nValeri Souchkov, Differentiating Among the Five Levels of Solutions,Web\nValeri Souchkov, “How to Define a Contradiction”, Web and PDF\nOpen Source TRIZ: TRIZ Power Tools: Formulating Contradictions: Webpage",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#additional-readings",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#additional-readings",
    "title": "TRIZ - A Contradictory Language",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nhttps://www.bbc.com/worklife/article/20201109-why-the-paradox-mindset-is-the-key-to-success\nhttps://hbr.org/2008/06/the-contradictions-that-drive-toyotas-success",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#footnotes",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#footnotes",
    "title": "TRIZ - A Contradictory Language",
    "section": "Footnotes",
    "text": "Footnotes\n\nClassical TRIZ started off with 39 Inventive Parameters. In recent years, the parameters have been increased to 48, after a study of an ever-increasing body of patent literature.↩︎",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/200-TRIZ-Advanced-Stuff/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/200-TRIZ-Advanced-Stuff/index.html#references",
    "title": "TRIZ - Substance Field Analysis, and ARIZ",
    "section": "References",
    "text": "References\n\nhttps://the-trizjournal.com/altshullers-greatest-discovery-beyond/\nProject TETRIS: Chapter 3: Short Review of ARIZ: Algorithm for Innovative Problem Solving and a Case Study (PDF)\nProject TETRIS: Chapter 4: Substance-Field ( “Su-Field” ) Analysis and Standard Solutions basic notions and rules (PDF)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - Substance Field Analysis, and ARIZ"
    ]
  },
  {
    "objectID": "content/courses/listing.html",
    "href": "content/courses/listing.html",
    "title": "Teaching",
    "section": "",
    "text": "Here are the courses that I teach:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\nData Analytics for Managers and Creators\n\n\nUsing R for Data Visualization, Analysis, and Inference\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\nTRIZ for Problem Solvers\n\n\nMethods and Process for Creative Thinking and Problem Solving\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\nMath Models for Creative Coders\n\n\nUsing matters of Life and Death Math and Code to Create Novel Experiences\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n🧭 Tech Tools for Creative Education\n\n\nMaking Explanable Explanations with Tech\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\nLiterary Jukebox: In Short, the World\n\n\nDesign Principles from Short Fiction\n\n\nDec 31, 2022\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html",
    "title": "\n Ranking",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nDumbbell and Radar Charts"
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#what-graphs-will-we-see-today",
    "title": "\n Ranking",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nDumbbell and Radar Charts"
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Ranking",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#inspiration",
    "title": "\n Ranking",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Energy Sources in the USA in 2024\n\n\n\n\n\n\n\n\n\n(b) 5 tools Players in Baseball\n\n\n\n\n\n\nFigure 1: Dumbbell and Radar Charts for Ranking\n\n\nWhat do we see here? From https://www.visualcapitalist.com/sp/americas-cheapest-sources-of-electricity-in-2024/ :\nFrom Figure 1 (a):\n\n\nOnshore wind power effectively costs $0 per megawatt-hour (MWh) when subsidies are included!\n\nDemand for storage solutions is rising quickly. If storage is included, the minimum cost for onshore wind increases to $8 per MWh.\n\nSolar photovoltaics (PV) have similarly attractive economics. With subsidies, the minimum cost is $6 per MWh. When including storage, $38 per MWh. Notably, the maximum cost of solar PV with storage has significantly increased from $102 in 2023 to $210 in 2024.\n\nFor gas-combined cycle plants, which combine natural gas and steam turbines for efficient electricity generation, the maximum price has climbed $7 year-over-year to $108 per MWh.\n\n\nAnd from From Figure 1 (b)?\n\nThere is a clear difference in the capabilities of the three players compared, though all of them are classified as “5 tools” players.\n\nEach player is better than the others at one unique skill: Betts at Throwing, Judge at Hit_power, and Trout at Hit_avg."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#how-do-these-charts-work",
    "title": "\n Ranking",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nDumbbell charts show changes in rank/attainment/performance of several entities over two “instants in time” or two “points of interest”. ( Note these two prepositions!! ) The chart is usually sorted to show the entity with the largest change at the very top, or the very bottom. The Y-axis is the “entity” variable (Qual!) and the X-axis is a SINGLE rank or measure of attainment/performance (Quant!). In the above chart, we saw different energy sources as “entities” and their cost as the performance measure, and the energy sources were (roughly) ranked in order of the change in cost. The shape is of course, a bar/dumbbell with endpoints. The length of the bar is proportional to the change.\nA Radar chart does not show change; it simply plots a set of static performance measures or ranks. However these measures or ranks are not a single performance measure but MULTIPLE. So how do we have multiple X-axes then? We use angle and create as many axes as we need depending upon the number of measures we wish to show, all axes diverging from a single point. The performance measure us marked off along each such angled axis, usually with the same scale (though that may require external pre-processing). The final shape is of course a polygon, and we can plot many “entities” as overlapping, semi-transparent polygons. In the plot above, the entities are the players, and the performance measures are the so-called 5 tools of baseball."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#plotting-a-dumbbell-chart",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#plotting-a-dumbbell-chart",
    "title": "\n Ranking",
    "section": "\n Plotting a Dumbbell Chart",
    "text": "Plotting a Dumbbell Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThere does not appear to be a way of plotting dumbbell charts in Orange. 😢.\n\n\nThere does not appear to be a way of plotting dumbbell charts in RAWgraphs. 😢.\n\n\nIn DataWrapper, a dumbbell plot is referred to as a range plot, which is also quite an appropriate name: https://academy.datawrapper.de/article/111-how-to-create-a-range-plot\nAnother rather similar and evocative plot on DataWrapper is the arrow plot: https://academy.datawrapper.de/article/123-how-to-create-an-arrow-plot\nHere is an example of a dumbbell chart/range chart created in DataWrapper. This chart ranks different countries on how much better off the nursing profession is in those countries. (The comparison is with the UK).\nHit the Get the data button, and then upload it into https://app.datawrapper.de/ and see if you can recreate this chart:\n\n\n\n\n\nThe direct link to this dataviz is https://www.datawrapper.de/_/9q1tJ/"
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#plotting-a-radar-chart",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#plotting-a-radar-chart",
    "title": "\n Ranking",
    "section": "\n Plotting a Radar Chart",
    "text": "Plotting a Radar Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nNopes.\n\n\n\nDownload this RAWgraphs project file to your machine and then upload to https://app.rawgraphs.io/:\n Download the RawGraphs Radar Chart Project File \n\n\nNopes."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#dataset-brood-parasites---cuckoo-eggs-and-host-eggs",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#dataset-brood-parasites---cuckoo-eggs-and-host-eggs",
    "title": "\n Ranking",
    "section": "\n Dataset: Brood Parasites - Cuckoo Eggs and Host Eggs",
    "text": "Dataset: Brood Parasites - Cuckoo Eggs and Host Eggs\nCuckoo birds drop their eggs into other birds’ nests, where they hatch and are looked after by the unwitting host-parent bird, often at the cost of their own babies, a phenomenon known as brood parasitism.\nThe data is available at Vincent Arel-Bundock’s website: https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/cuckoohosts.csv. Use this URL to directly import into Orange.\nThe dataset contains dimensions of the eggs of the host birds and compares them to that of the cuckoo. Import this dataset into Orange and look at the variables, their nature, and their summaries.\n\n Examine the Data\nA data frame with 10 observations on the following 12 variables. Each row corresponds to a host species bird.\n\n\n\n\n\n\n\n\n\n(a) Egg Data Table\n\n\n\n\n\n\n\n\n\n(b) Egg Data Table\n\n\n\n\n\n\nFigure 2: Egg Dimensions data\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t be confused with Figure 2 (b) showing means and sds, and the very variable names having means and sds! The table shows computed measures in these variables!\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nrownames: Not aptly named, but contains the names of the host bird species.\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nclength: mean length of cuckoo eggs in given host’s nest\n\ncl.sd: standard deviation of cuckoo egg lengths\n\ncbreadth: mean breadth of cuckoo eggs in given host’s nest\n\ncb.sd: standard deviation of cuckoo egg breadths\n\ncnum: number of cuckoo eggs\n\nhlength: length of host eggs\n\nhl.sd: standard deviation of host egg lengths\n\nhbreadth: breadth of host eggs\n\nhb.sd: standard deviation of host egg breadths\n\nhnum: number of host eggs\n\nmatch: number of eggs where color matched\n\nnomatch: number where color did not match\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQuestion #1\n\n\n\nQ1. How different are length, breadth (mean) of host eggs different from those of the cuckoo’s eggs\n\n\n\n\n\nFigure 3: Bird Eggs Radar Chart\n\n\n\n\n\n\n\n\n\n\nNoteQuestion #2\n\n\n\nQ2. Are the statistical measures (standard deviations) of the length/breadth different between cuckoo and host eggs?\n\n\n\n\n\nFigure 4: Bird Eggs Stats Radar Chart\n\n\n\n\n\n\n\nFigure 5: Bird Eggs Stats Radar Chart by Host Species\n\n\n\n\n\n What is the Story Here?\n\nThe Figure 3 shows that both mean-lengths and mean-breadths of the eggs are nearly the same between those of the host and the cuckoo! 😮. The poor host bird has little chance of detecting the parasite egg purely by dimensions….\nFrom Figure 4, the statistical variations are also nearly the same, except for a few host species where the variation (sd) in the host-egg-length is much larger.\nThis aspect is seen better in Figure 5, where for the Wren, the Robin, and the Hedge Sparrow, ….s-o-m-e.. times, the parasite cuckoo egg may be much smaller and perhaps detectable..but again small size may render it inconspicous!\nBut..is this over time? Are all the eggs the same age?…Ummm…\n\nWho was it who said:\nकाकः कृष्णः पिकः कृष्णः को भेदः पिककाकयोः ।\nवसन्तकाले संप्राप्ते काकः काकः पिकः पिकः ॥\n- कुवलयानन्द"
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#dataset-employment-vs-population-vs-gender",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#dataset-employment-vs-population-vs-gender",
    "title": "\n Ranking",
    "section": "\n Dataset: Employment vs Population vs Gender",
    "text": "Dataset: Employment vs Population vs Gender\nThis is a dataset from Our World in Data. Download this data and import into Orange to take a look at it. We might then decide what we wish to see by way of a chart and pre-process the data and saving it with Orange. Then we will send this data to RAWGraphs/DataWrapper to plot our charts.\n\n\n Employment Data\n\n\nWe will as usual examine the data in Orange, filter and process as needed, and then use the other tools to plot charts to answer our Questions. The workflow for Orange is downloadable with the button below:\n Download the Orange Workflow \n\n Examine the Data\n\n\nWe have converted the Entity and Code variable to Qual\nWe have used the Select Row widget to select just 7 rows from the 53K rows\n\n Data Dictionary\nA dataframe with 7 rows and 5 columns.\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nemployment-to-population-ratio, men(%): Population of men employed\n\nemployment-to-population-ratio, women(%): Population of women employed\n\nYear: year( = 2010)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nentity: country\n\ncode: code for the country\n\ncontinent: continent\n\n\n\nUse the Orange Save Data widget to save the filtered file as a new CSV and then upload into DataWrapper! Here is the dumbbell chart from DataWrapper. You can head off to DataWrapper here and edit a copy of this chart.\n\n\n\n\n\n\n What is the Story Here?\nWith a simple but effective chart like this, we can tell the story pretty quickly:\n\nIndia and Pakistan have huge differences between the employment percentages of women and men.\nAll countries shown in the chart have a higher percentage of men employed than women."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#bump-charts",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#bump-charts",
    "title": "\n Ranking",
    "section": "\n Bump Charts",
    "text": "Bump Charts\nDataWrapper does offer a way of creating bump charts for ranking, that look like this:\n\n\n\n\n\nFigure 6: Bump Chart\n\n\nThe chart shows the ranking of different chart types over the years. The procedure on DataWrapper is here: https://academy.datawrapper.de/article/347-how-to-create-a-bump-chart\nHowever, I think this procedure is not worth it and creating the plot with R code is far easier and more intuitive."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#your-turn",
    "title": "\n Ranking",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNote\n\n\n\n\nTry the Bird Eggs dataset with normalization and see if the story changes!\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nJapanese Sake Wines Find this dataset about the grading of Japanese Sake wines: https://vincentarelbundock.github.io/Rdatasets/csv/heplots/Sake.csv\" You should be able to use this URL directly in RAWGraphs/DataWrapper.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSea Weed Nutrition  Download the Seaweed Nutrition xls  Choose the right sheet in the xls! You may need to use Orange to pre-process this data using the Orange Widgets Select Columns, Select Rows, and Preprocess. With the Preprocess widget, you may wish to normalize each column into the range [0,1] for your Radar Charts."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#wait-but-why",
    "title": "\n Ranking",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can measure some Performance metric about entities such as Products, Brands, Shops, Companies, Stock Prices/Earnings and see how it changes over two instances of measurement, with a dumbbell chart.\nThe length of the dumbbells tells a very clear story.\nDumbbell Plots are clearly are more intuitive and clear than the corresponding bar chart:\n\n\n\n\n\n\nFigure 7: Employment Gender Bar Chart\n\n\n\nDifferences between the same set of data at two different aspects is very quickly apparent\n\nDifferences in differences(DID) are also quite easily apparent. Experiments do use these metrics and these plots would be very useful there.\nIf entities have their performance or quality measured over several different “aspects”, a radar chart would serve you well. Do you think Dumbledore could have used a Radar Chart to decide who could have won the House Trophy at Hogwarts?\nThe area(s) and non-overlapping parts of the (overlaid) radar chart are very evocative of superior performance."
  },
  {
    "objectID": "content/courses/NoCode/Modules/60-Rankings/index.html#readings",
    "href": "content/courses/NoCode/Modules/60-Rankings/index.html#readings",
    "title": "\n Ranking",
    "section": "\n Readings",
    "text": "Readings\n\nHighcharts Blog. Why you need to start using dumbbell chartshttps://github.com/hrbrmstr/ggalt#lollipop-charts\nSee this use of Radar Charts in Education. Choose the country/countries of choice and plot their ranks on various educational parameters in a radar chart. https://gpseducation.oecd.org/Home"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html",
    "title": "\n Space",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQual\nChoropleth and Symbols Maps, Cartograms"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#what-graphs-will-we-see-today",
    "title": "\n Space",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQual\nChoropleth and Symbols Maps, Cartograms"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#inspiration",
    "title": "\n Space",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Infosys in the EU\n\n\n\n\n\n\n\n\n\n(b) Population Cartogram\n\n\n\n\n\n\nFigure 1: Choropleth and Cartogram\n\n\n\n\n\n\n\n\n\n\n\n(a) Where’s the next Houthi attack?\n\n\n\n\n\n\n\n\n\n(b) Malacca\n\n\n\n\n\n\nFigure 2: Symbol Maps"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#how-do-these-charts-work",
    "title": "\n Space",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn Figure 1 (a), we have a choropleth map. What does choropleth1 mean? And what kind of information could this map represent? The idea is to colour a specific area of the map, a district or state, based on a Quant or a Qual variable.\nThe Figure 1 (b) deliberately distorts and scales portions of the map in proportion to a Quant variable, in this case, population in 2018.\nIn Figure 2 (a) and Figure 2 (b), symbols are used to indicate either the location/presence of an item of interest, or a quantity by scaling their size in proportion to a Quant variable"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#creating-maps",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#creating-maps",
    "title": "\n Space",
    "section": "\nCreating Maps",
    "text": "Creating Maps\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us use a built-in dataset in Orange to create a Symbol/bubble/Choropleth map. This dataset pertains to the climate in several European cities. Here is the workflow:\n Download Orange Map Workflow file \nLet us look first at the data table.\n\n\n\n\n\nFigure 3: European Cities Data Table\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nCity: City in Europe\n\nClimate: One of six types: oceanic, meditterranean, humid…etc.\n\nClimate(koppen): Some sort of climate classification(acronym?)\n\nQuantitative Data\n\nAll other variables are Quantitative. These pertain to temperature,rainfall, humidity, sunshine, and ultraviolet radiation.\n\nLongitude and Latitude are also Quant\n\n\n\n\n\n\n\n\n\nImportantLong and Lat\n\n\n\nNote the presence of specific Longitude Latitude columns in the data. Why am I saying this in that peculiar order ??💭\n\n\n\n\n\n\n\nFigure 4: European Cities Maps\n\n\nWe have directed the output of the Data Table widget to the Geo Map widget, so that the selected citie(s) show up as symbols on the map. The symbol colour is proportional to one of the other “non-locational” Quant variables. It can also be a Qual variable.\nWe can also connect the same output to the Choropleth Map widget. However in this case, Orange colours the country in which the chosen city is located, based on the selected Quant/Qual variable for colour.\n\n\n\n\n\n\nNoteBase Map\n\n\n\nNote how Orange “gets hold” of a base map of Europe to plot the cities on. These are built-in datasets / automatically downloaded by Orange using free map services such as Open Street Map\n\n\n\n\n\nNopes.\n\n\nSymbol Maps: https://academy.datawrapper.de/category/278-symbol-maps\nChoropleth Maps: https://academy.datawrapper.de/category/93-maps"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#dataset-ufo-sightings",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#dataset-ufo-sightings",
    "title": "\n Space",
    "section": "\n Dataset: UFO Sightings",
    "text": "Dataset: UFO Sightings\nSuppose we have our own data, of places we have visited. Let us cook up such a dataset manually (in Excel) without Long and Lat, and we can then Geo Code the places plot them based on some other parameter of interest.\n\nGeocoding widget extracts latitude/longitude pairs from region names or synthesizes latitude/longitude to return region name. If the region is large, say a country, encoder with return the latitude and longitude of geometric centre.\n\nHere is a Excel to download; you should plot this first and then edit the places and its characteristics to suit your own research.\n Download the UFO Sightings Dataset \nNote that this dataset does have longitude and latitude data. We will import this into Orange and deliberately Geo Code this, just to compare.\n\n Examine the Data\n\n\n\n\n\n\n\n\n\n(a) UFO Data Input\n\n\n\n\n\n\n\n\n\n(b) UFO Data Table\n\n\n\n\n\n\nFigure 5: UFO Sightings Dataset"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#dataset-animal-migration-tracks",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#dataset-animal-migration-tracks",
    "title": "\n Space",
    "section": "\n Dataset: Animal Migration Tracks",
    "text": "Dataset: Animal Migration Tracks\nSo far we have seen maps that place POINTS on a base map. Let us see if we can get tracks to show…or not, peasants.\n\n Examine the Data\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\n\n\n What is the Story Here?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#your-turn",
    "title": "\n Space",
    "section": "\n Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#wait-but-why",
    "title": "\n Space",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#references",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#references",
    "title": "\n Space",
    "section": "References",
    "text": "References\n\nhttps://github.com/CityOfNewYork/nyc-planimetrics/blob/main/Capture_Rules.md"
  },
  {
    "objectID": "content/courses/NoCode/Modules/70-Maps/index.html#footnotes",
    "href": "content/courses/NoCode/Modules/70-Maps/index.html#footnotes",
    "title": "\n Space",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html",
    "title": "\n Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nStatistical Tests\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\n\n\n\n\nFigure 1: Orange Data Mining GUI\n\n\nAll operations are done using a visual menu-driven interface. We drag and drop widgets that can have inputs and outputs. Widgets perform operations on data sent to them by other widgets. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.\nOrange also has add-ons that provide widgets for specific tasks such as Machine Learning, Time Series Analysis and so on."
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#introduction-to-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#introduction-to-orange",
    "title": "\n Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nStatistical Tests\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\n\n\n\n\nFigure 1: Orange Data Mining GUI\n\n\nAll operations are done using a visual menu-driven interface. We drag and drop widgets that can have inputs and outputs. Widgets perform operations on data sent to them by other widgets. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.\nOrange also has add-ons that provide widgets for specific tasks such as Machine Learning, Time Series Analysis and so on."
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#installing-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#installing-orange",
    "title": "\n Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#basic-usage-of-orange",
    "title": "\n Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#orange-workflows",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#orange-workflows",
    "title": "\n Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#widgets-and-channels",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#widgets-and-channels",
    "title": "\n Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#loading-data-into-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#loading-data-into-orange",
    "title": "\n Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n We are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#simple-visuals-using-orange",
    "title": "\n Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#reference",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#reference",
    "title": "\n Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/\nhttps://orangedatamining.com/blog/visualizations-101/\nStackExchange Orange Forum. https://datascience.stackexchange.com/questions/tagged/orange"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html",
    "href": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "",
    "text": "library(\"vcd\")\nlibrary(\"scatterplot3d\")\nlibrary(colorspace)\n## random seed\nmyseed &lt;- 1071\nset.seed(myseed)"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#data",
    "href": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#data",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Data",
    "text": "Data\n\ndata(\"Hospital\")\ndata(\"Arthritis\")\nart &lt;- xtabs(~ Treatment + Improved, data = Arthritis, subset = Sex == \"Female\")\nnames(dimnames(art))[2] &lt;- \"Improvement\"\n\n\ndata(\"UCBAdmissions\")\nnames(dimnames(UCBAdmissions)) &lt;- c(\"Admission\", \"Gender\", \"Department\")\n\n\ndata(\"Punishment\")\npunish &lt;- xtabs(Freq ~ memory + attitude + age + education, data = Punishment)\ndimnames(punish)[[3]][3] &lt;- \"40+\""
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#inference",
    "href": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#inference",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Inference",
    "text": "Inference\n\nset.seed(myseed)\nart_max &lt;- coindep_test(art, n = 5000)\n\nset.seed(myseed)\nucb_max &lt;- coindep_test(aperm(UCBAdmissions, c(3, 2, 1)), margin = \"Department\", n = 5000)\n\n\nset.seed(myseed)\nart_chisq &lt;- coindep_test(art, n = 5000, indepfun = function(x) sum(x^2))\n\n\nset.seed(myseed)\nhos_chisq &lt;- coindep_test(Hospital, n = 5000, indepfun = function(x) sum(x^2))"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#figures",
    "href": "content/courses/NoCode/Modules/35-Proportions/files/mosaic-tutorial.html#figures",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Figures",
    "text": "Figures\n\nFigure 12.1: Grouped bar chart for the hospital data\n\n\nbarplot(Hospital,\n  legend = rownames(Hospital), beside = TRUE,\n  xlab = \"Length of stay\", ylab = \"Number of patients\"\n)\n\n\n\n\n\n\n###################################################\n### Figure 12.2: 3D bar chart for the hospital data\n###################################################\nmyHospital &lt;- t(Hospital)[, 3:1]\nmydat &lt;- data.frame(\n  \"Length of stay\" = as.vector(row(myHospital)),\n  \"Visit frequency\" = as.vector(col(myHospital)),\n  \"Number of patients\" = as.vector(myHospital)\n)\nscatterplot3d(mydat,\n  type = \"h\", pch = \" \", lwd = 10,\n  x.ticklabs = c(\"2-9\", \"\", \"10-19\", \"\", \"20+\"),\n  y.ticklabs = c(\"Never\", \"\", \"Less than monthly\", \"\", \"Regular\"),\n  xlab = \"Length of stay\", ylab = \"Visit frequency\", zlab = \"Number of patients\",\n  y.margin.add = 0.2,\n  color = \"black\", box = FALSE\n)\n\n\n\n\n\n\n############################################################################\n### Figure 12.3: Construction of a mosaic plot for the hospital data, step 1\n############################################################################\nmosaic(margin.table(Hospital, 2), split = TRUE)\n\n\n\n\n\n\n\nFigure 12.4: Construction of a mosaic plot for the hospital data, step 2\n\nmosaic(t(Hospital),\n  split = TRUE, mar = c(left = 3.5),\n  labeling_args = list(\n    offset_labels = c(left = 0.5),\n    offset_varnames = c(left = 1, top = 0.5), set_labels =\n      list(\"Visit frequency\" = c(\"Regular\", \"Less than\\nmonthly\", \"Never\"))\n  )\n)\n\n\n\n\n\n\n###\n### Figure 12.5: Mosaic plot for the Hospital data, alternative splitting\n\nmosaic(Hospital)\n\n\n\n\n\n\n\nFigure 12.6: Sieve plot for the hospital data\n\nsieve(t(Hospital), split = TRUE, pop = FALSE, gp = gpar(lty = \"dotted\", col = \"black\"))\nlabeling_cells(text = t(Hospital), clip = FALSE, gp = gpar(fontface = 2, fontsize = 15))(t(Hospital))\n\n\n\n\n\n\n\nFigure 12.7: Association plot for the hospital data\n\nassoc(t(Hospital), split = TRUE)\n\n\n\n\n\n\n\nFigure 12.8: Qualitative color palette for HSV and HCL space\n\npar(mfrow = c(1, 2), mar = c(1, 1, 1, 1), oma = c(0, 0, 0, 0))\npie(rep(1, 9), radius = 1, col = rainbow(9), labels = 360 * 0:8 / 9)\npie(rep(1, 9), radius = 1, col = rainbow_hcl(9), labels = 360 * 0:8 / 9)\n\n\n\n\n\n\n\nFigure 12.9: Diverging color palettes for HSV and HCL space\n\npar(mfrow = c(1, 1), mar = c(1, 1, 1, 1), oma = c(0, 0, 0, 0))\nplot.new()\nrect(0:4 / 5, 0.2, 1:5 / 5, 0.5, border = 0, col = diverge_hcl(5))\nrect(0, 0.2, 1, 0.5, border = 1, col = NULL)\nrect(0:4 / 5, 0.55, 1:5 / 5, 0.85, border = 0, col = diverge_hsv(5))\nrect(0, 0.55, 1, 0.85, border = 1, col = NULL)\ntext(c(1:5 / 5 - 0.1), 0.11, c(\"(260, 100, 50)\", \"(260, 50, 70)\", \"(H, 0, 90)\", \"(0, 50, 70)\", \"(0, 100, 50)\"))\ntext(c(1:5 / 5 - 0.1), 0.91, c(\"(240, 100, 100)\", \"(240, 50, 100)\", \"(H, 0, 100)\", \"(0, 50, 100)\", \"(0, 100, 100)\"))\n\n\n\n\n\n\n\nFigure 12.10: Spine plot with highlighting for the hospital data\n\nmycol &lt;- rep(grey.colors(2)[2:1], 1:2)\nmosaic(t(Hospital),\n  mar = c(left = 3.5),\n  labeling_args = list(\n    offset_labels = c(left = 0.5),\n    offset_varnames = c(left = 1, top = 0.5), set_labels =\n      list(\"Visit frequency\" = c(\"Regular\", \"Less than\\nmonthly\", \"Never\"))\n  ),\n  split = TRUE, highlighting = 2, gp = gpar(fill = mycol, col = mycol)\n)\n\n\n\n\n\n\n\nFigure 12.11: Mosaic display of the hospital data with Friendly-like\n\n### color coding of the residuals\n\nmosaic(t(Hospital),\n  split = TRUE, shade = TRUE,\n  mar = c(left = 3.5),\n  gp_args = list(p.value = hos_chisq$p.value),\n  labeling_args = list(\n    offset_labels = c(left = 0.5),\n    offset_varnames = c(left = 1, top = 0.5), set_labels =\n      list(\"Visit frequency\" = c(\"Regular\", \"Less than\\nmonthly\", \"Never\"))\n  )\n)\n\n\n\n\n\n\n\nFigure 12.12: Association plot of the hospital data with\n\n### Friendly-like color coding of the residuals\n##########################################################################\nassoc(t(Hospital),\n  split = TRUE, shade = TRUE, keep = TRUE,\n  gp_args = list(p.value = hos_chisq$p.value)\n)\n\n\n\n\n\n\n\nFigure 12.13: Mosaic plot for the arthritis data, using the chi-squared test and fixed cut-off points for the shading\n\n###########################################################################\nmosaic(art, shade = TRUE, gp_args = list(p.value = art_chisq$p.value))\n\n\n\n\n\n\n\nFigure 12.14: Mosaic plot for the arthritis data, using the maximum test and data-driven cut-off points for the residuals\n\nset.seed(myseed)\nmosaic(art, gp = shading_max, gp_args = list(n = 5000))\n\n\n\n\n\n\n\nFigure 12.15: Pairs-plot for the UCB admissions data\n\npairs(UCBAdmissions)\n\n\n\n\n\n\n\nFigure 12.16: Doubledecker plot for the UCB admissions data\n\ndoubledecker(aperm(aperm(UCBAdmissions, c(1, 3, 2))[2:1, , ], c(2, 3, 1)),\n  margins = c(left = 0, right = 5), col = rev(grey.colors(2))\n)\n\n\n\n\n\n\n\nFigure 12.17: Mosic plot for the UCB admissions data\n\nmosaic(aperm(UCBAdmissions, c(3, 2, 1)),\n  data = UCBAdmissions, split = TRUE,\n  shade = TRUE, keep = FALSE, residuals = ucb_max$residuals,\n  gp_args = list(p.value = ucb_max$p.value), rep = c(Admission = FALSE)\n)\n\n\n\n\n\n\n\nFigure 12.18: Association plot for the UCB admissions data\n\nassoc(aperm(UCBAdmissions, c(3, 2, 1)),\n  split = c(TRUE, FALSE, TRUE), shade = TRUE,\n  residuals_type = \"Pearson\", residuals = ucb_max$residuals,\n  gp_args = list(p.value = ucb_max$p.value), rep = c(Admission = FALSE)\n)\n\n\n\n\n\n\n\nFigure 12.19: Conditional association plot for the UCB admissions data\n\ncotabplot(aperm(UCBAdmissions, c(2, 1, 3)),\n  panel = cotab_coindep, shade = TRUE,\n  legend = FALSE,\n  panel_args = list(type = \"assoc\", margins = c(2, 1, 1, 2), varnames = FALSE)\n)\n\n\n\n\n\n\n### Figure 12.20: Mosic plot with highlighting for the punishment data\n\n\nmosaic(~ age + education + memory + attitude,\n  data = punish, keep = FALSE,\n  gp = gpar(fill = grey.colors(2)), spacing = spacing_highlighting,\n  rep = c(attitude = FALSE)\n)\n\n\n\n\n\n\n\nFigure 12.21: Conditional mosaic plot for the punishment data\n\nset.seed(123)\ncotabplot(punish, panel = cotab_coindep, panel_args = list(varnames = FALSE, margins = c(2, 1, 1, 2)))"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html",
    "title": "\n Groups",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBox-Whisker Plots and Violin Plots\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#what-graphs-will-we-see-today",
    "title": "\n Groups",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBox-Whisker Plots and Violin Plots\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#inspiration",
    "title": "\n Groups",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on uh, shades1 of meaning for Jane Austen Gen-Z phrases! Bah."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#how-do-these-charts-work",
    "title": "\n Groups",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable. The internal process of this plot is as follows:\n\nmake groups of the Quant variable for each level of the Qual\nin each group, rank the Quant variable values in increasing order\nCalculate: median, IQR, outliers\n\nplot these as a vertical or horizontal box structure\n\nThe box can also be asymmetric “half plots” if needed…\n\n\n\n\n\n\nNoteHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\n\n\n\n\n\nFigure 2: Box Plot and Density\n\n\nIn the Figure 2, the boxplot is the one on the top. The box part represents the middle 50% of the data, in order of magnitude, and the two halves of the box are defined by the median line.\nThe boxplot in the figure compared with a density plot, which shows a symmetric normal density. Since the latter is symmetric, the median and the mean are identical, as seen by the correspondence with the boxplot in the figure above.\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 3: Box Plot Discussions\n\n\nIn the Figure 3 (a), we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 3 (b)."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#creating-box-plots",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#creating-box-plots",
    "title": "\n Groups",
    "section": "\n Creating Box Plots",
    "text": "Creating Box Plots\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nHere is the BoxPlot Widget description.\nLet us first plot a set of boxplots for the familiar iris dataset and then investigate other datasets using the same Orange workflow.\n Download the BoxPlot Workflow file \n\n\n\n\n\nFigure 4: Iris Box Plot\n\n\nFigure 4 shows the three horizontal box-plots for the chosen Quant variable, one for each level of iris(species). The IQR is also shown for each fo the groups. One can selectively compare either medians or means across these groups of measurements.\n\n\nhttps://youtu.be/Cax0cQ6caI8\n\n\nThere does not seem to be a way of creating Box Plots in DataWrapper ."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#dataset-salaries-in-academia",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#dataset-salaries-in-academia",
    "title": "\n Groups",
    "section": "\n Dataset: Salaries in Academia",
    "text": "Dataset: Salaries in Academia\nLet us examine this dataset in Orange.\n Download the Salaries data \n\n Examine the Data\n\n\n\n\n\nFigure 5: Salaries Data Table\n\n\nFigure 5 states that there are 397 teachers, with 6 variables in the dataset.\n\n\n\n\n\nFigure 6: SalariesData Table\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nsalary: (int) (Annual) Salary!\n\nyrs_service: (int) No. of Years they have served as teachers\n\nyrs_since_phd: (int) No. of Years after their PhD. (sigh)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ndiscipline: (chr) Nature of Expertise\n\nrank: (chr) Nature of Appointment\n\nsex: (chr) Male / Female. Note the imbalance in the counts!!\n\n\n\n\n\n\n\n\n\nNoteQual and Quant…\n\n\n\nCan any of the Quant variables be thought of as Quant variables? When, under what circumstances?\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Box Plots and Violins\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ1. What is the distribution of salary? If we split by sex?\n\n\n\n\n\n\n\n\n\n(a) Salaries Box Plot\n\n\n\n\n\n\n\n\n\n(b) Salaries Box Plot by Sex\n\n\n\n\n\n\nFigure 7: Salaries Data Box Plots\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ2. What is the distribution of salary, when we split by other Qual variables, such as rank?\n\n\n\n\n\nFigure 8: Salaries Box Plot by Sex\n\n\n\n\n\n What is the Story Here?\nSalaries have quite a wide distribution with some very highly paid individuals ( ~ $240K), while the median is still $107K. So some people are paid than 2X the median!\nWhen split by sex, we get two box plots that show the differences between group salaries. The means and medians are quite different between the two groups, an important inference that needs to be completely verified by a statistical t-test.\nWhen split by rank, we get three box plots that show the differences between group salaries, again an important inference that needs to be completely verified by a statistical ANOVA test."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#are-the-differences-significant",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#are-the-differences-significant",
    "title": "\n Groups",
    "section": "\n Are the Differences Significant?",
    "text": "Are the Differences Significant?\n\n\n\n\n\n\nImportantHunches and Hypotheses\n\n\n\nIn data analysis, we always want to know2, as in life, how important things are, whether they matter. To do this, we make up hunches, or more precisely, Hypotheses. We make two in fact:\n\\(H_0\\): Nothing is happening;\\(H_a\\): (“a” for Alternate): Something is happening and it is important enough to pay attention to.\nWe then pretend that \\(H_0\\) is true and ask that our data prove us wrong; if it does, we reject \\(H_0\\) in favour of \\(H_a\\).\nThis is a very important idea of Hypothesis Testing which helps you justify your hunch. Try to do this for the Package Opening and Closing Times.\n\n\n\n\n\n\n\n\nImportantt-test for two categories\n\n\n\nWhen comparing mean salaries vs sex in Figure 7 (b), note the annotation below the graph. This is the result of the t-test:\n\\[ Student's ~ t: 3.198~(p = 0.002, ~ N = 397) \\]\nThis indicates several things:\n\nThat the t-statistic is 3.198;\nIf we assume sex makes no difference to salary, then the probability that this difference could arise merely by chance is low \\(p = 0.002\\);\nAnd of course that there \\(397\\) data points to vouch for this estimate.\n\nThe test states that this difference is statistically significant and could be used to justify further actions based upon it. Look at the references below to get a fascinating history of statistical testing and its origins in …beer.\n\n\n\n\n\n\n\n\nImportantANOVA test for more than 2 levels\n\n\n\nNow observe the boxplots and annotations in Figure 8, where again we compare mean salaries vs rank. This is the result of the ANOVA-test:\n\\[ ANOVA: ~ 128.217~~(p = 0.000, ~ N = 397) \\]\nThis indicates several things:\n\nThat the ANOVA F-statistic is 128.217;\nIf we assume rank makes no difference to salary, then the probability that this difference could arise merely by chance is negligible \\(p = 0.000\\);\nAnd again that there \\(397\\) data points to vouch for this estimate.\n\nThe ANOVA test states that the (multiple) differences are statistically significant and could be used to justify further actions based upon it.\n\n\n\n\n\n\n\n\nNote ANOVA for the Cat-egorically Curious\n\n\n\nFor the intrepid, here is a brief, diagrammed, hand-calculated, and intuitive walk-through of ANOVA. Note that the t-test and ANOVA are identical tests, the former being used for 2-level comparisons of means, and the latter for comparisons of more than 2 means. Again, means, not medians."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#your-turn",
    "title": "\n Groups",
    "section": "\n Your Turn",
    "text": "Your Turn\nHere are a couple of datasets that you might want to analyze with box plots, and even perform t-tests and ANOVA-tests:\n\n\n\n\n\n\nNoteInsurance Data\n\n\n\n Download the Insurance data \n\n\n\n\n\n\n\n\nNotePolitical Donations\n\n\n\n Download the Donations data \n\n\n\n\n\n\n\n\nNoteUFO Encounters\n\n\n\n\n\n UFO Sighting data\n\n\nThe data dictionary for this dataset is here at the TidyTuesday Website.. The TidyTuesday Website is a treasure trove of interesting datasets!\n\n\n\n\n\n\n\n\nNoteGPT-based Language detectors are biased against non-native English writers.\n\n\n\n\n\n AI Dectector data\n\n\nWhat story can you tell, and deduction can you make from Figure 9 below? How would you replicate it? What would you add?\n\n\n\n\n\nFigure 9: AI Detectors"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#wait-but-why",
    "title": "\n Groups",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots are a powerful statistical graphic that give us a combined view of data ranges, quartiles, medians, and outliers.\nBox plots can compare groups within our Quant variable, based on levels of a Qual variable. This is a very common and important task in research! In your design research, you would have numerical Quant data that is accompanied by categorical Qual data pertaining to your target audience. Analyzing for differences in the Quant across levels of the Qual (e.g household expenditure across groups of people) is a vital step in justifying time, effort, and money for further actions in your project. Don’t faff this.\nThey are ideal for visualizing statistical tests for difference in mean values across groups (t-test and ANOVA)."
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#readings",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#readings",
    "title": "\n Groups",
    "section": "\n Readings",
    "text": "Readings\n\nBevans, R. (2023, June 22). An Introduction to t Tests | Definitions, Formula and Examples. Scribbr. https://www.scribbr.com/statistics/t-test/\nBrown, Angus. (2008). The Strange Origins of the t-test. Physiology News | No. 71 | Summer 2008| https://static.physoc.org/app/uploads/2019/03/22194755/71-a.pdf\nStephen T. Ziliak.(2008). Guinnessometrics: The Economic Foundation of “Student’s” t. Journal of Economic Perspectives—Volume 22, Number 4—Fall 2008—Pages 199–216. https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.22.4.199\nhttps://quillette.com/2024/08/03/xy-athletes-in-womens-olympic-boxing-paris-2024-controversy-explained-khelif-yu-ting/\nSenefeld JW, Lambelet Coleman D, Johnson PW, Carter RE, Clayburn AJ, Joyner MJ. Divergence in Timing and Magnitude of Testosterone Levels Between Male and Female Youths. JAMA. 2020;324(1):99–101. doi:10.1001/jama.2020.5655. https://jamanetwork.com/journals/jama/fullarticle/2767852\nDoriane Lambelet Coleman, Sex in Sport, 80 Law and Contemporary Problems 63-126 (2017). Available at: https://scholarship.law.duke.edu/lcp/vol80/iss4/5"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-Groups/index.html#footnotes",
    "href": "content/courses/NoCode/Modules/28-Groups/index.html#footnotes",
    "title": "\n Groups",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe term throwing a shade can be found in Jane Austen’s novel Mansfield Park (1814). Young Edmund Bertram is displeased with a dinner guest’s disparagement of the uncle who took her in: “With such warm feelings and lively spirits it must be difficult to do justice to her affection for Mrs. Crawford, without throwing a shade on the Admiral.”↩︎\n“Ah, Misha, he has a stormy spirit. His mind is in bondage. He is haunted by a great, unsolved doubt. He is one of those who don’t want millions, but an answer to their questions.” ― Fyodor Dostoevsky, The Brothers Karamazov: A Novel in Four Parts With Epilogue↩︎"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html",
    "href": "content/courses/NoCode/Modules/30-Change/index.html",
    "title": "\n Change",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#what-graphs-will-we-see-today",
    "title": "\n Change",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Change",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#inspiration",
    "title": "\n Change",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: ScatterPlot Inspiration http://www.calamitiesofnature.com/archive/?c=559\n\n\nDoes belief in Evolution depend upon the GSP of of the country? Where is the US in all of this? Does the Bible Belt tip the scales here?\nAnd India?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#how-do-these-charts-work",
    "title": "\n Change",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nScatter Plots take two separate Quant variables as inputs. Each of the variables is mapped to a position, or coordinate: one for the X-axis, and the other for the Y-axis, like an ordered pair. Each pair of observations from the two Quant variables ( which would be in one row!) thus gives us a point in the Scatter Plot.\nLooking at these clouds of points gives us an intuitive sense of the relationship between the two Quant variables, how one varies with the other. A cloud that slopes upward to the right indicates a positive relationship between the two; a cloud that slopes down to the right indicates a negative one. An amorphous cloud that does not discernibly slope in either way would lead us to infer that there is little or no relationship between the variables.\n\n\n\n\n\n\nImportantSlope and the Correlation Coefficient are Related\n\n\n\nUnder the assumption of a linear relationship between the two Quant variables, we plot a straight trend line, or regression line through the cloud of points, as a line that best represents that linear relationship. The slope of the regression line is directly linked to the Pearson Correlation Coefficient between the two variables."
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#plotting-a-scatter-plot",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#plotting-a-scatter-plot",
    "title": "\n Change",
    "section": "\n Plotting a Scatter Plot",
    "text": "Plotting a Scatter Plot\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nWe can use the now (overly) familiar iris dataset to plot our first scatter plot. Download the workflow file below:\n Download the Scatter Plot Workflow \n\n\n\n\n\nFigure 2: Iris Scatter Plot\n\n\n\nTry setting shapes and colours, and try plotting a “regression line”. Do you get one line, or several? Why, or why not? How can you switch between the two “methods”?\nTry other pairs of Quant variables in the dataset.\nWhich plot is the most informative? Why?\n\nWe can also add the correlations widget to evaluate correlations between all pairs of numerical/Quant variables. Then keeping that widget open along with the Scatter Plot widget we can visualize the relationship between the plot and the correlation score.\n\nWhen we do this, we might get a setup as shown below.\n\n\n\n\n\nFigure 3: Iris Correlations and Scatter Plot\n\n\nHere we can choose which correlation score we want to visualize in the correlations widget window and see the plot change in the scatter plot window.\nCan you spot Simpson’s Paradox here? More on that further below.\n\n\n\n\n\nhttps://academy.datawrapper.de/article/65-how-to-create-a-scatter-plot"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#what-is-the-story-here",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#what-is-the-story-here",
    "title": "\n Change",
    "section": "What is the Story here?",
    "text": "What is the Story here?\n\nThere are three species of iris flowers and they are “separable” based on combinations of their quantitative measurements.\nSome pairs of Quant variables create Scatter Plots that are quite disjoint and allow easy identification of the species variable.\nIn a ML model for this dataset, the species variable is most likely to be the target variable while the rest are predictors."
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#dataset-cancer",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#dataset-cancer",
    "title": "\n Change",
    "section": "\n Dataset: Cancer",
    "text": "Dataset: Cancer\nLet us examine a fairly complex dataset pertaining to cancer, and analyze that with scatter plots.\n Download the Cancer dataset \nWe can use the same Workflow as before.\n\n Examine the Data\n\n\n\n\n\nFigure 4: Cancer Data Table\n\n\nFrom Figure 4, we see that there is one Qual column Diagnosis, and all the remaining 31 columns seem to be some Quant measurements of a total of 569 tumours. (Not all columns are visible)\n\n\n\n\n\nFigure 5: Cancer Data Summary Table #1\n\n\nFigure 5 gives is histograms and statistics of all the 32 columns. Most histograms seem roughly symmetric, but a detailed look must be taken.\n\n\n\n\n\nFigure 6: Cancer Data Summary Table #2\n\n\nIn Figure 6, we see that there is some imbalance between the counts for the one Qual variable, Diagnosis.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\n“Id”\n\n\n\n“Radius (mean)”\n“Texture (mean)”\n\n\n“Perimeter (mean)”\n“Area (mean)”\n\n\n“Smoothness (mean)”\n“Compactness (mean)”\n\n\n“Concavity (mean)”\n“Concave points (mean)”\n\n\n“Symmetry (mean)”\n“Fractal dimension (mean)”\n\n\n“Radius (se)”\n“Texture (se)”\n\n\n“Perimeter (se)”\n“Area (se)”\n\n\n“Smoothness (se)”\n“Compactness (se)”\n\n\n“Concavity (se)”\n“Concave points (se)”\n\n\n“Symmetry (se)”\n“Fractal dimension (se)”\n\n\n“Radius (worst)”\n“Texture (worst)”\n\n\n“Perimeter (worst)”\n“Area (worst)”\n\n\n“Smoothness (worst)”\n“Compactness (worst)”\n\n\n“Concavity (worst)”\n“Concave points (worst)”\n\n\n“Symmetry (worst)”\n“Fractal dimension (worst)”\n\n\n\n\n\nMany of the Quant variables seem to be mean measurements, with the mean presumably taken over several “sites” within the same tumour.\nAlong with the mean, there are also measurements of se or standard error which is, roughly speaking, a measure of the standard deviation of the multiple measurements made. So for instance, Area(mean) and Area(se) are pairs of measurements created using multiple “sites” or “cross-sections” on one tumour.\nSome other variables are labelled as worst, which may be either the max or min of such a set of “multi-site” tumour measurements.\n\n\n\n\n\n\n\nImportantMay the (data) Source be with you\n\n\n\nIt is important to note that these are (educated?) guesses; one is best off connecting with the person/agency that provided the data for a precise understanding of variables. This will prevent nonsensical plots/models and inferences from showing up in your work.\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nDiagnosis: (text) (B)enign, or (M)alignant\n\n\n\nDo use the joint view of correlation score and scatter plot to answer these, and possibly other Research Questions.\n\n Research Questions\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ1. Are the mean and se observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\n\n(a) Cancer Scatter Plot #1\n\n\n\n\n\n\n\n\n\n(b) Cancer Scatter Plot #2\n\n\n\n\n\n\n\n\n\n\n\n(c) Cancer Scatter Plot #3\n\n\n\n\n\n\n\n\n\n(d) Cancer Scatter Plot #4\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ2. Are the mean and worst observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\nFigure 8: Cancer Scatter Plot #5\n\n\n\n\n\n\n\n\n\nFigure 9: Cancer Scatter Plot #6\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Cancer Scatter Plot #7\n\n\n\n\n\n\n\n\n\nFigure 11: Cancer Scatter Plot #8\n\n\n\n\n\n\n\n\n What is the Story Here?\nFrom Figure 7 (a), we see that the area(mean) and area(se) are somewhat correlated; moreover the correlation is slightly higher for the malignant tumours ( red dots, appropriately…). This trend shows up also for radius in Figure 7 (b), and for fractaldimension in Figure 7 (d). However, for smoothness, we see much lower correlation {#fig-cancer-smoothness-mean-se}.\nFor the mean vs worst scatter plots, we see decent correlations all around, with each of the graphs showing clouds tilted upward to the right.\n\n\n\n\n\n\nImportantSimpson’s Paradox\n\n\n\nTry to remove colours and then plot a regression line. This usually gives a more clear idea of the correlation, without running into problems such as the Simpson’s Paradox:\n\n\n\n\n\nFigure 12: Simpson’s Paradox\n\n\nAnd see also this:"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#a-variant",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#a-variant",
    "title": "\n Change",
    "section": "\n A Variant",
    "text": "A Variant"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#your-turn",
    "title": "\n Change",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry to play this online Correlation Game.\n\n\n\n\n\n\n\nNote2. School Expenditure and Grades.\n\n\n\n Download the School Data \n\n\n\n\n\n\n\n\nNote3. Gas Prices and Consumption\n\n\n\nAs described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n Download the Gas Consumption Data \n\n\n\n\n\n\n\n\nNote4. Horror Movies (Bah.You awful people..)\n\n\n\n Download the Horror Movie Data \n\n\n\n\n\n\n\n\nNote6. Food Delivery Times\n\n\n\n Download the Food Delivery Data"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#wait-but-why",
    "title": "\n Change",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\n\n\n\n\n\n\n\nImportant\n\n\n\nTarget variables are usually plotted on the Y-axis, while Predictor variables are on the X-Axis, in a Scatter Plot. Why? Because \\(y = mx + c\\) !\n\n\n\nCorrelation scores are good indicators of things that are, well, related. While one variable may not necessarily cause another, a good correlation score may indicate how to chose a good predictor.\nAlways, always, plot and test your data! Both numerical summaries and graphical summaries are necessary! See below!!\n\n\n\n\n\n\n\nWarningAnd How about these datasets?\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\naway\n54.26610\n47.83472\n16.76983\n26.93974\n-0.0641284\n\n\nbullseye\n54.26873\n47.83082\n16.76924\n26.93573\n-0.0685864\n\n\ncircle\n54.26732\n47.83772\n16.76001\n26.93004\n-0.0683434\n\n\ndino\n54.26327\n47.83225\n16.76514\n26.93540\n-0.0644719\n\n\ndots\n54.26030\n47.83983\n16.76774\n26.93019\n-0.0603414\n\n\nh_lines\n54.26144\n47.83025\n16.76590\n26.93988\n-0.0617148\n\n\nhigh_lines\n54.26881\n47.83545\n16.76670\n26.94000\n-0.0685042\n\n\nslant_down\n54.26785\n47.83590\n16.76676\n26.93610\n-0.0689797\n\n\nslant_up\n54.26588\n47.83150\n16.76885\n26.93861\n-0.0686092\n\n\nstar\n54.26734\n47.83955\n16.76896\n26.93027\n-0.0629611\n\n\nv_lines\n54.26993\n47.83699\n16.76996\n26.93768\n-0.0694456\n\n\nwide_lines\n54.26692\n47.83160\n16.77000\n26.93790\n-0.0665752\n\n\nx_shape\n54.26015\n47.83972\n16.76996\n26.93000\n-0.0655833\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot that cute T-Rex in Orange, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)"
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-Change/index.html#readings",
    "href": "content/courses/NoCode/Modules/30-Change/index.html#readings",
    "title": "\n Change",
    "section": "\n Readings",
    "text": "Readings\n\nRohrer JM. Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science. 2018;1(1):27-42. https://doi.org/10.1177/2515245917745629 PDF\nCase Study on Horror Movies. (Arvind: Bah.) https://notawfulandboring.blogspot.com/2024/04/using-pulse-rates-to-determine-scariest.html\nThe Datasaurus Package: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\nA superb web-scrolly on Sustainable Development Goals (SDGs)s! Go and see! https://datatopics.worldbank.org/sdgatlas/goal-1-no-poverty?lang=en\nHunter, W. G. (1981). Six Statistical Tales. The Statistician, 30(2), 107. doi:10.2307/2987563. https://sci-hub.ru/10.2307/2987563\nA cartoon+interactive explanation of Simpson’s Paradox. Real fun! https://pwacker.com/simpson.html\nFutility Closet blog. (December 12, 2014). English by Degrees. https://www.futilitycloset.com/2014/12/12/english-by-degrees/ A short article that seems to speak of LLMs/ChatGPT…in 1948!!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html",
    "title": "\n Networks",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQuant (optional)\nNetwork Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#what-graphs-will-we-see-today",
    "title": "\n Networks",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQuant (optional)\nNetwork Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Networks",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#inspiration",
    "title": "\n Networks",
    "section": "\n Inspiration",
    "text": "Inspiration"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#how-do-these-charts-work",
    "title": "\n Networks",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#plotting-a-network-chart",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#plotting-a-network-chart",
    "title": "\n Networks",
    "section": "\n Plotting a Network Chart",
    "text": "Plotting a Network Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#dataset-the-game-of-thrones",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#dataset-the-game-of-thrones",
    "title": "\n Networks",
    "section": "\n Dataset: The Game of Thrones",
    "text": "Dataset: The Game of Thrones\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\nDownload this data to your machine and use it in Orange.\n\n Examine the Data\n\n\n\n\n\nFigure 1: Banned Books Data Table\n\n\n\n\n\n\n\nFigure 2: Banned Books Data Summary\n\n\nFigure 1 states that we have 1586 rows, 7 columns. So 1586 banned books are on this list! 🙀 🙀 🙀\nThe Figure 2 already has a thumbnail-like bar chart. We will still make a “proper” one with the appropriate widget.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2.\n\n\n\n What is the Story Here?\n\nAnd what, Californians are too busy making money to care about book-banning!!! The state does not even show up in the chart! 😀"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#your-turn",
    "title": "\n Networks",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nAiRbnb Price Data on the French Riviera:\n\n\n\n AiRbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n\nFertility: This rather large and interesting Fertility related dataset from https://vincentarelbundock.github.io/Rdatasets/csv/AER/Fertility.csv"
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#wait-but-why",
    "title": "\n Networks",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nNetworks show up in a very diverse set of domains: society, epidemiology, trade, logistics, transportation, innovation, ecology, geopolitics, information technology, cyber***…So it’s worthwhile to have a good grounding in networks.\n\nIn SMI, we wave hands and fatuously say “Everything’s connected”! Well, this module will enable you to go beyond tawk and do a show instead. \nComplex relationships between entities are best represented with a graph. \nThere are also many graph layouts possible for the same dataset. Go see the pictures at David Schoch’s website. This can enable a very different view and insight about these relationships."
  },
  {
    "objectID": "content/courses/NoCode/Modules/100-Networks/index.html#references",
    "href": "content/courses/NoCode/Modules/100-Networks/index.html#references",
    "title": "\n Networks",
    "section": "\n References",
    "text": "References\n\nNetwork Repository. An Interactive Scientific Network Data Repository: the first scientific network data repository with interactive visual analytics. https://networkrepository.com &gt; A great source of network data from various domains!\nhttps://bookdown.org/markhoff/social_network_analysis/\nEpiModel: Mathematical Modeling of Infectious Disease Dynamics. https://www.epimodel.org\nHua Wang, & Wellman, B. (2010). Social Connectivity in America: Changes in Adult Friendship Network Size From 2002 to 2007. American Behavioral Scientist, 53(8), 1148–1169.\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians"
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html",
    "title": "\n Data",
    "section": "",
    "text": "(a) Composition VIII\n\n\n\n\n\n\n\n\n\n(b) Blue\n\n\n\n\n\n\nFigure 1: Kandinsky: Abstract Paintings, or Data Visualizations?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-where-data",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-where-data",
    "title": "\n Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of human curiosity and then of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-visualize",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-visualize",
    "title": "\n Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts and mere statements: for example:\n\n\n\n\n\n\nFigure 2: Rape Capital\n\n\n\n\n\n\n\nFigure 3: Data Reveals Crime"
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-analyze",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-analyze",
    "title": "\n Data",
    "section": "\n Why Analyze?",
    "text": "Why Analyze?\nFor us to draw good inferences and make actionable plans based on data, it has to satisfy specific statistical tests. These tests tell us how significant the data are and whether the data are telling us a story that can be justified, or it is just a random coincidence. Depending upon our Research Question, different statistical tests will be applicable."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-are-data-types",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-are-data-types",
    "title": "\n Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Variable Types\n\n\n\n\n\n\n\nFigure 5: Tidy Data\n\n\n\n\n\n\n\n\nImportantTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-types",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-types",
    "title": "\n Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions! Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\nWhat Human Purposes?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\nNA\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\nNA\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\nNA\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\nNA\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\n\n\n\nFigure 6: Type of Variables\n\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#some-examples-of-data-variables",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#some-examples-of-data-variables",
    "title": "\n Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\n\n\n\n\n\n\nNoteExample 1: AllCountries dataset\n\n\n\n\n\n\n  \n\n\n\nQuestions\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.2 The answer is \\(70.5\\%\\). Q2. What would be the units for LifeExpectancy? A2. Decimal Years.\n\n\n\n\n\n\n\n\n\nNoteExample 2:StudentSurveys\n\n\n\n\n\n\n  \n\n\n\nQuestions\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#questions-1",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#questions-1",
    "title": "\n Data",
    "section": "Questions",
    "text": "Questions\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-is-a-data-visualization",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-is-a-data-visualization",
    "title": "\n Data",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data + Geometry\n\n Shapes\nData Visualization is the act of “mapping” a geometric aspect/aesthetic to a variable in data. The aesthetic then varies in accordance with the data variable, creating (part of) a chart.\nWhat might be the geometric aesthetics available to us? An aesthetic is a geometric property, such as x-coordinate, y-coordinate, length/breadth/height,radius,shape,size, linewidth, linetype, and even colour…\n\n\n\n\n\nFigure 7: Common Geometric Aesthetics in Charts\n\n\n\n Mapping\nWhat does this “mapping” mean? That the geometric aesthetics are used to represent qualitative or quantitative variables from your data, by varying in accordance to the data variable.\nFor instance, length or height of a bar can be made proportional to theage or income of a person. Colour of points can be mapped to gender, with a unique colour for each gender. Position along an axis x can vary in accordance with a height variable and position along the y axis can vary with a bodyWeight variable.\n\n\n\n\n\n\n\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle,pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nNoteA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and [kind of variable] Section 5, Quantitative or Qualitative. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered a z-axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-viz",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-viz",
    "title": "\n Data",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram, Choropleth and Symbol Maps\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram"
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#conclusion",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#conclusion",
    "title": "\n Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nSo there we have it:\n\nQuestions lead to Types of Variables (Quant and Qual)\n\nFurther Questions lead to relationships between them, which we describe using Data Visualizations\n\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. So creating a chart is like stacking up a set of metaphors."
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#references",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#references",
    "title": "\n Data",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n\n\nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\nhttps://safetyculture.com/topics/design-of-experiments/\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\nTim C. Hesterberg (2015). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician, 69:4, 371-386, DOI:10.1080/00031305.2015.1089789. PDF here"
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#footnotes",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#footnotes",
    "title": "\n Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html",
    "title": "\n Rhythm",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nLine Plot"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-graphs-will-we-see-today",
    "title": "\n Rhythm",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nLine Plot"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Rhythm",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#inspiration",
    "title": "\n Rhythm",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Line Plot Inspiration https://fivethirtyeight.com/features/katie-ledecky-is-the-present-and-the-future-of-swimming/\n\n\n\n\n\n\n\n\n\n(b) Time Series Plot Inspiration\n\n\n\n\n\n\nFigure 1: Line Plots\n\n\nEk Ledecky bheegi-bhaagi si, is it? Yeh Ledecky hai, ya jal-pari?\nIn Figure 2 (a), the black line is the average of the 50 best times at each distance since 2000. The top 200 times for each distance since 2000 are also plotted, with light orange lines each representing one swimmer. Her races and her career essentially follow the same pattern — the more she swims, the more she separates from the field. Her 1500 metres record timing is better than the best time for 800m!!😱 (Update July 2024: Ledecky won the bronze at Paris 2024)\nAnd LA? The weather in California…ahh. But Seattle has more variation, and sudden changes too!\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends? And more in September?\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Births over Days of the Week\n\n\n\n\n\n\n\n\n\n(b) Births over Months of the Year\n\n\n\n\n\n\nFigure 2: Solomon Grundy, born on Monday… was an Accountant?\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\nAnd why more births in September? That should be a no-brainer!! 😹"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#how-do-these-charts-work",
    "title": "\n Rhythm",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nLine Plots take two separate Quant variables as inputs. Each of the variables is mapped to a position, or coordinate: one for the X-axis, and the other for the Y-axis. Each pair of observations from the two Quant variables ( which would be in one row!) give us a point. All this much is identical with the Scatter Plot.\nAnd here, the points are connected together and sometimes thrown away altogether, leaving just the line.\nLooking at the lines, we get a very function-al sense of the variation: is it upward or downward? Is it linear or nonlinear? Is it periodic or seasonal…all these questions can be answered with Line Charts.\n\n\n\n\n\n\nImportantWhen one variable is Time?\n\n\n\nLine charts often have one variable as a time variable. In such case the data is said to be a time series.\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#plotting-a-line-plot",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#plotting-a-line-plot",
    "title": "\n Rhythm",
    "section": "\n Plotting a Line Plot",
    "text": "Plotting a Line Plot\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us at least look at this data in Orange, and import it into this rather elaborate Orange Workflow:\n\n\n\n Download the Orange Time Series Workflow \n\n\n Download the RIAA Music Revenue data \n\n\n\nImport this into Orange and see…\n\n\n\n\n\nFigure 3: RIAA Revenues Data Table\n\n\nFigure 3 states that there are 432 data points, with 7 variables in the dataset; some missing data.\nFor now, the variables we need are :\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nYear: (int) Year in which RIAA revenue was logged\n\nValue (For Charting): (int) Revenue in million USD We can ignore the rest for now, unless we plan to work more with this data, and need to know more. The other numerical data showing billions of USD are not easily decipherable, an example of data that is not documented well…\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nCategory: (chr) Form of the Music released ( CD etc..)\n\n\n\nWe need to first form time series from the dataset: we will choose the year-date variable, and indicate that it starts on Jan 1, 1973:\n\n\n\n\n\n\n\n\n\n(a) Forming Time Series\n\n\n\n\n\n\n\n\n\n(b) Plotting Time Series\n\n\n\n\n\n\nFigure 4: Forming and Plotting Time Series in Orange\n\n\n\n\n\n\n\n\n Download the RIAA Music Revenue data \n\n\n Download the RAWGraphs Line Chart Tutorial File \n\n\n\nUpload this RAWgraphs project tutorial file into https://app.rawgraphs.io/ and play! Here is something we can create there:\n\n\n\nhttps://academy.datawrapper.de/article/23-how-to-create-a-line-chart\nHere be dragons: DataWrapper wants the data in wide format: each Format of music needs to have its figures in a separate column! 🤦. And this is not a data transformation that we can achieve within DataWrapper. Bah.\nWe are probably better off plotting a regular scatter plot. Here too there seem to be limitations because we are not able to colour the series based on type of music Format.\nThe Shape of You Data\nNever mind that silly song now.\nAs mentioned above, data can be in wide or long form. How does one imagine this shape-shifting that seems needed now and then? Let’s see.\n\n\n\n\n\n\nImportantLong Form and Wide Form Data\n\n\n\nSeveral tools such as DataWrapper (and others, yes, I agree, even with code, as we will see) need data transformed to a specific shape. this is usually mandated by the “shape or geometry” we intend to use in the visualization. We should now look at this idea of shape in data. Consider the data tables below:\n\n\n\n\n\nProduct\nPower\nCost\nHarmony\nStyle\nSize\nManufacturability\nDurability\nUniversality\n\n\n\nG1\n0.5858003\n0.2773750\n0.7244059\n0.0731445\n0.1000535\n0.4551024\n0.9622046\n0.9966129\n\n\nG2\n0.0089458\n0.8135742\n0.9060922\n0.7546750\n0.9540688\n0.9710557\n0.7617024\n0.5062709\n\n\nG3\n0.2937396\n0.2604278\n0.9490402\n0.2860006\n0.4156071\n0.5839880\n0.7145085\n0.4899432\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\nProduct\nParameter\nRating\n\n\n\nG1\nPower\n0.5858003\n\n\nG1\nCost\n0.2773750\n\n\nG1\nHarmony\n0.7244059\n\n\nG1\nStyle\n0.0731445\n\n\nG1\nSize\n0.1000535\n\n\nG1\nManufacturability\n0.4551024\n\n\nG1\nDurability\n0.9622046\n\n\nG1\nUniversality\n0.9966129\n\n\nG2\nPower\n0.0089458\n\n\nG2\nCost\n0.8135742\n\n\nG2\nHarmony\n0.9060922\n\n\nG2\nStyle\n0.7546750\n\n\nG2\nSize\n0.9540688\n\n\nG2\nManufacturability\n0.9710557\n\n\nG2\nDurability\n0.7617024\n\n\nG2\nUniversality\n0.5062709\n\n\nG3\nPower\n0.2937396\n\n\nG3\nCost\n0.2604278\n\n\nG3\nHarmony\n0.9490402\n\n\nG3\nStyle\n0.2860006\n\n\nG3\nSize\n0.4156071\n\n\nG3\nManufacturability\n0.5839880\n\n\nG3\nDurability\n0.7145085\n\n\nG3\nUniversality\n0.4899432\n\n\n\n\n\nFigure 6\n\n\n\nWhat we have done is:\n\nconvert all the variable names into a stacked column Parameter\n\nPut all the numbers into another column Rating\n\nRepeated the Product column values as many times as needed to cover all Parameters (8 times).\n\n\nSee the gif below to get an idea of how this transformation can be worked reversibly. (Yeah, never mind the code also.)\n\n\n\n\n\nFigure 7: Data Pivoting\n\n\n Download the RIAA Music Revenue data \n\n\nSo how can we actually do this? Two Ways.\nTurns out there are some nice people at U. San Diego who have built an R-oriented app called Radiant for Business Analytics that can do this pretty much click-and-point style, though it is nowhere as much fun as Orange. Head off there:\nhttps://vnijs.shinyapps.io/radiant\nWe upload our original data, pivot it, and download the pivotted data. Now the pivotted wide-form data should work in DataWrapper.\nAnd RAWgraphs also has a stack on column option that does pretty much the same thing. See here: https://www.rawgraphs.io/learning/how-to-stack-your-unstacked-data-or-meet-the-unpivoter\nWhatever, peasants."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-is-the-story-here",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#what-is-the-story-here",
    "title": "\n Rhythm",
    "section": "What is the Story here?",
    "text": "What is the Story here?\n\nOver the years different music formats have had their place in the sun\nAll physical forms are on the wane; streaming music is the current mode of music consumption."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#dataset-weather-at-new-york-city-airports",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#dataset-weather-at-new-york-city-airports",
    "title": "\n Rhythm",
    "section": "\n Dataset: Weather at New York City Airports",
    "text": "Dataset: Weather at New York City Airports\nTo get an idea of seasons, trends and to try our hand at time-series forecasting, let us look at a data set pertaining to the weather at New York city airports.\n\n\n\n Download the Weather data \n\n\n Download the Weather Workflow \n\n\n\n\n Examine the Data\nIncluded below is a PDF report from Orange, summarizing the data, generated from the Feature Summary widget::\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\nFigure 8: Weather Data Table\n\n\nWe should take the first column time_hour and see if we can use that as our time variable. All the weather related numerical data columns are individual time series which we can plot and analyse.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\ntime_hour(num): Numeric date-time variable. Does Orange spot this?\n\nyear(num): Just 2013.\n\nmonth, day, hour(num): components of the exact time of measurement of weather parameters\n\nhumid,temp,wind_dir, wind_speed, wind_gust, precip, pressure, visib (num): all numeric weather parameters\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\norigin (text): airport (JKF/EWR/LGR)\n\n\n\nLet us build an Orange workflow step-by-step for this dataset and its Research Questions.\n\n Research Questions\nThere are a lot of parameters to play with and investigate here.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ1. What is Temperature temp over time at each of three airports? \nThis is a Scatter Plot of course.\nIt seems the Line Chart widget in Orange cannot colour individual time series by colour using another Qualitative variable. 😢. Is there a better way? (You know the answer.)\nAlso note the utter busy-ness of this chart. This is a chart of 26K points, well beyond what we can digest at one time. We need to summarize/average etc.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ2. In the US, there is a lot of talk of “wind chill factor”. So, is there some graphical evidence of windchill? (temp and wind_speed)?\n\n\n\n\n\nFigure 9: NYC Airport Temperatures over Time\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ3. How do averaged plots look like, for temp, humid, and dewpoint?\nWe can use the Moving Transform widget in Orange to calculate monthly averages for these quantities, after converting the data into a time series.\n\n\n\n\n\nFigure 10: Time Averaged Weather Plots-3\n\n\n\n\n\n What is the Story Here?\n\nThere is a strong natural seasonal trend over the period of one year in the temperature at all three airports\nIf we plot temperature against windspeed, we see a fair negative slope/correlation, as we would expect.\nHumidity is high most times, except during some very dry winter months?\n\n\n\n\n\n\n\nNote\n\n\n\nDid you notice the serious outlier in the temp vs windspeed graph? Try to remove the Select Rows widget and see if you can spot it. Do you understand why that egregious reading had to be be filtered?\nSuch readings are called outliers."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#dataset-born-in-the-usa",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#dataset-born-in-the-usa",
    "title": "\n Rhythm",
    "section": "\n Dataset: Born in the USA",
    "text": "Dataset: Born in the USA\n\nTourist: Any famous people born around here?\nGuide: No sir, best we can do is babies.\n\nThe Time Series Line Chart widget in Orange is described here. https://orangedatamining.com/widget-catalog/time-series/line_chart/\nLet us take some Births related data and plot it in Orange.\n Download the US Births data \nAnd download the Line Chart workflow file for this data:\n Download the Time Series Line Chart Workflow \nNote how we have two widgets for the Line Charts. More shortly.\n\n Examine the Data\n\n\n\n\n\nFigure 11: Born in the USA\n\n\n\n\n\n\n\nFigure 12: Births Summary Table\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nyear, month, date_of_month: (int) Columns giving time information\n\nday-of_week: (int) Additional Time information\n\nbirths: (int) Total live births across the USA that day\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\nNone. Though we might covert day_of_week and month into Qual variables later.\n\n\nEvenly spread year, month, date_of_month and day_of_week variables…the bumps are curious though, no? day_of_week is of course neat. births are numerical data and have a good spread with a bimodal distribution distribution. Some numbers in the mid-range hardly occur at all… So a premonition of some two-valued phenomenon here already.\n\n Research Questions\n\n\n\n\n\n\nNoteQ1. What does the births data look like over the years?\n\n\n\n\n\n\n\n\nFigure 13: Births over the Years\n\n\nHmmm…very busy graph. The overall trend is a slight bump in births around 2007 and then a slow reduction in births. Large variations otherwise, which we need to see in finer detail on a magnified scale, a folded scale, or by averaging.\nConverting month or day_of_week to categorical in the File Menu does not provide us with a way of separating the time series by month or weekday…sad.. We will be able to average over month, day_of_week to see what happens.\n\n\n\n\n\n\n\n\nNoteQ2. What do births look like averaged over month?\n\n\n\n\nThis is good! We have converted the dataset to a timeseries, of course, and then added a moving transform widget, that allows us to take averages of births over weeks, months, or years. Play with this setting in the moving transform widget.\nWe see that averaging i.e. \\(aggregating\\) by Month of year clearly shows September as the month for the most number of births.\n\n\n\n\n\n\n\n\nNoteQ3. What do births look like averaged over day_of_week?\n\n\n\nHere too with the moving transform widget, choosing Day of Week as the aggregating parameter, we see a dip in births over weekends. Try!!\n\n\n\n\n\n\n\n\nImportantFolded Scale?\n\n\n\nLook at the figure below.\n\n\n\n\n\nFigure 14: Aggregate over Week of the Year\n\n\nIt should be apparent that the line chart shows averages based on “Week of Year”. What does that mean?\nImagine a carpenter’s folding footruler: \nImagine the entire time series stretched out and then folded over itself at intervals of a week. There will of course be overlapping data that represent data points for the same week year after year. THAT is what goes into the averaging!\nSo we see that the weeks in September show the highest average birth numbers, which seems right!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#other-plots",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#other-plots",
    "title": "\n Rhythm",
    "section": "\n Other Plots",
    "text": "Other Plots\nImagine that we follow this overlap routine and get the data by same-week-of-year, as before. We need not necessarily average that data; we can simply plot each (repeated) week’s worth of data as a box plot. This results in an array of boxplots, one per week, and is called a candlestick plot. Clearly we can do this for months, weeks, and even days of the week. Here is what it looks like; it does not seem possible to create these with any of the tools we are currently using.\n\n\n\n\n\nFigure 15\n\n\nAs before, the medians are the black lines across each boxplot, which is one for each month. Note that since the medians are towards the upper end of the boxplots, we can guess that the per-month distribution must be skewed to the left (lower than median values are less frequent).\nIf the Quantities that vary over time are not continuous but discrete values such as high, medium, and low,, a time-series heatmap is also a possibility.\n\n\n\n\n\nFigure 16\n\n\nVery arbitrarily slicing the birth numbers into three bins titled high, fine, and low, we can plot a heatmap like this. Orange does have a heatmap widget, however it seems suited to Machine Learning methods such as Clustering."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#your-turn",
    "title": "\n Rhythm",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteValentine’s Day Spending by Age\n\n\n\nA regular line plot, not a time series.\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv\n\n\n\n\n\n\n\n\nNoteWilliam Farr’s Data on Cholera in London, 1849\n\n\n\nhttps://vincentarelbundock.github.io/Rdatasets/csv/HistData/Cholera.csv\n\n\n\n\n\n\n\n\nNoteArctic and Antarctic Sea Ice coverage over time.\n\n\n\nIs global warming affecting ice coverage at the poles?\n Download the Sea Ice Data \nThis data is in wide form, and you may have to massage it into long form before pulling it into Orange!\n\n\n\n\n\n\n\n\nNotePurple Air\n\n\n\nIn the Air Tonight: Head over to Purple Rain Purple Air and download air quality data from community based air quality sensors. Plot these as time series, and try getting historical data, or data on festivals or important occasions in specific cities."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#wait-but-why",
    "title": "\n Rhythm",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nLine Charts show up functional relationships or overall trends in the data.\nThey can be made less cluttered than the corresponding scatter plots, especially with averaging.\nSeasonal cycles can also be spotted very easily.\nThe X-axis need not necessarily be time: it can often be other (independent) variables, and the Y-axis plots the target/dependent variable.\nHowever, we do encounter many things that vary over time: weather, wealth, No. of users or downloads of an app, hits to a webpage, customers at a supermarket, or population of animals or plants in a region.\nThese are best represented by Line Charts\nAs humans, we are also deeply interested in patterns of recurrence over time, and in forecasting for the future, using tech, and using say Oracles.\nBoth these purposes are amply served by Line Charts."
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-Rhythm/index.html#references",
    "href": "content/courses/NoCode/Modules/32-Rhythm/index.html#references",
    "title": "\n Rhythm",
    "section": "\n References",
    "text": "References\n\nCharles Chambliss (1989). The Mundanity of Excellence: An ethnographical report on Stratification and Olympic Swimmers.\n\nNijs V (2023). radiant: Business Analytics using R and Shiny. R package version 1.6.0, https://github.com/radiant-rstats/radiant.\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition).available online\n\n\nTime Series Analysis at Our Coding Club\n\n\nThe Nuclear Threat—The Shadow Peace, part 1\n\n\n11 Ways to Visualize Changes Over Time – A Guide"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#introduction",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#introduction",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#references",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/\nhttps://www.datacamp.com/tutorial/hierarchical-clustering-R\nhttps://www.datacamp.com/tutorial/k-means-clustering-r\nMichele Coscia. 2019. Who will Cluster the Cluster Makers? https://www.michelecoscia.com/?p=1709 Accessed 12 Jan 2024."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(prettydoc)\nlibrary(corrplot)\nlibrary(ggformula)\nlibrary(palmerpenguins) # Allison Horst's `penguins` data.\n##\nlibrary(tidymodels)\nlibrary(dials)\nlibrary(modeldata)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(parsnip)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex     \n Min.   :172.0     Min.   :2700   female:165  \n 1st Qu.:190.0     1st Qu.:3550   male  :168  \n Median :197.0     Median :4050   NA's  : 11  \n Mean   :200.9     Mean   :4202               \n 3rd Qu.:213.0     3rd Qu.:4750               \n Max.   :231.0     Max.   :6300               \n NA's   :2         NA's   :2                  \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` (14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor()\ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\", tl.cex = 1.0, )\n\n\n\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negatively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n\n  \n\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;%\n  recipe(species ~ .) %&gt;%\n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;% # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;- penguin_train %&gt;%\n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;- penguin_test %&gt;%\n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;-\n  penguin_model %&gt;%\n  fit(species ~ ., penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 1.01%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        85         0      0  0.00000000\nChinstrap      2        41      0  0.04651163\nGentoo         0         0     71  0.00000000\n\n# iris_ranger &lt;-\n#   rand_forest(trees = 100) %&gt;%\n#   set_mode(\"classification\") %&gt;%\n#   set_engine(\"ranger\") %&gt;%\n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%\n  dplyr::bind_cols(penguin_test_baked) %&gt;%\n  glimpse()\n\nRows: 134\nColumns: 8\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.8581235, -0.9312674, -0.5…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 1.74440040, 0.32252879, 0.22…\n$ flipper_length_mm &lt;dbl&gt; -1.42460769, -1.06786655, -0.78247365, -1.42460769, …\n$ body_mass_g       &lt;dbl&gt; -0.5676206, -0.5055254, -0.6918109, -0.7228585, -1.2…\n$ sex               &lt;fct&gt; male, female, male, female, female, male, male, fema…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%\n  dplyr::bind_cols(penguin_test_baked) %&gt;%\n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs &lt;-\n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 10\n$ .pred_Adelie      &lt;dbl&gt; 1.00, 0.99, 0.99, 1.00, 1.00, 0.99, 1.00, 0.98, 1.00…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.00, 0.01, 0.01, 0.00, 0.00, 0.01, 0.00, 0.02, 0.00…\n$ .pred_Gentoo      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.8581235, -0.9312674, -0.5…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 1.74440040, 0.32252879, 0.22…\n$ flipper_length_mm &lt;dbl&gt; -1.42460769, -1.06786655, -0.78247365, -1.42460769, …\n$ body_mass_g       &lt;dbl&gt; -0.5676206, -0.5055254, -0.6918109, -0.7228585, -1.2…\n$ sex               &lt;fct&gt; male, female, male, female, female, male, male, fema…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %&gt;%\n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n# ROC Plot\npenguin_fit_probs %&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n\n  \n\n\npenguin_recipe %&gt;% broom::tidy()\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n# penguin_model %&gt;% tidy()\n# penguin_fit %&gt;% tidy()\npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n# set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;%\n  training() %&gt;%\n  glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 6.7, 4.6, 6.0, 4.6, 6.0, 7.2, 5.5, 6.3, 4.9, 6.0, 5.7, 6.…\n$ Sepal.Width  &lt;dbl&gt; 3.0, 3.6, 2.2, 3.1, 2.7, 3.2, 2.3, 3.3, 3.1, 3.4, 2.8, 3.…\n$ Petal.Length &lt;dbl&gt; 5.2, 1.0, 5.0, 1.5, 5.1, 6.0, 4.0, 4.7, 1.5, 4.5, 4.1, 4.…\n$ Petal.Width  &lt;dbl&gt; 2.3, 0.2, 1.5, 0.2, 1.6, 1.8, 1.3, 1.6, 0.1, 1.6, 1.3, 1.…\n$ Species      &lt;fct&gt; virginica, setosa, virginica, setosa, versicolor, virgini…\n\niris_split %&gt;%\n  testing() %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 4.9, 5.4, 4.6, 5.0, 4.4, 4.8, 5.7, 5.1, 5.7, 5.1, 5.4, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.0, 3.9, 3.4, 3.4, 2.9, 3.0, 4.4, 3.5, 3.8, 3.8, 3.4, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.7, 1.4, 1.5, 1.4, 1.4, 1.5, 1.4, 1.7, 1.5, 1.7, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.4, 0.3, 0.3, 0.3, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;-\n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .) # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_center(all_predictors(), -all_outcomes()) %&gt;%\n  step_scale(all_predictors(), -all_outcomes()) %&gt;%\n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;-\n  iris_split %&gt;%\n  training() %&gt;%\n  bake(iris_recipe, .)\niris_training_baked\n\n\n  \n\n\niris_testing_baked &lt;-\n  iris_split %&gt;%\n  testing() %&gt;%\n  bake(iris_recipe, .)\niris_testing_baked\n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest, quietly = TRUE)\niris_rf &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -1.18429954, -0.57592649, -1.54932338, -1.06262493, -1.79…\n$ Sepal.Width  &lt;dbl&gt; -0.1010677, 1.9980297, 0.8318645, 0.8318645, -0.3343007, …\n$ Petal.Width  &lt;dbl&gt; -1.3448302, -1.0814250, -1.2131276, -1.3448302, -1.344830…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;-\n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.922996032, 0.981138889, 0.991000000, 0.980285714, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.072837302, 0.002500000, 0.007333333, 0.018047619, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.004166667, 0.016361111, 0.001666667, 0.001666667, 0…\n$ Sepal.Length     &lt;dbl&gt; -1.18429954, -0.57592649, -1.54932338, -1.06262493, -…\n$ Sepal.Width      &lt;dbl&gt; -0.1010677, 1.9980297, 0.8318645, 0.8318645, -0.33430…\n$ Petal.Width      &lt;dbl&gt; -1.3448302, -1.0814250, -1.2131276, -1.3448302, -1.34…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;-\n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.91, 0.98, 1.00, 1.00, 0.88, 0.91, 0.93, 1.00, 0.91,…\n$ .pred_versicolor &lt;dbl&gt; 0.06, 0.02, 0.00, 0.00, 0.12, 0.06, 0.07, 0.00, 0.09,…\n$ .pred_virginica  &lt;dbl&gt; 0.03, 0.00, 0.00, 0.00, 0.00, 0.03, 0.00, 0.00, 0.00,…\n$ Sepal.Length     &lt;dbl&gt; -1.18429954, -0.57592649, -1.54932338, -1.06262493, -…\n$ Sepal.Width      &lt;dbl&gt; -0.1010677, 1.9980297, 0.8318645, 0.8318645, -0.33430…\n$ Petal.Width      &lt;dbl&gt; -1.3448302, -1.0814250, -1.2131276, -1.3448302, -1.34…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.02 0.03 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.17 0.19 0.2 0.21 0.3 0.34 0.5 0.55 0.69 0.71 0.74 0.75 0.81 0.84 0.85 0.9 0.91 0.93 0.94 0.96 0.98  1\n                                                                                                                                                            \n 13    3    3    4    4    1    1    2   1    1    2    1    1   2    1   1    1   1    1    1    1    1    1    1    1    1   1    1    1    1    1    3  1\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.07 0.09 0.13 0.15 0.24 0.29 0.45 0.5 0.66 0.68 0.76 0.79 0.81 0.83 0.86 0.87 0.9 0.91 0.92 0.95 0.97 0.98  1\n                                                                                                                                           \n 21    1    3    4    1    1    2    1    1    1    1    1    1   1    1    1    2    1    1    1    1    1   2    1    1    2    2    2  1\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.04 0.07 0.08 0.1 0.16 0.18 0.19 0.85 0.88 0.91 0.93 0.95 0.98 0.99  1\n                                                                                     \n 24    2    3    2    1    1   1    1    1    2    1    1    4    1    2    2    1 10\n\n\nIris Classifier: Gain and ROC Curves\nWe can plot gain and ROC curves for each of these models\n\niris_ranger_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 139\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 2…\n$ .n_events       &lt;dbl&gt; 0, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 2…\n$ .percent_tested &lt;dbl&gt; 0.000000, 1.666667, 8.333333, 10.000000, 11.666667, 13…\n$ .percent_found  &lt;dbl&gt; 0.000000, 4.545455, 22.727273, 27.272727, 31.818182, 3…\n\niris_ranger_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\niris_ranger_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 142\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001250000, 0.001666667, 0.002000000, …\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.2631579, 0.2894737, 0.3421053, 0.3…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_ranger_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\niris_rf_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 83\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 10, 11, 13, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28,…\n$ .n_events       &lt;dbl&gt; 0, 10, 11, 13, 15, 16, 20, 21, 22, 22, 22, 22, 22, 22,…\n$ .percent_tested &lt;dbl&gt; 0.000000, 16.666667, 18.333333, 21.666667, 25.000000, …\n$ .percent_found  &lt;dbl&gt; 0.000000, 45.454545, 50.000000, 59.090909, 68.181818, …\n\niris_rf_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\niris_rf_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 86\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.04, 0.07, 0.08, 0.10, 0.16, 0.18…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.6315789, 0.6842105, 0.7631579, 0.8…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_rf_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  bind_cols(predict(iris_ranger, iris_testing_baked)) %&gt;%\n  bind_cols(select(iris_testing_baked, Species)) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.922996032, 0.981138889, 0.991000000, 0.980285714, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.072837302, 0.002500000, 0.007333333, 0.018047619, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.004166667, 0.016361111, 0.001666667, 0.001666667, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;%\n#   bind_cols(select(iris_testing_baked,Species)) %&gt;%\n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  bind_cols(predict(iris_ranger, iris_testing_baked)) %&gt;%\n  bind_cols(select(iris_testing_baked, Species)) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.91, 0.98, 1.00, 1.00, 0.88, 0.91, 0.93, 1.00, 0.91,…\n$ .pred_versicolor &lt;dbl&gt; 0.06, 0.02, 0.00, 0.00, 0.12, 0.06, 0.07, 0.00, 0.09,…\n$ .pred_virginica  &lt;dbl&gt; 0.03, 0.00, 0.00, 0.00, 0.00, 0.03, 0.00, 0.00, 0.00,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;%\n#   bind_cols(select(iris_testing_baked,Species)) %&gt;%\n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#references",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "References",
    "text": "References\n\nMachine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-right",
    "href": "content/slides/projects-slides/portfolio/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-left",
    "href": "content/slides/projects-slides/portfolio/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#section",
    "href": "content/slides/projects-slides/portfolio/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "href": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "href": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "href": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#quarto",
    "href": "content/slides/projects-slides/portfolio/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#bullets",
    "href": "content/slides/projects-slides/portfolio/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#code",
    "href": "content/slides/projects-slides/portfolio/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/slides/listing.html",
    "href": "content/slides/listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Introduction to Networks in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Networks in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\nArvind Venkatadri\n\n\nMay 13, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Nature of Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/r-slides-listing.html",
    "href": "content/slides/r-slides/r-slides-listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Introduction to Networks in R\n\n\nUsing tidygraph and visNetwork\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nThe Nature of Data\n\n\nHow does Human Experience link with Data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/networks/LICENSE.html",
    "href": "content/slides/r-slides/networks/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#introduction",
    "href": "content/slides/r-slides/working-in-R/index.html#introduction",
    "title": "Working in R",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "href": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "title": "Working in R",
    "section": "Data Structures",
    "text": "Data Structures"
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "",
    "text": "PlotX3D is a collection of Observable notebooks developed by Mark McClure, each of which defines functions that work well together. We need to import those notebooks into our work here and then use those functions to create the shapes we need."
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html#introduction",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html#introduction",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "",
    "text": "PlotX3D is a collection of Observable notebooks developed by Mark McClure, each of which defines functions that work well together. We need to import those notebooks into our work here and then use those functions to create the shapes we need."
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html#importing-ojs-packages",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html#importing-ojs-packages",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "Importing OJS packages",
    "text": "Importing OJS packages\nImport aquero, a dplyr equivalent in Observable:\n\nimport {aq, op} from \"@uwdata/arquero\"\n//PlotX3D libraries\nimport {\n  show_x3d,\n  create_pointSet,\n  create_indexedFaceSet,\n  create_indexedLineSet,\n  create_sphere,\n  create_text,\n  create_cylinder,\n  create_torus,\n  create_arrow,\n  create_box,\n  create_transform,\n  style\n} from \"@mcmcclur/x3dom-primitives\"\n//\nimport { create_surface } from \"@mcmcclur/parametric-surfaces\"\n//\nimport { create_tube } from \"@mcmcclur/space-curves-and-tubes\"\n//\nimport { create_hollow_cylinder } from \"@mcmcclur/hollow-cylinder\""
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html#making-shapes-with-plotx3d",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html#making-shapes-with-plotx3d",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "Making Shapes with PlotX3D",
    "text": "Making Shapes with PlotX3D\n\nhyperbolic_paraboloid = show_x3d([\n  create_surface((x, y) =&gt; [x, y, x ** 2 - y ** 2], [-1, 1], [-1, 1])\n])\n\n\n\n\n\n\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPut a interactive filter on the data:\n\nfiltered = penguins.filter(function(data) {\n  return bill_length_min &lt; data.bill_length_mm &&\n         islands.includes(data.island);\n})\n\n\n\n\n\n\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)"
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html#trying-counts-and-summaries-with-aquero",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html#trying-counts-and-summaries-with-aquero",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "Trying Counts and Summaries with aquero",
    "text": "Trying Counts and Summaries with aquero\n\ndf_penguins = aq.from(penguins)\nInputs.table(df_penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_penguins.groupby(\"species\").count().view()\n\n\n\n\n\n\n\ndf_penguins\n  .groupby(\"species\")\n  .rollup({ mean: (d) =&gt; op.mean(d.body_mass_g) })\n  .view()"
  },
  {
    "objectID": "content/projects/Modules/R/usingPlotX3D/index.html#discovering-plot",
    "href": "content/projects/Modules/R/usingPlotX3D/index.html#discovering-plot",
    "title": "Using PlotX3D with Observable in Quarto",
    "section": "Discovering plot",
    "text": "Discovering plot\n\nPlot.plot({\n  grid: true,\n  inset: 10,\n  //aspectRatio: 0.05,\n  color: {legend: true},\n  marks: [\n    Plot.frame(),\n    Plot.rectY(df_penguins, Plot.binX({y: \"count\"}, {x: \"body_mass_g\"}, \n                                   {color: \"species\"}, {fill: \"species\"})\n              ),\n    Plot.ruleY([0],)\n  ],\n  title: \"For charts, an informative title\",\n  subtitle: \"Subtitle to follow with additional context\",\n  caption: \"Figure 2. A chart with a title, subtitle, and caption.\",\n})\n\n\n\n\n\n\nTrying to master the syntax, step by step:\n\nPlot.boxX(\n// Data \n  df_penguins, \n  \n// Aesthetics\n          {x: \"body_mass_g\", y: \"species\", fill: \"species\", sort: {y: \"x\"}}\n)\n// Plot options, concatenated\n  .plot({\n    title: \"Box Plot in Observable\",\n    subtitle: \"Getting hold of the Syntax\",\n    caption: \"Box Plot on Log scale\",\n    height: 400,\n    marginLeft: 100,\n    \n// scales\n    x:{type:\"log\",}})\n\n\n\n\n\n\nLet’s try the same plot with facetting, AND with change in colour palette:\n\nPlot.boxX(\n// Data \n  df_penguins, \n  \n// Aesthetics\n          {x: \"body_mass_g\", y: \"species\", fy: \"island\", fill: \"species\", sort: {y: \"x\"}}\n)\n// Plot options, concatenated\n  .plot({\n    title: \"Box Plot in Observable\",\n    subtitle: \"Getting hold of the Syntax\",\n    caption: \"Box Plot on Log scale\",\n// scales\n    x:{type:\"log\"}})\n\n\n\n\n\n\n\nPlot.plot({\n\n//plots to overlay, inside \"marks\". Pass data for each! No inheritance!\n  marks: [\n    Plot.ruleY([0]), // x-axis intercept!!\n    Plot.areaY(aapl, {x: \"Date\", y: \"Close\", fillOpacity: 0.2}),\n    Plot.lineY(aapl, {x: \"Date\", y: \"Close\"})\n  ],\n  y: {\n    type: \"log\",\n    domain: [50, 300],\n    grid: true\n  }\n})\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  inset: 10,\n  aspectRatio: fixed? 1 : undefined, //works online ;-()\n  color: {legend: true},\n  marks: [\n    Plot.frame(),\n    Plot.dot(penguins, {x: \"culmen_length_mm\", y: \"culmen_depth_mm\", stroke: \"species\"})\n  ]\n})\n\n\n\n\n\n\n\nGrouping and Aggregation within Plot\n\nPlot.plot({\n  marginLeft: 80,\n  marginRight: 80,\n  aspectRatio: 0.05,\n  x: {labelAnchor: \"center\", domain: [0, 200]},\n  marks: [\n    Plot.barX(penguins, Plot.groupY({x: \"count\"}, {y: \"species\"})),\n    Plot.ruleX([0])\n  ]\n})"
  },
  {
    "objectID": "content/projects/Modules/R/usingObservable/index.html",
    "href": "content/projects/Modules/R/usingObservable/index.html",
    "title": "Using Observable",
    "section": "",
    "text": "Show the Code\n//Import `aquero`, a `dplyr` equivalent in Observable:\nimport {aq, op} from \"@uwdata/arquero\"\n\n\n\n\n\n\n\nLet’s import and view the penguins dataset:\n\n\nShow the Code\npenguins = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n\n\n\n\n\n\n\nShow the Code\n//df_penguins = aq.from(penguins)\n//Inputs.table(df_penguins)\n//\nInputs.table(penguins)"
  },
  {
    "objectID": "content/projects/Modules/R/usingObservable/index.html#setting-up-ojs-packages",
    "href": "content/projects/Modules/R/usingObservable/index.html#setting-up-ojs-packages",
    "title": "Using Observable",
    "section": "",
    "text": "Show the Code\n//Import `aquero`, a `dplyr` equivalent in Observable:\nimport {aq, op} from \"@uwdata/arquero\"\n\n\n\n\n\n\n\nLet’s import and view the penguins dataset:\n\n\nShow the Code\npenguins = FileAttachment(\"palmer-penguins.csv\").csv({ typed: true })\n\n\n\n\n\n\n\n\n\nShow the Code\n//df_penguins = aq.from(penguins)\n//Inputs.table(df_penguins)\n//\nInputs.table(penguins)"
  },
  {
    "objectID": "content/projects/Modules/R/usingObservable/index.html#discovering-plot-library-in-ojs",
    "href": "content/projects/Modules/R/usingObservable/index.html#discovering-plot-library-in-ojs",
    "title": "Using Observable",
    "section": "Discovering plot library in OJS",
    "text": "Discovering plot library in OJS\nLet’s create a slider for making variable histograms with facetting:\n\n\nShow the Code\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPut a interactive filter on the data:\n\n\nShow the Code\nfiltered = penguins.filter(function(data) {\n  return bill_length_min &lt; data.bill_length_mm &&\n         islands.includes(data.island);\n})\n\n\n\n\n\n\n\n\n\nShow the Code\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    title: \"Facetted Histogram\",\n    caption: \"Figure 1. A chart with a title, subtitle, and caption.\",\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\n\n\nShow the Code\nPlot.plot({\n  grid: true,\n  inset: 10,\n  //aspectRatio: 0.05,\n  color: {legend: true},\n  marks: [\n    Plot.frame(),\n    Plot.rectY(df_penguins, \n      Plot.binX({y: \"count\"}, {x: \"body_mass_g\"}, \n                {color: \"species\"}, {fill: \"species\"})\n              ),\n    Plot.ruleY([0])\n  ],\n  title: \"TITLE: Histogram of Body Mass of Penguins\",\n  subtitle: \"SUBTITLE: Using OJS\",\n  caption: \"Figure 1. A chart with a title, subtitle, and caption.\",\n})\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nFigure 1 is a histogram.\nTrying to master the syntax, step by step:\n\n\nShow the Code\nPlot.boxX(\n// Data \n  df_penguins, \n  \n// Aesthetics\n          {x: \"body_mass_g\", y: \"species\", fill: \"species\", sort: {y: \"x\"}}\n)\n// Plot options, concatenated\n  .plot({\n    title: \"Box Plot in Observable\",\n    subtitle: \"Getting hold of the Syntax\",\n    caption: \"Box Plot on Log scale\",\n    height: 400,\n    marginLeft: 100,\n    \n// scales\n    x:{type:\"log\",}})\n\n\n\n\n\n\n\nLet’s try the same plot with facetting, AND with change in colour palette:\n\n\nShow the Code\nPlot.boxX(\n// Data \n  df_penguins, \n  \n// Aesthetics\n          {x: \"body_mass_g\", y: \"species\", fy: \"island\", \n          fill: \"species\", \n          sort: {y: \"x\"},\n          }\n)\n// Plot options, concatenated\n  .plot({\n    title: \"Box Plot in Observable\",\n    subtitle: \"Getting hold of the Syntax\",\n    caption: \"Box Plot on Log scale\",\n// scales\n    x:{type:\"log\"}})\n\n\n\n\n\n\n\n\n\nShow the Code\nPlot.plot({\n\n//plots to overlay, inside \"marks\". Pass data for each! No inheritance!\n  marks: [\n    Plot.ruleY([0]), // x-axis intercept!!\n    Plot.areaY(aapl, {x: \"Date\", y: \"Close\", fillOpacity: 0.2}),\n    Plot.lineY(aapl, {x: \"Date\", y: \"Close\"})\n  ],\n  y: {\n    type: \"log\",\n    domain: [50, 300],\n    grid: true\n  }\n})\n\n\n\n\n\n\n\n\nScatter Plot\n\n\nShow the Code\nPlot.plot({\n  grid: true,\n  inset: 10,\n  color: {legend: true},\n  x: {labelAnchor: \"center\"},\n  y: {labelAnchor: \"center\"},\n  //title: \"Scatter Plot for Penguins\",\n  marginLeft: 40,\n  marks: [\n    Plot.frame(),\n    Plot.dot(penguins, \n      {x: \"bill_length_mm\", y: \"bill_depth_mm\", \n      stroke: \"black\", fill: \"species\", strokeWidth: 0.2})\n  ]\n})\n\n\n\n\n\n\n\n\n\nGrouping and Aggregation within Plot\n\n\nShow the Code\nPlot.plot({\n  marginLeft: 80,\n  marginRight: 80,\n  aspectRatio: 0.05,\n  x: {labelAnchor: \"center\", domain: [0, 200]},\n  marks: [\n    Plot.barX(penguins, Plot.groupY({x: \"count\"}, {y: \"species\"})),\n    Plot.ruleX([0])\n  ]\n})"
  },
  {
    "objectID": "content/projects/Modules/R/usingObservable/index.html#trying-counts-and-summaries-with-aquero",
    "href": "content/projects/Modules/R/usingObservable/index.html#trying-counts-and-summaries-with-aquero",
    "title": "Using Observable",
    "section": "Trying Counts and Summaries with aquero",
    "text": "Trying Counts and Summaries with aquero\n\n\nShow the Code\ndf_penguins = aq.from(penguins)\ndf_penguins.groupby(\"species\").count().view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the Code\ndf_penguins\n  .groupby(\"species\")\n  .rollup({ mean: (d) =&gt; op.mean(d.body_mass_g) })\n  .view()"
  },
  {
    "objectID": "content/projects/Modules/R/Usingmapgl/index.knit.html",
    "href": "content/projects/Modules/R/Usingmapgl/index.knit.html",
    "title": "Using mapgl",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(mapgl)"
  },
  {
    "objectID": "content/projects/Modules/R/Usingmapgl/index.knit.html#first-map-with-mapgl",
    "href": "content/projects/Modules/R/Usingmapgl/index.knit.html#first-map-with-mapgl",
    "title": "Using mapgl",
    "section": "First Map with mapgl",
    "text": "First Map with mapgl\n\nmapgl::mapboxgl(\n  style = mapbox_style(\"satellite\"),\n  projection = \"winkelTripel\"\n)\n\n\nmapgl::mapboxgl(\n  style = mapbox_style(\"streets\"),\n  projection = \"winkelTripel\"\n) %&gt;%\n  fly_to(\n    center = c(77.580643, 12.972442),\n    zoom = 18, pitch = 50, bearing = 140\n  )\n\n\nmaplibre(\n  style = maptiler_style(\"bright\"),\n  center = c(77.580643, 12.972442),\n  zoom = 16,\n  # pitch = 50, bearing = 140\n) %&gt;%\n  add_fullscreen_control(position = \"top-left\") %&gt;%\n  add_navigation_control()\n\n\n\n\n\n\nfl_age &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01002_001\",\n  state = \"FL\",\n  year = 2022,\n  geometry = TRUE\n)\n\n  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |=====                                                                 |   8%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  23%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |==========================                                            |  38%  |                                                                              |===========================                                           |  38%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  40%  |                                                                              |============================                                          |  41%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  77%  |                                                                              |=======================================================               |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |==========================================================            |  84%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%\n\nfl_age\n\n\n  \n\n\nrange(fl_age$estimate, na.rm = T)\n\n[1] 19.1 81.9\n\n\n\nfl_map &lt;- maplibre(style = maptiler_style(\"basic\"), bounds = fl_age)\n\nfl_map %&gt;%\n  add_fill_layer(\n    id = \"fl_tracts\",\n    source = fl_age,\n    fill_color =\n      interpolate(\n        column = \"estimate\",\n        values = c(20, 80),\n        stops = c(\"lightblue\", \"darkblue\"),\n        na_color = \"lightgrey\"\n      )\n  ) %&gt;%\n  add_legend(\n    \"Median Age in Florida\",\n    values = c(20, 80),\n    colors = c(\"lightblue\", \"darkblue\")\n  )\n\n\n\n\n\n\nfl_map %&gt;%\n  add_fill_layer(\n    id = \"fl_tracts\",\n    source = fl_age,\n    fill_color = step_expr(\n      column = \"estimate\",\n      values = c(20, 50, 80),\n      base = \"white\",\n      stops = c(\"lightblue\", \"blue\", \"darkblue\"),\n      na_color = \"lightgrey\"\n    )\n  ) %&gt;%\n  add_legend(\n    \"Median Age in Florida\",\n    values = c(20, 80),\n    colors = c(\"lightblue\", \"blue\", \"darkblue\")\n  )"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html",
    "href": "content/projects/Modules/R/tidygraph/index.html",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5. I have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#introduction",
    "href": "content/projects/Modules/R/tidygraph/index.html#introduction",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5. I have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#contents",
    "href": "content/projects/Modules/R/tidygraph/index.html#contents",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "CONTENTS",
    "text": "CONTENTS\n\nWorking with colors in R plots\nReading in the network data\nNetwork plots in ‘igraph’\nPlotting two-mode networks\nPlotting multiplex networks\nQuick example using ‘network’\nSimple plot animations in R\nInteractive JavaScript networks\nInteractive and dynamic networks with ndtv-d3\nPlotting networks on a geographic map"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#dataset-1-edgelist--",
    "href": "content/projects/Modules/R/tidygraph/index.html#dataset-1-edgelist--",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 1: edgelist ~~——-",
    "text": "——-~~ DATASET 1: edgelist ~~——-\n\n# Read in the data:\nnodes &lt;- read.csv(\"./Dataset1-Media-Example-NODES.csv\", header = T, as.is = T)\nlinks &lt;- read.csv(\"./Dataset1-Media-Example-EDGES.csv\", header = T, as.is = T)\n\n\n# Examine the data:\nhead(nodes)\n\n\n  \n\n\nhead(links)\n\n\n  \n\n\n\nConverting the data to an igraph object:\nThe graph_from_data_frame() function takes two data frames: ‘d’ and ‘vertices’. - ‘d’ describes the edges of the network - it should start with two columns containing the source and target node IDs for each network tie. - ‘vertices’ should start with a column of node IDs. It can be omitted. - Any additional columns in either data frame are interpreted as attributes.\nNOTE: ID columns need not be numbers or integers!!\n\nnet &lt;- graph_from_data_frame(d = links, vertices = nodes, directed = T)\n\n# Examine the resulting object:\nclass(net)\n\n[1] \"igraph\"\n\nnet\n\nIGRAPH a04c2ba DNW- 17 49 -- \n+ attr: name (v/c), media (v/c), media.type (v/n), type.label (v/c),\n| audience.size (v/n), type (e/c), weight (e/n)\n+ edges from a04c2ba (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\n\nThe description of an igraph object starts with four letters:\n- D or U, for a directed or undirected graph - N for a named graph (where nodes have a name attribute) - W for a weighted graph (where edges have a weight attribute) -B for a bipartite (two-mode) graph (where nodes have a type attribute) The two numbers that follow (17 49) refer to the number of nodes and edges in the graph. The description also lists node & edge attributes.\nWe can access the nodes, edges, and their attributes:\n\nE(net)\n\n+ 49/49 edges from a04c2ba (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\nV(net)\n\n+ 17/17 vertices, named, from a04c2ba:\n [1] s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17\n\nE(net)$type\n\n [1] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"hyperlink\" \"hyperlink\"\n [7] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\"\n[13] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"  \n[19] \"hyperlink\" \"mention\"   \"mention\"   \"hyperlink\" \"hyperlink\" \"mention\"  \n[25] \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[31] \"mention\"   \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[37] \"mention\"   \"hyperlink\" \"mention\"   \"hyperlink\" \"mention\"   \"mention\"  \n[43] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"  \n[49] \"hyperlink\"\n\nV(net)$media\n\n [1] \"NY Times\"            \"Washington Post\"     \"Wall Street Journal\"\n [4] \"USA Today\"           \"LA Times\"            \"New York Post\"      \n [7] \"CNN\"                 \"MSNBC\"               \"FOX News\"           \n[10] \"ABC\"                 \"BBC\"                 \"Yahoo News\"         \n[13] \"Google News\"         \"Reuters.com\"         \"NYTimes.com\"        \n[16] \"WashingtonPost.com\"  \"AOL.com\"            \n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(edges) %&gt;%\n  select(type)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 3 (active)\n    from    to type     \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1     1     2 hyperlink\n 2     1     3 hyperlink\n 3     1     4 hyperlink\n 4     1    15 mention  \n 5     2     1 hyperlink\n 6     2     3 hyperlink\n 7     2     9 hyperlink\n 8     2    10 hyperlink\n 9     3     1 hyperlink\n10     3     4 hyperlink\n# ℹ 39 more rows\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ℹ 14 more rows\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  select(media)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 1 (active)\n   media              \n   &lt;chr&gt;              \n 1 NY Times           \n 2 Washington Post    \n 3 Wall Street Journal\n 4 USA Today          \n 5 LA Times           \n 6 New York Post      \n 7 CNN                \n 8 MSNBC              \n 9 FOX News           \n10 ABC                \n11 BBC                \n12 Yahoo News         \n13 Google News        \n14 Reuters.com        \n15 NYTimes.com        \n16 WashingtonPost.com \n17 AOL.com            \n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\n\nOr find specific nodes and edges by attribute:(that returns objects of type vertex sequence / edge sequence)\n\nV(net)[media == \"BBC\"]\n\n+ 1/17 vertex, named, from a04c2ba:\n[1] s11\n\nE(net)[type == \"mention\"]\n\n+ 20/49 edges from a04c2ba (vertex names):\n [1] s01-&gt;s15 s03-&gt;s10 s04-&gt;s06 s04-&gt;s11 s04-&gt;s17 s05-&gt;s01 s05-&gt;s15 s06-&gt;s17\n [9] s07-&gt;s03 s07-&gt;s08 s07-&gt;s14 s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s12-&gt;s06 s12-&gt;s14\n[17] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s16-&gt;s17\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  filter(media == \"BBC\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# A rooted tree\n#\n# Node Data: 1 × 5 (active)\n  id    media media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s11   BBC            2 TV                    34\n#\n# Edge Data: 0 × 4\n# ℹ 4 variables: from &lt;int&gt;, to &lt;int&gt;, type &lt;chr&gt;, weight &lt;int&gt;\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(edges) %&gt;%\n  filter(type == \"mention\")\n\n# A tbl_graph: 17 nodes and 20 edges\n#\n# A directed simple graph with 3 components\n#\n# Edge Data: 20 × 4 (active)\n    from    to type    weight\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n 1     1    15 mention     20\n 2     3    10 mention      2\n 3     4     6 mention      1\n 4     4    11 mention     22\n 5     4    17 mention      2\n 6     5     1 mention      1\n 7     5    15 mention     21\n 8     6    17 mention     21\n 9     7     3 mention      1\n10     7     8 mention     22\n11     7    14 mention      4\n12     8     7 mention     21\n13     8     9 mention     23\n14     9    10 mention     21\n15    12     6 mention      2\n16    12    14 mention     22\n17    13    17 mention      1\n18    14    11 mention      1\n19    14    13 mention     21\n20    16    17 mention     21\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ℹ 14 more rows\n\n\nIf you need them, you can extract an edge list or a matrix back from the igraph networks.\n\nas_edgelist(net, names = T)\n\n      [,1]  [,2] \n [1,] \"s01\" \"s02\"\n [2,] \"s01\" \"s03\"\n [3,] \"s01\" \"s04\"\n [4,] \"s01\" \"s15\"\n [5,] \"s02\" \"s01\"\n [6,] \"s02\" \"s03\"\n [7,] \"s02\" \"s09\"\n [8,] \"s02\" \"s10\"\n [9,] \"s03\" \"s01\"\n[10,] \"s03\" \"s04\"\n[11,] \"s03\" \"s05\"\n[12,] \"s03\" \"s08\"\n[13,] \"s03\" \"s10\"\n[14,] \"s03\" \"s11\"\n[15,] \"s03\" \"s12\"\n[16,] \"s04\" \"s03\"\n[17,] \"s04\" \"s06\"\n[18,] \"s04\" \"s11\"\n[19,] \"s04\" \"s12\"\n[20,] \"s04\" \"s17\"\n[21,] \"s05\" \"s01\"\n[22,] \"s05\" \"s02\"\n[23,] \"s05\" \"s09\"\n[24,] \"s05\" \"s15\"\n[25,] \"s06\" \"s06\"\n[26,] \"s06\" \"s16\"\n[27,] \"s06\" \"s17\"\n[28,] \"s07\" \"s03\"\n[29,] \"s07\" \"s08\"\n[30,] \"s07\" \"s10\"\n[31,] \"s07\" \"s14\"\n[32,] \"s08\" \"s03\"\n[33,] \"s08\" \"s07\"\n[34,] \"s08\" \"s09\"\n[35,] \"s09\" \"s10\"\n[36,] \"s10\" \"s03\"\n[37,] \"s12\" \"s06\"\n[38,] \"s12\" \"s13\"\n[39,] \"s12\" \"s14\"\n[40,] \"s13\" \"s12\"\n[41,] \"s13\" \"s17\"\n[42,] \"s14\" \"s11\"\n[43,] \"s14\" \"s13\"\n[44,] \"s15\" \"s01\"\n[45,] \"s15\" \"s04\"\n[46,] \"s15\" \"s06\"\n[47,] \"s16\" \"s06\"\n[48,] \"s16\" \"s17\"\n[49,] \"s17\" \"s04\"\n\nas_adjacency_matrix(net, attr = \"weight\")\n\n17 x 17 sparse Matrix of class \"dgCMatrix\"\n\n\n                                                     \ns01  . 22 22 21 .  .  .  .  .  .  .  .  .  . 20  .  .\ns02 23  . 21  . .  .  .  .  1  5  .  .  .  .  .  .  .\ns03 21  .  . 22 1  .  .  4  .  2  1  1  .  .  .  .  .\ns04  .  . 23  . .  1  .  .  .  . 22  3  .  .  .  .  2\ns05  1 21  .  . .  .  .  .  2  .  .  .  .  . 21  .  .\ns06  .  .  .  . .  1  .  .  .  .  .  .  .  .  . 21 21\ns07  .  .  1  . .  .  . 22  . 21  .  .  .  4  .  .  .\ns08  .  .  2  . .  . 21  . 23  .  .  .  .  .  .  .  .\ns09  .  .  .  . .  .  .  .  . 21  .  .  .  .  .  .  .\ns10  .  .  2  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns11  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns12  .  .  .  . .  2  .  .  .  .  .  . 22 22  .  .  .\ns13  .  .  .  . .  .  .  .  .  .  . 21  .  .  .  .  1\ns14  .  .  .  . .  .  .  .  .  .  1  . 21  .  .  .  .\ns15 22  .  .  1 .  4  .  .  .  .  .  .  .  .  .  .  .\ns16  .  .  .  . . 23  .  .  .  .  .  .  .  .  .  . 21\ns17  .  .  .  4 .  .  .  .  .  .  .  .  .  .  .  .  .\n\n# Using tidygraph\n# No direct command seems available ...\n\n\n# Or data frames describing nodes and edges:\nigraph::as_data_frame(x = net, what = \"edges\")\n\n\n  \n\n\nigraph::as_data_frame(x = net, what = \"vertices\")\n\n\n  \n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\n\n\n  \n\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(edges) %&gt;%\n  as_tibble()\n\n\n  \n\n\n\n\n# You can also access the network matrix directly:\nnet[1, ]\n\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17 \n  0  22  22  21   0   0   0   0   0   0   0   0   0   0  20   0   0 \n\nnet[5, 7]\n\n[1] 0\n\n# Using tidygraph\n# Does not seem possible, even with `as.matrix()`.\n# Returns tibbles only as in the code chunk above\n\n\n# First attempt to plot the graph:\nplot(net) # not pretty!\n\n\n\n\n\n\n# Removing loops from the graph:\nnet &lt;-\n  igraph::simplify(net, remove.multiple = F, remove.loops = T)\n\n# Let's and reduce the arrow size and remove the labels:\nplot(net, edge.arrow.size = .4, vertex.label = NA)\n\n\n\n\n\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    # clears an area near the node\n\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 8, shape = 21, fill = \"orange\") +\n  geom_node_text(aes(label = id), size = 3)\n\n\n\n\n\n\n# Removing loops from the graph:\n# From the docs:\n# convert() is a shorthand for performing both `morph` and `crystallise` along with extracting a single tbl_graph (defaults to the first). For morphs w(h)ere you know they only create a single graph, and you want to keep it, this is an easy way.\n#\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  convert(to_simple) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\")"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#dataset-2-matrix",
    "href": "content/projects/Modules/R/tidygraph/index.html#dataset-2-matrix",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 2: matrix ——–",
    "text": "——-~~ DATASET 2: matrix ——–\n\n# Read in the data:\nnodes2 &lt;- read.csv(\"./Dataset2-Media-User-Example-NODES.csv\", header = T, as.is = T)\nlinks2 &lt;- read.csv(\"./Dataset2-Media-User-Example-EDGES.csv\", header = T, row.names = 1)\n\n# Examine the data:\nhead(nodes2)\n\n\n  \n\n\nhead(links2)\n\n\n  \n\n\n# links2 is a matrix for a two-mode network:\nlinks2 &lt;- as.matrix(links2)\ndim(links2)\n\n[1] 10 20\n\ndim(nodes2)\n\n[1] 30  5\n\n\nNote: What is a two-mode network? A network that as a node$type variable and can be a bipartite or a k-partite network as a result.\n\n# Create an igraph network object from the two-mode matrix:\nnet2 &lt;- igraph::graph_from_incidence_matrix(links2)\n\n# To transform a one-mode network matrix into an igraph object,\n# we would use graph_from_adjacency_matrix()\n\n# A built-in vertex attribute 'type' shows which mode vertices belong to.\ntable(V(net2)$type)\n\n\nFALSE  TRUE \n   10    20 \n\n# Basic igraph plot\nplot(net2, vertex.label = NA)\n\n\n\n\n\n\n\n\n# using tidygraph\n# For all objects that are not node and edge data_frames\n# tidygraph uses `as_tbl_graph()`\n#\ngraph &lt;- as_tbl_graph(links2)\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\n\n\n  \n\n\ngraph %&gt;%\n  activate(edges) %&gt;%\n  as_tibble()\n\n\n  \n\n\ngraph %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21, size = 6,\n    color = \"black\"\n  )\n\n\n\n\n\n\n\n\n# Examine the resulting object:\nclass(net2)\n\n[1] \"igraph\"\n\nnet2\n\nIGRAPH a6d0165 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from a6d0165 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\n\nNote: The remaining attributes for the nodes ( in data frame nodes2) are not (yet) a part of the graph, either with igraph or with tidygraph."
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#plot-parameters-in-igraph",
    "href": "content/projects/Modules/R/tidygraph/index.html#plot-parameters-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——~~ Plot parameters in igraph ——–",
    "text": "——~~ Plot parameters in igraph ——–\nCheck out the node options (starting with ‘vertex.’) and the edge options (starting with ‘edge.’).Type ?igraph.plotting in your console\n\n?igraph.plotting\n\nWe can set the node & edge options in two ways - one is to specify them in the plot() function, as we are doing below.\n\nPlot with curved edges (edge.curved = .1) and reduce arrow size:\n\n\nplot(net, edge.arrow.size = .4, edge.curved = .1)\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n   id    media               media.type type.label audience.size\n   &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\ngraph %&gt;% ggraph(., layout = \"graphopt\") +\n  geom_edge_arc(\n    color = \"grey\",\n    strength = 0.1,\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 8,\n    color = \"black\"\n  ) +\n  geom_node_text(aes(label = id), size = 3)\n\n\n\n\n\n\n\n\nSet node color to orange and the border color to hex 555555\nReplace the vertex label with the node names stored in “media”\n\n\nplot(\n  net,\n  edge.arrow.size = .2,\n  edge.curved = 0,\n  vertex.color = \"orange\",\n  vertex.frame.color = \"#555555\",\n  vertex.label = V(net)$media,\n  vertex.label.color = \"black\",\n  vertex.label.cex = .7\n)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %&gt;%\n  ggraph(., layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(.3, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 6,\n    color = \"#555555\"\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\n\n\n\nThe second way to set attributes is to add them to the igraph object.\n\nGenerate colors based on media type:\n\n\ncolrs &lt;- c(\"gray50\", \"tomato\", \"gold\")\nV(net)$color &lt;- colrs[V(net)$media.type]\nplot(net)\n\n\n\n\n\n\n\n\nCompute node degrees (#links) and use that to set node size:\n\n\ndeg &lt;- igraph::degree(net, mode = \"all\")\nV(net)$size &lt;- deg * 3\n# Alternatively, we can set node size based on audience size:\nV(net)$size &lt;- V(net)$audience.size * 0.7\nV(net)$size\n\n [1] 14.0 17.5 21.0 22.4 14.0 35.0 39.2 23.8 42.0 16.1 23.8 23.1 16.1  8.4 16.8\n[16] 19.6 23.1\n\n# The labels are currently node IDs.\n# Setting them to NA will render no labels:\nV(net)$label.color &lt;- \"black\"\nV(net)$label &lt;- NA\n\n# Set edge width based on weight:\nE(net)$width &lt;- E(net)$weight / 6\n\n# change arrow size and edge color:\nE(net)$arrow.size &lt;- .2\nE(net)$edge.color &lt;- \"gray80\"\n\n# We can even set the network layout:\ngraph_attr(net, \"layout\") &lt;- layout_with_lgl\nplot(net)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = type.label, size = size),\n    shape = 21,\n    color = \"black\"\n  ) +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\")\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range = c(2, 16)) +\n\n  guides(fill = guide_legend(\n    title = \"Media Type\",\n    override.aes = list(pch = 21, size = 4)\n  ))\n\n\n\n\n\n\n\nWe can also override the attributes explicitly in the plot:\n\nplot(net, edge.color = \"orange\", vertex.color = \"gray50\")\n\n\n\n\n\n\n\nWe can also add a legend explaining the meaning of the colors we used:\n\nplot(net)\nlegend(\n  x = -2.1, y = -1.1,\n  c(\"Newspaper\", \"Television\", \"Online News\"),\n  pch = 21, col = \"#777777\",\n  pt.bg = colrs, pt.cex = 2.5, bty = \"n\", ncol = 1\n)\n\n\n\n\n\n\n# legends are automatic with the tidygraph + ggraph flow\n\nSometimes, especially with semantic networks, we may be interested in plotting only the labels of the nodes:\n\nplot(net,\n  vertex.shape = \"none\", vertex.label = V(net)$media,\n  vertex.label.font = 2, vertex.label.color = \"gray40\",\n  vertex.label.cex = .7, edge.color = \"gray85\"\n)\n\n\n\n\n\n\n# using tidygraph\n\nggraph(net, layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey80\", width = 2,\n    end_cap = circle(0.5, \"cm\"),\n    start_cap = circle(0.5, \"cm\")\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\n\n\n\nLet’s color the edges of the graph based on their source node color. We’ll get the starting node for each edge with ends().\nNote: Edge attribute is being set by start node.\n\nedge.start &lt;- ends(net, es = E(net), names = F)[, 1]\nedge.col &lt;- V(net)$color[edge.start] # How simple this is !!!\n# The three colors are recycled\n#\nplot(net, edge.color = edge.col, edge.curved = .4)\n\n\n\n\n\n\n\nNOTE: The source node colour has been set using the media.type, which is a node attribute. Node attributes are not typically accessible to edges. So we need to build a combo data frame using dplyr, so that edges can use this node attribute. ( There may be other ways…)\n\n# Using tidygraph\n# Make a \"combo\" data frame of nodes *and* edges with left_join()\n# Join by `from` so that type.label is based on from = edge.start\n\nlinks %&gt;%\n  left_join(., nodes, by = c(\"from\" = \"id\")) %&gt;%\n  tbl_graph(edges = ., nodes = nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_arc(\n    aes(\n      color = type.label,\n      width = weight\n    ),\n    strength = 0.3\n  ) +\n  geom_node_point(\n    aes(\n      fill = type.label,\n      # type.label is now available as edge attribute\n      size = size\n    ),\n    shape = 21,\n    color = \"black\"\n  ) +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\"),\n    guide = \"legend\"\n  ) +\n  scale_edge_color_manual(\n    name = \"Source Type\",\n    values = c(\"grey80\", \"gold\", \"tomato\")\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range = c(2, 16)) +\n  # not \"limits\"!\n  guides(fill = guide_legend(override.aes = list(\n    pch = 21,\n    size = 4\n  )))"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#network-layouts-in-igraph",
    "href": "content/projects/Modules/R/tidygraph/index.html#network-layouts-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Network Layouts in ‘igraph’ ——–",
    "text": "——-~~ Network Layouts in ‘igraph’ ——–\nNetwork layouts are algorithms that return coordinates for each node in a network.\nLet’s generate a slightly larger 100-node graph using a preferential attachment model (Barabasi-Albert).\n\nnet.bg &lt;- sample_pa(n = 100, power = 1.2)\nV(net.bg)$size &lt;- 8\nV(net.bg)$frame.color &lt;- \"white\"\nV(net.bg)$color &lt;- \"orange\"\nV(net.bg)$label &lt;- \"\"\nE(net.bg)$arrow.mode &lt;- 0\nplot(net.bg)\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 4) +\n  theme_graph()\n\n\n\n\n\n\n\nNow let’s plot this network using the layouts available in igraph. You can set the layout in the plot function:\n\nplot(net.bg, layout = layout_randomly)\n\n\n\n\n\n\n\nOr calculate the vertex coordinates in advance:\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = \"circle\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nl is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. You can generate your own:\n\nl &lt;- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))\nl\n\n       [,1] [,2]\n  [1,]    1    1\n  [2,]    2  100\n  [3,]    3   99\n  [4,]    4   98\n  [5,]    5   97\n  [6,]    6   96\n  [7,]    7   95\n  [8,]    8   94\n  [9,]    9   93\n [10,]   10   92\n [11,]   11   91\n [12,]   12   90\n [13,]   13   89\n [14,]   14   88\n [15,]   15   87\n [16,]   16   86\n [17,]   17   85\n [18,]   18   84\n [19,]   19   83\n [20,]   20   82\n [21,]   21   81\n [22,]   22   80\n [23,]   23   79\n [24,]   24   78\n [25,]   25   77\n [26,]   26   76\n [27,]   27   75\n [28,]   28   74\n [29,]   29   73\n [30,]   30   72\n [31,]   31   71\n [32,]   32   70\n [33,]   33   69\n [34,]   34   68\n [35,]   35   67\n [36,]   36   66\n [37,]   37   65\n [38,]   38   64\n [39,]   39   63\n [40,]   40   62\n [41,]   41   61\n [42,]   42   60\n [43,]   43   59\n [44,]   44   58\n [45,]   45   57\n [46,]   46   56\n [47,]   47   55\n [48,]   48   54\n [49,]   49   53\n [50,]   50   52\n [51,]   51   51\n [52,]   52   50\n [53,]   53   49\n [54,]   54   48\n [55,]   55   47\n [56,]   56   46\n [57,]   57   45\n [58,]   58   44\n [59,]   59   43\n [60,]   60   42\n [61,]   61   41\n [62,]   62   40\n [63,]   63   39\n [64,]   64   38\n [65,]   65   37\n [66,]   66   36\n [67,]   67   35\n [68,]   68   34\n [69,]   69   33\n [70,]   70   32\n [71,]   71   31\n [72,]   72   30\n [73,]   73   29\n [74,]   74   28\n [75,]   75   27\n [76,]   76   26\n [77,]   77   25\n [78,]   78   24\n [79,]   79   23\n [80,]   80   22\n [81,]   81   21\n [82,]   82   20\n [83,]   83   19\n [84,]   84   18\n [85,]   85   17\n [86,]   86   16\n [87,]   87   15\n [88,]   88   14\n [89,]   89   13\n [90,]   90   12\n [91,]   91   11\n [92,]   92   10\n [93,]   93    9\n [94,]   94    8\n [95,]   95    7\n [96,]   96    6\n [97,]   97    5\n [98,]   98    4\n [99,]   99    3\n[100,]  100    2\n\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = l) +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph()\n\n\n\n\n\n\n\nThis layout is just an example and not very helpful - thankfully igraph has a number of built-in layouts, including:\n\nRandomly placed vertices\n\n\nl &lt;- layout_randomly(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_randomly(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\n\nCircle layout\n\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_in_circle(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n3D sphere layout\n\n\nl &lt;- layout_on_sphere(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_on_sphere(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\")\n\n\n\n\n\n\n\n\nThe Fruchterman-Reingold force-directed algorithm: Nice but slow, most often used in graphs smaller than ~1000 vertices.\n\n\nl &lt;- layout_with_fr(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_fr(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\")\n\n\n\n\n\n\n\nYou will also notice that the F-R layout is not deterministic - different runs will result in slightly different configurations. Saving the layout in l allows us to get the exact same result multiple times.\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1, 1))\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = l)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n\nBy default, the coordinates of the plots are rescaled to the [-1,1] interval for both x and y. You can change that with the parameter rescale = FALSE and rescale your plot manually by multiplying the coordinates by a scalar. You can use norm_coords to normalize the plot with the boundaries you want. This way you can create more compact or spread out layout versions.\n\n# Get the layout coordinates:\nl &lt;- layout_with_fr(net.bg)\n# Normalize them so that they are in the -1, 1 interval:\nl &lt;- norm_coords(l, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n\npar(mfrow = c(2, 2), mar = c(0, 0, 0, 0))\nplot(net.bg, rescale = F, layout = l * 0.4)\nplot(net.bg, rescale = F, layout = l * 0.8)\nplot(net.bg, rescale = F, layout = l * 1.2)\nplot(net.bg, rescale = F, layout = l * 1.6)\n\n\n\n\n\n\n# Using tidygraph\n# Can't do this with tidygraph ( multiplying layout * scalar ), it seems\n\nAnother popular force-directed algorithm that produces nice results for connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to minimize the energy in a spring system.\n\nl &lt;- layout_with_kk(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_kk(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nThe MDS (multidimensional scaling) algorithm tries to place nodes based on some measure of similarity or distance between them. More similar/less distant nodes are placed closer to each other. By default, the measure used is based on the shortest paths between nodes in the network. That can be changed with the dist parameter.\n\nplot(net.bg, layout = layout_with_mds)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_mds(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nThe LGL algorithm is for large connected graphs. Here you can specify a root- the node that will be placed in the middle of the layout.\n\nplot(net.bg, layout = layout_with_lgl)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_lgl(.)) +\n  geom_edge_link0(colour = \"grey\") +\n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nBy default, igraph uses a layout called layout_nicely which selects an appropriate layout algorithm based on the properties of the graph. Check out all available layouts in igraph:Type ?igraph::layout_ in your console\n\nlayouts &lt;- grep(\"^layout_\", ls(\"package:igraph\"), value = TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\nlayouts &lt;- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\n\npar(mfrow = c(3, 3), mar = c(1, 1, 1, 1))\n\nfor (layout in layouts) {\n  print(layout)\n  l &lt;- do.call(layout, list(net))\n  plot(net, edge.arrow.mode = 0, layout = l, main = layout)\n}\n\n[1] \"layout_as_star\"\n\n\n[1] \"layout_components\"\n\n\n[1] \"layout_in_circle\"\n\n\n[1] \"layout_nicely\"\n\n\n[1] \"layout_on_grid\"\n\n\n[1] \"layout_on_sphere\"\n\n\n[1] \"layout_randomly\"\n\n\n[1] \"layout_with_dh\"\n\n\n[1] \"layout_with_drl\"\n\n\n\n\n\n\n\n\n[1] \"layout_with_fr\"\n\n\n[1] \"layout_with_gem\"\n\n\n[1] \"layout_with_graphopt\"\n\n\n[1] \"layout_with_kk\"\n\n\n[1] \"layout_with_lgl\"\n\n\n[1] \"layout_with_mds\""
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#highlighting-specific-nodes-or-links",
    "href": "content/projects/Modules/R/tidygraph/index.html#highlighting-specific-nodes-or-links",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Highlighting specific nodes or links ——–",
    "text": "——-~~ Highlighting specific nodes or links ——–\nSometimes we want to focus the visualization on a particular node or a group of nodes. Let’s represent distance from the NYT:\n\n\ndistances() calculates shortest path from vertices in ‘v’ to ones in ‘to’.\n\n\ndist.from.NYT &lt;- distances(net,\n  v = V(net)[media == \"NY Times\"],\n  to = V(net),\n  weights = NA\n)\n\n# Set colors to plot the distances:\noranges &lt;- colorRampPalette(c(\"dark red\", \"gold\"))\ncol &lt;- oranges(max(dist.from.NYT) + 1)\ncol &lt;- col[dist.from.NYT + 1]\n\n# Let's have same coordinates for Nodes in both graph renderings\n# Then we can verify that the distance calculations are the same for both renderings\ncoords &lt;- igraph::layout_nicely(net)\nplot(net,\n  vertex.label = dist.from.NYT,\n  vertex.color = col, vertex.label.color = \"black\",\n  layout = coords\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n   id    media               media.type type.label audience.size\n   &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\n# Set up NY Times as root node first\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need an integer node id.\nroot_nyt &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"NY Times\") %&gt;%\n  select(node_id) %&gt;%\n  as_vector()\nroot_nyt\n\nnode_id \n      1 \n\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  # new stuff:\n  # breadth first search for all distances from the root node\n  mutate(order = bfs_dist(root = root_nyt)) %&gt;%\n  ggraph(., layout = coords) + # same layout\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    aes(\n      fill = order,\n      size = size\n    ),\n    shape = 21,\n    color = \"black\"\n  ) +\n\n  geom_node_text(aes(label = order)) +\n\n  scale_fill_gradient(\n    name = \"Distance from NY Times\",\n    low = \"dark red\",\n    high = \"gold\",\n    guide = \"legend\"\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range = c(2, 16)) +\n  guides(fill = guide_legend(override.aes = list(\n    pch = 21,\n    size = 4\n  )))\n\n\n\n\n\n\n\nOr, a bit more readable:\n\nplot(net,\n  vertex.color = col,\n  vertex.label = dist.from.NYT, edge.arrow.size = .6,\n  vertex.label.color = \"white\",\n  vertex.size = V(net)$size * 1.6,\n  edge.width = 2,\n  layout = norm_coords(layout_with_lgl(net)) * 1.4, rescale = F\n)"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#path-highlighting",
    "href": "content/projects/Modules/R/tidygraph/index.html#path-highlighting",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Path Highlighting",
    "text": "Path Highlighting\nWe can also highlight paths between the nodes in the network.\n\nSay here between MSNBC and the New York Post\n\n\nnews.path &lt;- shortest_paths(net,\n  from = V(net)[media == \"MSNBC\"],\n  to = V(net)[media == \"New York Post\"],\n  output = \"both\"\n) # both path nodes and edges\nnews.path.distance &lt;- distances(\n  net,\n  V(net)[media == \"MSNBC\"],\n  V(net)[media == \"New York Post\"]\n)\nnews.path\n\n$vpath\n$vpath[[1]]\n+ 4/17 vertices, named, from 5c2c2d9:\n[1] s08 s03 s12 s06\n\n\n$epath\n$epath[[1]]\n+ 3/48 edges from 5c2c2d9 (vertex names):\n[1] s08-&gt;s03 s03-&gt;s12 s12-&gt;s06\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\nnews.path.distance\n\n    s06\ns08   5\n\n# Generate edge color variable to plot the path:\necol &lt;- rep(\"gray80\", ecount(net))\necol[unlist(news.path$epath)] &lt;- \"orange\"\n\n# Generate edge width variable to plot the path:\new &lt;- rep(2, ecount(net))\new[unlist(news.path$epath)] &lt;- 4\n\n# Generate node color variable to plot the path:\nvcol &lt;- rep(\"gray40\", vcount(net))\nvcol[unlist(news.path$vpath)] &lt;- \"gold\"\n\nplot(net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = ew,\n  edge.arrow.mode = 0,\n  ## added lines\n  vertex.label = V(net)$media,\n  vertex.label.font = 2,\n  vertex.label.color = \"gray40\",\n  vertex.label.cex = .7,\n  layout = coords * 1.5\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# We need to use:\n# to_shortest_path(graph, from, to, mode = \"out\", weights = NULL)\n# Let's set up `to` and `from` nodes\n#\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need integer node ids for `from` and `to` in `to_shortest_path`\n\nmsnbc &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"MSNBC\") %&gt;%\n  select(node_id) %&gt;%\n  as_vector()\nmsnbc\n\nnode_id \n      8 \n\nnypost &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"New York Post\") %&gt;%\n  select(node_id) %&gt;%\n  as_vector()\nnypost\n\nnode_id \n      6 \n\n# Let's create a fresh graph object using morph\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\n#\n# # Can do this to obtain a separate graph\n# convert(to_shortest_path,from = msnbc,to = nypost)\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\nmsnbc_nyp &lt;-\n  graph %&gt;%\n  # first mark all nodes and edges as *not* on the shortest path\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = FALSE) %&gt;%\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = FALSE) %&gt;%\n  # Find shortest path between MSNBC and NY Post\n  morph(to_shortest_path, from = msnbc, to = nypost) %&gt;%\n  # Now to mark the shortest_path nodes as TRUE\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = TRUE) %&gt;%\n  # Now to mark the shortest_path edges as TRUE\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = TRUE) %&gt;%\n  #\n  # Merge back into main graph; Still saving it as a `msnbc_nyp`\n  unmorph()\nmsnbc_nyp\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 5 (active)\n    from    to type      weight shortest_path_edge\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;lgl&gt;             \n 1     1     2 hyperlink     22 FALSE             \n 2     1     3 hyperlink     22 FALSE             \n 3     1     4 hyperlink     21 FALSE             \n 4     1    15 mention       20 FALSE             \n 5     2     1 hyperlink     23 FALSE             \n 6     2     3 hyperlink     21 FALSE             \n 7     2     9 hyperlink      1 FALSE             \n 8     2    10 hyperlink      5 FALSE             \n 9     3     1 hyperlink     21 FALSE             \n10     3     4 hyperlink     22 TRUE              \n# ℹ 39 more rows\n#\n# Node Data: 17 × 6\n  id    media             media.type type.label audience.size shortest_path_node\n  &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt; &lt;chr&gt;              &lt;int&gt; &lt;lgl&gt;             \n1 s01   NY Times                   1 Newspaper             20 FALSE             \n2 s02   Washington Post            1 Newspaper             25 FALSE             \n3 s03   Wall Street Jour…          1 Newspaper             30 TRUE              \n# ℹ 14 more rows\n\nmsnbc_nyp %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(layout = coords) +\n  # geom_edge_link0(colour = \"grey\") +\n  geom_edge_link0(aes(\n    colour = shortest_path_edge,\n    width = shortest_path_edge\n  )) +\n\n  geom_node_point(aes(\n    size = size,\n    fill = shortest_path_node\n  ), shape = 21) +\n  geom_node_text(aes(label = media)) +\n\n  scale_size_continuous(\"Degree\", range = c(2, 16)) +\n  scale_fill_manual(\"Shortest Path\",\n    values = c(\"grey\", \"gold\")\n  ) +\n\n  scale_edge_width_manual(values = c(1, 4)) +\n\n  scale_edge_colour_manual(values = c(\"grey\", \"orange\")) +\n  guides(\n    fill = guide_legend(override.aes = list(\n      pch = 21,\n      size = 6\n    )),\n    edge_colour = \"none\",\n    edge_width = \"none\"\n  )\n\n\n\n\n\n\n\n\nHighlight the edges going into or out of a vertex, for instance the WSJ. For a single node, use incident(), for multiple nodes use incident_edges()\n\n\n\ninc.edges &lt;-\n  incident(net, V(net)[media == \"Wall Street Journal\"], mode = \"all\")\n\n# Set colors to plot the selected edges.\necol &lt;- rep(\"gray80\", ecount(net))\necol[inc.edges] &lt;- \"orange\"\nvcol &lt;- rep(\"grey40\", vcount(net))\nvcol[V(net)$media == \"Wall Street Journal\"] &lt;- \"gold\"\nplot(\n  net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = 2,\n  layout = coords\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"Wall Street Journal\") %&gt;%\n  select(node_id) %&gt;%\n  as_vector()\n\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(\n    wsj_adjacent = node_is_adjacent(\n      to = wsj, mode = \"all\",\n      include_to = TRUE\n    ),\n    size = centrality_degree()\n  ) %&gt;%\n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;%\n  activate(edges) %&gt;%\n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;%\n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) +\n\n  geom_node_point(aes(\n    fill = WSJ,\n    size = size\n  ), shape = 21) +\n\n  geom_node_text(aes(label = media), repel = TRUE) +\n\n  scale_fill_manual(\"WSJ Neighbours\",\n    values = c(\"grey\", \"gold\"),\n    guide = guide_legend(\n      override.aes =\n        list(\n          pch = 21,\n          size = 5\n        )\n    )\n  ) +\n  scale_edge_colour_manual(\"WSJ Links\",\n    values = c(\"grey\", \"orange\")\n  ) +\n  scale_size(\"Degree\", range = c(2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(\n    shape = \"none\", fill = \"none\" # , colour = \"none\"\n  )"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#highlight-neighbours",
    "href": "content/projects/Modules/R/tidygraph/index.html#highlight-neighbours",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Highlight Neighbours",
    "text": "Highlight Neighbours\nOr we can highlight the immediate neighbors of a vertex, say WSJ. The neighbors function finds all nodes one step out from the focal actor. To find the neighbors for multiple nodes, use adjacent_vertices(). To find node neighborhoods going more than one step out, use function ego() with parameter order set to the number of steps out to go from the focal node(s).\n\nneigh.nodes &lt;- neighbors(net, V(net)[media == \"Wall Street Journal\"], mode = \"out\")\n\n# Set colors to plot the neighbors:\nvcol[neigh.nodes] &lt;- \"#ff9d00\"\nplot(net, vertex.color = vcol)\n\n\n\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"Wall Street Journal\") %&gt;%\n  select(node_id) %&gt;%\n  as_vector()\n\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(\n    wsj_adjacent = node_is_adjacent(\n      to = wsj, mode = \"all\",\n      # remove WSJ from the list!\n      # highlight only the neighbours\n\n      include_to = FALSE\n    ),\n    size = centrality_degree()\n  ) %&gt;%\n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;%\n  activate(edges) %&gt;%\n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;%\n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) +\n\n  geom_node_point(aes(\n    fill = wsj_adjacent,\n    size = size\n  ), shape = 21) +\n\n  geom_node_text(aes(label = media), repel = TRUE) +\n\n  scale_fill_manual(\"WSJ Neighbours\",\n    values = c(\"grey\", \"gold\"),\n    guide = guide_legend(\n      override.aes =\n        list(\n          pch = 21,\n          size = 5\n        )\n    )\n  ) +\n  scale_edge_colour_manual(\"WSJ Links\",\n    values = c(\"grey\", \"orange\")\n  ) +\n  scale_size(\"Degree\", range = c(2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(\n    shape = \"none\", fill = \"none\" # , colour = \"none\"\n  )\n\n\n\n\n\n\n\nAnother way to draw attention to a group of nodes: (This is generally not recommended since, depending on layout, nodes that are not ‘marked’ can accidentally get placed on top of the mark)\n\nplot(net, mark.groups = c(1, 4, 5, 8), mark.col = \"#C5E5E7\", mark.border = NA)\n\n\n\n\n\n\n# Mark multiple groups:\nplot(net,\n  mark.groups = list(c(1, 4, 5, 8), c(15:17)),\n  mark.col = c(\"#C5E5E7\", \"#ECD89A\"), mark.border = NA\n)"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#interactive-plotting-with-tkplot",
    "href": "content/projects/Modules/R/tidygraph/index.html#interactive-plotting-with-tkplot",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Interactive plotting with ‘tkplot’ ——–",
    "text": "——-~~ Interactive plotting with ‘tkplot’ ——–\nR and igraph offer interactive plotting capabilities (mostly helpful for small networks)\n\ntkid &lt;- tkplot(net) # tkid is the id of the tkplot\n\nl &lt;- tkplot.getcoords(tkid) # grab the coordinates from tkplot\nplot(net, layout = l)"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#other-ways-to-represent-a-network",
    "href": "content/projects/Modules/R/tidygraph/index.html#other-ways-to-represent-a-network",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Other ways to represent a network ——–",
    "text": "——-~~ Other ways to represent a network ——–\nOne reminder that there are other ways to represent a network:\n\nHeatmap of the network matrix:\n\n\nnetm &lt;- as_adjacency_matrix(net, attr = \"weight\", sparse = F)\ncolnames(netm) &lt;- V(net)$media\nrownames(netm) &lt;- V(net)$media\n\npalf &lt;- colorRampPalette(c(\"gold\", \"dark orange\"))\n\n# The Rowv & Colv parameters turn dendrograms on and off\nheatmap(netm[, 17:1],\n  Rowv = NA, Colv = NA, col = palf(20),\n  scale = \"none\", margins = c(10, 10)\n)\n\n\n\n\n\n\n\n\nDegree distribution\n\n\ndeg.dist &lt;- degree_distribution(net, cumulative = T, mode = \"all\")\n# degree is available in `sna` too\nplot(x = 0:max(igraph::degree(net)), y = 1 - deg.dist, pch = 19, cex = 1.4, col = \"orange\", xlab = \"Degree\", ylab = \"Cumulative Frequency\")\n\n\n\n\n\n\n# Using Tidygraph\n# https://stackoverflow.com/questions/18356860/cumulative-histogram-with-ggplot2\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(degree = centrality_degree(mode = \"all\")) %&gt;%\n  as_tibble() %&gt;%\n  ggplot(aes(x = degree, y = stat(count))) +\n  # geom_histogram(aes(y = cumsum(..count..)), binwidth = 1) +\n  stat_bin(aes(y = cumsum(after_stat(count))),\n    binwidth = 1, # Ta-Da !!\n    geom = \"point\", color = \"orange\", size = 5\n  )"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#plotting-two-mode-networks",
    "href": "content/projects/Modules/R/tidygraph/index.html#plotting-two-mode-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "4. Plotting two-mode networks",
    "text": "4. Plotting two-mode networks\n\nhead(nodes2)\n\n\n  \n\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\nnet2\n\nIGRAPH a6d0165 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from a6d0165 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\nplot(net2)\n\n\n\n\n\n\n\nThis time we will make nodes look different based on their type. Media outlets are blue squares, audience nodes are orange circles:\n\nV(net2)$color &lt;- c(\"steel blue\", \"orange\")[V(net2)$type + 1]\nV(net2)$shape &lt;- c(\"square\", \"circle\")[V(net2)$type + 1]\n\n# Media outlets will have name labels, audience members will not:\nV(net2)$label &lt;- \"\"\nV(net2)$label[V(net2)$type == F] &lt;- nodes2$media[V(net2)$type == F]\nV(net2)$label.cex &lt;- .6\nV(net2)$label.font &lt;- 2\n\nplot(net2, vertex.label.color = \"white\", vertex.size = (2 - V(net2)$type) * 8)\n\n\n\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n\n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n\n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n\n  scale_size_manual(values = c(10, 4), guide = \"none\")\n\n\n\n\n\n\n\nigraph has a built-in bipartite layout, though it’s not the most helpful:\n\nplot(net2, vertex.label = NA, vertex.size = 7, layout = layout_as_bipartite)\n\n\n\n\n\n\n# using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(., layout = \"igraph\", algorithm = \"bipartite\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n\n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n\n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n\n  scale_size_manual(values = c(10, 4), guide = \"none\")\n\n\n\n\n\n\n\n\nUsing text as nodes:\n\n\npar(mar = c(0, 0, 0, 0))\nplot(net2,\n  vertex.shape = \"none\", vertex.label = nodes2$media,\n  vertex.label.color = V(net2)$color, vertex.label.font = 2,\n  vertex.label.cex = .95, edge.color = \"gray70\", edge.width = 2\n)\n\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(\n    end_cap = circle(.4, \"cm\"),\n    start_cap = circle(0.4, \"cm\")\n  ) +\n  # geom_node&gt;point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = media, colour = type), size = 4) +\n\n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 4))\n  ) +\n\n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n\n  scale_size_manual(values = c(10, 4), guide = \"none\")\n\n\n\n\n\n\n\n\nUsing images as nodes You will need the ‘png’ package to do this:\n\n\n# install.packages(\"png\")\nlibrary(\"png\")\n\nimg.1 &lt;- readPNG(\"./images/news.png\")\nimg.2 &lt;- readPNG(\"./images/user.png\")\n\nV(net2)$raster &lt;- list(img.1, img.2)[V(net2)$type + 1]\n\npar(mar = c(3, 3, 3, 3))\n\nplot(net2,\n  vertex.shape = \"raster\", vertex.label = NA,\n  vertex.size = 16, vertex.size2 = 16, edge.width = 2\n)\n\n\n# By the way, you can also add any image you want to any plot. For example, many #network graphs could be improved by a photo of a puppy carrying a basket full of kittens.\nimg.3 &lt;- readPNG(\"./images/puppy.png\")\nrasterImage(img.3, xleft = -1.7, xright = 0, ybottom = -1.2, ytop = 0)\n\n\n\n\n\n\n# The numbers after the image are coordinates for the plot.\n# The limits of your plotting area are given in par()$usr\n\n\n# Using ~~tidygraph~~ visNetwork\n# See this cheatsheet:\n# system.file(\"fontAwesome/Font_Awesome_Cheatsheet.pdf\", package = \"visNetwork\")\nlibrary(visNetwork)\n\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  # visNetwork needs a \"group\" variable for grouping...\n  mutate(group = as.character(type)) %&gt;%\n  visIgraph(.) %&gt;%\n  visGroups(\n    groupname = \"FALSE\", shape = \"icon\",\n    icon = list(code = \"f26c\", size = 75, color = \"orange\")\n  ) %&gt;%\n  visGroups(\n    groupname = \"TRUE\", shape = \"icon\",\n    icon = list(code = \"f007\", size = 75)\n  ) %&gt;%\n  addFontAwesome()\n\n\n\n\n\nWe can also generate and plot bipartite projections for the two-mode network : (co-memberships are easy to calculate by multiplying the network matrix by its transposed matrix, or using igraph’s bipartite.projection function)\n\nnet2.bp &lt;- bipartite.projection(net2)\n\n# We can calculate the projections manually as well:\nas_incidence_matrix(net2) %*% t(as_incidence_matrix(net2))\n\n    s01 s02 s03 s04 s05 s06 s07 s08 s09 s10\ns01   3   0   0   0   0   0   0   0   0   1\ns02   0   3   0   0   0   0   0   0   1   0\ns03   0   0   4   1   0   0   0   0   1   0\ns04   0   0   1   3   1   0   0   0   0   1\ns05   0   0   0   1   3   1   0   0   0   1\ns06   0   0   0   0   1   3   1   1   0   0\ns07   0   0   0   0   0   1   3   1   0   0\ns08   0   0   0   0   0   1   1   4   1   0\ns09   0   1   1   0   0   0   0   1   3   0\ns10   1   0   0   1   1   0   0   0   0   2\n\nt(as_incidence_matrix(net2)) %*% as_incidence_matrix(net2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\nU01   2   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\nU02   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU03   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU04   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU05   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU06   0   0   0   0   0   2   1   1   1   0   0   0   0   0   0   0   0   0   1\nU07   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU08   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU09   0   0   0   0   0   1   1   1   2   1   1   0   0   0   0   0   0   0   0\nU10   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\nU11   1   0   0   0   0   0   0   0   1   1   3   1   1   0   0   0   0   0   0\nU12   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\nU13   0   0   0   0   0   0   0   0   0   0   1   1   2   1   0   0   1   0   0\nU14   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   1   1   0   0\nU15   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0\nU16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   2   1   1   1\nU17   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   2   1   1\nU18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1\nU19   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   1   2\nU20   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1\n    U20\nU01   0\nU02   0\nU03   0\nU04   1\nU05   1\nU06   1\nU07   0\nU08   0\nU09   0\nU10   0\nU11   0\nU12   0\nU13   0\nU14   0\nU15   0\nU16   0\nU17   0\nU18   0\nU19   1\nU20   2\n\npar(mfrow = c(1, 2))\n\nplot(\n  net2.bp$proj1,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[!is.na(nodes2$media.type)]\n)\n\nplot(\n  net2.bp$proj2,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[is.na(nodes2$media.type)]\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# Calculate projections and add attributes/labels\nproj1 &lt;-\n  as_incidence_matrix(net2) %*% t(as_incidence_matrix(net2)) %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\nproj2 &lt;-\n  t(as_incidence_matrix(net2)) %*% as_incidence_matrix(net2) %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\n\np1 &lt;- proj1 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(size = 6, colour = \"orange\") +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np2 &lt;- proj2 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(\n    aes(colour = media.type),\n    size = 6,\n    shape = 15,\n    colour = \"dodgerblue\"\n  ) +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np1 + p2"
  },
  {
    "objectID": "content/projects/Modules/R/tidygraph/index.html#plotting-multiplex-networks",
    "href": "content/projects/Modules/R/tidygraph/index.html#plotting-multiplex-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "5. Plotting multiplex networks",
    "text": "5. Plotting multiplex networks\nIn some cases, the networks we want to plot are multigraphs: they can have multiple edges connecting the same two nodes. A related concept, multiplex networks, contain multiple types of ties – e.g. friendship, romantic, and work relationships between individuals.\nIn our example network, we also have two tie types: hyperlinks and mentions. One thing we can do is plot each type of tie separately:\n\nE(net)$width &lt;- 2\nplot(\n  net,\n  edge.color = c(\"dark red\", \"slategrey\")[(E(net)$type == \"hyperlink\") +\n    1],\n  vertex.color = \"gray40\",\n  layout = layout_in_circle,\n  edge.curved = .3\n)\n\n\n\n\n\n\n# Another way to delete edges using the minus operator:\nnet.m &lt;- net - E(net)[E(net)$type == \"hyperlink\"]\nnet.h &lt;- net - E(net)[E(net)$type == \"mention\"]\n\n# Plot the two links separately:\npar(mfrow = c(1, 2))\n\nplot(net.h,\n  vertex.color = \"orange\",\n  layout = layout_with_fr,\n  main = \"Tie: Hyperlink\"\n)\nplot(net.m,\n  vertex.color = \"lightsteelblue2\",\n  layout = layout_with_fr,\n  main = \"Tie: Mention\"\n)\n\n\n\n\n\n\n\n\nMake sure the nodes stay in the same place in both plots:\n\n\npar(mfrow = c(1, 2), mar = c(1, 1, 4, 1))\n\nl &lt;- layout_with_fr(net)\nplot(net.h,\n  vertex.color = \"orange\",\n  layout = l,\n  main = \"Tie: Hyperlink\"\n)\nplot(net.m,\n  vertex.color = \"lightsteelblue2\",\n  layout = l,\n  main = \"Tie: Mention\"\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\n\nlayout &lt;- layout_in_circle(net)\np1 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  activate(edges) %&gt;%\n  filter(type == \"hyperlink\") %&gt;%\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size),\n    shape = 21,\n    fill = \"orange\"\n  ) +\n  scale_size(range = c(2, 12)) +\n  labs(title = \"Tie: Hyperlink\") +\n  theme(\n    aspect.ratio = 1, ,\n    legend.position = \"bottom\"\n  )\n\np2 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  activate(edges) %&gt;%\n  filter(type == \"mention\") %&gt;%\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size),\n    shape = 21,\n    fill = \"lightsteelblue2\"\n  ) +\n  scale_size(range = c(2, 12)) +\n  labs(title = \"Tie: Mention\") +\n  theme(aspect.ratio = 1, legend.position = \"bottom\")\n\nwrap_plots(p1, p2, guides = \"collect\") &\n  # note this \"pipe\" for patchwork!\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nIn our example network, we don’t have node dyads connected by multiple types of connections (we never have both a ‘hyperlink’ and a ‘mention’ tie between the same two news outlets) – however that could happen.\nNote: See the edges between s03 and s10…these are in opposite directions. So no dyads.\n\nlayout &lt;- layout_in_circle(net)\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05, aes(colour = type)) +\n  geom_node_point(aes(size = size),\n    shape = 21,\n    fill = \"orange\"\n  ) +\n  geom_node_text(aes(label = id), repel = TRUE) +\n  scale_size(range = c(2, 12)) +\n  labs(title = \"Tie: Hyperlink\") +\n  theme(\n    aspect.ratio = 1, ,\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\nOne challenge in visualizing multiplex networks is that multiple edges between the same two nodes may get plotted on top of each other in a way that makes them impossible to distinguish. For example, let us generate a simple multiplex network with two nodes and three ties between them:\n\nmultigtr &lt;- graph(edges = c(1, 2, 1, 2, 1, 2), n = 2)\n\nl &lt;- layout_with_kk(multigtr)\n\n# Let's just plot the graph:\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = 0.1,\n  layout = l\n)\n\n\n\n\n\n\n# Using tidygraph\nmultigtr %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(edges) %&gt;%\n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;%\n  ggraph(., layout = l) +\n  geom_edge_arc(strength = 0.1, aes(colour = edge_col)) +\n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nBecause all edges in the graph have the same curvature, they are drawn over each other so that we only see the last one. What we can do is assign each edge a different curvature. One useful function in ‘igraph’ called curve_multiple() can help us here. For a graph G, curve.multiple(G) will generate a curvature for each edge that maximizes visibility.\n\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = curve_multiple(multigtr),\n  layout = l\n)\n\n\n\n\n\n\n\n\nmultigtr %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(edges) %&gt;%\n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;%\n  ggraph(., layout = l) +\n  geom_edge_fan(strength = 0.1, aes(colour = edge_col), width = 2) +\n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAnd that is the end of this reoworked tutorial! Hope you enjoyed it and found it useful!!"
  },
  {
    "objectID": "content/projects/Modules/SMI/Resources-For-Order-and-Chaos/index.html",
    "href": "content/projects/Modules/SMI/Resources-For-Order-and-Chaos/index.html",
    "title": "Resources for Order and Chaos",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#let-us-start-with-some-pictures",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#let-us-start-with-some-pictures",
    "title": "Teaching in Low Resource Environments",
    "section": "Let us start with some pictures!",
    "text": "Let us start with some pictures!"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#the-govt-school-system-in-blr",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#the-govt-school-system-in-blr",
    "title": "Teaching in Low Resource Environments",
    "section": "The Govt School System in BLR",
    "text": "The Govt School System in BLR"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#organization",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#organization",
    "title": "Teaching in Low Resource Environments",
    "section": "Organization",
    "text": "Organization\n\n\n\n9 Blocks\n4 South + Anekal\n4 North\nWe are sitting in North-4\nThe nearest school is GKLPS Gantiganahalli\n\n\n\n1400+ Primary schools\nMedium of Instruction: Kannada (1204), Urdu (178), Tamil (30), Telugu(8)\nServes over 250,000 children"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#so-what-are-the-challenges",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#so-what-are-the-challenges",
    "title": "Teaching in Low Resource Environments",
    "section": "So What are the Challenges?",
    "text": "So What are the Challenges?\nInfrastructure\n\n\n\nVery few Rooms\nCleanliness, Hygiene\nPlayground?\nNo Security: Public Access after school hours\n\n\n\nComputers?\nStorage?\nReliable Electricity, Wiring, Grounding?\nDrinking Water, Toilets\n\n~ 75% schools had e.coli bacteria in drinking water \n\n\nBut there is a wide variety here and we must not generalize!"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#so-what-are-the-challenges-1",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#so-what-are-the-challenges-1",
    "title": "Teaching in Low Resource Environments",
    "section": "So What are the Challenges?",
    "text": "So What are the Challenges?\nPersonnel\n\n\n\nSome HMs, Teachers are unsympathetic\nSchool Dept Mgmt Structure\n\nMany idlers and politickers\n\n“PT Masters”. Nuff said.\nSome do not show up on time\n\n\n\nSome astonishingly committed Teachers\nSome very creative Teachers (Maths Sir at Kaggalipura)\nSome travel astonishing distances to get to school\n\nEpic commutes (GKLPS Ragihalli)!!"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#aksharas-library-system",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#aksharas-library-system",
    "title": "Teaching in Low Resource Environments",
    "section": "Akshara’s Library System",
    "text": "Akshara’s Library System\n\n\n\n400 Hubs\n1000 Spokes\nAll mediums of Instruction\n~ 600 Akshara-employed staff\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nHub1\n\nHub1\n\n\n\nHub2\n\nHub2\n\n\n\nspoke0\n\nSpoke\n\n\n\nspoke0--Hub1\n\n\n\n\nspoke1\n\nSpoke\n\n\n\nspoke1--Hub1\n\n\n\n\nspoke2\n\nSpoke\n\n\n\nspoke2--Hub1\n\n\n\n\nspoke3\n\nSpoke\n\n\n\nspoke3--Hub2\n\n\n\n\nspoke4\n\nSpoke\n\n\n\nspoke4--Hub2\n\n\n\n\nspoke5\n\nSpoke\n\n\n\nspoke5--Hub2\n\n\n\n\nspoke6\n\nSpoke\n\n\n\nspoke6--Hub2\n\n\n\n\n\n\n\nFigure 1: Library Hub Spoke Model\n\n\n\n\n\n\n\n\n\nSpokes and Hubs may have different Mediums of Instructions\nSpokes and Hubs are not necessarily, but sometimes co-located"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#components-of-the-akshara-library-programme",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#components-of-the-akshara-library-programme",
    "title": "Teaching in Low Resource Environments",
    "section": "Components of the Akshara Library Programme",
    "text": "Components of the Akshara Library Programme\n\n\n\nGetting 400+ rent-free rooms from the School Dept\nHub-Spoke Allocation (Travelling Salesman Problem)\nContent Identification (Books, Languages)\nContent Allocation\nContent Management\n\n\n\nHiring 600+ people\nTraining in three Languages\nPerformance Monitoring\nAdvocacy and Reporting\nCompetition with other NGOs…\n\n\n\n\n\nWe were 5 Designers in the Resource Team\nAnd sadly, some internal competition too.."
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#components-of-the-akshara-library-programme-1",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#components-of-the-akshara-library-programme-1",
    "title": "Teaching in Low Resource Environments",
    "section": "Components of the Akshara Library Programme",
    "text": "Components of the Akshara Library Programme\nThe “Creative Stuff”\n\n\n\nRPGs based on Books\nUsing ICT in the Libraries\nUsing LEGO in the Libraries\nResearch Experiment ( RCT )\nMay I say this? POSH Training\n\n\n\nCreating Volunteer Events\nCreating Community Festivals\n“Designers at School” Competition\nPromoting and Testing for Creativity\n\n\n\n\n\nWhere do you think Play and Invent came from?\nAnd Community and Practices…"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#who-helped-us",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#who-helped-us",
    "title": "Teaching in Low Resource Environments",
    "section": "Who Helped Us?",
    "text": "Who Helped Us?\n\n\nPartners\n\nDepartment of Public Instruction\nDonors:\n\n\nUnited Way India\n\n\nING Vysya\nInventure Academy\n40K Foundation (Sydney, AU)\n\n\n\nVolunteers\n\n\n\n\nING Vysya\n\nFIS Global"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#what-did-the-libraries-look-like",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#what-did-the-libraries-look-like",
    "title": "Teaching in Low Resource Environments",
    "section": "What Did the Libraries Look Like?",
    "text": "What Did the Libraries Look Like?"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-pictures-1",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-pictures-1",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG: Pictures-1",
    "text": "RPG: Pictures-1\n\n\n\n\n\n\n\n\n\nBuying and Selling\nProfit and Loss\nSet, Groups, Taking Common…"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-pictures-2",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-pictures-2",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG: Pictures-2",
    "text": "RPG: Pictures-2\n\n\n\n\n\n\n\n\n\nSolar System, Eclipses…\nTime Zones\nRotation , Revolution of Planets"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Method",
    "text": "RPG Design Method\n\n\nStep#1: Book Mining\n\n\n\nRead the book aloud!\nFind Characters, Events, Places, Emotions…\nDocument these for each Book in a searchable Excel (Horrors!)\n\n\n\n\n\nThis was the template for the BookFace Student Review Activity…"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-1",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-1",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Method",
    "text": "RPG Design Method\n\n\nStep#2: Curriculum Mining\n\n\n\nCurriculum is specified in terms of Minimum Learning Levels (MLLs)\nEach Subject, each class\nShort Crisp phrases or sentences:\n\nMath in real life\nSimple Bills\nUse Road Signs\nTraffic Rules\n\n\n\n\n\n\nAbout 15 Groups of MLLs\nMined about 100 books across all Reading Levels\nO&C at SMI uses Book Mining (Hamlet, The Three Musketeers.. )"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-2",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-2",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Method",
    "text": "RPG Design Method\n\n\nStep3: Socratic Questions to Start\n\nQuestions(3-4) bring a specific aspect of the book into focus\nBook Mining shows which aspects to focus on and to develop questions on\nAllow one to depart from the specifics of book and go more general\nGeneral Ideas can motivate an Activity\n\n\n\nE.g. “The Village Fair” → Things available in a fair → Buy and Sell → Like things unlike things"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-3",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-3",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Method",
    "text": "RPG Design Method\n\n\nStep4: Create Role-Play-Game\n\nBook Mining done earlier + Questions created above\nBrainstorming, Word-Association Games\nClues from Miming and Dumb Charades\nUsing modifications of common childhood games\nGoogle News, You Tube videos\nPublic events, News Stories and TV advertisements provide a fertile ground for Activity creation.\n\n\n\nEg: “Grandpa Fish and the Radio” → methods of communication → TV, Radio, Phone, Post, Telegram, Visual Communication Signs\n\n\n\nActivity was “bought” from Akshara by Karadi Tales…Hah!"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-4",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-method-4",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Method",
    "text": "RPG Design Method\n\n\nStep5: Verification: The Right Questions at the End\n\nGo from General to Specific\nMap the Role Play learnings / concepts back to events and characters in the book\nSocratic Questions(3-4) as to how the role play relates to the book\n\n\n\nE.g “Ruby Red Rose Red”:\n\nHow did Raju share his fruit?\nHow did they cut it?\nWhich fruit in the book already nicely divided up? ( Fractions activity with Origami)"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-process-metaphor",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#rpg-design-process-metaphor",
    "title": "Teaching in Low Resource Environments",
    "section": "RPG Design Process Metaphor",
    "text": "RPG Design Process Metaphor\n\nThink of the RPG Design Process as a Spindle shape…\nNarrow to Broad: Book -&gt; Questions -&gt; Game: Specific to General Concept\nBroad to Narrow: Game -&gt; Questions -&gt; Book: General to Specific\nBook is “Specific”\nConcept ( i.e. MLL) is “General”\nNeed to leave the specific of the book to the general of the concept and return."
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#what-did-we-achieve",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#what-did-we-achieve",
    "title": "Teaching in Low Resource Environments",
    "section": "What did we achieve?",
    "text": "What did we achieve?\n\n\n\nSet Theory with Class 3 kids!! \nRayleigh Scattering with only body as props\nSolar System and Eclipses\nTime Zones -…..\n\n\n\nTraffic\nMath in Daily Life\nShapes, Sizes and Objects\nCommunication Tech and How it works\n“Citizen” Systems and Services ( Police / Ration Shops…)\n\n\n\n\n\nLanguage: Kan / Urd / Tam / Tel\nEVS: Geography, History, Civics\nMaths"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#sample-rpg-document",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#sample-rpg-document",
    "title": "Teaching in Low Resource Environments",
    "section": "Sample RPG Document",
    "text": "Sample RPG Document\n\n\n\nYup it is the one about Rayleigh Scattering!\n\n  Download PDF File\n   \n    Unable to display PDF file. Download instead."
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activity-design-method",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activity-design-method",
    "title": "Teaching in Low Resource Environments",
    "section": "Lego Activity Design Method",
    "text": "Lego Activity Design Method\n\n\n\nVery Similar Process with Book Mining\nActivity Used Lego Model building before reading the Book\nModels first, then Socratic Questions\nThen read the Book and more Socratic Questions\n\n\n\nEven more free and spontaneous\nRPG had a “process” and “steps”;\nWith LEGO, there was more freedom up front in model-making\nMapping the Models to Book was documented for the Librarian"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activity-pictures",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activity-pictures",
    "title": "Teaching in Low Resource Environments",
    "section": "Lego Activity Pictures",
    "text": "Lego Activity Pictures"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activities-document",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-activities-document",
    "title": "Teaching in Low Resource Environments",
    "section": "Lego Activities Document",
    "text": "Lego Activities Document"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#a-lego-community-festival",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#a-lego-community-festival",
    "title": "Teaching in Low Resource Environments",
    "section": "A Lego Community Festival",
    "text": "A Lego Community Festival"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-habba-in-the-press",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#lego-habba-in-the-press",
    "title": "Teaching in Low Resource Environments",
    "section": "Lego Habba in the Press",
    "text": "Lego Habba in the Press\n\nhttps://akshara.org.in/lego-habba-begins-with-a-bang/\nhttps://akshara.org.in/lego-habba-2012-a-big-success/"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#using-edubuntu-linux-in-the-libraries",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#using-edubuntu-linux-in-the-libraries",
    "title": "Teaching in Low Resource Environments",
    "section": "Using Edubuntu Linux in the Libraries",
    "text": "Using Edubuntu Linux in the Libraries"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#what-does-edubuntu-offer",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#what-does-edubuntu-offer",
    "title": "Teaching in Low Resource Environments",
    "section": "What Does Edubuntu Offer?",
    "text": "What Does Edubuntu Offer?\n\n\nGames\n\nTuxMath\nStellarium\nCelestia\n\n\nRegular Stuff\n\nOffice Suite\nBrowser\nAudio and Movie Player with Codecs"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#what-does-edubuntu-offer-1",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#what-does-edubuntu-offer-1",
    "title": "Teaching in Low Resource Environments",
    "section": "What Does Edubuntu Offer?",
    "text": "What Does Edubuntu Offer?\n\nCelestia: a 3D Space Simulator Celestia lets you explore our universe in three dimensions.\n\nCelestia simulates many different types of celestial objects. From planets and moons to star clusters and galaxies, you can visit every object in the expandable database and view it from any point in space and time. The position and movement of solar system objects is calculated accurately in real time at any rate desired."
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#why-edubuntu",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#why-edubuntu",
    "title": "Teaching in Low Resource Environments",
    "section": "Why Edubuntu?",
    "text": "Why Edubuntu?\n\n\n\nFree!! Like Sunshine and Fresh Air!\nResistant to Power Failure\nResistant to Viruses \nLots of Games and Software\n\n\n\nDoes not need to be installed!!\nCan boot from DVD/USB or even from a local network\nEdubuntu LTSP Project\n\ncan have a lot of very old and simple desktops function as clients\nOne machine acts as server\nhttps://www.eifl.net/system/files/resources/201609/ltspguide-english.pdf"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#so-how-did-we-resolve-the-challenges",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#so-how-did-we-resolve-the-challenges",
    "title": "Teaching in Low Resource Environments",
    "section": "So How did We Resolve the Challenges?",
    "text": "So How did We Resolve the Challenges?\n\n\n\nDesigning Activities is hard, so it was done centrally\nLibrarians were trained on execution only\nSome Librarians were involved in “user testing” and even conceptualization stage\nTime Tables so as to minimize commuting for Librarians\nEven designed a Akshara Librarians Bag! (Didn’t work…)\n\n\n\nRejuvenated over 400 computers with Edubuntu\nEven left Edubuntu DVDs in Libraries in case\nTrained Librarians in Edubuntu + Activities"
  },
  {
    "objectID": "content/projects/Modules/SMI/project-talk-2024/index.html#the-end",
    "href": "content/projects/Modules/SMI/project-talk-2024/index.html#the-end",
    "title": "Teaching in Low Resource Environments",
    "section": "The End",
    "text": "The End\n Thank You! Questions?\n arvind.venkatadri@gmail.com"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, and the Great Bubble Barrier.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "A TRIZ Analysis of the Dyson O-Wind Generator",
    "text": "A TRIZ Analysis of the Dyson O-Wind Generator\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nIn the video itself, we heard about how electrical power consumption centers are the urban areas and these are far away from the generation sites. This leads to capital costs in HT Transmission equipment; we go to HT transmission to reduce losses on the way. This is already a Contradiction, which we might solve using Segmentation to arrive at Local Generation of Power. Local generation is a good idea to reduce these costs. This leads easily to Solar Panels on rooftops for example. Again while this may be cheaper than the electrical distribution system, it still uses a fair bit of capex and space and is centralized per building. Can we take Segmentation even further and think of a hyper-local household-based power generation unit, using the Wind?\nWhat would be the problems with using Wind based power generation around the home? Here below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem. Let us state at some of our Problems: I have marked some of these with question marks since I am using imagination here and not direct primary research or information to formulate these. Note that some these may sound naive, but that is exactly way to start!\n\nI would like to have access my generator, but it needs to be not too close to the walls for it to harness the wind.\nHow to tap the power from the generator? What if the connection wires get twisted?\nDo I need a conventional Commutator? Won’t that be heavy?\nWhat voltage and current will I get? Will it be compatible with my 230V AC mains?\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state perhaps the most interesting problem (to me!) that the inventors have solved as demonstrated in the video above. We will state this as an Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\nNoteAdministrative Contradiction\n\n\n\nAC: Winds help to generate power by making something rotate, but winds can change direction and slow down the existing rotation.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\nImportantIdeal Final Result\n\n\n\nTorque must be in one direction only (irrespective of wind direction)\n\n\n\n\n\n\n\n\nCautionUnidirectional Assumption\n\n\n\nI have made a strong assumption here about the the unidirectional movement: the main intent is for the rotating generator to be able to harness winds from any direction to establish or continue rotation in one direction (CW or CCW). Alternating current power generation is in principle immune to direction of rotation.\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix(PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradiction both ways1:\n\n\n\n\n\n\nNoteTechnical Contradictions\n\n\n\n\n\nTC 1: Improve (15)Force/Torque while not worsening (3)Angle/Length of Moving Object\n\n\nTC 2: Improve (3)Angle/Length of Moving Object while not worsening (15)Force/Torque\n\n\n\n\nAgain we have chosen the TRIZ Parameters based on our IFR. Other metaphoric TRIZ Parameters that may suggest themselves are 12(Duration of Action on a Moving Object), 14(Speed), and (40)Harmful Effects Acting on the System.\nIs there a Physical Contradiction(PC)2 possible here?\n\n\n\n\n\n\nWarningPhysical Contradiction\n\n\n\nThe Rotor must yield and not yield to the Wind at the same time. In other words, the rotor must be “porous and non-porous”3 to the wind at the same time.\n\n\nLet us now apply the TCs to the Contradiction Matrix and obtain the TRIZ Inventive Principles."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC-1:\n\n17(Another Dimension) !!\n4( Asymmetry)\n14(Curvature) !!!\n\n10(Preliminary Action)\nand with TC-2:\n\n3(Local Quality)\n9(Preliminary Anti-Action)\n35(Parameter Change)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Curvature, Another Dimension and Asymmetry. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n(14)Curvature: Hmm…nothing new here, or is there? Of course the rotor has to be curved and kind of sphere-like….\n17(Another Dimension): A near-spherical thing has really only one dimension..the radius. And that points in all directions / dimensions! Should there be changes in radius then? Should the radius change create bumps ( positive change ) or depressions ( negative change?) Should the bump be like a welt, and the depression like a groove? How can a bump or a depression itself be curved, as 14(Curvature) suggests?\n4(Asymmetry): The bumps or depressions…..they have to be asymmetric? So….not like longitudes and nor latitudes, but may be like those great circles.\n\n3(Local Quality): OK, the bumps or depressions are already “local”….can we go further? Here is where I stretch and go hyper-local: Should there be structures on or inside them, like flaps or fins or vanes? How can these be asymmetric, then? By acting like miniature flaps or trapdoors, that yield / fall flat when pushed in one direction and stand up / resist when pushed in the other direction…somewhat like a dog or cat’s fur? Then push and pull work differently…\n\n\n\n\n\n\nTipFrom Flaps to ….Funnels!\n\n\n\nMaking these flaps movable as the above paragraph seems to suggest would probably not be a good idea, from an engineering standpoint. But once we have the image of wind + flaps / fins / vanes and differences in pressure or movement, the Bernoulli Principle and Venturi effect suggest themselves immediately!! So what could this vane-fin-fur-flap thingy be then? Oh good heavens, a funnel !!!\n\n\n{HappyApple, Public domain, via Wikimedia Commons}\nSo each of those bumps are segmented into funnel-like structures that cause differences in air pressure when the wind blow. These differences are unidirectional and create movement/rotation! And because the bumps are curved along the surface of the sphere, and they are not parallel to one another (asymmetry), at least some of the internal funnels will always be “in the wind” 4, and capable of creating rotation using Bernoulli/Venturi effect!\n\n9(Preliminary Anti-Action): What do we wish to guard against? Counter acting wind forces. Well, the funnel structures work only with wind blowing into the broad opening and so we are fine!\n\nSo finally we could just imagine a spherical object, mounted on a spindle, with spiral arc-like bumps at different places on the surfaces. Within the arc-like bumps are funnel-like structures that create differentials in pressure when subject to the wind, and that creates rotation. Since the funnels are asymmetric by nature, our final rotation is unidirectional. Whew! ( Yes, that “whew” is also very suggestive here 😃!)"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction.\nOur IFR states that we want the rotor to yield one way and to not yield when pushed the other way so it needs to be both hard and soft at the same time. This is a Physical Contradiction! In this case we can easily see and application of Separation in Space and also Separation on Condition. However I think in this case, it would not be easy to arrive at the Solution using just these.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#references",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#references",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "References",
    "text": "References\n\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#footnotes",
    "href": "content/projects/Modules/TRIZ/2023-11-03-Owind/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎\nSo the Rotor must have…holes? How do holes “work in one direction only”? We will see…↩︎\nMathematically, the Wind direction vector will be (nearly) normal to the aperture of some funnel.↩︎"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#introduction",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#introduction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrence-of-arabia-a-summary",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrence-of-arabia-a-summary",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence of Arabia: a Summary",
    "text": "Lawrence of Arabia: a Summary\nThe movie is the story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks. The stellar cast includes Peter O’Toole as Lawrence, Omar Sharif as Ali, Alec Guinness as Prince Feisal, Anthony Quinn as Auda Abu Tayi, Claude Raines as Dryden, and Anthony Quayle as Col. Brighton. The director was David Lean. The editing of the film by Anne Coates is also much admired. ( https://womenfilmeditors.princeton.edu/tag/lawrence-of-arabia/ )\nLawrence is a complex, talented, and yet simple man, who is extremely well read (Greek philosophy and the Koran, for example) and is also an expert in Arab affairs and has considerable skill at map-making. Due to his being interpreted as insolent and insubordinate , he is given a lowly job at the HQ in Cairo. Dryden manages to convince the General that Lawrence should be allowed to go into Arabia and to find out what kind of long-term plans Prince Feisal is making for Arabia.\nHere is the map of the events that are unfolding in the movie at this time.1\n\n\n\n\n\nLawrence encounters Ali in dramatic fashion at the Masturah Well, on the way to meet Feisal, and his Arab guide is shot by Ali, a direct experience for Lawrence of inter-tribe rivalry in Arabia. (Ali is a Harith, and Tafas the guide was a Hashemi). Lawrence peremptorily rejects an offer of help from Ali, and finds his way alone to Wadi Safra, where Feisal is camped. He is met by Col. Brighton as he nears the camp. Both enter camp just in time to witness another bombing raid by Turkish airplanes.\nLater in the meeting with Feisal, Brighton tries to convince Feisal to retreat to Yenbo (Yanbu) and be out of range for the Turks, and where the British Army would supply them, train them to fight against the Turks. Feisal reluctantly accepts this plan, though he would rather the British navy take the port city of Aqaba and supply his army from there. Brighton simply scoffs at that idea, because the Turkish have 12 inch guns at Aqaba and the British have other things to do.\nLawrence has already intrigued Feisal by completing a verse from the Koran as it was being read by Selim, the cleric. At the end of the meeting, Feisal confronts Lawrence alone, as to his intentions in Arabia and finds out, to his astonishment, that Lawrence has his own interpretation of what his tasks and loyalties were, and these did not necessarily coincide with those of Brighton. In fact, Lawrence is not in favour of the Arab Army’s retreat to Yenbo, as it would become one small part of the British Army. As a parting remark, Feisal says to Lawrence that the Arabs need what no man can provide, a miracle.\nHere is the video of that terrific tent meeting scene."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrences-problem",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrences-problem",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence’s Problem",
    "text": "Lawrence’s Problem\nLawrence does not sleep that night. Provoked by Feisal’s parting remark, he sits up all night on a sand dune close to the camp, thinking about how Aqaba could be taken, since he wants the Arabs to continue fighting from where they were, and even advance if possible with British help. His detailed understanding of the Arabian geography, his knowledge of the Aqaba port and its fortifications, all come to the fore here. Aqaba is a port at the head end of a narrow gulf to the east of the Sinai Peninsula.\nIn the early morning, seemingly in a eureka moment, he decides that attacking Aqaba from the landward side would be a good solution, since the guns there could not be turned around.\nHere is Lawrence trying to convince Ali about this plan:\n\n\n\n\nLawrence does not inform Brighton of his plans, nor even Feisal. It is Ali who informs Feisal of this enterprise. Clearly, Lawrence does not consider Brighton as a member of his Field (as defined by Csikszentmihalyi in his Creativity Systems Model), but Feisal is a Field Member to Ali.\nApropos, the act of sitting up all night can be seen as the Incubation and Elaboration stages of the 5 Stages of Creativity from Csikszentmihalyi (Preparation, Incubation, Insight, Elaboration, Execution)."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "A TRIZ Analysis of the Plan to Take Aqaba",
    "text": "A TRIZ Analysis of the Plan to Take Aqaba\nFor a TRIZ workflow, we proceed as follows:\nFirst, using the method described in Open Source TRIZ, (https://www.youtube.com/watch?v=cah0OhCH55k), we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram for this purpose:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the British allies attack Aqaba, they may win, BUT they may lose a few warships. If the Arabs want to attack, they are too small in number and have no warships, and hence their chances of success are very slim. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The Arabs need the British to supply them via Aqaba port. Aqaba has huge guns and they will sink the British ships in that narrow gulf if they try a naval attack. So the Arabs need to take Aqaba without losing British ships.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define the Ideal Final Result:\n\n\nIFR: The Arabs need to attack and take Aqaba port, and the big guns there should have no effect.\n\n\nNote how the tone of this IFR is like a “eat my cake and have it too”. Very typical for IFRs, this impossible-sounding tone!\nLet us take the AC and convert it into a Technical Contradiction(TC). We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can attempt, stating the Contradiction both ways2:\n\n- TC 1: Increase Duration of Action of a Moving Object (12) and not worsen Stability of Objects Composition (21)- TC 2: Increase Stability of Objects Composition (21) and not worsen Duration of Action of a Moving Object (12)\n\nHere we choose these Parameters based on our IFR that the guns at Aqaba should not affect the Arab attack at all. The Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Power(18) or Illumination Intensity(23) to “metaphorize” the effectiveness of the attack, if our imaginations run in that direction. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could even stretch to making a Physical Contradiction(PC)3 happen:\n\n\nPC: The Ships must be near the guns but not be near enough to be shot at. (They must be near and not near at the same time)"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#solving-the-technical-contradiction",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#solving-the-technical-contradiction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\nThe Inventive Principles are:(TC1)\n\nIP 13 (The Other Way Around)\n\nIP 35 (Parameter Change)\n\nIP 24 (Intermediary)\n\nIP 40 (Composite Materials)\n\nand (TC2)\n\nIP 10 (Preliminary Action)\n\nIP 5 (Merging)\n\nIP 35 (Parameter Change)\n\nIP 13 (The Other Way Around)\n\nHow are we to apply these Inventive Principles? Here again is an imaginative exercise as we map these Generalized Solutions back into the Problem at hand:\n\nIP 13: The Other Way Around. Change the Direction? Of what? Two aspects have “direction” as attributes: the attack, and the guns, which Lawrence can’t control. So how does he use this? Not attack by sea? Wait…ATTACK BY LAND!! Change the DIRECTION of Attack! So attack from the other side, the land side!! (We could retrospectively add this parameter to the Ishikawa Diagram too). Will this work? Yes, the guns can’t turn around!!\nIP 35: Parameter Change. But “ships” on land?? Note, the desert is an ocean into which no oar is dipped. Sand and Water are both Resources in the problem, as we have duly noted in the Ishikawa Diagram. So a different kind of ocean and therefore a different kind of ship? At a stretch, we can say the warships of the British Navy are being substituted with the use of ….Camels!! And, metaphorically speaking, it is still an attack using ships….The Ships of the Desert!! So water becomes sand, and the warships become camels using Parameter Change !\nIP 10: Prior Action. How? Lawrence and Ali are far from Aqaba and cannot do anything “in advance”. What could this be?\nIP 5: Merging. However, on the way to Aqaba, Lawrence and Ali must recruit the Howeitat tribe “in advance” of their attack !! As Lawrence tells Ali, If 50 men came out of the Nefud Desert, they might be 50 men other men would join. This is in accordance with what IP 10 is suggesting, to get other tribes to join in, in advance of the attack.\nIP 40: Composite Materials. What object within the situation can we reconstitute with smaller pieces of different types? The British Army…so an army made up of pieces? Yes! The Tribes need to unite into one composite army.\nAnd, instead of large warships, the Arabs switch to a composite force with camels…\n\nSo IP 13 works nicely now, along with IP 35 and IP 40, to give us a camel-borne attack from the landward side. IP 10 also teams up with IP 40 and IP 5 to give the idea of tribe unification.\nAnd so Lawrence and Ali, with the help of Auda Abu Tayi, attack Aqaba port from the landward side by crossing the Nefud desert on camels, and take it! And we have justified their decision using TRIZ !!\nHere is the final solution in action !!\n\n\n\n\n I hope that was as much fun to read as it was for me to write it up !!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#points-to-ponder",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#points-to-ponder",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Points to Ponder",
    "text": "Points to Ponder\n\nDo we each of us need a Dryden to vouch for us and help us get access to the Field?\nDoes TRIZ work in both mundane and industrial contexts? (Yes of course!)\nCan we just take the 40 Inventive Principles directly and throw them at every Problem, without necessarily going through the process of creating Contradictions and IFR? Hipple’s book has a remark in this direction."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#references",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#references",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "References",
    "text": "References\n\nLawrence of Arabia at the Internet Movie Data Base https://www.imdb.com/title/tt0056172/\n\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#footnotes",
    "href": "content/projects/Modules/TRIZ/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#footnotes",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Footnotes",
    "text": "Footnotes\n\nI was not able to ascertain who is the author of this map. I would be happy to write to obtain permission and use it with acknowledgement.↩︎\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/listing.html",
    "href": "content/projects/listing.html",
    "title": "Blogs and Writings on Code, TRIZ and other things",
    "section": "",
    "text": "A Tidygraph version of a Popular Network Science Tutorial\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA TRIZ Analysis of Lawrence of Arabia\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTRIZ – An Inventive Problem-Solving Method\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nData, DataViz, and Stats with the Stars\n\n\n\n\n\nJun 28, 2025\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Blogs and Talks"
    ]
  },
  {
    "objectID": "content/posts/p5.js/index.html",
    "href": "content/posts/p5.js/index.html",
    "title": "Embedding p5.js Sketches in Quarto",
    "section": "",
    "text": "Trying embed p5.js sketches in Quarto."
  },
  {
    "objectID": "content/posts/p5.js/index.html#introduction",
    "href": "content/posts/p5.js/index.html#introduction",
    "title": "Embedding p5.js Sketches in Quarto",
    "section": "",
    "text": "Trying embed p5.js sketches in Quarto."
  },
  {
    "objectID": "content/posts/p5.js/index.html#sketches-to-embed",
    "href": "content/posts/p5.js/index.html#sketches-to-embed",
    "title": "Embedding p5.js Sketches in Quarto",
    "section": "Sketches to Embed",
    "text": "Sketches to Embed\n\nOkati\n\n\n\nTwo-kati\n\n\n\nvanilla kati"
  },
  {
    "objectID": "content/posts/p5.js/index.html#references",
    "href": "content/posts/p5.js/index.html#references",
    "title": "Embedding p5.js Sketches in Quarto",
    "section": "References",
    "text": "References\n\nEmbedding p5.js sketches. https://toolness.github.io/p5.js-widget/#src"
  },
  {
    "objectID": "content/posts/listing.html",
    "href": "content/posts/listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons\n\n\n\n\n\n\nArvind V.\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\nNutshell: Expandable Explanations\n\n\n\n\n\n\nDavid Schoch\n\n\n\n\n\n\n\n\n\n\n\nUsing sketch\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\nDiagrams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbedding p5.js Sketches in Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Fake Data in R\n\n\n\n\n\n\nArvind V\n\n\n\n\n\n\n\n\n\n\n\nMaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnap Slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Posts"
    ]
  },
  {
    "objectID": "content/posts/16-FakeData/index.html",
    "href": "content/posts/16-FakeData/index.html",
    "title": "Generating Fake Data in R",
    "section": "",
    "text": "Often we need to generate fake data for teaching and demo purposes. This post uncovers several different packages for this purpose."
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#introduction",
    "href": "content/posts/16-FakeData/index.html#introduction",
    "title": "Generating Fake Data in R",
    "section": "",
    "text": "Often we need to generate fake data for teaching and demo purposes. This post uncovers several different packages for this purpose."
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#set-up-the-r-packages",
    "href": "content/posts/16-FakeData/index.html#set-up-the-r-packages",
    "title": "Generating Fake Data in R",
    "section": "Set Up the R Packages",
    "text": "Set Up the R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(vtable)\n\n# Generate Data\n# library(simulate) TO BE FOUND AND INSTALLED!!!!\nlibrary(regressinator)\nlibrary(holodeck) # Simulating Multivariate Data\nlibrary(explore)\nlibrary(charlatan)\nlibrary(ids) # animals, adjectives, sentences, and proquints\nlibrary(rcorpora)\nlibrary(simstudy)\nlibrary(faux) # for simulating data for factorial designs\nlibrary(wakefield) # for generating fake data"
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#using-simulate",
    "href": "content/posts/16-FakeData/index.html#using-simulate",
    "title": "Generating Fake Data in R",
    "section": "Using simulate",
    "text": "Using simulate\n\nsim_bernoulli(prob = 0.2, params = NULL, data = df)\nsim_beta(shape1 = 0.2, shape2 = 0.8, params = NULL)"
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#using-regressinator",
    "href": "content/posts/16-FakeData/index.html#using-regressinator",
    "title": "Generating Fake Data in R",
    "section": "Using regressinator",
    "text": "Using regressinator\nhttps://www.refsmmat.com/regressinator/\n\nThe regressinator is a pedagogical tool for conducting simulations of regression analyses and diagnostics. It can:\n\nSimulate populations with predictor variables from arbitrary distributions\nSimulate response variables that are functions of the predictor variables plus error, or are drawn from a distribution related to the predictors\nGiven a model, simulate from the population sampling distribution of that model’s estimates\nGiven a model fit to data, generate new simulated data based on the model fit\nFacilitate lineup plots comparing diagnostics on the fitted model to diagnostics where all model assumptions are met.\n\n\n\nlibrary(regressinator)\n\nlinear_pop &lt;- population(\n  x1 = predictor(\"rnorm\", mean = 4, sd = 10),\n  x2 = predictor(\"runif\", min = 0, max = 10),\n  y = response(\n    0.7 + 2.2 * x1 - 0.2 * x2, # relationship between X and Y\n    family = gaussian(), # link function and response distribution\n    error_scale = 1.5 # sd; errors are scaled by this amount\n  )\n)\n\nIn general, population() defines a population according to the following relationship:\n\\[\nY ∼ Some ~ Distribution\\\\\n\\] \\[\n~g(E[Y | X = x]) = \\mu(x)\\\\\n\\] \\[\nwhere ~ μ(x)=any~function~of~x\\\\\n\\]\nIf family is not specified the default is Gaussian, and the link function g is identity.\nWe can create a population with binary outcomes and a logistic link function:\n\nlogistic_pop &lt;- population(\n  x1 = predictor(\"rnorm\", mean = 0, sd = 10),\n  x2 = predictor(\"runif\", min = 0, max = 10),\n  y = response(0.7 + 2.2 * x1 - 0.2 * x2,\n    family = binomial(link = \"logit\")\n  )\n)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html",
    "href": "content/labs/r-labs/graphics/wizardy.html",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#introduction",
    "href": "content/labs/r-labs/graphics/wizardy.html#introduction",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#goals",
    "href": "content/labs/r-labs/graphics/wizardy.html#goals",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Goals",
    "text": "Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/wizardy.html#setting-up-r-packages",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\nLet’s load up a few packages that we need to start:\n\nlibrary(tidyverse) ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)\nlibrary(scico) ## scico color palettes(http://www.fabiocrameri.ch/colourmaps.php) in R\nlibrary(ggtext) ## add improved text rendering to ggplot2\nlibrary(ggforce) ## add missing functionality to ggplot2\nlibrary(ggdist) ## add uncertainty visualizations to ggplot2\nlibrary(magick) ## load images into R\nlibrary(patchwork) ## combine outputs from ggplot2\nlibrary(kableExtra) ## Produces attractive tables\nlibrary(palmerpenguins)\n\nlibrary(showtext) ## add google fonts to plots\n\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\nshowtext_auto() # set the google fonts as default\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#data",
    "href": "content/labs/r-labs/graphics/wizardy.html#data",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\n\n## Create a nicely formatted table\n## uses `kableExtra` package\n##\npenguins %&gt;%\n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmale\n2007\n\n\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmale\n2007\n\n\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmale\n2007\n\n\nAdelie\nTorgersen\n34.4\n18.4\n184\n3325\nfemale\n2007\n\n\nAdelie\nTorgersen\n46.0\n21.5\n194\n4200\nmale\n2007\n\n\nAdelie\nBiscoe\n37.8\n18.3\n174\n3400\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.7\n18.7\n180\n3600\nmale\n2007\n\n\nAdelie\nBiscoe\n35.9\n19.2\n189\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nBiscoe\n38.8\n17.2\n180\n3800\nmale\n2007\n\n\nAdelie\nBiscoe\n35.3\n18.9\n187\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.6\n18.6\n183\n3550\nmale\n2007\n\n\nAdelie\nBiscoe\n40.5\n17.9\n187\n3200\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.9\n18.6\n172\n3150\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.5\n18.9\n180\n3950\nmale\n2007\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n2007\n\n\nAdelie\nDream\n37.2\n18.1\n178\n3900\nmale\n2007\n\n\nAdelie\nDream\n39.5\n17.8\n188\n3300\nfemale\n2007\n\n\nAdelie\nDream\n40.9\n18.9\n184\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.4\n17.0\n195\n3325\nfemale\n2007\n\n\nAdelie\nDream\n39.2\n21.1\n196\n4150\nmale\n2007\n\n\nAdelie\nDream\n38.8\n20.0\n190\n3950\nmale\n2007\n\n\nAdelie\nDream\n42.2\n18.5\n180\n3550\nfemale\n2007\n\n\nAdelie\nDream\n37.6\n19.3\n181\n3300\nfemale\n2007\n\n\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmale\n2007\n\n\nAdelie\nDream\n36.5\n18.0\n182\n3150\nfemale\n2007\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.0\n18.5\n186\n3100\nfemale\n2007\n\n\nAdelie\nDream\n44.1\n19.7\n196\n4400\nmale\n2007\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n2007\n\n\nAdelie\nDream\n39.6\n18.8\n190\n4600\nmale\n2007\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n2007\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nAdelie\nDream\n42.3\n21.2\n191\n4150\nmale\n2007\n\n\nAdelie\nBiscoe\n39.6\n17.7\n186\n3500\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.1\n18.9\n188\n4300\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n190\n3450\nfemale\n2008\n\n\nAdelie\nBiscoe\n42.0\n19.5\n200\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n34.5\n18.1\n187\n2900\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.4\n18.6\n191\n3700\nmale\n2008\n\n\nAdelie\nBiscoe\n39.0\n17.5\n186\n3550\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.6\n18.8\n193\n3800\nmale\n2008\n\n\nAdelie\nBiscoe\n36.5\n16.6\n181\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n37.6\n19.1\n194\n3750\nmale\n2008\n\n\nAdelie\nBiscoe\n35.7\n16.9\n185\n3150\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.3\n21.1\n195\n4400\nmale\n2008\n\n\nAdelie\nBiscoe\n37.6\n17.0\n185\n3600\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n18.2\n192\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n36.4\n17.1\n184\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.6\n18.0\n192\n3950\nmale\n2008\n\n\nAdelie\nBiscoe\n35.5\n16.2\n195\n3350\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n19.1\n188\n4100\nmale\n2008\n\n\nAdelie\nTorgersen\n35.9\n16.6\n190\n3050\nfemale\n2008\n\n\nAdelie\nTorgersen\n41.8\n19.4\n198\n4450\nmale\n2008\n\n\nAdelie\nTorgersen\n33.5\n19.0\n190\n3600\nfemale\n2008\n\n\nAdelie\nTorgersen\n39.7\n18.4\n190\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n39.6\n17.2\n196\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n2008\n\n\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.8\n18.5\n195\n4250\nmale\n2008\n\n\nAdelie\nTorgersen\n40.9\n16.8\n191\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n37.2\n19.4\n184\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n36.2\n16.1\n187\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n2008\n\n\nAdelie\nTorgersen\n34.6\n17.2\n189\n3200\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmale\n2008\n\n\nAdelie\nTorgersen\n36.7\n18.8\n187\n3800\nfemale\n2008\n\n\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmale\n2008\n\n\nAdelie\nDream\n37.3\n17.8\n191\n3350\nfemale\n2008\n\n\nAdelie\nDream\n41.3\n20.3\n194\n3550\nmale\n2008\n\n\nAdelie\nDream\n36.3\n19.5\n190\n3800\nmale\n2008\n\n\nAdelie\nDream\n36.9\n18.6\n189\n3500\nfemale\n2008\n\n\nAdelie\nDream\n38.3\n19.2\n189\n3950\nmale\n2008\n\n\nAdelie\nDream\n38.9\n18.8\n190\n3600\nfemale\n2008\n\n\nAdelie\nDream\n35.7\n18.0\n202\n3550\nfemale\n2008\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n2008\n\n\nAdelie\nDream\n34.0\n17.1\n185\n3400\nfemale\n2008\n\n\nAdelie\nDream\n39.6\n18.1\n186\n4450\nmale\n2008\n\n\nAdelie\nDream\n36.2\n17.3\n187\n3300\nfemale\n2008\n\n\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmale\n2008\n\n\nAdelie\nDream\n38.1\n18.6\n190\n3700\nfemale\n2008\n\n\nAdelie\nDream\n40.3\n18.5\n196\n4350\nmale\n2008\n\n\nAdelie\nDream\n33.1\n16.1\n178\n2900\nfemale\n2008\n\n\nAdelie\nDream\n43.2\n18.5\n192\n4100\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n192\n3725\nfemale\n2009\n\n\nAdelie\nBiscoe\n41.0\n20.0\n203\n4725\nmale\n2009\n\n\nAdelie\nBiscoe\n37.7\n16.0\n183\n3075\nfemale\n2009\n\n\nAdelie\nBiscoe\n37.8\n20.0\n190\n4250\nmale\n2009\n\n\nAdelie\nBiscoe\n37.9\n18.6\n193\n2925\nfemale\n2009\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n2009\n\n\nAdelie\nBiscoe\n38.6\n17.2\n199\n3750\nfemale\n2009\n\n\nAdelie\nBiscoe\n38.2\n20.0\n190\n3900\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n17.0\n181\n3175\nfemale\n2009\n\n\nAdelie\nBiscoe\n43.2\n19.0\n197\n4775\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n16.5\n198\n3825\nfemale\n2009\n\n\nAdelie\nBiscoe\n45.6\n20.3\n191\n4600\nmale\n2009\n\n\nAdelie\nBiscoe\n39.7\n17.7\n193\n3200\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.2\n19.5\n197\n4275\nmale\n2009\n\n\nAdelie\nBiscoe\n39.6\n20.7\n191\n3900\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.7\n18.3\n196\n4075\nmale\n2009\n\n\nAdelie\nTorgersen\n38.6\n17.0\n188\n2900\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.3\n20.5\n199\n3775\nmale\n2009\n\n\nAdelie\nTorgersen\n35.7\n17.0\n189\n3350\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n2009\n\n\nAdelie\nTorgersen\n36.2\n17.2\n187\n3150\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.7\n19.8\n198\n3500\nmale\n2009\n\n\nAdelie\nTorgersen\n40.2\n17.0\n176\n3450\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.4\n18.5\n202\n3875\nmale\n2009\n\n\nAdelie\nTorgersen\n35.2\n15.9\n186\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n40.6\n19.0\n199\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.8\n17.6\n191\n3275\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.5\n18.3\n195\n4300\nmale\n2009\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n44.1\n18.0\n210\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.5\n17.9\n190\n3325\nfemale\n2009\n\n\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmale\n2009\n\n\nAdelie\nDream\n36.8\n18.5\n193\n3500\nfemale\n2009\n\n\nAdelie\nDream\n37.5\n18.5\n199\n4475\nmale\n2009\n\n\nAdelie\nDream\n38.1\n17.6\n187\n3425\nfemale\n2009\n\n\nAdelie\nDream\n41.1\n17.5\n190\n3900\nmale\n2009\n\n\nAdelie\nDream\n35.6\n17.5\n191\n3175\nfemale\n2009\n\n\nAdelie\nDream\n40.2\n20.1\n200\n3975\nmale\n2009\n\n\nAdelie\nDream\n37.0\n16.5\n185\n3400\nfemale\n2009\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n2009\n\n\nAdelie\nDream\n40.2\n17.1\n193\n3400\nfemale\n2009\n\n\nAdelie\nDream\n40.6\n17.2\n187\n3475\nmale\n2009\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n2009\n\n\nAdelie\nDream\n40.7\n17.0\n190\n3725\nmale\n2009\n\n\nAdelie\nDream\n37.3\n16.8\n192\n3000\nfemale\n2009\n\n\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmale\n2009\n\n\nAdelie\nDream\n39.2\n18.6\n190\n4250\nmale\n2009\n\n\nAdelie\nDream\n36.6\n18.4\n184\n3475\nfemale\n2009\n\n\nAdelie\nDream\n36.0\n17.8\n195\n3450\nfemale\n2009\n\n\nAdelie\nDream\n37.8\n18.1\n193\n3750\nmale\n2009\n\n\nAdelie\nDream\n36.0\n17.1\n187\n3700\nfemale\n2009\n\n\nAdelie\nDream\n41.5\n18.5\n201\n4000\nmale\n2009\n\n\nGentoo\nBiscoe\n46.1\n13.2\n211\n4500\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n16.3\n230\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n48.7\n14.1\n210\n4450\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n47.6\n14.5\n215\n5400\nmale\n2007\n\n\nGentoo\nBiscoe\n46.5\n13.5\n210\n4550\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.4\n14.6\n211\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.7\n15.3\n219\n5200\nmale\n2007\n\n\nGentoo\nBiscoe\n43.3\n13.4\n209\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.8\n15.4\n215\n5150\nmale\n2007\n\n\nGentoo\nBiscoe\n40.9\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.0\n16.1\n216\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n45.5\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.4\n14.6\n213\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n45.8\n14.6\n210\n4200\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.3\n15.7\n217\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n42.0\n13.5\n210\n4150\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.2\n15.2\n221\n6300\nmale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.5\n209\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.7\n15.1\n222\n5350\nmale\n2007\n\n\nGentoo\nBiscoe\n50.2\n14.3\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.5\n14.5\n213\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.3\n15.8\n215\n5050\nmale\n2007\n\n\nGentoo\nBiscoe\n42.9\n13.1\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.1\n15.1\n215\n5100\nmale\n2007\n\n\nGentoo\nBiscoe\n47.8\n15.0\n215\n5650\nmale\n2007\n\n\nGentoo\nBiscoe\n48.2\n14.3\n210\n4600\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.3\n220\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n47.3\n15.3\n222\n5250\nmale\n2007\n\n\nGentoo\nBiscoe\n42.8\n14.2\n209\n4700\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n207\n5050\nfemale\n2007\n\n\nGentoo\nBiscoe\n59.6\n17.0\n230\n6050\nmale\n2007\n\n\nGentoo\nBiscoe\n49.1\n14.8\n220\n5150\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.4\n16.3\n220\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n42.6\n13.7\n213\n4950\nfemale\n2008\n\n\nGentoo\nBiscoe\n44.4\n17.3\n219\n5250\nmale\n2008\n\n\nGentoo\nBiscoe\n44.0\n13.6\n208\n4350\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.7\n15.7\n208\n5350\nmale\n2008\n\n\nGentoo\nBiscoe\n42.7\n13.7\n208\n3950\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n16.0\n225\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.7\n210\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n222\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n43.6\n13.9\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n13.9\n210\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n225\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n44.9\n13.3\n213\n5100\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n15.8\n215\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n46.6\n14.2\n210\n4850\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.5\n14.1\n220\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.1\n14.4\n210\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.1\n15.0\n225\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.4\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.0\n15.4\n220\n5050\nmale\n2008\n\n\nGentoo\nBiscoe\n43.8\n13.9\n208\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n15.0\n220\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n43.2\n14.5\n208\n4450\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.4\n15.3\n224\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.8\n208\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.2\n14.9\n221\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.7\n13.9\n214\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n54.3\n15.7\n231\n5650\nmale\n2008\n\n\nGentoo\nBiscoe\n45.8\n14.2\n219\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.8\n16.8\n230\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n49.5\n16.2\n229\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n43.5\n14.2\n220\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.6\n221\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.8\n217\n5200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.0\n216\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.6\n16.0\n230\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n47.5\n14.2\n209\n4600\nfemale\n2008\n\n\nGentoo\nBiscoe\n51.1\n16.3\n220\n6000\nmale\n2008\n\n\nGentoo\nBiscoe\n45.2\n13.8\n215\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n2008\n\n\nGentoo\nBiscoe\n49.1\n14.5\n212\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.5\n15.6\n221\n5450\nmale\n2009\n\n\nGentoo\nBiscoe\n47.4\n14.6\n212\n4725\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.0\n15.9\n224\n5350\nmale\n2009\n\n\nGentoo\nBiscoe\n44.9\n13.8\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n43.4\n14.4\n218\n4600\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.3\n14.2\n218\n5300\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n14.0\n212\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.1\n17.0\n230\n5550\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n15.0\n218\n4950\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.2\n17.1\n228\n5400\nmale\n2009\n\n\nGentoo\nBiscoe\n45.5\n14.5\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.5\n16.1\n224\n5650\nmale\n2009\n\n\nGentoo\nBiscoe\n44.5\n14.7\n214\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n15.7\n226\n5200\nmale\n2009\n\n\nGentoo\nBiscoe\n49.4\n15.8\n216\n4925\nmale\n2009\n\n\nGentoo\nBiscoe\n46.9\n14.6\n222\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.4\n14.4\n203\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.1\n16.5\n225\n5250\nmale\n2009\n\n\nGentoo\nBiscoe\n48.5\n15.0\n219\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.9\n17.0\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n15.5\n215\n4975\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.1\n15.0\n228\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.8\n16.1\n215\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n41.7\n14.7\n210\n4700\nfemale\n2009\n\n\nGentoo\nBiscoe\n53.4\n15.8\n219\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n43.3\n14.0\n208\n4575\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.1\n15.1\n209\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n50.5\n15.2\n216\n5000\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmale\n2009\n\n\nGentoo\nBiscoe\n43.5\n15.2\n213\n4650\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.5\n16.3\n230\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.1\n16.0\n230\n5850\nmale\n2009\n\n\nGentoo\nBiscoe\n48.8\n16.2\n222\n6000\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n13.7\n214\n4925\nfemale\n2009\n\n\nGentoo\nBiscoe\n46.8\n14.3\n215\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.4\n15.7\n222\n5750\nmale\n2009\n\n\nGentoo\nBiscoe\n45.2\n14.8\n212\n5200\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.9\n16.1\n213\n5400\nmale\n2009\n\n\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n2007\n\n\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n2007\n\n\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n2007\n\n\nChinstrap\nDream\n45.4\n18.7\n188\n3525\nfemale\n2007\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n2007\n\n\nChinstrap\nDream\n45.2\n17.8\n198\n3950\nfemale\n2007\n\n\nChinstrap\nDream\n46.1\n18.2\n178\n3250\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n18.2\n197\n3750\nmale\n2007\n\n\nChinstrap\nDream\n46.0\n18.9\n195\n4150\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n19.9\n198\n3700\nmale\n2007\n\n\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nfemale\n2007\n\n\nChinstrap\nDream\n51.7\n20.3\n194\n3775\nmale\n2007\n\n\nChinstrap\nDream\n47.0\n17.3\n185\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n18.1\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n45.9\n17.1\n190\n3575\nfemale\n2007\n\n\nChinstrap\nDream\n50.5\n19.6\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n50.3\n20.0\n197\n3300\nmale\n2007\n\n\nChinstrap\nDream\n58.0\n17.8\n181\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n46.4\n18.6\n190\n3450\nfemale\n2007\n\n\nChinstrap\nDream\n49.2\n18.2\n195\n4400\nmale\n2007\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n2007\n\n\nChinstrap\nDream\n48.5\n17.5\n191\n3400\nmale\n2007\n\n\nChinstrap\nDream\n43.2\n16.6\n187\n2900\nfemale\n2007\n\n\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmale\n2007\n\n\nChinstrap\nDream\n46.7\n17.9\n195\n3300\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n19.0\n197\n4150\nmale\n2007\n\n\nChinstrap\nDream\n50.5\n18.4\n200\n3400\nfemale\n2008\n\n\nChinstrap\nDream\n49.5\n19.0\n200\n3800\nmale\n2008\n\n\nChinstrap\nDream\n46.4\n17.8\n191\n3700\nfemale\n2008\n\n\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmale\n2008\n\n\nChinstrap\nDream\n40.9\n16.6\n187\n3200\nfemale\n2008\n\n\nChinstrap\nDream\n54.2\n20.8\n201\n4300\nmale\n2008\n\n\nChinstrap\nDream\n42.5\n16.7\n187\n3350\nfemale\n2008\n\n\nChinstrap\nDream\n51.0\n18.8\n203\n4100\nmale\n2008\n\n\nChinstrap\nDream\n49.7\n18.6\n195\n3600\nmale\n2008\n\n\nChinstrap\nDream\n47.5\n16.8\n199\n3900\nfemale\n2008\n\n\nChinstrap\nDream\n47.6\n18.3\n195\n3850\nfemale\n2008\n\n\nChinstrap\nDream\n52.0\n20.7\n210\n4800\nmale\n2008\n\n\nChinstrap\nDream\n46.9\n16.6\n192\n2700\nfemale\n2008\n\n\nChinstrap\nDream\n53.5\n19.9\n205\n4500\nmale\n2008\n\n\nChinstrap\nDream\n49.0\n19.5\n210\n3950\nmale\n2008\n\n\nChinstrap\nDream\n46.2\n17.5\n187\n3650\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n19.1\n196\n3550\nmale\n2008\n\n\nChinstrap\nDream\n45.5\n17.0\n196\n3500\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n17.9\n196\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmale\n2009\n\n\nChinstrap\nDream\n50.1\n17.9\n190\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.0\n19.6\n212\n4300\nmale\n2009\n\n\nChinstrap\nDream\n51.5\n18.7\n187\n3250\nmale\n2009\n\n\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n48.1\n16.4\n199\n3325\nfemale\n2009\n\n\nChinstrap\nDream\n51.4\n19.0\n201\n3950\nmale\n2009\n\n\nChinstrap\nDream\n45.7\n17.3\n193\n3600\nfemale\n2009\n\n\nChinstrap\nDream\n50.7\n19.7\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n42.5\n17.3\n187\n3350\nfemale\n2009\n\n\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmale\n2009\n\n\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nfemale\n2009\n\n\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmale\n2009\n\n\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n2009\n\n\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmale\n2009\n\n\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n2009\n\n\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmale\n2009\n\n\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nfemale\n2009"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up:\n\n## simple plot: data + mappings + geometry\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing ( most of ) the theme-able aspects of a ggplot plot.\nFor more info, type ?theme in your console.\n\n\n\n\n\nggplot Theme Elements\n\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales:\n\nggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(aes(color = body_mass_g),\n    alpha = .6,\n    size = 3.5\n  ) +\n\n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n\n  ## custom colors from the scico package\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n\n  ## custom labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) PLoS ONE\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggtext}\n",
    "text": "Using {ggtext}\n\nFrom Claus Wilke’s website (www.wilkelab.org/ggtext)\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n\nelement_markdown()\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theming command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, strip text\n\n## assign plot to `gt` - we can add new things to this plot later\n## (wrapped in parenthesis so it is assigned and plotted in one step)\n\n(gt &lt;- ggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n\n  # New code starts here: Two Step Procedure with ggtext\n  # 1. Markdown formatting of labels and title, using asterisks\n  # To create italics and bold text in titles\n\n\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\n)\n\n\n\n\n\n\n\n\nelement_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions:\n\n## use HTML syntax to change text color\ngt_mar &lt;- gt +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n## use HTML syntax to change font and text size\ngt_mar +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\nAdding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n## use HTML syntax to add images to text elements\ngt_mar +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\nAnnotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox().\ngeom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\ngt_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = tibble(\n\n      # Three rich text labels, so three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54), y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      lab = c(\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n      ),\n    ),\n\n    # Now pass these data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  rcartocolor::scale_color_carto_d(\n    palette = \"Bold\",\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\nError in loadNamespace(x): there is no package called 'rcartocolor'\n\ngt_rich\n\nError: object 'gt_rich' not found\n\n\nFormatted Text boxes on ggplots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping\n\ngt_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  rcartocolor::scale_color_carto_d(palette = \"Bold\", guide = \"none\") +\n\n  ## add text annotations for each species\n  ## Creating a tibble for the labels!\n  ggtext::geom_richtext(\n    data = tibble(\n      # Three rich text labels\n      # So three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54),\n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      notes = c(\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n      )\n    ),\n\n    # Now pass these data variables as aesthetics\n    aes(\n      x,\n      y,\n      label = notes,\n      color = species,\n      angle = angle\n    ),\n    size = 4,\n    fill = NA,\n    label.color = NA,\n    lineheight = .3\n  ) +\n\n\n  # Now for the Plot Titles and Labels, as before\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\nError in loadNamespace(x): there is no package called 'rcartocolor'\n\ngt_box\n\nError: object 'gt_box' not found\n\n\ngeom_textbox() → formatted text boxes with word wrapping\n\ngt_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"),\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\nError: object 'gt_box' not found"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggforce}\n",
    "text": "Using {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n## plot that we will annotate with ggforce afterwards\n(gf &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  ))\n\nError in loadNamespace(x): there is no package called 'rcartocolor'\n\n\n\n## ellipsoids for all groups\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species),\n    alpha = .15, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nError: object 'gf' not found\n\n\n\n## ellipsoids for specific subset\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n#   coord_cartesian(xlim = c(25, 65), ylim = c(10, 25))\n)\n\nError: object 'gf' not found\n\n\n\n## circles\n(gf +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nError: object 'gf' not found\n\n\n\n## rectangles\n(gf +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nError: object 'gf' not found\n\n\n\nlibrary(concaveman)\n## hull\n(gf +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == \"Gentoo\"),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nError: object 'gf' not found"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "href": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "ggplot tricks",
    "text": "ggplot tricks\n\n(gg0 &lt;-\n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n  scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )\n)\n\n\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\n(gg1b &lt;- gg1 + theme(plot.caption.position = \"panel\"))\n\n\n\n\n\n\n\nLegend Design\n\n(gg2 &lt;- gg1b + theme(legend.position = \"top\"))\n\n\n\n\n\n\nggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\n(gg2b &lt;- gg2 +\n  guides(color = guide_colorbar(\n    title.position = \"top\",\n    title.hjust = .5,\n    barwidth = unit(20, \"lines\"),\n    barheight = unit(.5, \"lines\")\n  )))\n\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg3 &lt;- gg2b +\n  annotation_custom(img, ymin = 18.5, ymax = 30.5, xmin = 55, xmax = 65.5) +\n  labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg3"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {patchwork}\n",
    "text": "Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;%\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;%\n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg4 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) +\n  geom_violin() +\n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg3 | (gg4 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ngg3 / (gg3 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n)) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#resources",
    "href": "content/labs/r-labs/graphics/wizardy.html#resources",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Resources",
    "text": "Resources\n\n\nIntro to R (one of many good online tutorials)\n“R for Data Science” book (open-access)\nggplot2 Book (open-access)\nR Graph Gallery\nSlides of Cedric Scherer’s talk\nExtensive ggplot2 tutorial\n“Evolution of a ggplot” blog post by Cedric Scherer\n\n#TidyTuesday project (#TidyTuesday on Twitter)\n\n#TidyTuesday Contributions by Cedric Scherer incl. all codes\n\nR4DS learning community (huge Slack community for people learning R incl. a mentoring program)\n\nIllustrations by Allison Horst (more general about data and stats + R-related)\nR Packages:\n\nggplot2\nggtext\nggforce\nggdist\nggraph\nggstream\nggbump\ngggibous\nwaffle\ngeofacet\ncartogram\npatchwork\nsf"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html",
    "href": "content/labs/r-labs/graphics/colors.html",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path = \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\nlibrary(scales) # Create special ( % or $ ) scales\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n# library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col\nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/colors.html#setting-up-r-packages",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path = \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\nlibrary(scales) # Create special ( % or $ ) scales\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n# library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col\nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#introduction",
    "href": "content/labs/r-labs/graphics/colors.html#introduction",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#goals",
    "href": "content/labs/r-labs/graphics/colors.html#goals",
    "title": "Lab 05: Colors with Penguins",
    "section": "Goals",
    "text": "Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "title": "Lab 05: Colors with Penguins",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#data",
    "href": "content/labs/r-labs/graphics/colors.html#data",
    "title": "Lab 05: Colors with Penguins",
    "section": "Data",
    "text": "Data\nWe will use the penguins dataset built into the palmerpenguins package. Your should try other datasets too!\nHere is a glimpse of the data:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNote that the unit of observation here is one-row-per-penguin.\nVariables you need for this lab:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "href": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colour vs fill aesthetic",
    "text": "Colour vs fill aesthetic\nFill and colour scales in ggplot2 can use the same palettes. Some shapes such as lines only accept the colour aesthetic, while others, such as polygons, accept both colour and fill aesthetics. In the latter case, the colour refers to the border of the shape, and the fill to the interior.\n\n## A look at all 25 symbols\ndf &lt;- data.frame(x = 1:5,\n                 y = rep(rev(seq(0, 24, by = 5)), each = 5),\n                 z = 1:25)\ns &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_text(aes(label = z, y = y - 1)) +\n  theme_void()\ns + geom_point(aes(shape = z), size = 4) + scale_shape_identity()\n\n\n\n\n\n\n\nAll symbols have a foreground colour, so if we add color = \"navy\", they all are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\")  + scale_shape_identity()\n\n\n\n\n\n\n\nWhile all symbols have a foreground colour, symbols 21-25 also take a background colour (fill). So if we add fill = \"orchid\", only the last row of symbols are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\", fill = \"orchid\")  + scale_shape_identity()"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete vs continuous variables",
    "text": "Discrete vs continuous variables\nWHAT IS THE DIFFERENCE BETWEEN CATEGORICAL, ORDINAL AND INTERVAL VARIABLES?\nIn order to use color with your data, most importantly, you need to know if you’re dealing with discrete or continuous variables."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "href": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "title": "Lab 05: Colors with Penguins",
    "section": "Some Colour Palette Packages in R",
    "text": "Some Colour Palette Packages in R\nWe have the following example packages that offer palettes in R:\n\nRColorBrewer\nwesanderson\npaletteer\ncolorspace\n\nSee Appendix for a detailed graphical analysis of these palette packages."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "href": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colour Palette Types",
    "text": "Colour Palette Types\nThese palettes can be:\n\nSequential (type = “seq”) palettes are suited to ordered data that progress from low to high. Lightness steps dominate the look of these schemes, with light colors for low data values to dark colors for high data values. (for numerical data, that are ordered)\n\n\nDiverging (type = “div”) palettes put equal emphasis on mid-range critical values and extremes at both ends of the data range. The critical class or break in the middle of the legend is emphasized with light colors and low and high extremes are emphasized with dark colors that have contrasting hues.(for numerical data that can be positive or negative, often representing deviations from some norm or baseline)\n\n\nQualitative (type = “qual”) palettes do not imply magnitude differences between legend classes, and hues are used to create the primary visual differences between classes. Qualitative schemes are best suited to representing nominal or categorical data. (for qualitative unordered data)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "href": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "title": "Lab 05: Colors with Penguins",
    "section": "Create a simple set of scatter plots",
    "text": "Create a simple set of scatter plots\nWe will create simple base plots in ggplot and see how we may alter the colour scales using palettes.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\np1 &lt;- penguins %&gt;% \n  drop_na() %&gt;% \n  # pipe data into ggplot\n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = species # COLOUR = DISCRETE/QUAL VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P1: DISCRETE/QUAL Colour Palette\")\n\n\np2 &lt;- \npenguins %&gt;% \n  drop_na() %&gt;% \n  # pipe the data into ggplot, \n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = bill_length_mm # COLOUR = CONT/QUANT VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P2: CONTINUOUS/QUANT Colour Palette\")\n\np1\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\nNote that these use the default colours in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colours for Discrete (QUAL) Variables",
    "text": "Colours for Discrete (QUAL) Variables\nThe commands below are used to fill colours based on Qualitative Variables:\n\nscale_colour/fill_discrete\n\nscale_colour/fill_brewer # RColorBrewer\n….\n\nNow to use these!"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Plotting Colours based on Discrete Variables",
    "text": "Plotting Colours based on Discrete Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete n-Colour palettes from RColorBrewer\n",
    "text": "Discrete n-Colour palettes from RColorBrewer\n\n\nRColorBrewer::brewer.pal.info\n\n\n  \n\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n\np1 +\n  # default palette = \"Blues\"\n  scale_colour_brewer() +\n  labs(title = \"Brewer Palette = Blues\")\n\n\n\n\n\n\np1 +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"Brewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete Colour scales using wesanderson palettes",
    "text": "Discrete Colour scales using wesanderson palettes\n\nwesanderson::wes_palettes %&gt;% names()\n\n [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n[10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n[13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n[16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n[19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n[22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"    \n\n\n\np1 +\n  scale_colour_discrete(type = wes_palette(name = \"GrandBudapest1\",\n                                           n = 3)) +\n  labs(title = \"Wes Anderson Palette: GrandBudapest\")\n\n\n\n\n\n\n# We can also specify colour codes ourselves with scale_x_discrete.\n# Use argument \"values\" instead of \"type\"\nmanual_colours &lt;- c(\"#afc4b8\", \"#f1a4b2\", \"#ffb1e1\") \nmanual_colours\n\n[1] \"#afc4b8\" \"#f1a4b2\" \"#ffb1e1\"\n\np1 +\n  scale_colour_manual(values =  manual_colours) +\n  labs(title = \"Manual Colours\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete n-Colour palettes from RColorBrewer\n",
    "text": "Discrete n-Colour palettes from RColorBrewer\n\n\n# scale_x_brewer() for DISCRETE data\np1 +\n  scale_colour_brewer(palette = \"Spectral\") +\n  \n  labs(title = \"RColorBrewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete Colour scales using paletteer palettes",
    "text": "Discrete Colour scales using paletteer palettes\n\npalettes_d_names\n\n\n  \n\n\npalettes_dynamic_names\n\n\n  \n\n\npaletteer_d(\"dutchmasters::pearl_earring\")\n\n&lt;colors&gt;\n#A65141FF #E7CDC2FF #80A0C7FF #394165FF #FCF9F0FF #B1934AFF #DCA258FF #100F14FF #8B9DAFFF #EEDA9DFF #E8DCCFFF \n\npaletteer_dynamic(\"ggthemes_ptol::qualitative\", n = 3)\n\n&lt;colors&gt;\n#4477AAFF #DDCC77FF #CC6677FF \n\np1 +\n  scale_colour_paletteer_d(\"ggthemes_ptol::qualitative\", \n                           dynamic = TRUE) +\n  \n  labs(title = \"ggThemes Palette: Qualitative\", \n          subtitle = \"\")\n\n\n\n\n\n\n# I like Vermeer's \"Girl with the Pearl Earring\"!\np1 +\n  scale_colour_paletteer_d(\"dutchmasters::pearl_earring\",\n                           dynamic = FALSE) +\n  \n  labs(title = \"Palettes from `paletteer`\", \n          subtitle = \" Palette from Vermeer: Girl with Pearl Earring\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colours for Continuous (QUANT) Variables",
    "text": "Colours for Continuous (QUANT) Variables\nThe commands below are used to fill colours based on Quantitative Variables:\n\n\nscale_colour/fill_gradient (Two colour gradient)\n\nscale_colour/fill_gradient2 (Three colour gradient)\n\nscale_colour/fill_gradientn (Specify Palette, from other packages also, like wesanderson )\n\nscale_colour/fill_distiller (Palettes from RColorBrewer)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Plotting Colours based on Continuous Variables",
    "text": "Plotting Colours based on Continuous Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Two Colour Gradients",
    "text": "Continuous Two Colour Gradients\nCreates a pallete containing continuous shades between two colours:\n\np2 +\n    scale_color_gradient(\n      low = \"yellow\", # Play with this in the chunk below\n      high = \"purple\") + # Play with this in the chnk below\n  \n  labs(title = \"Two Colour Gradients\",\n          subtitle = \"P2: Continuous 2-Colour Pallete\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Three Colour Gradients",
    "text": "Continuous Three Colour Gradients\nSometimes we want a palette this way: a midpoint colour, and colours for the two extremes of a continuous variable:\n\ncolour_midpoint &lt;- mean(penguins$bill_length_mm, \n                         na.rm = TRUE) # remove missing values\n# Struggled all morning on 22 Aug 2020 to get at this ;-D\n\n# Play with the function: 0/mean/median/mode/max/min\n\np2 +\n  scale_colour_gradient2(\n  low = \"brown\", # Play with this in the chunk below\n  mid = \"white\", # Play with this in the chunk below\n  high = \"purple\", # Play with this in the chunk below\n  midpoint = colour_midpoint, # see above\n  space = \"Lab\", # don't mess with this!\n  na.value = \"grey50\")  +\n  labs(title = \"Three colour continuous gradient\", \n          subtitle = \"Mid Colour mapped to midpoint of data variable\",\n          caption = \"Colours inspired by my favourite cocker spaniel, Lord Chestnut\") # Play with these"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour Gradients - grDevices package",
    "text": "Continuous n-Colour Gradients - grDevices package\n\n# grDevices Palettes\np2 +\n  scale_colour_gradientn(\n    colours = terrain.colors(10)) +\n  # Try these:\n  # heat.colors() / topo.colors() / cm.colors() / rainbow()\n  \n  labs(title = \"N-colour continuous gradients\", \n          subtitle = \"Palettes from grDevices\",\n          caption = \"Palette: terrain.colors\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour Gradients - wesanderson Palettes",
    "text": "Continuous n-Colour Gradients - wesanderson Palettes\n\nwes_palettes\n\n$BottleRocket1\n[1] \"#A42820\" \"#5F5647\" \"#9B110E\" \"#3F5151\" \"#4E2A1E\" \"#550307\" \"#0C1707\"\n\n$BottleRocket2\n[1] \"#FAD510\" \"#CB2314\" \"#273046\" \"#354823\" \"#1E1E1E\"\n\n$Rushmore1\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Rushmore\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Royal1\n[1] \"#899DA4\" \"#C93312\" \"#FAEFD1\" \"#DC863B\"\n\n$Royal2\n[1] \"#9A8822\" \"#F5CDB4\" \"#F8AFA8\" \"#FDDDA0\" \"#74A089\"\n\n$Zissou1\n[1] \"#3B9AB2\" \"#78B7C5\" \"#EBCC2A\" \"#E1AF00\" \"#F21A00\"\n\n$Zissou1Continuous\n [1] \"#3A9AB2\" \"#6FB2C1\" \"#91BAB6\" \"#A5C2A3\" \"#BDC881\" \"#DCCB4E\" \"#E3B710\"\n [8] \"#E79805\" \"#EC7A05\" \"#EF5703\" \"#F11B00\"\n\n$Darjeeling1\n[1] \"#FF0000\" \"#00A08A\" \"#F2AD00\" \"#F98400\" \"#5BBCD6\"\n\n$Darjeeling2\n[1] \"#ECCBAE\" \"#046C9A\" \"#D69C4E\" \"#ABDDDE\" \"#000000\"\n\n$Chevalier1\n[1] \"#446455\" \"#FDD262\" \"#D3DDDC\" \"#C7B19C\"\n\n$FantasticFox1\n[1] \"#DD8D29\" \"#E2D200\" \"#46ACC8\" \"#E58601\" \"#B40F20\"\n\n$Moonrise1\n[1] \"#F3DF6C\" \"#CEAB07\" \"#D5D5D3\" \"#24281A\"\n\n$Moonrise2\n[1] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n\n$Moonrise3\n[1] \"#85D4E3\" \"#F4B5BD\" \"#9C964A\" \"#CDC08C\" \"#FAD77B\"\n\n$Cavalcanti1\n[1] \"#D8B70A\" \"#02401B\" \"#A2A475\" \"#81A88D\" \"#972D15\"\n\n$GrandBudapest1\n[1] \"#F1BB7B\" \"#FD6467\" \"#5B1A18\" \"#D67236\"\n\n$GrandBudapest2\n[1] \"#E6A0C4\" \"#C6CDF7\" \"#D8A499\" \"#7294D4\"\n\n$IsleofDogs1\n[1] \"#9986A5\" \"#79402E\" \"#CCBA72\" \"#0F0D0E\" \"#D9D0D3\" \"#8D8680\"\n\n$IsleofDogs2\n[1] \"#EAD3BF\" \"#AA9486\" \"#B6854D\" \"#39312F\" \"#1C1718\"\n\n$FrenchDispatch\n[1] \"#90D4CC\" \"#BD3027\" \"#B0AFA2\" \"#7FC0C6\" \"#9D9C85\"\n\n$AsteroidCity1\n[1] \"#0A9F9D\" \"#CEB175\" \"#E54E21\" \"#6C8645\" \"#C18748\"\n\n$AsteroidCity2\n[1] \"#C52E19\" \"#AC9765\" \"#54D8B1\" \"#b67c3b\" \"#175149\" \"#AF4E24\"\n\n$AsteroidCity3\n[1] \"#FBA72A\" \"#D3D4D8\" \"#CB7A5C\" \"#5785C1\"\n\nnames(wes_palettes)\n\n [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n[10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n[13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n[16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n[19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n[22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"    \n\n\n\np2 +\n    scale_colour_gradientn(\n      colors = wes_palette(name = \"GrandBudapest1\", \n                           n = 4), # Keep an eye on \"n\".\n      na.value = \"grey\") +\n  # Try these:\n  # \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"\n  # \"Rushmore\"       \"Royal1\"         \"Royal2\"\n  # \"Zissou1\"        \"Darjeeling1\"    \"Darjeeling2\"   \n  # \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n  # \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"   \n  # \"GrandBudapest1\" \"GrandBudapest2\" \"IsleofDogs1\"   \n  # \"IsleofDogs2\"   \n  # Keep an eye on \"n\".\n  \n  labs(title = \"N-colour continuous gradients\", \n       subtitle = \"Palettes from wesanderson\",\n       caption = \"Palette: GrandBudapest1\") # Change this caption based on palette choice"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour palettes from RColorBrewer\n",
    "text": "Continuous n-Colour palettes from RColorBrewer\n\nRecall Palette types\n\n\nseq for continuous data mapped to colour\n\nqual for categorical data mapped to colour ( discrete)\n\ndiv continuous data mapped to colour, that has pos and neg extremes from a middle value\n\n\nbrewer.pal.info\n\n\n  \n\n\n\n\n# scale_color_distiller() and scale_fill_distiller() \n# are used to apply the ColorBrewer colour scales \n# to continuous data.\n\np2 +\n  scale_colour_distiller(\n    palette = \"YlGnBu\") + # Play with this palette\n  \n  labs(title = \"RColorBrewer Palette\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Colour scales using paletteer palettes",
    "text": "Continuous Colour scales using paletteer palettes\nThis palette seems to have everything accessible in a simple way! NOTE: In order to access some palettes in paletteer, you may be asked to install other packages. E.g. harrypotter or scico. These need not be brought into your session using library() but are accessed directly by paletteer which is very convenient!!\n\n# What continuous palettes are there in paletteer?\npaletteer::palettes_c_names\n\n\n  \n\n\n\nOK, one of the Games of Thrones Palettes, and Harry Potter!\n\np2 +\n  # scale_colour_paletteer_c(\"gameofthrones::jon_snow\") +\n  # Temporarily not available in paletteer?\n  \n  scale_colour_paletteer_c(`\"harrypotter::dracomalfoy\"`) +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Hary Potter: Draco Malfoy\",\n       caption = \"Oh you awful Srishti people...\")\n\nError in `gen_fun()`:\n! The package \"harrypotter\" is required.\n\n  # Harry Potter Gryffindor Palette.\n  # Will ask for `harrypotter` package to be installed. Say yes!\n  p2 +\n  scale_colour_paletteer_c(\"harrypotter::gryffindor\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Harry Potter: Gryffindor\")\n\nError in `gen_fun()`:\n! The package \"harrypotter\" is required."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html",
    "href": "content/labs/r-labs/tidy/dplyr.html",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#introduction",
    "href": "content/labs/r-labs/tidy/dplyr.html#introduction",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "title": "Introduction to the dplyr package",
    "section": "Setting up the Packages",
    "text": "Setting up the Packages\n\nknitr::opts_chunk$set(message = TRUE) # Want tidylog messages\nlibrary(tidyverse)\nlibrary(tidylog) ## Explains what happens with dplyr verbs"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "href": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "title": "Introduction to the dplyr package",
    "section": "Tidy Data",
    "text": "Tidy Data\n\ndata(starwars)\ndim(starwars)\n\n[1] 87 14\n\nstarwars\n\n\n  \n\n\n\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R.\n\n\nTidy Data\n\nThe three features described in the figure above define the nature of tidy data:\n\nVariables in Columns\n\nObservations in Rows and\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row).\nWhen working with data you must:\n\nFigure out what you want to do. (Purpose)\nDescribe those tasks in the form of a computer program. (Plain English to R Code)\nExecute the program.\n\nThe dplyr package makes these steps fast and easy:\n\nBy constraining your options, it helps you think about your data manipulation challenges.\nIt provides simple “verbs”, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.\nIt uses efficient backends, so you spend less time waiting for the computer.\n\n\nNe’er you mind about backends ;-) See Shakespeare’s Hamlet.\n\nThis document introduces you to dplyr’s basic set of tools, and shows you how to apply them to data frames. dplyr also supports databases via the dbplyr package, once you’ve installed, read vignette(\"dbplyr\") to learn more."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "href": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "title": "Introduction to the dplyr package",
    "section": "Data: starwars",
    "text": "Data: starwars\nTo explore the basic data manipulation verbs of dplyr, we’ll use the dataset starwars. This dataset contains 87 characters and comes from the Star Wars API, and is documented in ?starwars\n\nThis means: type ?starwars in the Console. Try.\n\nNote that starwars is a tibble, a modern re-imagining of the data frame. It’s particularly useful for large datasets because it only prints the first few rows. You can learn more about tibbles at https://tibble.tidyverse.org; in particular you can convert data frames to tibbles with as_tibble().\n\nCheck your Environment Tab to inspect starwars in a separate tab."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "Single table verbs",
    "text": "Single table verbs\ndplyr aims to provide a function for each basic verb of data manipulation. These verbs can be organised into three categories based on the component of the dataset that they work with:\n\nRows:\n\n\nfilter() chooses rows based on column values.\n\nslice() chooses rows based on location.\n\narrange() changes the order of the rows.\n\n\nColumns:\n\n\nselect() changes whether or not a column is included.\n\nrename() changes the name of columns.\n\nmutate() changes the values of columns and creates new columns.\n\nrelocate() changes the order of the columns.\n\n\nGroups of rows:\n\n\nsummarise() collapses a group into a single row.\n\n\n\n\nThink of the parallels from Microsoft Excel.\n\nThe pipe\nAll of the dplyr functions take a data frame (or tibble) as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so the result from one step is then “piped” into the next step. You can use the pipe to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”).\nFilter rows with filter()\n\nfilter() allows you to select a subset of rows in a data frame. Like all single verbs, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE.\nFor example, we can select all character with light skin color and brown eyes with:\n\nNote the double equal to sign (==) below! Equivalent to MS Excel Data -&gt; Filter\n\n\nstarwars %&gt;% filter(skin_color == \"light\", eye_color == \"brown\")\n\nfilter: removed 80 rows (92%), 7 rows remaining\n\n\n\n  \n\n\n\nArrange rows with arrange()\n\narrange() works similarly to filter() except that instead of filtering or selecting rows, it reorders them. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:\n\nstarwars %&gt;% arrange(height, mass)\n\n\n  \n\n\n\nUse desc() to order a column in descending order:\n\nstarwars %&gt;% arrange(desc(height))\n\n\n  \n\n\n\nChoose rows using their position with slice()\n\nslice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows.\n\nThis is an important step in Prediction, Modelling and Machine Learning.\n\nWe can get characters from row numbers 5 through 10.\n\nstarwars %&gt;% slice(5:10)\n\nslice: removed 81 rows (93%), 6 rows remaining\n\n\n\n  \n\n\n\nIt is accompanied by a number of helpers for common use cases:\n\n\nslice_head() and slice_tail() select the first or last rows.\n\n\nstarwars %&gt;% slice_head(n = 3)\n\nslice_head: removed 84 rows (97%), 3 rows remaining\n\n\n\n  \n\n\n\n\n\nslice_sample() randomly selects rows. Use the option prop to choose a certain proportion of the cases.\n\n\nstarwars %&gt;% slice_sample(n = 5)\n\nslice_sample: removed 82 rows (94%), 5 rows remaining\n\n\n\n  \n\n\nstarwars %&gt;% slice_sample(prop = 0.1)\n\nslice_sample: removed 79 rows (91%), 8 rows remaining\n\n\n\n  \n\n\n\nUse replace = TRUE to perform a bootstrap sample. If needed, you can weight the sample with the weight argument.\n\nBootstrap samples are a special statistical sampling method. Counterintuitive perhaps, since you sample with replacement. Should remind you of your high school Permutation and Combination class, with all those urn models and so on. If you remember.\n\n\n\nslice_min() and slice_max() select rows with highest or lowest values of a variable. Note that we first must choose only the values which are not NA.\n\n\nstarwars %&gt;%\n  filter(!is.na(height)) %&gt;%\n  slice_min(height, n = 3)\n\nfilter: removed 6 rows (7%), 81 rows remaining\nslice_min: removed 78 rows (96%), 3 rows remaining\n\n\n\n  \n\n\n\nSelect columns with select()\n\nOften you work with large datasets with many columns but only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset using operations that usually only work on numeric variable positions:\n\n# Select columns by name\nstarwars %&gt;% select(hair_color, skin_color, eye_color)\n\nselect: dropped 11 variables (name, height, mass, birth_year, sex, …)\n\n\n\n  \n\n\n# Select all columns between hair_color and eye_color (inclusive)\nstarwars %&gt;% select(hair_color:eye_color)\n\nselect: dropped 11 variables (name, height, mass, birth_year, sex, …)\n\n\n\n  \n\n\n# Select all columns except those from hair_color to eye_color (inclusive)\nstarwars %&gt;% select(!(hair_color:eye_color))\n\nselect: dropped 3 variables (hair_color, skin_color, eye_color)\n\n\n\n  \n\n\n# Select all columns ending with color\nstarwars %&gt;% select(ends_with(\"color\"))\n\nselect: dropped 11 variables (name, height, mass, birth_year, sex, …)\n\n\n\n  \n\n\n\nThere are a number of helper functions you can use within select(), like starts_with(), ends_with(), matches() and contains(). These let you quickly match larger blocks of variables that meet some criterion. See ?select for more details.\nYou can rename variables with select() by using named arguments:\n\nstarwars %&gt;% select(home_world = homeworld)\n\nselect: renamed one variable (home_world) and dropped 13 variables\n\n\n\n  \n\n\n\nBut because select() drops all the variables not explicitly mentioned, it’s not that useful. Instead, use rename():\n\nstarwars %&gt;% rename(home_world = homeworld)\n\nrename: renamed one variable (home_world)\n\n\n\n  \n\n\n\nAdd new columns with mutate()\n\nBesides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate():\n\nstarwars %&gt;% mutate(height_m = height / 100)\n\nmutate: new variable 'height_m' (double) with 46 unique values and 7% NA\n\n\n\n  \n\n\n\nWe can’t see the height in meters we just calculated, but we can fix that using a select command.\n\nstarwars %&gt;%\n  mutate(height_m = height / 100) %&gt;%\n  select(height_m, height, everything())\n\nmutate: new variable 'height_m' (double) with 46 unique values and 7% NA\nselect: columns reordered (height_m, height, name, mass, hair_color, …)\n\n\n\n  \n\n\n\ndplyr::mutate() is similar to the base transform(), but allows you to refer to columns that you’ve just created:\n\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\n\nmutate: new variable 'height_m' (double) with 46 unique values and 7% NA\n        new variable 'BMI' (double) with 59 unique values and 32% NA\nselect: columns reordered (BMI, name, height, mass, hair_color, …)\n\n\n\n  \n\n\n\nIf you only want to keep the new variables, use transmute():\n\nstarwars %&gt;%\n  transmute(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  )\n\ntransmute: dropped 14 variables (name, height, mass, hair_color, skin_color, …)\ntransmute: dropped 14 variables (name, height, mass, hair_color, skin_color, …)\n           new variable 'height_m' (double) with 46 unique values and 7% NA\n           new variable 'BMI' (double) with 59 unique values and 32% NA\n\n\n\n  \n\n\n\nChange column order with relocate()\n\nUse a similar syntax as select() to move blocks of columns at once\n\nstarwars %&gt;% relocate(sex:homeworld, .before = height)\n\nrelocate: columns reordered (name, sex, gender, homeworld, height, …)\n\n\n\n  \n\n\n\nSummarise values with summarise()\n\nThe last verb is summarise(). It collapses a data frame to a single row.\n\nstarwars %&gt;% summarise(mean_height = mean(height, na.rm = TRUE))\n\nsummarise: now one row and one column, ungrouped\n\n\n\n  \n\n\n\nIt’s not that useful until we learn the group_by() verb below.\nCommonalities\nYou may have noticed that the syntax and function of all these verbs are very similar:\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame. You can refer to columns in the data frame directly without using $.\nThe result is a new data frame\n\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result.\nThese five functions provide the basis of a language of data manipulation. At the most basic level, you can only alter a tidy data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise())."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "href": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "title": "Introduction to the dplyr package",
    "section": "Combining functions with %>%\n",
    "text": "Combining functions with %&gt;%\n\nThe dplyr API is functional in the sense that function calls don’t have side-effects. You must always save their results. This doesn’t lead to particularly elegant code, especially if you want to do many operations at once. You either have to do it step-by-step:\n\na1 &lt;- group_by(starwars, species, sex)\na2 &lt;- select(a1, height, mass)\na3 &lt;- summarise(a2,\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nOr if you don’t want to name the intermediate results, you need to wrap the function calls inside each other:\n\nsummarise(\n  select(\n    group_by(starwars, species, sex),\n    height, mass\n  ),\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\ngroup_by: 2 grouping variables (species, sex)\nAdding missing grouping variables: `species`, `sex`\nselect: dropped 10 variables (name, hair_color, skin_color, eye_color, birth_year, …)\nsummarise: now 41 rows and 4 columns, one group variable remaining (species)\n\n\n\n  \n\n\n\nThis is difficult to read because the order of the operations is from inside to out. Thus, the arguments are a long way away from the function. To get around this problem, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so you can use it to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”):\n\nstarwars %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE)\n  )\n\ngroup_by: 2 grouping variables (species, sex)\nsummarise: now 41 rows and 4 columns, one group variable remaining (species)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "href": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "title": "Introduction to the dplyr package",
    "section": "Patterns of operations",
    "text": "Patterns of operations\nThe dplyr verbs can be classified by the type of operations they accomplish (we sometimes speak of their semantics, i.e., their meaning). It’s helpful to have a good grasp of the difference between select and mutate operations.\nSelecting operations\nOne of the appealing features of dplyr is that you can refer to columns from the tibble as if they were regular variables. However, the syntactic uniformity of referring to bare column names hides semantical differences across the verbs. A column symbol supplied to select() does not have the same meaning as the same symbol supplied to mutate().\nSelecting operations expect column names and positions. Hence, when you call select() with bare variable names, they actually represent their own positions in the tibble. The following calls are completely equivalent from dplyr’s point of view:\n\n# `name` represents the integer 1\nselect(starwars, name)\n\nselect: dropped 13 variables (height, mass, hair_color, skin_color, eye_color,\n…)\n\n\n\n  \n\n\nselect(starwars, 1)\n\nselect: dropped 13 variables (height, mass, hair_color, skin_color, eye_color,\n…)\n\n\n\n  \n\n\n\nBy the same token, this means that you cannot refer to variables from the surrounding context if they have the same name as one of the columns. In the following example, height still represents 2, not 5:\n\nheight &lt;- 5\nselect(starwars, height)\n\nselect: dropped 13 variables (name, mass, hair_color, skin_color, eye_color, …)\n\n\n\n  \n\n\n\nOne useful subtlety is that this only applies to bare names and to selecting calls like c(height, mass) or height:mass. In all other cases, the columns of the data frame are not put in scope. This allows you to refer to contextual variables in selection helpers:\n\nname &lt;- \"color\"\nselect(starwars, ends_with(name))\n\nselect: dropped 11 variables (name, height, mass, birth_year, sex, …)\n\n\n\n  \n\n\n\nThese semantics are usually intuitive. But note the subtle difference:\n\nname &lt;- 5\nselect(starwars, name, identity(name))\n\nselect: dropped 12 variables (height, mass, hair_color, eye_color, birth_year,\n…)\n\n\n\n  \n\n\n\nIn the first argument, name represents its own position 1. In the second argument, name is evaluated in the surrounding context and represents the fifth column.\nMutating operations\nMutate semantics are quite different from selection semantics. Whereas select() expects column names or positions, mutate() expects column vectors. We will set up a smaller tibble to use for our examples.\n\ndf &lt;- starwars %&gt;% select(name, height, mass)\n\nselect: dropped 11 variables (hair_color, skin_color, eye_color, birth_year,\nsex, …)\n\n\nWhen we use select(), the bare column names stand for their own positions in the tibble. For mutate() on the other hand, column symbols represent the actual column vectors stored in the tibble. Consider what happens if we give a string or a number to mutate():\n\nmutate(df, \"height\", 2)\n\nmutate: new variable '\"height\"' (character) with one unique value and 0% NA\n        new variable '2' (double) with one unique value and 0% NA\n\n\n\n  \n\n\n\nmutate() gets length-1 vectors that it interprets as new columns in the data frame. These vectors are recycled so they match the number of rows. That’s why it doesn’t make sense to supply expressions like \"height\" + 10 to mutate(). This amounts to adding 10 to a string! The correct expression is:\n\nmutate(df, height + 10)\n\nmutate: new variable 'height + 10' (double) with 46 unique values and 7% NA\n\n\n\n  \n\n\n\nIn the same way, you can unquote values from the context if these values represent a valid column. They must be either length 1 (they then get recycled) or have the same length as the number of rows. In the following example we create a new vector that we add to the data frame:\n\nvar &lt;- seq(1, nrow(df))\nmutate(df, new = var)\n\nmutate: new variable 'new' (integer) with 87 unique values and 0% NA\n\n\n\n  \n\n\n\nA case in point is group_by(). While you might think it has select semantics, it actually has mutate semantics. This is quite handy as it allows to group by a modified column:\n\ngroup_by(starwars, sex)\n\ngroup_by: one grouping variable (sex)\n\n\n\n  \n\n\ngroup_by(starwars, sex = as.factor(sex))\n\ngroup_by: one grouping variable (sex)\n\n\n\n  \n\n\ngroup_by(starwars, height_binned = cut(height, 3))\n\ngroup_by: one grouping variable (height_binned)\n\n\n\n  \n\n\n\nThis is why you can’t supply a column name to group_by(). This amounts to creating a new column containing the string recycled to the number of rows:\n\ngroup_by(df, \"month\")\n\ngroup_by: one grouping variable (\"month\")"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "Two table verbs",
    "text": "Two table verbs\nSometimes our data is spread across more than one table. Often these tables are linked by some common, or common-looking, variable columns. dplyr allows us to work with such data that is spread over more than one table. More information is available here: Two Table Verbs in dplyr\nThe operations/verbs used to manipulate two-table verbs are:\n\nMutating joins, which add new variables to one table from matching rows in another.\n\ninner_join()\n\n\n\n\n\n\n\n\n\n\n\n\nleft_join()\n\n\n\n\n\n\n\n\n\n\nright_join()\n\n\n\n\n\n\n\n\n\n\nfull_join()\n\n\n\n\n\n\n\n\n\n\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\n\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nanti_join(x, y) drops all observations in x that have a match in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nunion()\n\n\n\n\n\n\n\n\n\n\n\n\nunion_all(),\n\n\n\n\n\n\n\n\n\n\nintersect(),\n\n\n\n\n\n\n\n\n\n\nsetdiff()\n\n\n\n\n\n\n\n\n\n\nTidyr Operations:\npivot_longer()\npivot_wider()"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html",
    "href": "content/labs/r-labs/networks/index.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#setting-up-r-packages",
    "href": "content/labs/r-labs/networks/index.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#introduction",
    "href": "content/labs/r-labs/networks/index.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#goals",
    "href": "content/labs/r-labs/networks/index.html#goals",
    "title": "The Grammar of Networks",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "href": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "href": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n - The World Wide Web is an example of a directed network because\n hyperlinks connect one Web page to another, but not necessarily \n the other way around.\n\n - Co-authorship networks represent examples of un-directed networks,\nwhere nodes are authors and they are connected by an edge if they\nhave written a publication together\n\n - When people send e-mail to each other, the distinction between the\nsender (source) and the recipient (target) is clearly meaningful,\ntherefore the network is directed.\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "href": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer -1",
    "text": "Predict/Run/Infer -1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Nodes \n\ngrey_nodes &lt;- read_csv(\"../../../materials/data/networks/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"../../../materials/data/networks/grey_edges.csv\")\n\ngrey_nodes\n\n\n  \n\n\ngrey_edges\n\n\n  \n\n\n\n\n\nQuestions and Inferences #1:\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e. extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(\n  nodes = grey_nodes,\n  edges = grey_edges,\n  directed = FALSE\n)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\nQuestions and Inferences #2:\n\n\nQuestions and Inferences: What information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: Describe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line\n# About what that line does for the overall graph\n\nggraph(graph = ga, layout = \"kk\") +\n  #\n\n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n\n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  #\n\n  labs(\n    title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing **cool** things in the other\n       classes...\"\n  )\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: What parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc.\n\nggraph(graph = ga, layout = \"kk\") +\n  geom_edge_link(width = 2) +\n  geom_node_point(\n    shape = 21, size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  labs(\n    title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other\n       classes...\"\n  )\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\nQuestions and Inferences: What did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n\n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5:\n\n\nQuestions and Inferences: Describe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "href": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4, colour = \"red\") +\n  geom_node_text(aes(label = name),\n    repel = TRUE, size = 3,\n    max.overlaps = 20\n  ) +\n  labs(edge_width = \"Weight\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "href": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nset_graph_style()\n\n# This dataset contains the graph that describes the class\n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;- tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") +\n  geom_edge_diagonal() +\n  labs(title = \"Dendrogram\")\n\n\n\n\n\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) +\n  geom_edge_diagonal() +\n  geom_node_point(aes(filter = leaf)) +\n  coord_fixed() +\n  labs(title = \"Circular Dendrogram\")\n\n\n\n\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth), size = 0.25) +\n  labs(title = \"Rectangular Tree Map\")\n\n\n\n\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) +\n  coord_fixed() +\n  labs(title = \"Circular Tree Map\")\n\n\n\n\n\n\n# icicle\nggraph(graph, layout = \"partition\") +\n  geom_node_tile(aes(y = -y, fill = depth))\n\n\n\n\n\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() +\n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8:\n\n\nQuestions and Inferences: How do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#faceting",
    "href": "content/labs/r-labs/networks/index.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\n# setting theme_graph\nset_graph_style()\n\n\n# facet edges by type\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link(aes(color = type)) +\n  geom_node_point() +\n  facet_edges(~type) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n# facet nodes by sex\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link() +\n  geom_node_point() +\n  facet_nodes(~race) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n# facet both nodes and edges\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link(aes(color = type)) +\n  geom_node_point() +\n  facet_graph(type ~ race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #9:\n\n\nQuestions and Inferences: Does splitting up the main graph into subnetworks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "href": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\n\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;%\n  filter(degree &gt; 0) %&gt;%\n  activate(edges) %&gt;%\n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n# setting theme_graph\nset_graph_style()\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %&gt;%\n  activate(edges) %&gt;%\n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %&gt;%\n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) +\n\n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\")\n\n\n\n\n\n\n# or even less typing\nggraph(ga, layout = \"nicely\") +\n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n  geom_node_point(aes(\n    colour = centrality_degree(),\n    size = centrality_degree()\n  )) +\n  scale_color_gradient(\n    guide = \"legend\",\n    low = \"green\",\n    high = \"red\"\n  )\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #10:\n\n\nQuestions and Inferences: How do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n# setting theme_graph\nset_graph_style()\n\n\n# visualize communities of nodes\nga %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link() +\n  geom_node_point(aes(color = community), size = 5)\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #11:\n\n\nQuestions and Inferences: Is the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n  \n\n\ngrey_edges\n\n\n  \n\n\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;%\n  rowid_to_column(var = \"id\") %&gt;%\n  rename(\"label\" = name) %&gt;%\n  mutate(sex = case_when(\n    sex == \"F\" ~ \"Female\",\n    sex == \"M\" ~ \"Male\"\n  )) %&gt;%\n  replace_na(., list(sex = \"Transgender?\")) %&gt;%\n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n  \n\n\ngrey_edges_vis &lt;- grey_edges %&gt;%\n  select(from, to) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"from\" = \"label\")\n  ) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"to\" = \"label\")\n  ) %&gt;%\n  select(\"from\" = id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n  \n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;%\n  visNodes(font = list(size = 40)) %&gt;%\n  # Colour and icons for each of the gender-groups\n  visGroups(\n    groupname = \"Female\", shape = \"icon\",\n    icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\", shape = \"icon\",\n    icon = list(code = \"f183\", size = 75, color = \"slateblue\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\", shape = \"icon\",\n    icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  # visLegend() %&gt;%\n  # Add the fontawesome icons!!\n  addFontAwesome() %&gt;%\n  # Add Interaction Controls\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis, ) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-edges.csv\")\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;-\n  starwars_nodes %&gt;%\n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;-\n  starwars_edges %&gt;%\n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;%\n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;%\n  # Select \"id.x\" and \"id.y\" ONLY\n  # Rename them as \"from\" and \"to\"\n  # keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n  \n\n\nstarwars_edges_vis\n\n\n  \n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(\n    font = list(size = 30), shape = \"icon\",\n    icon = list(code = \"f1e3\", size = 75)\n  ) %&gt;%\n  addFontAwesome() %&gt;%\n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(font = list(size = 30)) %&gt;%\n  visEdges(color = \"red\")"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#your-assignments",
    "href": "content/labs/r-labs/networks/index.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\nMake1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;%\n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(“karate”,package= “igraphdata”) karate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT &lt;- read_rds(\"../../../materials/data/networks/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\nMake-2: Literary Network with TV Show / Book / Story / Play\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don’t mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#read-more",
    "href": "content/labs/r-labs/networks/index.html#read-more",
    "title": "The Grammar of Networks",
    "section": "Read more",
    "text": "Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n———. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html",
    "href": "content/labs/r-labs/time/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-R",
    "href": "content/labs/r-labs/time/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Check in",
    "text": "Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Check in",
    "text": "Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#save-and-share",
    "href": "content/labs/r-labs/time/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Save and share",
    "text": "Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nUpload this exported plot to Teams -&gt; Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture 🐈 first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html",
    "title": "Playing With Mapview",
    "section": "",
    "text": "In this tutorial, we will learn to create interactive maps in R, using a package called mapview, which is a simpler way to access leaflet, which is a wellknown package to create interactive maps.\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nWhereas leaflets code becomes lengthy fairly quickly, mapview allows full functionality of leaflet using sensible defaults. Type ?mapview in the console for more help."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#introduction",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#introduction",
    "title": "Playing With Mapview",
    "section": "",
    "text": "In this tutorial, we will learn to create interactive maps in R, using a package called mapview, which is a simpler way to access leaflet, which is a wellknown package to create interactive maps.\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nWhereas leaflets code becomes lengthy fairly quickly, mapview allows full functionality of leaflet using sensible defaults. Type ?mapview in the console for more help."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#more-information",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#more-information",
    "title": "Playing With Mapview",
    "section": "More Information",
    "text": "More Information\nMore information on mapview is available at https://r-spatial.github.io/mapview/.\nThere are also two wonderful talks by Tim Appelhans, the creator of mapview that are available here:\n\nMapview package tutorial (Part 1) - TIB AV-Portal\nMapview package tutorial (Part 2) - TIB AV-Portal\n\n\nlibrary(tidyverse)\nlibrary(sf)\n##\n# Mapview and allied packages\nlibrary(mapview)\nlibrary(leaflet)\nlibrary(leafem) # Provides extensions for packages 'leaflet' & 'mapdeck', many of which are used by package 'mapview'.\nlibrary(leafgl) # High-Performance 'WebGl' Rendering for Package 'leaflet'\nlibrary(leafsync) # Create small multiples of several leaflet web maps with (optional) synchronised panning and zooming control.\n##\nlibrary(slideview) # Create a side-by-side view of raster(image)s with an interactive slider to switch between regions of the images.\nlibrary(cubeview) # View 3D Raster Cubes Interactively\nlibrary(plainview) # Provides methods for plotting potentially large (raster) images interactively on a plain HTML canvas.\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n# library(osmplotr) # Creating maps with OSM data in R. Package is no longer maintained, so not used."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#basic-maps-using-mapview",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#basic-maps-using-mapview",
    "title": "Playing With Mapview",
    "section": "Basic Maps using mapview",
    "text": "Basic Maps using mapview\nfranconia , trails, and breweries are geospatial datasets of class sf from the mapview package. franconia contains MULTIPOLYGON, trails contains MULTILINESTRING, and breweries contains POINT geometries.\n\nclass(franconia)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(franconia, 1)\n\n\n  \n\n\n\n\nclass(trails)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(trails, 1)\n\n\n  \n\n\n\n\nclass(breweries)\n\n[1] \"sf\"         \"data.frame\"\n\nhead(breweries, 1)\n\n\n  \n\n\n\nPlotting these is a simple one-liner:\n\nmapview(franconia)\n\n\n\n\nmapview(breweries)\n\n\n\n\nmapview(trails)\n\n\n\n\n\nmapview has automagically added shapes to the map by detecting the geometry column in each sf dataframe. (rather like geom_sf in ggplot). The map is interactive and clicking on any of the shapes provides a popup containing all the remaining attribute information ( from the non-geometry columns)\nNote that there are multiple basemaps available by default in mapview. The layers icon on the left allows the user to interactively choose the base map style. There are other basemaps that can be specified programmatically.\nWe can also plot these maps as overlays ( since they all pertain to the same geographical area.) Each of the maps can also be given a layer name:\n\n# Single overlay plot with layer names\nmapview(franconia, layer.name = \"1-Franconia\") +\n  mapview(trails, layer.name = \"2-Brewery Trails\") +\n  mapview(breweries, layer.name = \"3-Breweries\")\n\n\n\n\n\nAdd Colours to Shapes\nmapview offers a simple way of adding colours to shapes, based on any of the other columns in the respective dataframe, by passing that column name(in quotes!) to the parameter zcol in mapview():\n\nmapview(franconia,\n  zcol = \"district\",\n  col.regions = grDevices::hcl.colors\n) + # set colour palette\n  mapview(breweries, col.regions = \"red\")\n\n\n\n\n\nLegends\nNote that legends are created by default. They can be turned off with ,legend = FALSE inside the mapview() function. Note also the home button at the bottom right: that re-centres and resets the map.\nMap Stack with All Attributes\nOne can get a stack of maps where the shapes are coloured by all variables simultaneously by using , burst = TRUE instead of zcol:\n\nmapview(franconia, burst = TRUE)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#using-mapview-with-external-geospatial-data",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#using-mapview-with-external-geospatial-data",
    "title": "Playing With Mapview",
    "section": "Using mapview with external geospatial data",
    "text": "Using mapview with external geospatial data\nOn to something more complex. We want to plot a known set of locations on a mapview map. mapview takes in geographical data in many ways and we will explore most of them."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#data-sources-for-mapview",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#data-sources-for-mapview",
    "title": "Playing With Mapview",
    "section": "Data Sources for mapview",
    "text": "Data Sources for mapview\n\nObjects of the following spatial classes are supported in mapview:\n\nsf\n\nraster (Layer, Stack, Brick and SpatialPixels* / SpatialGridDataFrame)\nstars\n\nsp (Points, Polygons, Lines and their DataFrame version)\nsatellite\n\n\nWhich means we cannot give mapview simple vectors / matrices/ dataframes containing lon / lat information: they need to be converted into sf format first. (Leaflet could natively do this! Hmm…)\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at India Airports Locations.\nYou can either download it, save a copy, and read it in as usual, or use the URL itself to read it in directly from data.world. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (very simple!) instructions are available here at data.world\n\n# library(devtools)\n# devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nindia_airports &lt;-\n  read_csv(file = \"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;%\n  slice(-1) %&gt;% # Drop the first row which contains labels\n\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;%\n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;%\n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083)) %&gt;%\n  # Convert to `sf` dataframe\n  st_as_sf(\n    coords = c(\"lon\", \"lat\"),\n    remove = FALSE, # retain the original lon and lat columns\n    sf_column_name = \"geometry\",\n    crs = 4326 # specify Projection,else no basemap will be plotted\n  )\n\nindia_airports %&gt;% head()\n\n\n  \n\n\n\nLet us plot this in `mapview`, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\n# Change the order of basemaps in mapview\n# Male OpenTopoMap the default\nmapviewOptions(basemaps = c(\"OpenTopoMap\", \"CartoDB.Positron\", \"CartoDB.DarkMatter\", \"OpenStreetMap\", \"Esri.WorldImagery\"))\n\nmapview(india_airports,\n  zcol = \"type\"\n)\n\n\n\n\n\nUsing popups and labels\nBy default, mapview provides a mouseover label information (feature ID, or a zcol attribute if zcol has been set), and a popup table containing all attribute fields. This can be customized to show the user wants. There are various options for popups in mapview:\n\npopup = popupTable() Text/table based popup\npopup = popupImage() Images in popups\npopup = popupGraph() a data visualization in the popup\n\npopup = popupIframe() URL, Image, Video in a popup using iframe\nWe will download a small dataset of restaurants in say Malleswaram, Bangalore and plot them with mapview, adding popups and labels:\n\n\n\n# library(osmdata)\n\nbbox &lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nrestaurants &lt;-\n  osmdata::opq(bbox = bbox) %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = \"restaurant\"\n  ) %&gt;%\n  osmdata_sf() %&gt;% # Convert to Simple Features format\n  purrr::pluck(\"osm_points\") # Pull out the data frame of interest\n\nrestaurants &lt;- restaurants %&gt;%\n  dplyr::filter(cuisine == \"indian\")\nrestaurants\n\n\n  \n\n\n\nLet us add popups containing the restaurant name and cuisine; we need to add the R package leafpop to add popups\n\nlibrary(leafpop)\nmapviewOptions(basemaps = \"OpenStreetMap\") # set basemap to OSM\nmapview(\n  restaurants,\n  col.regions = \"green\", # Point Fill colour\n  cex = 10, # Point Size\n  color = \"red\", # Points Border\n  popup = popupTable(restaurants, zcol = c(\"name\", \"cuisine\"))\n)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#using-icons-for-markers",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#using-icons-for-markers",
    "title": "Playing With Mapview",
    "section": "Using icons for markers",
    "text": "Using icons for markers\nWe can also change the icon for each airport. Let us try one of the several icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n\n# Based on data in india_airports\n\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- leaflet::makeIcon(\n\n  \"./images/iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the mapview map\n\nmapview(india_airports) %&gt;%\n  popupImage(\n    img = iata_icon,\n    embed = TRUE,\n    popup = popup\n  )\n\nmapview(\n  x = india_airports,\n  popup = popupImage(\n    img = iata_icon, embed = TRUE,\n    popup = popup\n  )\n)\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of [ionicons](http://ionicons.com/), [glyphicons](https://icons.getbootstrap.com/#icons), and [FontAwesomeIcons](http://fontawesome.io/icons/)\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in [Using Leaflet Markers @JLA-Data.net](https://www.jla-data.net/eng/leaflet-markers-in-r/)\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\n\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"\n    ),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348\n    ),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216\n    )\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# Vanilla leaflet map\n\nleaflet(companies_itpl) %&gt;%\n  addTiles() %&gt;%\n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\n\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg%22\",\n    iconWidth = 25, iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc%22\",\n    iconWidth = 45, iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico%22\",\n    iconWidth = 25, iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png%22\",\n    iconWidth = 45, iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png%22\",\n    iconWidth = 75, iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png%22\",\n    iconWidth = 25, iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png%22\",\n    iconWidth = 35, iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg%22\",\n    iconWidth = 25, iconHeight = 25\n  )\n)\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;%\n  addMarkers(\n    icon = ~ favicons[ticker], # lookup based on ticker\n\n    label = ~ companies_itpl$ticker,\n    labelOptions = labelOptions(noHide = F, offset = c(15, -25))\n  ) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\n\nmetro\n\n\n  \n\n\nleaflet(data = metro) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  # Add CartoDB.Positron\n\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n\n  # Add Markers for each airport\n\n  addCircleMarkers(\n    radius = ~ sqrt(pop2030) / 350,\n    color = \"red\",\n    popup = paste(\n      \"Name: \", metro$name, \"&lt;br&gt;\",\n      \"Population 2030: \", metro$pop2030\n    )\n  )\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM datahttps://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox &lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;-\n  osmdata::opq(bbox = bbox) %&gt;%\n  osmdata::add_osm_feature(key = \"amenity\", value = \"restaurant\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_points\") %&gt;%\n  dplyr::select(name, cuisine, geometry) %&gt;%\n  dplyr::filter(cuisine == \"indian\")\n\nlocations %&gt;% head()\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(\n  data = locations,\n  options = leafletOptions(minZoom = 12)\n) %&gt;%\n  addProviderTiles(providers$CartoDB.Voyager) %&gt;%\n  # Regular `leaflet` code\n  addAwesomeMarkers(\n    icon = awesomeIcons(\n      icon = \"fa-coffee\",\n      library = \"fa\",\n      markerColor = \"blue\",\n      iconColor = \"black\",\n      iconRotate = TRUE\n    ),\n    popup = paste(\n      \"Name: \", locations$name, \"&lt;br&gt;\",\n      \"Food: \", locations$cuisine\n    )\n  )\n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\n\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\nleaflet(data = locations) %&gt;%\n  addProviderTiles(providers$CartoDB.Voyager) %&gt;%\n  # Workaround code\n\n  addAwesomeMarkers(\n    icon = coffee,\n    popup = paste(\n      \"Name: \", locations$name, \"&lt;br&gt;\",\n      \"Food: \", locations$cuisine, \"&lt;br&gt;\"\n    )\n  )\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(\n  c(\n    runif(5, 76.652985 - 0.01, 76.652985 + 0.01),\n    runif(5, 12.311827 - 0.01, 12.311827 + 0.01)\n  ),\n  nrow = 5\n)\n\nmysore5\n\n         [,1]     [,2]\n[1,] 76.64748 12.30449\n[2,] 76.66189 12.31432\n[3,] 76.65584 12.30750\n[4,] 76.64625 12.31081\n[5,] 76.64661 12.31920\n\nleaflet(data = mysore5) %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  # Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n\n  addAwesomeMarkers(\n    icon = awesomeIcons(\n      icon = \"music\",\n      iconColor = \"black\",\n      library = \"glyphicon\"\n    ),\n    popup = \"Carnatic Music !!\"\n  )"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing With Mapview",
    "section": "Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nlibrary(osmdata)\n# Option 1\n# Gives too large a bbox\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\n# bbox\n\n# Setting bbox manually is better\namsterdam_coords &lt;- matrix(c(4.85, 4.95, 52.325, 52.375),\n  byrow = TRUE,\n  nrow = 2, ncol = 2,\n  dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n)\namsterdam_coords\n\n     min    max\nx  4.850  4.950\ny 52.325 52.375\n\ncolleges &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = \"college\"\n  ) %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")\n\nparks &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(key = \"landuse\", value = \"grass\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")\n\nroads &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(\n    key = \"highway\",\n    value = \"primary\"\n  ) %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_lines\")\n\ncyclelanes &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(key = \"cycleway\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_lines\")\n\nWe have 12 colleges in our data and 3371 parks in our data.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(data = colleges, popup = ~ colleges$name) %&gt;%\n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;%\n  addPolylines(data = roads, color = \"red\") %&gt;%\n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing With Mapview",
    "section": "Chapter 3: Using Raster Data in leaflet\n",
    "text": "Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\nusing the OpenStreetMap package\n\n\nlibrary(terra)\n\nlibrary(maptiles)\n\n# library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingWithMapview.html#adding-legendswork-in-progress",
    "href": "content/labs/r-labs/maps/PlayingWithMapview.html#adding-legendswork-in-progress",
    "title": "Playing With Mapview",
    "section": "Adding Legends[Work in Progress!]",
    "text": "Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\n\ndf &lt;- data.frame(\n  lat = runif(20, min = 11.97, max = 13.07),\n  lng = runif(20, min = 77.48, max = 77.68),\n  col = sample(c(\"red\", \"blue\", \"green\"), 20,\n    replace = TRUE\n  ),\n  stringsAsFactors = FALSE\n)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\n\n\n\n\n\nNoteLeaflet\n\n\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n\n# devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\nlibrary(data.world)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#introduction",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#introduction",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\n\n\n\n\n\nNoteLeaflet\n\n\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n\n# devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\nlibrary(data.world)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#basic-features-of-a-leaflet-map",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#basic-features-of-a-leaflet-map",
    "title": "Playing with Leaflet",
    "section": "Basic Features of a leaflet Map",
    "text": "Basic Features of a leaflet Map\n\n# Set value for the minZoom and maxZoom settings.\n# leaflet(options = leafletOptions(minZoom = 0, maxZoom = 18))\n\nm &lt;- leaflet() %&gt;%\n  # Add default OpenStreetMap map tiles\n  addTiles() %&gt;%\n  # Set view to be roughly centred on Bangalore City\n  setView(lng = 77.580643, lat = 12.972442, zoom = 12)\n\nm\n\n\n\n\n# Click on the map to zoom in; Shift+Click to zoom out\n\nleaflet by default uses Open Street Map as its base map. We can use other base maps too, as we will see later."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "Add Shapes to a Map",
    "text": "Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\nAdd Markers with popups\n\nm %&gt;% addMarkers(\n  lng = 77.580643, lat = 12.972442,\n  popup = \"The birthplace of Rvind\"\n)\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\nAdding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: &lt;a href = https://arvindvenkatadri.com&gt;Arvind V's Website &lt;/a&gt;\",\n      \"&lt;br&gt;\"\n    ),\n    ## Ensuring we cannot close the popup,\n    ## else we will not be able to find where it is,\n    ## since there is no Marker\n\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\nAdding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n\n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(\n      noHide = TRUE, # Label always visible\n      textOnly = F,\n      textsize = 20\n    ),\n\n    # And here is the popup defn.\n    popup = paste(\n      \"PopUp Text: &lt;a href = https://arvindvenkatadri.com&gt;Arvind V's Website &lt;/a&gt;\",\n      \"&lt;br&gt;\"\n    )\n  )\n\n\n\n\n\nAdding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest.\n\n\n\n\n\n\nNoteaddCircles and addCircleMarkers\n\n\n\nThe radius argument works differently in addCircles() and addCircleMarkers().\n\n\n\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\", \"Frederick\", \"Rockville\", \"Gaithersburg\", \"Bowie\", \"Hagerstown\", \"Annapolis\", \"College Park\", \"Salisbury\", \"Laurel\"),\n  pop = c(619493, 66169, 62334, 61045, 55232, 39890, 38880, 30587, 30484, 25346),\n  lat = c(39.2920592, 39.4143921, 39.0840, 39.1434, 39.0068, 39.6418, 38.9784, 38.9897, 38.3607, 39.0993),\n  lng = c(-76.6077852, -77.4204875, -77.1528, -77.2014, -76.7791, -77.7200, -76.4922, -76.9378, -75.5994, -76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(\n    radius = ~ pop / 1000, # Pixels!!\n    color = \"blue\",\n    stroke = FALSE, # no border for the Markers\n    opacity = 0.8\n  ) %&gt;%\n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n  )\n\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\nAdding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;%\n  addRectangles(\n    lat1 = 10.3858, lng1 = 75.0595,\n    lat2 = 12.8890, lng2 = 77.9625\n  )\n\n\n\n\n\nAdd Polygons to a Map\n\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;%\n  # arbitrary vector data for lat and lng\n  addPolygons(\n    lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n    lat = c(10.12, 11.04, 11.87, 12.04, 10.7)\n  )\n\n\n\n\n\nAdd PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;%\n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n  # Two Vectors\n  addPolylines(\n    lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n    lat = c(10.12, 11.04, 11.87, 12.04, 10.7)\n  ) %&gt;%\n  # Add Waypoint Icons\n  # Same Two Vectors\n  addMarkers(\n    lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n    lat = c(10.12, 11.04, 11.87, 12.04, 10.7)\n  )\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\n\n\n\n\n\n\nImportantThe mapedit package\n\n\n\nhttps://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-leaflet-with-external-geospatial-data",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-leaflet-with-external-geospatial-data",
    "title": "Playing with Leaflet",
    "section": "Using leaflet with External Geospatial Data",
    "text": "Using leaflet with External Geospatial Data\nOn to something more complex. We want to plot an external user-defined set of locations on a leaflet map. leaflet takes in geographical data in many ways and we will explore most of them.\nPOINT Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nVectors: Simply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nMatrices: Two-column numeric matrices (first column is longitude, second is latitude)\n\n\nData Frames: Data frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\n\nPackage sp” SpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPackage sf: POINT,sfc_POINT, andsfobjects (from thesf` package); only X and Y dimensions will be considered\n\n\n\n\n\n\n\n\nWarningNot using sp\n\n\n\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\n\nPoints using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a and, for poor peasants especially, also by clicking the download button below. Save it in a convenient data folder in your project and read it in.\n\n\n\n\n\n\nNoteUsing data.world\n\n\n\nYou will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n\n\n\n\n  \n\n\n\nHere is the data:\n\n\n\n Airports data\n\n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the default OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n\n  # Add Markers for each airport\n  addCircleMarkers(\n    lng = ~lon, lat = ~lat,\n    # Optional, variables stated for clarity\n    # leaflet can automatically detect lon-lat columns\n    # if they are appropriately named in the data\n    # longitude/lon/lng\n    # latitude/lat\n    radius = 2, # Pixels\n    color = \"red\",\n    opacity = 1\n  )\n\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of the several icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons. Here is the IATA icon: download and save it and make sure this code below has the proper path to this .png file!\n\n\nIATA Logo\n\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"images/iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons.\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"\n    ),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348\n    ),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216\n    )\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;%\n  addTiles() %&gt;%\n  addMarkers()\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, as we know, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n  \n\n\nleaflet(data = metro) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n\n  # Add Markers for each airport\n  addCircleMarkers(\n    radius = ~ sqrt(pop2030) / 350,\n    color = \"red\",\n    popup = paste(\n      \"Name: \", metro$name, \"&lt;br&gt;\",\n      \"Population 2030: \", metro$pop2030\n    )\n  )\n\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data https://www.openstreetmap.org/#map=16/12.9766/77.5888. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox &lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;-\n  osmdata::opq(bbox = bbox) %&gt;%\n  osmdata::add_osm_feature(key = \"amenity\", value = \"restaurant\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_points\") %&gt;%\n  dplyr::select(name, cuisine, geometry) %&gt;%\n  dplyr::filter(cuisine == \"indian\")\n\nlocations %&gt;% head()\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(\n  data = locations,\n  options = leafletOptions(minZoom = 12)\n) %&gt;%\n  addProviderTiles(providers$CartoDB.Voyager) %&gt;%\n  # Regular `leaflet` code\n  addAwesomeMarkers(\n    icon = awesomeIcons(\n      icon = \"fa-coffee\",\n      library = \"fa\",\n      markerColor = \"blue\",\n      iconColor = \"black\",\n      iconRotate = TRUE\n    ),\n    popup = paste(\n      \"Name: \", locations$name, \"&lt;br&gt;\",\n      \"Food: \", locations$cuisine\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nNoteFontawesome Workaround**\n\n\n\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. Aiyooo….;-()\n( Update Dec 2023: Seems OK now…)\n\n\n\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;%\n  addProviderTiles(providers$CartoDB.Voyager) %&gt;%\n  # Workaround code\n\n  addAwesomeMarkers(\n    icon = coffee,\n    popup = paste(\n      \"Name: \", locations$name, \"&lt;br&gt;\",\n      \"Food: \", locations$cuisine, \"&lt;br&gt;\"\n    )\n  )\n\n\n\n\n\n\n\nNoteleaflet detects sf POINT geometry\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(\n  c(\n    runif(5, 76.652985 - 0.01, 76.652985 + 0.01),\n    runif(5, 12.311827 - 0.01, 12.311827 + 0.01)\n  ),\n  nrow = 5\n)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.64561 12.30481\n[2,] 76.64341 12.30308\n[3,] 76.64324 12.31361\n[4,] 76.64518 12.31857\n[5,] 76.64842 12.31452\n\nleaflet(data = mysore5) %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  # Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(\n    icon = awesomeIcons(\n      icon = \"music\",\n      iconColor = \"black\",\n      library = \"glyphicon\"\n    ),\n    popup = \"Carnatic Music !!\"\n  )\n\n\n\n\n\nPolygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLINE and POLYGON data can also come from a variety of sources:\n\n\nsf package: MULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\nsp package: SpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n**sp package:SpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\nmaps package:map objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nMatrices:Two-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; Sounds very clumsy and better not attempt. Use sf instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nlibrary(osmdata)\n# Option 1\n# Gives too large a bbox\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\n# bbox\n\n# Setting bbox manually is better\namsterdam_coords &lt;- matrix(c(4.85, 4.95, 52.325, 52.375),\n  byrow = TRUE,\n  nrow = 2, ncol = 2,\n  dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n)\namsterdam_coords\n\n     min    max\nx  4.850  4.950\ny 52.325 52.375\n\ncolleges &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = \"college\"\n  ) %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")\n\nparks &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(key = \"landuse\", value = \"grass\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")\n\nroads &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(\n    key = \"highway\",\n    value = \"primary\"\n  ) %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_lines\")\n\ncyclelanes &lt;- amsterdam_coords %&gt;%\n  osmdata::opq() %&gt;%\n  osmdata::add_osm_feature(key = \"cycleway\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_lines\")\n\nWe have 12 colleges, 3371 parks, 309 roads, and 281 cycle lanes in our data.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    data = colleges, color = \"yellow\",\n    popup = ~ colleges$name\n  ) %&gt;%\n  addPolygons(data = parks, color = \"seagreen\", popup = parks$name) %&gt;%\n  addPolylines(data = roads, color = \"red\") %&gt;%\n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-raster-data-in-leafletwork-in-progress",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-raster-data-in-leafletwork-in-progress",
    "title": "Playing with Leaflet",
    "section": "Using Raster Data in leaflet[Work in Progress!]",
    "text": "Using Raster Data in leaflet[Work in Progress!]\nSo far all the geospatial data we have plotted in leaflet has been vector data.\nWe will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegetation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\nusing the OpenStreetMap package\n\n\nlibrary(terra)\nlibrary(maptiles)\n# library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#bells-and-whistles-in-leaflet-layers-groups-legends-and-graticules",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#bells-and-whistles-in-leaflet-layers-groups-legends-and-graticules",
    "title": "Playing with Leaflet",
    "section": "Bells and Whistles in leaflet: layers, groups, legends, and graticules",
    "text": "Bells and Whistles in leaflet: layers, groups, legends, and graticules\nAdding Legends\n\n## Generate some random lat lon data around Bangalore\ndf &lt;- tibble(\n  lat = runif(20, min = 11.97, max = 13.07),\n  lng = runif(20, min = 77.48, max = 77.68),\n  col = sample(c(\"red\", \"blue\", \"green\"), 20,\n    replace = TRUE\n  ),\n  stringsAsFactors = FALSE\n)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n\n\n\n\n\nUsing Web Map Services (WMS) [Work in Progress!]\nTo be included."
  },
  {
    "objectID": "content/labs/doe/index.html",
    "href": "content/labs/doe/index.html",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students."
  },
  {
    "objectID": "content/labs/doe/index.html#introduction",
    "href": "content/labs/doe/index.html#introduction",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students."
  },
  {
    "objectID": "content/labs/doe/index.html#structure",
    "href": "content/labs/doe/index.html#structure",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Structure",
    "text": "Structure\nThe total number of students were 17. Eight Pairs of students were created randomly to create eight different Test tools for Short Term Memory testing.\nThe binary ( two - level ) variables/parameters that were used in the tests, were, following Lawrance:\n\nWL: Word List Length ( 7 and 15 words )\n\nSL: Syllables in the Words ( 2 and 5 syllables )\n\nST: Study Time allowed for the Respondents ( 15 and 30 seconds )\n\nOther parameters considered were a) Language b) Structure/Depiction of the Word Lists ( e.g. word clouds, matrices, columns…), c) Whether the words would be shown or read aloud, and d) whether the respondents had to speak out, or write down, the recollected words. These parameters were discussed and abandoned as too complex to mechanize, though they could have had an impact on the STM scores.\nHence a total of 8 Tests were created by 8 pairs of students, and each team tested the remaining 15 students ( Due to COVID restrictions, this testing was carried out entirely online on MS Teams, using individual breakout rooms for the Test Teams. )\nThe data were entered into a Google Sheet and the STM scores were converted to percentages so as to be comparable across WL.\nThe data was then “flattened” for each of the binary parameters; this was logical to do since for each parameter, the other two parameters were balanced out by the Test structure. For instance, for WL = 5, the SL and ST parameters used all the four combinations ( SL = 5, 15 ) and (ST = 15, 30 ). Hence the “common sense” analysis could proceed for each of the parameters individually. Joint effects were not considered for this preliminary class."
  },
  {
    "objectID": "content/labs/doe/index.html#data",
    "href": "content/labs/doe/index.html#data",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Data",
    "text": "Data\n\n\n\n  \n\n\n\nThe data has scores that have been combined into single columns for each setting for each of the parameters. For example, the column syllable_2 contains STM scores for all tests that used SL = 2-syllables in their tests. The Word Length WL and Study Time ST go through all their combinations in this column. The other columns are constructed similarly.\nBasic Plots\nWe will use Box Plots and Density Plots to compare the STM score distributions for each Parameter. To do this we need to pivot_longer the adjacent columns ( e.g. syllable_2 and syllable_5) and use these names as categorical variables:\nSyllable Parameter SL\n\n\n\n  \n\n\n\n\n\n\n\n\n\nStudy Time Parameter ST\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWord List Length Parameter WL"
  },
  {
    "objectID": "content/labs/doe/index.html#preliminary-observations",
    "href": "content/labs/doe/index.html#preliminary-observations",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Preliminary Observations",
    "text": "Preliminary Observations\nClearly, based on visual inspection of the Plots, the Word Count seems to have a large effect on STM Test Scores, with fewer words ( 7 ) being easier to recall. Study Time ( 15 and 30 seconds ) also seems to have a more modest positive effect on STM scores, while Syllable Count ( 2 or 5 syllables ) seems to have a modest negative effect on STM scores."
  },
  {
    "objectID": "content/labs/doe/index.html#analysis",
    "href": "content/labs/doe/index.html#analysis",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Analysis",
    "text": "Analysis\nWe wish to establish the significance of the effect size due to each of the Parameters. Already from the Density Plots, we can see that none of the scores are normally distributed. A quick Shapiro-Wilkes Test for each of them confirms that the scores are not normally distributed.\nHence we go for a Permutation Test to check for significance of effect.\nOn the other hand, as remarked in Ernst2, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp, as I can testify from direct observation in this class. There is no need to discuss sampling distributions and means, t-tests and the like. Permutations are easily executed in R, using packages such as mosaic3.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_2\nW = 0.95508, p-value = 0.02716\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_5\nW = 0.95321, p-value = 0.02211\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_15\nW = 0.9068, p-value = 0.0002348\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_30\nW = 0.95539, p-value = 0.0281\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_7\nW = 0.90542, p-value = 0.0002085\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_15\nW = 0.92806, p-value = 0.001645"
  },
  {
    "objectID": "content/labs/doe/index.html#permutation-tests",
    "href": "content/labs/doe/index.html#permutation-tests",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nWe proceed with a Permutation Test for each of the Parameters. We start with the Syllable Parameter SL. We shuffle the labels ( SL- = 2 and SL+ = 5) between the scores and determine the null distribution. This is then compared with the difference in mean scores between the unpermuted sets. We continue similarly for the other two parameters.\n\n\n[1] 0.0153731\n\n\n\n  \n\n\n\n[1] 0.08526183\n\n\n\n  \n\n\n\n[1] 0.2887539"
  },
  {
    "objectID": "content/labs/doe/index.html#conclusions",
    "href": "content/labs/doe/index.html#conclusions",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above null distribution plots obtained using Permutation tests, it is clear that both Study Time ( ST ) and List Word Length ( WL) have significant effects on the Short Term Memory Scores. The probability that the observed value is obtained or exceeded by any permutation of scores is very low in both cases.\nOn the other hand, Syllable Count (SL) does not seem to affect the STM scores significantly."
  },
  {
    "objectID": "content/labs/doe/index.html#references",
    "href": "content/labs/doe/index.html#references",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/labs/doe/index.html#footnotes",
    "href": "content/labs/doe/index.html#footnotes",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Footnotes",
    "text": "Footnotes\n\nLawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364.↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html",
    "href": "content/labs/r-labs/maps/gram-maps.html",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#introduction",
    "href": "content/labs/r-labs/maps/gram-maps.html#introduction",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#goals",
    "href": "content/labs/r-labs/maps/gram-maps.html#goals",
    "title": "The Grammar of Maps",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n- know the types and structures of spatial data and be able to work with them\n- understand the basics of modern spatial packages in R\n- be able to specify and download spatial data from the web, using R from sources such as naturalearth and Open Streep Map\n- plot static and interactive maps using ggplot, tmap and leaflet packages\n- add symbols and markers for places and regions of our own interest in these maps.\n- plot maps on a globe using the threejs package"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#pedagogical-note",
    "href": "content/labs/r-labs/maps/gram-maps.html#pedagogical-note",
    "title": "The Grammar of Maps",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nAll jargon words will be capitalized and in bold font."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "href": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "title": "The Grammar of Maps",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#setup-the-packages",
    "href": "content/labs/r-labs/maps/gram-maps.html#setup-the-packages",
    "title": "The Grammar of Maps",
    "section": "Setup the Packages",
    "text": "Setup the Packages\n\nRun this in your Console first: devtools::install_github(\"ropensci/rnaturalearthhires\")\nInstall all packages that are flagged by RStudio when you open this RMarkdown file\n\n\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# Run this in your console first\n# devtools::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires)\n\n# Plotting Maps\nlibrary(tidyverse) # Maps using ggplot + geom_sf\nlibrary(tmap) # Thematic Maps, static and interactive\nlibrary(tmaptools)\nlibrary(tmap.mapgl)\nlibrary(osmdata) # Fetch map data from osmdata.org\n## Interactive Maps\nlibrary(leaflet) # interactive Maps\nlibrary(leaflet)\nlibrary(leaflet.providers)\nlibrary(leaflet.extras)\nlibrary(threejs) # Globe maps in R. Part of the htmlwidgets family of packages\n\n# For Spatial Data Frame Processing\nlibrary(sf)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#introduction-to-maps-in-r",
    "href": "content/labs/r-labs/maps/gram-maps.html#introduction-to-maps-in-r",
    "title": "The Grammar of Maps",
    "section": "Introduction to Maps in R",
    "text": "Introduction to Maps in R\nWe will take small steps in making maps using just two of the several map making packages in R.\nThe steps we will use are:\n\nSearch for an area of interest\nLearn how to access spatial/map data using osmdata\n\nPlot and dress up our map using ggplot and tmap\n\nCreate interactive maps with leaflet using a variety of map data providers. (Note: tmap can also do interactive maps which we will explore also.)\n\nBas. Onwards and Map-wards!!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#step1---specifying-an-area-of-interest",
    "href": "content/labs/r-labs/maps/gram-maps.html#step1---specifying-an-area-of-interest",
    "title": "The Grammar of Maps",
    "section": "Step1 - Specifying an area of interest",
    "text": "Step1 - Specifying an area of interest\nIn R, we need to specify a “BOUNDING BOX” first, to declare our area of interest. God made me a BengaluR-kaR…I think..Let’s see if we can declare an area of interest. Then we can order on Swiggy and…never mind.\nWe can declare a BOUNDING BOX in several ways.\n\nUsing a longitude latitude info from Bounding Box Tool which gives bounding boxes in many different formats.\n\n\nLocate the place of interest using the search box.\nclick on the “box with arrow” tool on the upper left. This will create a rectangular shape.\n\nMove/resize this box and then copy the bounding box from the menu at the bottom. Ensure you copy in CSV format.\n\n\n# https://boundingbox.klokantech.com\n# CSV: 77.574028,12.917262,77.595073,12.939895\nbbox_1 &lt;- matrix(\n  c(77.574028, 12.917262, 77.595073, 12.939895),\n  byrow = FALSE,\n  nrow = 2,\n  ncol = 2,\n  dimnames = list(c(\"x\", \"y\"), c(\"min\", \"max\"))\n)\nbbox_1\n\n       min      max\nx 77.57403 77.59507\ny 12.91726 12.93989\n\n\n\nUsing a place name to look up a BOUNDING BOX with osmdata::getbb. This may not always work if the place name is know well known.\n\n\n# Using getbb command from the osmdata package\nbbox_2 &lt;- osmdata::getbb(\"Malleswaram, Bangalore, India\")\nbbox_2\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\n\nLet us examine both the calculated BOUNDING BOXes:\n\nbbox_1\n\n       min      max\nx 77.57403 77.59507\ny 12.91726 12.93989\n\nbbox_2\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\n\nBoth look similar in size; bbox_2 is slightly bigger.\nWe will use the bbox_2 from the above, to ensure we have a decent collection of features. If the download becomes too hefty, we can fall back on the smaller bbox!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#step2---get-map-data",
    "href": "content/labs/r-labs/maps/gram-maps.html#step2---get-map-data",
    "title": "The Grammar of Maps",
    "section": "Step2 - Get Map data",
    "text": "Step2 - Get Map data\n\nOpenStreetMap (OSM) provides maps of the world mostly created by volunteers. They are completely free to browse and use, with attribution to © OpenStreetMap contributors and adherence to the ODbL license required, and are used by many public and private organisations. OSM data can be downloaded in vector format and used for our own purposes. In this tutorial, we will obtain data from OSM using a query. A query is a request for data from a database. Simple queries can be performed more easily using the osmdata library for R, which automatically constructs the query and imports the data in a convenient format.\n\nOpen Street Map features have attributes in key-value pairs. We can use them to download the specific data we need. These features can easily be explored in the web browser, by using the ‘Query features’ button on OpenStreetMap (OSM):\n\n\n\n\n\nOSM Features\n\nHead off to OSM Street Map to try this out and to get an intuitive understanding of what OSM key-value pairs are, for different types of map features. Look for places of interest to you (features) and see what key-value pairs attach to those features.\nNOTE: key-value pairs are also referred to as tags.\nUseful key-value pairs / tags include:\n\n\n\n\n\n\nKEY\nVALUEs\n\n\n\nbuilding\nyes (all), house residential, apartments\n\n\nhighway\nresidential, service, track, unclassified, footway, path\n\n\namenity\nparking, parking_space, bench; place_of_worship; restaurant, cafe, fast_food; school, waste_basket, fuel, bank, toilets…\n\n\nshop\nconvenience, supermarket, clothes, hairdresser, car-repair…\n\n\nname\nactual name of the place e.g. Main_Street, McDonald’s, Pizza Hut, Subway\n\n\n\nwaterway\n\n\n\nnatural\n\n\n\nboundary\n\n\n\n\nFor more information see: OSM Tags for a nice visual description of popular key-value pairs that we can use. See what the highway tag looks like tag:highway\n\n\n\n\n\n\nImportantRapidEditor.Org for OSM Maps\n\n\n\nFor a user-friendly tutorial on how to edit and add features to the OSM map, head off to Rapid Editor. Here you will learn about OSM itself and about how you can add value to it by adding data from your own surroundings.\n\n\nThe osmdata commands available_features and available_tags can help also us get the associated key-value pairs to retrieve data from OSM.\nosmdata::available_features() %&gt;%\n  as_tibble() %&gt;%\n  reactable::reactable(data = ., filterable = TRUE, minRows = 10)\navailable_tags(feature = \"highway\") %&gt;% reactable::reactable(data = ., filterable = TRUE, minRows = 10)\navailable_tags(feature = \"amenity\") %&gt;% reactable::reactable(data = ., filterable = TRUE, minRows = 10)\navailable_tags(feature = \"natural\") %&gt;% reactable::reactable(data = ., filterable = TRUE, minRows = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use these key-value pairs to download different types of map data. Within our bbox for Jayanagar, Bangalore, we want to download diverse kinds of FEATURE data. Remember that a FEATURE is any object that can be “seen” on a map. This is done using the OPQ query in the osmdata package. The main parameters for this command are:\n\nbbox\n\nKEY / VALUE pairs (“TAGS”) to specify the kind of feature you need\n\nThe query returns a list data structure, with all geometries and features within the bounding box, and we can use any or all of them. Now we know the map features we are interested in. We also know what key-value pairs will be used to get this info from OSM.\n\n\n\n\n\n\nWarningData Downloads from OSM\n\n\n\nDo not run these commands too many times. Re-run this ONLY if you have changed your BOUNDING BOX. We will get our map data from OSM and then save it avoid repeated downloads. So, please copy/paste and run the following commands in your console.\n\n\n\n# This code is for reference\n# Run these commands ONCE in your Console\n# Or run this chunk manually one time\n\n# Get all restaurants, atms, colleges within my bbox\nlocations &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = c(\"restaurant\", \"atm\", \"college\")\n  ) %&gt;%\n  osmdata_sf() %&gt;% # Convert to Simple Features format\n  purrr::pluck(\"osm_points\") # Pull out the data frame of interest\n\n# Get all buildings within my bbox\ndat_buildings &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(key = \"building\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")\n\n# Get all residential roads within my bbox\ndat_roads &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(key = \"highway\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_lines\")\n\n# Get all parks / natural /greenery areas and spots within my bbox\ndat_natural &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(\n    key = \"natural\",\n    value = c(\"tree\", \"water\", \"wood\")\n  ) %&gt;%\n  osmdata_sf()\ndat_natural\n\ndat_trees &lt;-\n  dat_natural %&gt;%\n  purrr::pluck(\"osm_points\")\n\ndat_greenery &lt;-\n  dat_natural %&gt;% pluck(\"osm_polygons\")\n\nLet us save this data, so we don’t need to download all this again! We will store the downloaded data as .gpkg files on our local hard drives to use when we run this file again later. We will name our stored files as buildings, roads, and greenery, and trees, each with the .gpkg file extension, e.g. trees.gpkg.\nCheck your local project folder for a subfolder titles /gpkg-data for these files after executing these commands.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n# Or manually run this chunk once\n\nst_write(dat_roads,\n  dsn = \"./gpkg-data/roads.gpkg\",\n  append = FALSE, quiet = FALSE\n)\n\nst_write(dat_buildings,\n  dsn = \"./gpkg-data/buildings.gpkg\",\n  append = FALSE,\n  quiet = FALSE\n)\n\nst_write(dat_greenery,\n  dsn = \"./gpkg-data/greenery.gpkg\",\n  append = FALSE, quiet = FALSE\n)\n\nst_write(dat_trees,\n  dsn = \"./gpkg-data/trees.gpkg\",\n  append = FALSE, quiet = FALSE\n)\n\n\n\n\n\n\n\nWarningWork from here when you resume!\n\n\n\nAlways work from here to avoid repeated downloads from OSM. Start from the top ONLY if you intend to map new locations and need to modify your Bounding Box.\n\n\nLet us now read back the saved Data:\n\nbuildings &lt;- st_read(\"./gpkg-data/buildings.gpkg\")\n\nReading layer `buildings' from data source \n  `/Users/arvindv/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/gpkg-data/buildings.gpkg' \n  using driver `GPKG'\nSimple feature collection with 25599 features and 127 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.54975 ymin: 12.982 xmax: 77.59085 ymax: 13.02331\nGeodetic CRS:  WGS 84\n\ngreenery &lt;- st_read(\"./gpkg-data/greenery.gpkg\")\n\nReading layer `greenery' from data source \n  `/Users/arvindv/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/gpkg-data/greenery.gpkg' \n  using driver `GPKG'\nSimple feature collection with 79 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.55102 ymin: 12.98239 xmax: 77.59089 ymax: 13.02776\nGeodetic CRS:  WGS 84\n\ntrees &lt;- st_read(\"./gpkg-data/trees.gpkg\")\n\nReading layer `trees' from data source \n  `/Users/arvindv/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/gpkg-data/trees.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2123 features and 18 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.55037 ymin: 12.98239 xmax: 77.59089 ymax: 13.02776\nGeodetic CRS:  WGS 84\n\nroads &lt;- st_read(\"./gpkg-data/roads.gpkg\")\n\nReading layer `roads' from data source \n  `/Users/arvindv/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/gpkg-data/roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 5306 features and 138 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 77.5457 ymin: 12.97867 xmax: 77.59599 ymax: 13.03094\nGeodetic CRS:  WGS 84\n\n\nHow many rows? ( Rows -&gt; Features ) What kind of geom column in each data set?\n\n# How many buildings?\nnrow(buildings)\n\n[1] 25599\n\nbuildings$geom\n\nGeometry set for 25599 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.54975 ymin: 12.982 xmax: 77.59085 ymax: 13.02331\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\nclass(buildings$geom)\n\n[1] \"sfc_POLYGON\" \"sfc\"        \n\n\nSo the buildings dataset has 25599 buildings and their geometry is naturally a POLYGON type of geometry column.\nDo this check for all the other spatial data, in the code chunk below. What kind of geom column does each dataset have?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#my-first-map-in-r",
    "href": "content/labs/r-labs/maps/gram-maps.html#my-first-map-in-r",
    "title": "The Grammar of Maps",
    "section": "My first Map in R",
    "text": "My first Map in R\nThere are two ways of plotting maps that we will learn:\nggplot and geom_sf()\nFirst we will plot with ggplot and geom_sf() : recall that our data is stored in 5 files: buildings, parks, roads, trees, and greenery.\n\nggplot() +\n  geom_sf(\n    data = buildings, fill = \"gold\", color = \"grey\",\n    linewidth = 0.025\n  ) + # POLYGONS\n  geom_sf(data = roads, color = \"#ff9999\", linewidth = 0.5) + # LINES\n  geom_sf(\n    data = greenery, col = \"darkseagreen\",\n    fill = \"lightgreen\",\n    linewidth = 0.025\n  ) + # POLYGONS\n  geom_sf(data = trees, col = \"darkgreen\", size = 0.5) + # POINTS\n\n  # Set plot limits to exactly the bbox_2\n  # coord_sf(xlim = c(bbox_2[1,1], bbox_2[1,2]),\n  #          ylim = c(bbox_2[2,1], bbox_2[2,2]),\n  #          expand = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\nNote how geom_sf is capable of handling any geometry in the sfc column !!\n\ngeom_sf() is an unusual geom because it will draw different geometric objects depending on what simple features are present in the data: you can get points, lines, or polygons.\n\nSo there, we have our first map!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "href": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "title": "The Grammar of Maps",
    "section": "Map using tmap package",
    "text": "Map using tmap package\nWe can also create a map using a package called tmap. Here we also have the option of making the map interactive. tmap plots are made with code in “groups”: each group starts with a tm_shape() command.\n\n## Let's see if we can Malleswaram look like Venice\n## Red roof tops\n# Group-1\ntm_shape(buildings) +\n  tm_polygons(fill = \"firebrick\", col = \"firebrick\") +\n\n  # Group-2\n  tm_shape(roads) +\n  tm_lines(col = \"black\", lwd = 0.5) +\n\n  # Group-3\n  tm_shape(greenery) +\n  tm_polygons(fill = \"limegreen\", col = \"limegreen\") +\n\n\n  # Group-4\n  # Malleswaram has only Banyan Trees, suppose\n  tm_shape(trees) +\n  tm_dots(fill = \"darkgreen\", size = 0.25)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-tmap",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-tmap",
    "title": "The Grammar of Maps",
    "section": "Using data from tmap\n",
    "text": "Using data from tmap\n\nLike many other packages ( e.g. ggplot ) tmap also has a few built-in spatial datasets: World and metro, rivers, land and a few others. Check help on these. Let’s plot a first map using datasets built into tmap.\n\ndata(\"World\")\nhead(World, n = 3)\n\n\n  \n\n\n\nWe have several 14 attribute variables in World. Attribute variables such as gdp_cap_est, HPI are numeric. Others such as income_grp appear to be factors. iso_a3 is the standard three letter name for the country. name is of course, the name for each country!\n\ndata(\"metro\")\nhead(metro, n = 3)\n\n\n  \n\n\n\nHere too we have attribute variables for the metros, and they seem predominantly numeric. Again iso_a3 is the three letter name for the city.\n\ntmap_mode(\"plot\") # Making this a static plot\n\n# Group 1\ntm_shape(World) + # dataset = World.\n  tm_polygons(\"HPI\") + # Colour polygons by HPI numeric variable\n\n  # Note the \"+\" sign continuation\n\n  # Group 2\n  tm_shape(metro) + # dataset = metro\n  tm_bubbles(\n    size = \"pop2030\",\n    col = \"red\"\n  ) +\n  # Plot cities as bubbles\n  # Size proportional to numeric variable `pop2030`\n\n  tm_crs(\"auto\")\n\n\n\n\n\n\n\n\n# tmap_mode(\"view\")\n# Let's use WaterColor Map this time!!\n# tm_tiles(\"OpenTopoMap\") +\ntm_shape(World) +\n  tm_polygons(\"HPI\") + # Color by Happiness Index\n\n\n  tm_shape(metro) +\n  tm_bubbles(\n    size = \"pop2030\", # Size City Markers by Population in 2020\n    col = \"red\"\n  )"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "title": "The Grammar of Maps",
    "section": "Using data from rnaturalearth\n",
    "text": "Using data from rnaturalearth\n\nThe rnaturalearth package allows us to download shapes of countries. We can use it to get borders and also internal state/district boundaries.\n\nindia &lt;-\n  ne_states(\n    country = \"india\",\n    returnclass = \"sf\"\n  ) # gives a ready sf dataframe !\n\nindia_neighbours &lt;-\n  ne_states(\n    country = (c(\n      \"sri lanka\", \"pakistan\",\n      \"afghanistan\", \"nepal\", \"bangladesh\", \"bhutan\"\n    )\n    ),\n    returnclass = \"sf\"\n  )\n\nLet’s look at the attribute variable columns to colour our graph and to shape our symbols:\n\nnames(india)\n\n  [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n  [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n [11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n [16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n [21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n [26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n [31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n [36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n [41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n [46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n [51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n [56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n [61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n [66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n [71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n [76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n [81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"name_he\"    \"name_uk\"   \n [86] \"name_ur\"    \"name_fa\"    \"name_zht\"   \"FCLASS_ISO\" \"FCLASS_US\" \n [91] \"FCLASS_FR\"  \"FCLASS_RU\"  \"FCLASS_ES\"  \"FCLASS_CN\"  \"FCLASS_TW\" \n [96] \"FCLASS_IN\"  \"FCLASS_NP\"  \"FCLASS_PK\"  \"FCLASS_DE\"  \"FCLASS_GB\" \n[101] \"FCLASS_BR\"  \"FCLASS_IL\"  \"FCLASS_PS\"  \"FCLASS_SA\"  \"FCLASS_EG\" \n[106] \"FCLASS_MA\"  \"FCLASS_PT\"  \"FCLASS_AR\"  \"FCLASS_JP\"  \"FCLASS_KO\" \n[111] \"FCLASS_VN\"  \"FCLASS_TR\"  \"FCLASS_ID\"  \"FCLASS_PL\"  \"FCLASS_GR\" \n[116] \"FCLASS_IT\"  \"FCLASS_NL\"  \"FCLASS_SE\"  \"FCLASS_BD\"  \"FCLASS_UA\" \n[121] \"FCLASS_TLC\" \"geometry\"  \n\nnames(india_neighbours)\n\n  [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n  [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n [11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n [16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n [21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n [26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n [31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n [36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n [41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n [46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n [51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n [56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n [61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n [66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n [71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n [76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n [81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"name_he\"    \"name_uk\"   \n [86] \"name_ur\"    \"name_fa\"    \"name_zht\"   \"FCLASS_ISO\" \"FCLASS_US\" \n [91] \"FCLASS_FR\"  \"FCLASS_RU\"  \"FCLASS_ES\"  \"FCLASS_CN\"  \"FCLASS_TW\" \n [96] \"FCLASS_IN\"  \"FCLASS_NP\"  \"FCLASS_PK\"  \"FCLASS_DE\"  \"FCLASS_GB\" \n[101] \"FCLASS_BR\"  \"FCLASS_IL\"  \"FCLASS_PS\"  \"FCLASS_SA\"  \"FCLASS_EG\" \n[106] \"FCLASS_MA\"  \"FCLASS_PT\"  \"FCLASS_AR\"  \"FCLASS_JP\"  \"FCLASS_KO\" \n[111] \"FCLASS_VN\"  \"FCLASS_TR\"  \"FCLASS_ID\"  \"FCLASS_PL\"  \"FCLASS_GR\" \n[116] \"FCLASS_IT\"  \"FCLASS_NL\"  \"FCLASS_SE\"  \"FCLASS_BD\"  \"FCLASS_UA\" \n[121] \"FCLASS_TLC\" \"geometry\"  \n\n# Look only at attributes\nindia %&gt;%\n  st_drop_geometry() %&gt;%\n  head()\n\n\n  \n\n\nindia_neighbours %&gt;%\n  st_drop_geometry() %&gt;%\n  head()\n\n\n  \n\n\n\nIn the india data frame:\n\nColumn iso_a2 contains the country name.\n\nColumn name contains the name of the state\n\nIn the india_neighbours data frame:\n- Column gu_a3 contains the country abbreviation\n- Column name contains the name of the state\n- Column iso_3166_2 contains the abbreviation of the state within each neighbouring country.\n\ntmap_mode(\"plot\")\n\n# Plot India\ntm_shape(india) +\n  tm_polygons(\"name\", # Colour by States in India\n    fill.legend = tm_legend_hide()\n  ) +\n\n  # Plot Neighbours\n  tm_shape(india_neighbours) +\n  tm_fill(col = \"gu_a3\") + # Colour by Country Name\n\n  # Plot the cities in India alone\n  tm_shape(metro %&gt;% dplyr::filter(iso_a3 == \"IND\")) +\n\n  tm_dots(\n    size = \"pop2020\",\n    size.legend = tm_legend_hide()\n  ) +\n  # size by population in 2020\n\n  tm_layout(legend.show = FALSE) +\n  tm_credits(\"Geographical Boundaries are not accurate\",\n    size = 0.5,\n    position = \"right\"\n  ) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scalebar(position = \"left\") +\n  tmap_style(style = \"white\")\n\n\n\n\n\n\n# Try other map styles\n# cobalt #gray #white #watercolor #beaver #classic #watercolor #albatross #bw #col_blind"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "href": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "title": "The Grammar of Maps",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nCan you try to download a map area of your home town and plot it as we have above?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "title": "The Grammar of Maps",
    "section": "Adding my favourite Restaurants to the map",
    "text": "Adding my favourite Restaurants to the map\nIs it time to order on Swiggy…\nLet us adding interesting places to our map: say based on your favourite restaurants etc. We need restaurant data: lat/long + name + maybe type of restaurant. This can be manually created ( like all of OSMdata ) or if it is already there we can download using key-value pairs in our OSM data query.\nRestaurants can be downloaded using key= \"amenity\", value = \"restaurant\" or \"cafe\" etc. There are also other tags to explore!Searching for McDonalds for instance…( key = \"name\", value = \"McDonalds\"). Since we want JUST their location, and not the restaurant BUILDINGs, we extract osm_points.\n\n# Again, run these commands in your Console\ndat_R &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = c(\"restaurant\")\n  ) %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_points\")\n\n# Save the data for future use\nwrite_sf(dat_R, dsn = \"restaurants.gpkg\", append = FALSE, quiet = FALSE)\n\nNow reading the saved Restaurant Data:\n\nrestaurants &lt;- st_read(\"./restaurants.gpkg\")\n\nReading layer `restaurants' from data source \n  `/Users/arvindv/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/restaurants.gpkg' \n  using driver `GPKG'\nSimple feature collection with 194 features and 66 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.55053 ymin: 12.98415 xmax: 77.5902 ymax: 13.02244\nGeodetic CRS:  WGS 84\n\n\nHow many restaurants have we got?\n\nrestaurants %&gt;% nrow()\n\n[1] 194\n\n\nSo the restaurants dataset has 194 restaurants and their geometry is naturally a POINT type of geom column.\nThese are the names of columns in the Restaurant Data: Note the cuisine column.\n\nglimpse(restaurants)\n\nRows: 194\nColumns: 67\n$ osm_id                   &lt;chr&gt; \"456029893\", \"461539222\", \"577020540\", \"57796…\n$ name                     &lt;chr&gt; \"Hallimane\", \"Adiga's\", \"Vishnu Sagar\", \"Emir…\n$ addr.city                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.country             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.district            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.floor               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.full                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.hamlet              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.housename           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.housenumber         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.postcode            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.state               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.street              &lt;chr&gt; NA, \"Sampige Road\", NA, \"Suberdar Chatram Roa…\n$ addr.suburb              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ addr.unit                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ air_conditioning         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ alt_name                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ amenity                  &lt;chr&gt; \"restaurant\", \"restaurant\", \"restaurant\", \"re…\n$ bar                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ brand                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ brand.wikidata           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ brand.wikipedia          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ capacity                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ check_date               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ check_date.opening_hours &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ contact.instagram        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ cuisine                  &lt;chr&gt; NA, \"indian\", NA, NA, \"indian\", \"chinese\", NA…\n$ delivery                 &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ description              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ diet.halal               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ diet.non.vegetarian      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ diet.vegan               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ diet.vegetarian          &lt;chr&gt; NA, NA, NA, NA, \"only\", NA, NA, NA, NA, \"only…\n$ entrance                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ fax                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ happycow.id              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ indoor_seating           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ internet_access          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ level                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ mapillary                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ name.en                  &lt;chr&gt; NA, \"Brahmin's Coffee Bar\", NA, NA, NA, NA, N…\n$ name.kn                  &lt;chr&gt; \"ಹಳ್ಳಿಮನೆ\", \"ಬ್ರಾಹ್ಮಿನ್ಸ್ ಕಾಫಿ ಬಾರ್\", \"ವಿಷ್ಣು ಸಾಗರ\", \"ಎಮಿರೇಟ್ಸ್\", …\n$ official_name            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ old_name                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ opening_hours            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ operator                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ organic                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ outdoor_seating          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ payment.cash             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ payment.credit_cards     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ payment.debit_cards      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ phone                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ phone_1                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ reservation              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ shop                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ smoking                  &lt;chr&gt; NA, \"no\", NA, NA, \"no\", NA, NA, NA, NA, NA, N…\n$ source                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ start_date               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ takeaway                 &lt;chr&gt; NA, \"yes\", NA, NA, \"yes\", NA, NA, NA, NA, NA,…\n$ toilets                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ toilets.wheelchair       &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ user_defined             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ website                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wheelchair               &lt;chr&gt; NA, \"no\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ wikidata                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ wikipedia                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ geom                     &lt;POINT [°]&gt; POINT (77.57177 12.99553), POINT (77.57…\n\n\nSo let us plot the restaurants as POINTs using the restaurants data we have downloaded. The cuisine attribute looks interesting; let us colour the POINT based on the cuisine offered at that restaurant.\nSo Let’s look therefore at the cuisine column!\n\n# ( I want pizza...)\nrestaurants$cuisine %&gt;% unique()\n\n [1] NA                            \"indian\"                     \n [3] \"chinese\"                     \"regional\"                   \n [5] \"chicken;portuguese\"          \"italian\"                    \n [7] \"japanese\"                    \"Bengali\"                    \n [9] \"chats\"                       \"fast_food\"                  \n[11] \"indian;seafood\"              \"indian,chinese\"             \n[13] \"pizza\"                       \"indian;juice;ice_cream\"     \n[15] \"asian\"                       \"persian\"                    \n[17] \"american\"                    \"french\"                     \n[19] \"tex-mex\"                     \"punjabi\"                    \n[21] \"south_indian\"                \"mexican\"                    \n[23] \"regional;coffee_shop;indian\" \"french;burger\"              \n\n\nBig mess…many NAs, some double entries, separated by commas and semicolons….\n\n\n\n\n\n\nNoteThe cuisine attribute:\n\n\n\nNote: The cuisine variable has more than one entry for a given restaurant. We use tidyr::separate_*_*() to make multiple columns out of the cuisine column and retain the first one only. Since the entries are badly entered using both “;” and “,” we need to do this twice ;-() Bad Data entry!!\n\n\nLet’s get one cuisine entry per restaurant, and drop off the ones that do not mention a cuisine at all:\n\nrestaurants &lt;- restaurants %&gt;%\n  drop_na(cuisine) %&gt;% # Knock off nondescript restaurants\n\n  # Some have more than one classification ;-()\n  # Separated by semicolon or comma, so....\n  separate_wider_delim(\n    cols = cuisine,\n    names = c(\"cuisine\", NA, NA),\n    delim = \";\",\n    too_few = \"align_start\",\n    too_many = \"drop\"\n  ) %&gt;%\n  separate_wider_delim(\n    cols = cuisine,\n    names = c(\"cuisine\", NA, NA),\n    delim = \",\",\n    too_few = \"align_start\",\n    too_many = \"drop\"\n  )\n\n# Finally good food?\nrestaurants$cuisine\n\n  [1] \"indian\"       \"indian\"       \"chinese\"      \"indian\"       \"indian\"      \n  [6] \"indian\"       \"regional\"     \"regional\"     \"indian\"       \"regional\"    \n [11] \"regional\"     \"indian\"       \"chicken\"      \"italian\"      \"chinese\"     \n [16] \"regional\"     \"indian\"       \"japanese\"     \"regional\"     \"indian\"      \n [21] \"regional\"     \"Bengali\"      \"regional\"     \"chats\"        \"regional\"    \n [26] \"regional\"     \"indian\"       \"indian\"       \"indian\"       \"fast_food\"   \n [31] \"fast_food\"    \"indian\"       \"indian\"       \"indian\"       \"fast_food\"   \n [36] \"indian\"       \"regional\"     \"chinese\"      \"indian\"       \"chinese\"     \n [41] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [46] \"regional\"     \"regional\"     \"pizza\"        \"regional\"     \"regional\"    \n [51] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [56] \"regional\"     \"regional\"     \"regional\"     \"indian\"       \"regional\"    \n [61] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"indian\"      \n [66] \"indian\"       \"indian\"       \"regional\"     \"regional\"     \"indian\"      \n [71] \"regional\"     \"indian\"       \"regional\"     \"regional\"     \"pizza\"       \n [76] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [81] \"asian\"        \"persian\"      \"american\"     \"regional\"     \"regional\"    \n [86] \"regional\"     \"regional\"     \"regional\"     \"french\"       \"tex-mex\"     \n [91] \"indian\"       \"pizza\"        \"asian\"        \"punjabi\"      \"south_indian\"\n [96] \"indian\"       \"mexican\"      \"regional\"     \"mexican\"      \"asian\"       \n[101] \"indian\"       \"regional\"     \"pizza\"        \"french\"      \n\n\nLooks clean! Each entry is only ONE and not multiple any more. Now let’s plot the Restaurants as POINTs:\n\n# http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\n#\nggplot() +\n  geom_sf(data = buildings, colour = \"firebrick\") +\n  geom_sf(data = roads, colour = \"gold\", linewidth = 0.25) +\n  geom_sf(\n    data = restaurants %&gt;% drop_na(cuisine),\n    aes(fill = cuisine, geometry = geom),\n    colour = \"black\",\n    shape = 21,\n    size = 3\n  ) +\n  # Set plot limits to exactly the bbox_2\n  # coord_sf(xlim = c(bbox_2[1,1], bbox_2[1,2]),\n  #          ylim = c(bbox_2[2,1], bbox_2[2,2]),\n  #          expand = FALSE) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(\n    title = \"Restaurants in Malleswaram, Bangalore\",\n    caption = \"Based on osmdata\"\n  )\n\n\n\n\n\n\n\nWe could have done a (much!) better job, by combining cuisines into simpler and fewer categories, ( South_India and South_Indian ), but that is for another day!!\nBy now we know that we can use geom_sf() multiple number of times with different datasets to create layered maps in R."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#some-fancy-stuff",
    "href": "content/labs/r-labs/maps/gram-maps.html#some-fancy-stuff",
    "title": "The Grammar of Maps",
    "section": "Some fancy stuff",
    "text": "Some fancy stuff\nLet us try making glob based maps with the package threejs. This package is one of the family of packages in the htmlwidgets group of packages. It allows the use of some ( famous!) JavaScript graphing libraries directly and natively in R.\n\nglobejs usage\nThe globejs command from the package threejs allows one to plot points, arcs and images on a globe in 3D. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\nWe will generate some random locations and plot them on the 3D globe.\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n\nglobejs(lat = lat, long = long)\n\n\n\n\n\nAs seen, “spikes” are created at the random lat-lon locations. We can control the height/width/colour of the spikes, as well as the initial view of the globe itself: zoom, location and so on\n\nglobejs(\n  lat = lat,\n  long = long,\n\n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#scope-and-packages-for-exploration",
    "href": "content/labs/r-labs/maps/gram-maps.html#scope-and-packages-for-exploration",
    "title": "The Grammar of Maps",
    "section": "Scope and Packages for Exploration!!",
    "text": "Scope and Packages for Exploration!!\nsfnetworks / tmap networks\nmapsf\nggspatial"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#resources",
    "href": "content/labs/r-labs/maps/gram-maps.html#resources",
    "title": "The Grammar of Maps",
    "section": "Resources",
    "text": "Resources\n\nFree Map Tile services. https://alexurquhart.github.io/free-tiles/\nMartijn Tennekes and Jakub Nowosad (2025). Elegant and informative maps with tmap. https://tmap.geocompx.org\nEmine Fidan, Guide to Creating Interactive Maps in R\nNikita Voevodin,R, Not the Best Practices\nRapidEditor.Org. Web-based editor for community-data addition to OSM Maps. Can be used to really add value to local mappers and for your own projects."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#assignments",
    "href": "content/labs/r-labs/maps/gram-maps.html#assignments",
    "title": "The Grammar of Maps",
    "section": "Assignments",
    "text": "Assignments\n\nDraw a map of your home-town with your favourite restaurants shown. Pop-ups for each restaurant will win bonus points.\nDownload bird migration data from movebank.org. Import these into R and plot a migration map using tmap. Include the graticule, compass, legend, and credits.\n\nInspiration\n\nBurkhart, Christian. n.d. “Streetmaps.” StreetMaps\n\n\n\n\n\nMaking Vector Maps, Computing for the Social Sciences, Univ. of Chicago"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html",
    "href": "content/labs/r-labs/installation/installation.html",
    "title": "Lab 01 - Installation",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "title": "Lab 01 - Installation",
    "section": "Check in",
    "text": "Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "title": "Lab 01 - Installation",
    "section": "Check in",
    "text": "Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#save-and-share",
    "href": "content/labs/r-labs/installation/installation.html#save-and-share",
    "title": "Lab 01 - Installation",
    "section": "Save and share",
    "text": "Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nThat’s it! You have created and saved your first chart in R!!!"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\n  \"id\", \"state_fips\", \"county_fips\", \"name\", \"year\",\n  \"?\", \"?\", \"?\", \"rate\"\n)\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text),\n    colour = alpha(\"white\", 1 / 2), size = 0.2\n  ) +\n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") +\n  theme_void()\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap(~variable) +\n  theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\n  \"id\", \"state_fips\", \"county_fips\", \"name\", \"year\",\n  \"?\", \"?\", \"?\", \"rate\"\n)\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text),\n    colour = alpha(\"white\", 1 / 2), size = 0.2\n  ) +\n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") +\n  theme_void()\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap(~variable) +\n  theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "title": "ggplotly: various examples",
    "section": "Row",
    "text": "Row\nFaithful Eruptions\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d() +\n  xlim(0.5, 6) +\n  ylim(40, 110)\nggplotly(m)\n\n\n\n\n\nFaithful Eruptions (polygon)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  xlim(0.5, 6) +\n  ylim(40, 110)\nggplotly(m)\n\n\n\n\n\nFaithful Eruptions (hex)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_hex()\nggplotly(m)\n\nError in loadNamespace(x): there is no package called 'hexbin'"
  },
  {
    "objectID": "content/labs/r-labs/tidy/tidyversebooster/index.html",
    "href": "content/labs/r-labs/tidy/tidyversebooster/index.html",
    "title": "A Tidyverse Booster",
    "section": "",
    "text": "I am going to replicate the fantastic tutorial material from Christian Burkhart’s website. It is going to be long, but boy, is it worth it! I expect that thsi tutorial when complete will offer a cookbook when I want to achieve specific things in R, reading, arranging, plotting, graphing, inference, right upto ML, since these tasks are capable of automating and solving so many otherwise knotty problems.\n(This is also an atonement of sorts: I had done this two years ago and not saved my file on the cloud.)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/tidyversebooster/index.html#introduction",
    "href": "content/labs/r-labs/tidy/tidyversebooster/index.html#introduction",
    "title": "A Tidyverse Booster",
    "section": "",
    "text": "I am going to replicate the fantastic tutorial material from Christian Burkhart’s website. It is going to be long, but boy, is it worth it! I expect that thsi tutorial when complete will offer a cookbook when I want to achieve specific things in R, reading, arranging, plotting, graphing, inference, right upto ML, since these tasks are capable of automating and solving so many otherwise knotty problems.\n(This is also an atonement of sorts: I had done this two years ago and not saved my file on the cloud.)"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html",
    "href": "content/labs/r-labs/pronouns/pronouns.html",
    "title": "Lab-02: Pronouns and Data",
    "section": "",
    "text": "Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific geometric aspect in the data visualization.\nUnderstand how ask Questions of Data to develop Visualizations"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "href": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "title": "Lab-02: Pronouns and Data",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "The penguins dataset",
    "text": "The penguins dataset\n\nnames(penguins) # Column, i.e. Variable names\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(penguins) # first six rows\n\n\n  \n\n\ntail(penguins) # Last six rows\n\n\n  \n\n\ndim(penguins) # Size of dataset\n\n[1] 344   8\n\n# Check for missing data\nany(is.na(penguins) == TRUE)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNoteInspect the Data\n\n\n\n\nWhat are the variable names()?\nWhat would be the Question you might have asked to obtain each of the variables?\nWhat further questions/meta questions would you ask to “process” that variable? ( Hint: Add another word after any of the Interrogative Pronouns, e.g. How…MANY?)\nWhere might the answers take your story?\n\n\n\n\n\n\n\n\n\nNoteYour Turn #1\n\n\n\nState a few questions after discussion with your friend and state possible variables, or what you could DO with the variables, as an answer.\nE.g. Q. How many penguins? A. We need to count…rows?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "href": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "title": "Lab-02: Pronouns and Data",
    "section": "Pronouns and Variables",
    "text": "Pronouns and Variables\nIn the Table below, we have a rough mapping of interrogative pronouns to the kinds of variables in the data:\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers. Each variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens (https://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf)\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.\n\nDo think about this as you work with data.\n\nDo take a look at these references:\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\nhttps://www.freecodecamp.org/news/types-of-data-in-statistics-nominal-ordinal-interval-and-ratio-data-types-explained-with-examples/"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "The mpg dataset",
    "text": "The mpg dataset\n\nnames(mpg) # Column, i.e. Variable names\n\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"       \n\nhead(mpg) # first six rows\n\n\n  \n\n\ntail(mpg) # Last six rows\n\n\n  \n\n\ndim(mpg) # Size of dataset\n\n[1] 234  11\n\n# Check for missing data\nany(is.na(mpg) == TRUE)\n\n[1] FALSE\n\n\nYOUR TURN-2\nLook carefully at the variables here. How would you interpret say the cyl variable? Is it a number and therefore Quantitative, or could it be something else?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "Single Qualitative/Categorical/ Nominal Variable",
    "text": "Single Qualitative/Categorical/ Nominal Variable\n\nQuestions: Which? What Kind? How? How many of each Kind?\n\n\nIsland ( Which island ? )\nSpecies ( Which Species? )\n\n\nCalculations: No of levels / Counts for each level\n\n\n\n\ncount / tally of no. of penguins on each island or in each species\n\nsort and order by island or species\n\n\nCharts: Bar Chart / Pie Chart / Tree Map\n\n\n\ngeom_bar / geom_bar + coord_polar() / Find out!!\n\n\npenguins %&gt;% count(species)\n\n\n  \n\n\n\n\nggplot(penguins) +\n  geom_bar(aes(x = island))\n\n\n\n\n\n\nggplot(penguins) +\n  geom_bar(aes(x = sex))\n\n\n\n\n\n\n\nYOUR TURN-3"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "Single Quantitative Variable",
    "text": "Single Quantitative Variable\n\nQuestions: How many? How few? How often? How much?\nCalculations: max / min / mean / mode / (units)\n\n\n\nmax(), min(), range(), mean(), mode(), summary()\n\n\n\nCharts: Bar Chart / Histogram / Density\n\n\ngeom_histogram() / geom_density()\n\n\n\n\n\nmax(penguins$bill_length_mm)\n\n[1] 59.6\n\nrange(penguins$bill_length_mm, na.rm = TRUE)\n\n[1] 32.1 59.6\n\nsummary(penguins$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    172     190     197     201     213     231 \n\n\n\nggplot(penguins) +\n  geom_density(aes(bill_length_mm))\n\n\n\n\n\n\nggplot(penguins) +\n  geom_histogram(aes(x = bill_length_mm))\n\n\n\n\n\n\n\nYOUR TURN-4\nAre all the above Quantitative variables ratio variables? Justify."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Quantitative vs Quantitative",
    "text": "Two Variables: Quantitative vs Quantitative\nWe can easily extend our intuition about one quantitative variable, to a pair of them. What Questions can we ask?\n\nQuestions: How many of this vs How many of that? Does this depend upon that? How are they related? (Remember \\(y = mx + c\\) and friends?)\nCalculations: Correlation / Covariance / T-test / Chi-Square Test for Two Means etc. We won’t go into this here !\nCharts: Scatter Plot / Line Plot / Regression i.e. best fit lines\n\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm)\n\n[1] -0.2286256\n\n\n\nggplot(penguins) +\n  geom_point(aes(\n    x = flipper_length_mm,\n    y = body_mass_g\n  ))\n\n\n\n\n\n\nggplot(penguins) +\n  geom_point(aes(\n    x = flipper_length_mm,\n    y = bill_length_mm\n  ))\n\n\n\n\n\n\n\nYOUR TURN-5"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Categorical vs Categorical",
    "text": "Two Variables: Categorical vs Categorical\nWhat sort of question could we ask that involves two categorical variables?\n\nQuestions: How Many of this Kind( ~x) are How Many of that Kind( ~y ) ?\n\nCalculations: Counts and Tallies sliced by Category\n\n\ncounts , tally\n\n\n\n\nCharts: Stacked Bar Charts / Grouped Bar Charts / Segmented Bar Chart / Mosaic Chart\n\ngeom_bar()\nUse the second Categorical variables to modify fill, color.\nAlso try to vary the parameter position of the bars.\n\n\n\n\nggplot(penguins) +\n  geom_bar(\n    aes(\n      x = island,\n      fill = species\n    ),\n    position = \"stack\"\n  )\n\n\n\n\n\n\n\nStoryline: तीन पेनगीन। और तुम भी तीन(Oh never mind!)\nYOUR TURN-6"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Quantitative vs Qualitative",
    "text": "Two Variables: Quantitative vs Qualitative\nFinally, what if we want to look at Quant variables and Qual variables together? What questions could we ask?\n\nQuestions: How much of this is Which Kind of that? How many vs Which? How many vs How?\nCalculations: Counts, Means, Ranges etc., grouped by Categorical variable.\n\n\nggplot(penguins) +\n  geom_density(\n    aes(\n      x = body_mass_g,\n      color = island,\n      fill = island\n    ),\n    alpha = 0.3\n  )\n\n\n\n\n\n\n\n\nCharts: Bar Chart using group / density plots by group / violin plots by group / box plots by group\n\n\n\ngeom_bar / geom_density / geom_violin / geom_boxplot using Categorical Variable for grouping\n\n\nggplot(penguins) +\n  geom_density(\n    aes(\n      x = body_mass_g,\n      color = island,\n      fill = island\n    ),\n    alpha = 0.3\n  )\n\n\n\n\n\n\nggplot(penguins) +\n  geom_histogram(aes(\n    x = flipper_length_mm,\n    fill = sex\n  ))\n\n\n\n\n\n\n\nYOUR TURN-7\nTime to Play\n\nCreate a fresh RMarkdown and similarly analyse two datasets of the following data sets\n\n\nAny dataset in your R installation. Type data() in your console to see what is available.\ndiamonds . This dataset is part of the tidyverse package so just type diamonds in your code and there it is.\ngapminder !! Yes!!You will need to install the gapminder package to access this dataset\nmosaicData package datasets. Install mosaicData\ndata.world: Find Datasets of your choice: https://docs.data.world/en/64499-64516-Quickstarts-and-tutorials.html\nkaggle: https://www.kaggle.com/datasets"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html",
    "href": "content/labs/r-labs/graphics/index.html",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#introduction",
    "href": "content/labs/r-labs/graphics/index.html#introduction",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#goals",
    "href": "content/labs/r-labs/graphics/index.html#goals",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should: - know the types and structures of tidy data and be able to work with them - be able to create data visualizations using ggplot - Understand aesthetics and scales in `ggplot"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#set-up",
    "href": "content/labs/r-labs/graphics/index.html#set-up",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggstance)\n# A collection of historical datasets\nlibrary(HistData)\nlibrary(sf)\nlibrary(sfheaders)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "href": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "A Teaser from John Snow",
    "text": "A Teaser from John Snow"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "href": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Review of Tidy Data",
    "text": "Review of Tidy Data\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R. (The syntax to bring in a web-figure is ![caption](url))\n The three features described in the figure above define the nature of tidy data:\n\n\nVariables in Columns\n\n\nObservations in Rows and\n\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "href": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Kinds of Variables",
    "text": "Kinds of Variables\nKinds of Variable are defined by the kind of questions they answer to:\n\nWhat/Who/Where? -&gt; Some kind of Name. Categorical variable\nWhat Kind? How? -&gt; Some kind of “Type”. Factor variable\nHow Many? How large? -&gt; Some kind of Quantity. Numerical variable. Most Figures in R are computed with variables, and therefore, with columns."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "href": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Interrogations and Graphs",
    "text": "Interrogations and Graphs\nCreating graphs from data is an act of asking questions and viewing answers in a geometric way. Let us write some simple English descriptions of measures and visuals and see what commands they use in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#components-of-the-layered-grammar-of-graphics",
    "href": "content/labs/r-labs/graphics/index.html#components-of-the-layered-grammar-of-graphics",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Components of the layered grammar of graphics",
    "text": "Components of the layered grammar of graphics\nLayers are used to create the objects on a plot. They are defined by five basic parts:\n\nData (What dataset/spreadsheet am I using?)\nMapping (What does each column do in my graph?)\nStatistical transformation (stat) (Do I have count something first?)\nGeometric object (geom) (What shape, colour, size…do I want?)\nPosition adjustment (position) (Where do I want it on the graph?)\n\nData\nWe will use “real world” data. Let’s use the penguins dataset in the palmerpenguins package. Run ?penguins in the console to get more information about this dataset.\nHead\n\nhead(penguins)\n\n\n  \n\n\n\nTail\n\ntail(penguins)\n\n\n  \n\n\n\nDim\n\ndim(penguins)\n\n[1] 344   8\n\n\nSo we know what our data looks like. We pass this data to ggplot use to plot as follows: in R this creates an empty graph sheet!! Because we have not (yet) declared the geometric shapes we want to use to plot our information.\n\nggplot(data = penguins) # Creates an empty graphsheet, ready for plotting!!\n\n\n\n\n\n\n\nMapping\nNow that we have told R what data to use, we need to state what variables to plot and how.\nAesthetic Mapping defines how the variables are applied to the plot, i.e. we take a variable from the data and “metaphorize” it into a geometric feature. We can map variables metaphorically to a variety of geometric things: coordinate, length, height, size, shape, colour, alpha(how dark?)….\nThe syntax uses: aes(some_geometric_thing = some_variable)\nRemember variable = column.\nSo if we were graphing information from penguins, we might map a penguin’s flipper_length_mm column to the \\(x\\) position, and the body_mass_g column to the \\(y\\) position.\nMapping Example-1\nWe can try another example of aesthetic mapping with the same dataset:\nPlot-1a\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nPlot-1b\n\nggplot(penguins) +\n\n  # Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g)\n  )\n\n\n\n\n\n\n\nPlot-1c\n\nggplot(penguins) +\n\n  # Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(\n      x = body_mass_g,\n      fill = island\n    ) # color aesthetic = another variable\n  )\n\n\n\n\n\n\n\nMapping Example-2\nWe can try another example of aesthetic mapping with the same dataset:\nPlot-2a\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nPlot-2b\n\nggplot(penguins) +\n\n  # Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g)\n  ) \n\n\n\n\n\n\n\nPlot-2c\n\nggplot(penguins) +\n\n  # Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(\n      x = body_mass_g,\n      fill = species\n    ) #&lt;&lt; # color aesthetic = another variable\n  )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "href": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality:\n\n0 dimensions - point, text\n1 dimension - path, line\n2 dimensions - polygon, interval\n\nEach geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics.\nWe can also stack up geoms on top of one another to add layers to the graph.\nPlot1\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_line()\n\n\n\n\n\n\n\nPlot2\n\nggplot(data = penguins) +\n  geom_line(aes(\n    x = bill_length_mm,\n    y = body_mass_g\n  ))\n\n\n\n\n\n\n\nPlot3\n\nggplot(data = penguins) +\n  geom_point(\n    aes(\n      x = bill_length_mm,\n      y = body_mass_g,\n      color = island,\n      shape = species\n    ),\n    size = 3\n  ) +\n  ggtitle(\"A point geom with position and color and shape aesthetics\")\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  aes(x = species)\n) + # x position =&gt; ?\n  # No need to type \"mapping\"...\n  geom_bar() + # Where does the height come from?\n  ggtitle(\"A bar geom with position and height aesthetics\")\n\n\n\n\n\n\n\n\nggplot(data = penguins, aes(x = island)) +\n  geom_bar() +\n  ggtitle(\"A bar geom with position and height aesthetics\")\n\n\n\n\n\n\n\n\nPosition determines the starting location (origin) of each bar\nHeight determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each possible species."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "href": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Position adjustment",
    "text": "Position adjustment\nSometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap:\nSometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting.\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = species,\n    y = body_mass_g\n  )\n) +\n  geom_point() +\n  ggtitle(\"A point geom with obscured data points\")\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = species,\n    y = body_mass_g\n  )\n) +\n  geom_jitter() +\n  ggtitle(\"A point geom with jittered data points\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "href": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Statistical transformation",
    "text": "Statistical transformation\nA statistical transformation (stat) pre-transforms the data, before plotting. For instance, in a bar graph you might summarize the data by counting the total number of observations within a set of categories, and then plotting the count.\nCount\n\ncount(x = penguins, island)\n\n\n  \n\n\n\nCount and Bar Graph\n\nmydat &lt;- count(penguins, island)\n\nggplot(data = mydat) +\n  geom_col(aes(x = island, y = n))\n\n\n\n\n\n\n\nTidy Count and Bar Graph\n\n\n\n\n\n\n\n\nCount inside the Plot\n\npenguins %&gt;% # Our pipe Operator\n\n  ggplot(.) + # \".\" becomes the penguins dataset\n\n  geom_bar(aes(x = island)) # Note: geom_BAR !! y = count, and is computed internally!!\n\n\n\n\n\n\n\nSometimes you don’t need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#scale",
    "href": "content/labs/r-labs/graphics/index.html#scale",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Scale",
    "text": "Scale\nA scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color:\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_depth_mm,\n    y = bill_length_mm,\n    color = species\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\nThe scale can be changed to use a different color palette:\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm,\n    y = body_mass_g,\n    color = species\n  )\n) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\", direction = -1)\n\n\n\n\n\n\n\nNow we are using a different palette, but the scale is still consistent: all Adelie penguins utilize the same color, whereas Chinstrap use a new color but each Adelie still uses the same, consistent color."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "href": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Coordinate system",
    "text": "Coordinate system\nA coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system:\n\nx1 &lt;- c(1, 10)\ny1 &lt;- c(1, 5)\np &lt;- qplot(\n  x = x1, y = y1,\n  geom = \"point\", # Quick Plot. Deprecated, don't use\n  xlab = NULL, ylab = NULL\n) +\n  theme_bw()\np +\n  ggtitle(label = \"Cartesian coordinate system\")\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) +\n  geom_point() +\n  coord_polar()\n\n\n\n\n\n\n\nThis system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis:\n\np +\n  coord_trans(y = \"log10\") +\n  ggtitle(label = \"Semi-log coordinate system\")\n\n\n\n\n\n\n\nOr could even be drawn using polar coordinates:\n\np +\n  coord_polar() +\n  ggtitle(label = \"Polar coordinate system\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#faceting",
    "href": "content/labs/r-labs/graphics/index.html#faceting",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Faceting",
    "text": "Faceting\nFaceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged.\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm,\n    y = body_mass_g\n  )\n) +\n  geom_point() +\n  facet_wrap(~island)\n\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm,\n    y = body_mass_g,\n    color = sex\n  )\n) +\n  geom_point() +\n  facet_grid(species ~ island, scales = \"free_y\")\n\n\n\n\n\n\n# Ria's explanation: This code did not work because....\n#"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#defaults",
    "href": "content/labs/r-labs/graphics/index.html#defaults",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Defaults",
    "text": "Defaults\nRather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position.\nConsider the following scenario: you wish to generate a scatterplot visualizing the relationship between penguins’ bill_length and their body_mass. With no defaults, the code to generate this graph is:\n\nggplot() +\n  layer(\n    data = penguins,\n    mapping = aes(\n      x = bill_length_mm,\n      y = body_mass_g\n    ),\n    geom = \"point\",\n    stat = \"identity\",\n    position = \"identity\"\n  ) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  coord_cartesian()\n\n\n\n\n\n\n\nThe above code:\n\nCreates a new plot object (ggplot)\n\nAdds a layer (layer)\n\nSpecifies the data (penguins)\nMaps engine bill length to the \\(x\\) position and body mass to the \\(y\\) position (mapping)\nUses the point geometric transformation (geom = \"point\")\nImplements an identity transformation and position (stat = \"identity\" and position = \"identity\")\n\n\nEstablishes two continuous position scales (scale_x_continuous and scale_y_continuous)\nDeclares a cartesian coordinate system (coord_cartesian)\n\nHow can we simplify this using intelligent defaults?\n\nWe only need to specify one geom and stat, since each geom has a default stat.\nCartesian coordinate systems are most commonly used, so it should be the default.\n\nDefault scales can be added based on the aesthetic and type of variables.\n\nContinuous values are transformed with a linear scaling.\nDiscrete values are mapped to integers.\nScales for aesthetics such as color, fill, and size can also be intelligently defaulted.\n\n\n\nUsing these defaults, we can rewrite the above code as:\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(\n      x = bill_length_mm,\n      y = body_mass_g\n    )\n  )\n\n\n\n\n\n\n\nThis generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function:\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm,\n    y = body_mass_g\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\nAnd as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping:\n\nggplot(penguins, aes(bill_length_mm, body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html",
    "href": "content/posts/09-using-lordicons/index.html",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#introduction",
    "href": "content/posts/09-using-lordicons/index.html#introduction",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#installation",
    "href": "content/posts/09-using-lordicons/index.html#installation",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Installation",
    "text": "Installation\nType these in your Terminal:\n\nIconify: quarto install extension mcanouil/quarto-iconify\nFontAwesome: quarto install extension quarto-ext/fontawesome\nLordicons: quarto install extension jmgirard/lordicon\nAcademicons: quarto install extension schochastics/academicons\n\nThese extensions allows you to use a variety of icons in your Quarto HTML documents."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Lordicon Shortcodes",
    "text": "Using Lordicon Shortcodes\nThe {{&lt; li &gt;}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{&lt; lif &gt;}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n\n{{&lt; li code &gt;}}\n{{&lt; li wlpxtupd &gt;}}\n\n\n\n{{&lt; lif file &gt;}}\n{{&lt; lif church.json &gt;}}\n\n\n\n\n\nTriggers\ntrigger controls the icon’s animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li wxnxiano &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=click &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=morph &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=boomerang &gt;}}\n\n\n\n\n\n\nSpeed\nspeed controls how quickly the icon’s animation plays.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=0.5 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=1.0 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=2.0 &gt;}}\n\n\n\n\n\n\nColors\ncolors controls the icon’s coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gold &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gray,secondary:orange &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:#4030e8,secondary:#ee66aa &gt;}}\n\n\n\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc stroke=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=100 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=150 &gt;}}\n\n\n\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc scale=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=100 &gt;}}\n\n\n\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc x=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=100 &gt;}}\n\n\n\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc y=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=100 &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Academicons Shortcodes",
    "text": "Using Academicons Shortcodes\nThis extension allows you to use academicons in your Quarto HTML documents. It provides an {{&lt; ai &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; ai &lt;icon-name&gt; &gt;}}\nOptional &lt;size=...&gt;:\n{{&lt; ai &lt;icon-name&gt; &lt;size=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; ai arxiv &gt;}}\n\n\n\n{{&lt; ai google-scholar &gt;}}\n\n\n\n{{&lt; ai open-access &gt;}}\n\n\n\n{{&lt; ai open-access size=5x &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "href": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Fontawesome Icons",
    "text": "Using Fontawesome Icons\nThis extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Iconify Shortcodes",
    "text": "Using Iconify Shortcodes\nThis extension allows you to use Iconify icons in your Quarto HTML documents. It provides an {{&lt; iconify &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; iconify &lt;icon-name&gt; &gt;}}\nOptional &lt;set&gt; (default is fluent-emoji) &lt;size=...&gt;, &lt;width=...&gt;, &lt;height=...&gt;, &lt;flip=...&gt;, and &lt;rotate=...&gt;:\n{{&lt; iconify &lt;set&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;width=...&gt; &lt;height=...&gt; &lt;flip=...&gt; &lt;rotate=...&gt; &gt;}}\nIf &lt;size=...&gt; is defined, &lt;width=...&gt; and &lt;height=...&gt; are not used.\nSee https://docs.iconify.design/iconify-icon/ for more details.\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; iconify exploding-head &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=2xl &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=5x rotate=180deg &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=Huge &gt;}}\n\n\n\n{{&lt; iconify fluent-emoji-high-contrast 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify twemoji 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify line-md loading-alt-loop &gt;}}\n\n\n\n{{&lt; iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical &gt;}}"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html",
    "href": "content/posts/15-js/Using_sketch.html",
    "title": "Using sketch",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nknitr::knit_engines$set(sketch = sketch::eng_sketch)\nlibrary(tidyverse)\nlibrary(sketch)\n\n# devtools::install_github(\"seankross/p5\")\nlibrary(p5)"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html#introduction",
    "href": "content/posts/15-js/Using_sketch.html#introduction",
    "title": "Using sketch",
    "section": "Introduction",
    "text": "Introduction\nTrying to replicate this: https://kcf-jackson.github.io/sketch-website/docs/\n\nprint(\"'sketch' has its own knitr engine from version 1.0.5!\")\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/main.R\", id = \"sketch_1\",\n  width = 500, height = 400\n)\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/dots.R\", id = \"sketch_2\", deparsers = default_2_deparsers(),\n  width = 800, height = 600\n)\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/animated_dots.R\", id = \"sketch_2\",\n  deparsers = default_2_deparsers(),\n  width = 800, height = 600\n)\n\n\n\n\n\np5::p5() |&gt;\n  createCanvas(800, 600) |&gt;\n  background(\"#F4F8FC\") |&gt;\n  fill(\"yellow\") |&gt;\n  ellipse(~mouseX, ~mouseY, 30, 30)\n\n\n\n\n\n\nstripes &lt;- tibble(\n  x = rep(0, 7),\n  y = cumsum(c(0, rep(30, 6))),\n  w = rep(300, 7),\n  h = rep(15, 7)\n)\nstripes\n\n\n  \n\n\nstripes %&gt;%\n  p5() %&gt;%\n  createCanvas(300, 200) %&gt;%\n  fill(\"#FF0000\") %&gt;%\n  noStroke() %&gt;%\n  rect()\n\n\n\n\n\n\n#! load_script(src = \"https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js\")\nsetup &lt;- function() {\n  createCanvas(400, 300)\n}\n\ndraw &lt;- function() {\n  background(0, 0, 33) # RGB colors\n\n  for (i in 1:3) {\n    dia &lt;- sin(frameCount * 0.025) * 30 * i\n    fill(255, 70 * i, 0) # RGB colors\n    circle(100 * i, 150, dia) # (x, y, diameter)\n  }\n}"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html",
    "href": "content/posts/10-using-nutshell/nutshell.html",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "href": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#test",
    "href": "content/posts/10-using-nutshell/nutshell.html#test",
    "title": "Nutshell: Expandable Explanations",
    "section": "Test",
    "text": "Test\nThis is a senseless paragraph"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "href": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "title": "Nutshell: Expandable Explanations",
    "section": "Testing Links",
    "text": "Testing Links\n\n:link to senseless paragraph\n:link to wikipedia article\n:link to invisible sections"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "href": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "title": "Nutshell: Expandable Explanations",
    "section": ":x invisible",
    "text": ":x invisible\nUse ## :x header to include an invisible section that can be linked to via nutshell"
  },
  {
    "objectID": "content/posts/40-diagrams/index.html",
    "href": "content/posts/40-diagrams/index.html",
    "title": "Diagrams",
    "section": "",
    "text": "grViz(\"digraph{\n\n      graph[rankdir = LR]\n\n      node[shape = rectangle, style = filled]\n\n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n\n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n\n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n\n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n\n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n\n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n\n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n\n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n\n      }\")\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/posts/50-maps/index.html",
    "href": "content/posts/50-maps/index.html",
    "title": "Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(ggformula)"
  },
  {
    "objectID": "content/posts/50-maps/index.html#defining-a-bounding-box",
    "href": "content/posts/50-maps/index.html#defining-a-bounding-box",
    "title": "Maps",
    "section": "Defining a bounding box",
    "text": "Defining a bounding box\n\nbbox_2 &lt;- osmdata::getbb(\"Jayanagar, Bangalore, India\")\nbbox_2\n\n       min      max\nx 77.56242 77.60242\ny 12.90927 12.94927\n\n\n\nlocations &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(\n    key = \"amenity\",\n    value = c(\"restaurant\", \"atm\", \"college\")\n  ) %&gt;%\n  osmdata_sf() %&gt;% # Convert to Simple Features format\n  purrr::pluck(\"osm_points\") # Pull out the data frame of interest\n###\ndat_buildings &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;%\n  osmdata::add_osm_feature(key = \"building\") %&gt;%\n  osmdata_sf() %&gt;%\n  purrr::pluck(\"osm_polygons\")"
  },
  {
    "objectID": "content/posts/50-maps/index.html#inspecting-osm-data",
    "href": "content/posts/50-maps/index.html#inspecting-osm-data",
    "title": "Maps",
    "section": "Inspecting OSM data",
    "text": "Inspecting OSM data\n\nstr(locations)\n\nClasses 'sf' and 'data.frame':  580 obs. of  113 variables:\n $ osm_id                        : chr  \"302493798\" \"308729154\" \"362446925\" \"362446927\" ...\n $ name                          : chr  \"Citibank\" \"Citibank ATM Jayanagar 7th Block\" NA NA ...\n $ access                        : chr  NA NA NA NA ...\n $ addr:city                     : chr  NA NA NA NA ...\n $ addr:country                  : chr  NA NA NA NA ...\n $ addr:district                 : chr  NA NA NA NA ...\n $ addr:floor                    : chr  NA NA NA NA ...\n $ addr:full                     : chr  NA NA NA NA ...\n $ addr:hamlet                   : chr  NA NA NA NA ...\n $ addr:housename                : chr  NA NA NA NA ...\n $ addr:housenumber              : chr  NA NA NA NA ...\n $ addr:postcode                 : chr  NA NA NA NA ...\n $ addr:street                   : chr  NA \"Kanakapura Road\" NA NA ...\n $ addr:suburb                   : chr  NA NA NA NA ...\n $ air_conditioning              : chr  NA NA NA NA ...\n $ alt_name                      : chr  NA NA NA NA ...\n $ amenity                       : chr  \"atm\" \"atm\" NA NA ...\n $ bar                           : chr  NA NA NA NA ...\n $ barrier                       : chr  NA NA NA NA ...\n $ branch                        : chr  NA NA NA NA ...\n $ brand                         : chr  \"Citibank\" \"Citibank\" NA NA ...\n $ brand:en                      : chr  NA NA NA NA ...\n $ brand:hi                      : chr  NA NA NA NA ...\n $ brand:kn                      : chr  NA NA NA NA ...\n $ brand:pa                      : chr  NA NA NA NA ...\n $ brand:pnb                     : chr  NA NA NA NA ...\n $ brand:ur                      : chr  NA NA NA NA ...\n $ brand:wikidata                : chr  \"Q857063\" \"Q857063\" NA NA ...\n $ brand:wikipedia               : chr  NA NA NA NA ...\n $ brand:wikipedia:pa            : chr  NA NA NA NA ...\n $ brewery                       : chr  NA NA NA NA ...\n $ building                      : chr  NA NA NA NA ...\n $ capacity                      : chr  NA NA NA NA ...\n $ cash_in                       : chr  NA NA NA NA ...\n $ changing_table                : chr  NA NA NA NA ...\n $ check_date                    : chr  NA NA NA NA ...\n $ contact:facebook              : chr  NA NA NA NA ...\n $ contact:instagram             : chr  NA NA NA NA ...\n $ covered                       : chr  NA NA NA NA ...\n $ cuisine                       : chr  NA NA NA NA ...\n $ currency:INR                  : chr  NA NA NA NA ...\n $ delivery                      : chr  NA NA NA NA ...\n $ description                   : chr  NA NA NA NA ...\n $ diet:gluten_free              : chr  NA NA NA NA ...\n $ diet:halal                    : chr  NA NA NA NA ...\n $ diet:jain                     : chr  NA NA NA NA ...\n $ diet:non-vegetarian           : chr  NA NA NA NA ...\n $ diet:vegan                    : chr  NA NA NA NA ...\n $ diet:vegetarian               : chr  NA NA NA NA ...\n $ dog                           : chr  NA NA NA NA ...\n $ door                          : chr  NA NA NA NA ...\n $ drive_through                 : chr  NA NA NA NA ...\n $ email                         : chr  NA NA NA NA ...\n $ fee                           : chr  NA NA NA NA ...\n $ highchair                     : chr  NA NA NA NA ...\n $ image:menu                    : chr  NA NA NA NA ...\n $ image:menu:0                  : chr  NA NA NA NA ...\n $ indoor                        : chr  NA NA NA NA ...\n $ indoor_seating                : chr  NA NA NA NA ...\n $ internet_access               : chr  NA NA NA NA ...\n $ internet_access:fee           : chr  NA NA NA NA ...\n $ internet_access:ssid          : chr  NA NA NA NA ...\n $ level                         : chr  NA NA NA NA ...\n $ microbrewery                  : chr  NA NA NA NA ...\n $ mobile                        : chr  NA NA NA NA ...\n $ name:en                       : chr  NA NA NA NA ...\n $ name:hi                       : chr  NA NA NA NA ...\n $ name:kn                       : chr  \"ಸಿಟಿಬ್ಯಾಂಕ್\" NA NA NA ...\n $ name:pa                       : chr  NA NA NA NA ...\n $ name:pnb                      : chr  NA NA NA NA ...\n $ name:ur                       : chr  NA NA NA NA ...\n $ note                          : chr  NA NA NA NA ...\n $ official_name                 : chr  NA NA NA NA ...\n $ opening_hours                 : chr  NA NA NA NA ...\n $ operator                      : chr  \"Citibank\" \"Citibank\" NA NA ...\n $ operator:wikidata             : chr  \"Q857063\" \"Q857063\" NA NA ...\n $ organic                       : chr  NA NA NA NA ...\n $ outdoor_seating               : chr  NA NA NA NA ...\n $ payment:american_express      : chr  NA NA NA NA ...\n $ payment:app                   : chr  NA NA NA NA ...\n $ payment:cards                 : chr  NA NA NA NA ...\n $ payment:cash                  : chr  NA NA NA NA ...\n $ payment:contactless           : chr  NA NA NA NA ...\n $ payment:credit_cards          : chr  NA NA NA NA ...\n $ payment:debit_cards           : chr  NA NA NA NA ...\n $ payment:diners_club           : chr  NA NA NA NA ...\n $ payment:discover_card         : chr  NA NA NA NA ...\n $ payment:google_pay            : chr  NA NA NA NA ...\n $ payment:maestro               : chr  NA NA NA NA ...\n $ payment:mastercard            : chr  NA NA NA NA ...\n $ payment:mastercard_contactless: chr  NA NA NA NA ...\n $ payment:visa                  : chr  NA NA NA NA ...\n $ payment:visa_debit            : chr  NA NA NA NA ...\n $ phone                         : chr  NA NA NA NA ...\n $ ref:vatin                     : chr  NA NA NA NA ...\n $ reservation                   : chr  NA NA NA NA ...\n $ service:electricity           : chr  NA NA NA NA ...\n $ shop                          : chr  NA NA NA NA ...\n $ short_name                    : chr  \"Citi\" \"Citi\" NA NA ...\n  [list output truncated]\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:112] \"osm_id\" \"name\" \"access\" \"addr:city\" ...\n\nstr(dat_buildings)\n\nClasses 'sf' and 'data.frame':  35808 obs. of  137 variables:\n $ osm_id                     : chr  \"27554059\" \"32425083\" \"43196401\" \"43196402\" ...\n $ name                       : chr  \"Jayanagar 4th Block Shopping Complex\" \"Paakashala\" NA NA ...\n $ access                     : chr  NA NA NA NA ...\n $ addr:block                 : chr  NA NA NA NA ...\n $ addr:city                  : chr  NA \"Bengaluru\" NA NA ...\n $ addr:district              : chr  NA \"Bengaluru Urban\" NA NA ...\n $ addr:full                  : chr  NA NA NA NA ...\n $ addr:housename             : chr  NA NA NA NA ...\n $ addr:housenumber           : chr  NA \"502/37\" NA NA ...\n $ addr:parentstreet          : chr  NA NA NA NA ...\n $ addr:place                 : chr  NA NA NA NA ...\n $ addr:postcode              : chr  NA \"560070\" NA NA ...\n $ addr:state                 : chr  NA NA NA NA ...\n $ addr:street                : chr  NA \"40th cross, 1st Main Rd, 8th Block, Jayanagar\" NA NA ...\n $ addr:suburb                : chr  NA NA NA NA ...\n $ air_conditioning           : chr  NA NA NA NA ...\n $ alt_name                   : chr  NA NA NA NA ...\n $ amenity                    : chr  NA \"restaurant\" NA NA ...\n $ amenity_1                  : chr  NA NA NA NA ...\n $ atm                        : chr  NA NA NA NA ...\n $ bench                      : chr  NA NA NA NA ...\n $ brand                      : chr  NA NA NA NA ...\n $ brand:en                   : chr  NA NA NA NA ...\n $ brand:hi                   : chr  NA NA NA NA ...\n $ brand:kn                   : chr  NA NA NA NA ...\n $ brand:pa                   : chr  NA NA NA NA ...\n $ brand:pnb                  : chr  NA NA NA NA ...\n $ brand:ur                   : chr  NA NA NA NA ...\n $ brand:wikidata             : chr  NA NA NA NA ...\n $ brand:wikipedia            : chr  NA NA NA NA ...\n $ brand:wikipedia:pa         : chr  NA NA NA NA ...\n $ building                   : chr  \"yes\" \"commercial\" NA NA ...\n $ building:colour            : chr  NA NA NA NA ...\n $ building:flats             : chr  NA NA NA NA ...\n $ building:levels            : chr  NA NA NA NA ...\n $ building:levels:underground: chr  NA NA NA NA ...\n $ building:material          : chr  NA NA NA NA ...\n $ building:prefabricated     : chr  NA NA NA NA ...\n $ building:use               : chr  NA NA NA NA ...\n $ bus                        : chr  NA NA NA NA ...\n $ capacity                   : chr  NA NA NA NA ...\n $ check_date                 : chr  NA NA NA NA ...\n $ club                       : chr  NA NA NA NA ...\n $ community_centre           : chr  NA NA NA NA ...\n $ construction               : chr  NA NA NA NA ...\n $ contact:email              : chr  NA NA NA NA ...\n $ contact:mobile             : chr  NA NA NA NA ...\n $ content                    : chr  NA NA NA NA ...\n $ cuisine                    : chr  NA NA NA NA ...\n $ currency:others            : chr  NA NA NA NA ...\n $ delivery                   : chr  NA NA NA NA ...\n $ denomination               : chr  NA NA NA NA ...\n $ description                : chr  NA NA NA NA ...\n $ designation                : chr  NA NA NA NA ...\n $ diet:lacto_vegetarian      : chr  NA NA NA NA ...\n $ diet:vegan                 : chr  NA NA NA NA ...\n $ drive_through              : chr  NA NA NA NA ...\n $ ele                        : chr  NA NA NA NA ...\n $ email                      : chr  NA NA NA NA ...\n $ emergency                  : chr  NA NA NA NA ...\n $ fee                        : chr  NA NA NA NA ...\n $ female                     : chr  NA NA NA NA ...\n $ fixme                      : chr  NA NA NA NA ...\n $ government                 : chr  NA NA NA NA ...\n $ healthcare                 : chr  NA NA NA NA ...\n $ healthcare:speciality      : chr  NA NA NA NA ...\n $ height                     : chr  NA NA NA NA ...\n $ heritage                   : chr  NA NA NA NA ...\n $ heritage:operator          : chr  NA NA NA NA ...\n $ heritage:website           : chr  NA NA NA NA ...\n $ house                      : chr  NA NA NA NA ...\n $ internet_access            : chr  NA NA NA NA ...\n $ isced:level                : chr  NA NA NA NA ...\n $ landuse                    : chr  NA NA NA NA ...\n $ layer                      : chr  NA NA NA NA ...\n $ leisure                    : chr  NA NA NA NA ...\n $ level                      : chr  NA NA NA NA ...\n $ lit                        : chr  NA NA NA NA ...\n $ loc_name                   : chr  NA NA NA NA ...\n $ male                       : chr  NA NA NA NA ...\n $ man_made                   : chr  NA NA NA NA ...\n $ max_age                    : chr  NA NA NA NA ...\n $ max_level                  : chr  NA NA NA NA ...\n $ min_age                    : chr  NA NA NA NA ...\n $ min_level                  : chr  NA NA NA NA ...\n $ name:en                    : chr  NA NA NA NA ...\n $ name:etymology:wikidata    : chr  NA NA NA NA ...\n $ name:hi                    : chr  NA NA NA NA ...\n $ name:kn                    : chr  NA NA NA NA ...\n $ name:ml                    : chr  NA NA NA NA ...\n $ name:pa                    : chr  NA NA NA NA ...\n $ name:pnb                   : chr  NA NA NA NA ...\n $ name:ta                    : chr  NA NA NA NA ...\n $ name:ur                    : chr  NA NA NA NA ...\n $ note                       : chr  NA NA NA NA ...\n $ office                     : chr  NA NA NA NA ...\n $ official_name              : chr  NA NA NA NA ...\n $ old_name                   : chr  NA NA NA NA ...\n $ opening_hours              : chr  NA NA NA NA ...\n  [list output truncated]\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:136] \"osm_id\" \"name\" \"access\" \"addr:block\" ..."
  },
  {
    "objectID": "content/posts/50-maps/index.html#plotting-base-maps-with-ggplotggformula",
    "href": "content/posts/50-maps/index.html#plotting-base-maps-with-ggplotggformula",
    "title": "Maps",
    "section": "Plotting Base Maps with ggplot/ggformula",
    "text": "Plotting Base Maps with ggplot/ggformula\n\nggplot() +\n  geom_sf(data = locations, fill = \"gold\", color = \"grey\", linewidth = 0.025) +\n  geom_sf(data = dat_buildings, fill = \"purple\") +\n  coord_sf()\n\n\n\n\n\n\ngf_sf(geometry = ~geometry, data = locations) %&gt;%\n  gf_sf(geometry = ~geometry, dat = dat_buildings)"
  },
  {
    "objectID": "content/posts/50-maps/index.html#plotting-with-tmap",
    "href": "content/posts/50-maps/index.html#plotting-with-tmap",
    "title": "Maps",
    "section": "Plotting with tmap",
    "text": "Plotting with tmap\n\ntm_shape(locations) +\n  tm_dots(size = 0.5) +\n  tm_shape(dat_buildings) +\n  tm_polygons() +\n  tm_compass(position = c(\"right\", \"top\"))"
  },
  {
    "objectID": "content/posts/50-maps/index.html#adding-user-data-to-a-base-map",
    "href": "content/posts/50-maps/index.html#adding-user-data-to-a-base-map",
    "title": "Maps",
    "section": "Adding User Data to a Base Map",
    "text": "Adding User Data to a Base Map"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#introduction",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#introduction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#where-did-the-bear-cross-the-road",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#where-did-the-bear-cross-the-road",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Where did the Bear cross the Road?",
    "text": "Where did the Bear cross the Road?\n\nWildlife roadkill was a serious problem in Banff National Park in Canada, for both wildlife and motorists. The problem was tackled beautifully by Parks Canada using a system of tunnels and stunning natural-looking overpasses.\nWithout further ado, let us do a TRIZ Analysis of this remarkable set of inventions."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#a-triz-analysis-of-the-banff-wildlife-crossings",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#a-triz-analysis-of-the-banff-wildlife-crossings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n A TRIZ Analysis of the Banff Wildlife Crossings",
    "text": "A TRIZ Analysis of the Banff Wildlife Crossings\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem statements. Let us state some of our Problems\n\n\nSignage would help drivers slow down, but slowing down may make journey less enjoyable.\n\nSlowing Down may improve animal movement but may endanger humans.\n\nClearing the Vegetation may make animals more visible, but may also make vehicles visible to animals and affect their movement\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state our Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\nNote Administrative Contradiction\n\n\n\nAC: We wish to drive at high speeds, but not kill migrating wild animals nor endanger our vehicles.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\nImportant Ideal Final Result\n\n\n\nThe Animals and Humans should both use the Road whenever they want without being mutually affected!\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradictions both ways1:\n\n\n\n\n\n\nNote Technical Contradictions\n\n\n\n\n\nTC1: Improve Loss of Time(26) and not worsen Duration of Action by Moving Object(12)\n\n\n\n\nSince our IFR is all about time, we have chosen the TRIZ Parameters that are time-oriented. We could have also tried the following:\n\n\nTC2: Improve Volume of Moving Object(7) and not worsen Loss of Time(26)\n\n\nTC3: Improve Other harmful factors Acting on the System(40) and not worsen Duration of Action by Stationary Object(13)2\n\nThese include Volume and External Factors which are not quite there in out IFR. Is there a Physical Contradiction(PC)3 possible here?\n\n\n\n\n\n\n\nWarning Physical Contradiction\n\n\n\nIn fact our IFR is nearly worded as a PC: The Vehicles and the Animals must use the Road at the Same Time."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#solving-the-technical-contradictions",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#solving-the-technical-contradictions",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Solving the Technical Contradictions",
    "text": "Solving the Technical Contradictions\nLet us take the set of TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC1:\n\n3(Local Quality)\n17(Another Dimension)\n28(Mechanics Substitution)\n8(Anti-Weight); and\n19(Periodic Action),\n10(Preliminary Action)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Another Dimension and Local Quality. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n3(Local Quality): So something that is local…local where? Well, along the highway, of course. So something that is located a specific points along the highway. Nice but not really clear enough to be actionable, yet.\n17(Another Dimension): Well, well. The Road is a linear thing and has length and breadth. What would we use for another dimension? Height, of course! So, we need to go either above the road or below! And that leads us to a …bridge and a tunnel !!!\n\nThe other Inventive Principles are, to me, not evocative enough in this instance. But we already have a decent idea: we could just imagine a set of Local bridges and tunnels that occur at Periodic Intervals along the highway. And that is exactly what Parks Canada have done.\n\nHere is the solution in action:"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#using-triz-separation-principles",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction. Our IFR states that we want the humans and animals to use the road at the same TIME, and hence Separation in Space becomes a nice way to think of a solution.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#readings",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#readings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Readings",
    "text": "Readings\n\nhttps://discoverapega.ca/stories/wildlife-crossings-key-to-highway-safety-in-banff/\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k\n\nScrucca (2004)\nCano, Moguerza, and Redchuk (2012)"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "href": "content/projects/Modules/TRIZ/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nAnimals are nearly stationary compared to the vehicles.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#introduction",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#introduction",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\nTRIZ is a system of Inventive Problem Solving created by Genrikh Altshuller. Altshuller, was born in Russia in 1926, made his first invention at age 14 (9th Grade), and was later educated as a mechanical engineer. At the time he started working on TRIZ, in 1946, he was employed in the patent department of the Soviet navy, assisting inventors in filing their patents, in Baku, Azerbaijan. While there he became intrigued by the question of how an invention happens:\n\nIs it a matter of luck? The result of a mental “light bulb” turning on, as in the comics? Or can inventions be seen as the result of systematic patterns of inventive thinking?\n\nAltshuller adopted an empirical approach to answering this question. He studied thousands of patents, looking for commonalities, repetitive patterns, and principles of inventive thought. As he found these, he codified and documented them. His results, when eventually published, attracted many enthusiasts who continued and expanded the work over the years, reviewing what is now estimated to be more than three million patents worldwide. TRIZ is actively used in Companies such as Boeing, Bridgestone, Eastman Kodak, Ford Motor Company, Harley-Davidson Motor Company, Hewlett-Packard, Illinois Tool Works, Inficon, Ingersoll Rand, Kimberly-Clark, L.G. Electronics, Lucent Technologies, Michelin, National Semiconductor, NASA, Philips, Rolls-Royce, Samsung, Siemens, Western Digital, and Xerox, among others.\nAltshuller found that the most inventive of patents did two things:\n\nThey stated PROBLEMS as CONTRADICTIONs (using just 48 unique phrases)\nThese CONTRADICTIONs were resolved across a wide variety of patents using an astonishingly few INVENTIVE PRINCIPLEs. (only 40 in number!)"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#there-is-world-of-problems",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#there-is-world-of-problems",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n There is World of Problems!",
    "text": "There is World of Problems!\nLet us take our first step into the world of TRIZ. What did you think of immediately when you saw the first picture on this page? You surely saw the Contradiction: it is graffiti but claiming not to be! In TRIZ, the fundamental way of looking at an Inventive Design Problem is to discover and propose Contradictions. These are rendered in as simple and stark a language as possible…the starker the better!\nWhat sort of Contradictions do we see in these familiar objects below ? What is good and what is not so good? Could that be the source of a problem to solve?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContemplate and note down for each Object: Does it embody a CONTRADICTION?\nYes, each one does, in its own way: The pizza box lid collapses on the pizza when it is hot and spoils the topping. The wrench must be turned by pushing against its narrower edge; we would find it easier and less painful if we could apply force on the broader surface of the handle. And the chain? Everyone know it needs to be stiff and strong to be able to pull the wheel, and yet flexible enough to go around the sprocket…and get horribly entangled, leading to greasy fingers!\nIn this way we look at OBJECTS, PROCESSES, METHODS, PRODUCTS, and indeed CIRCUMSTANCES to find CONTRADICTIONs!\nHere are some more examples in the figure below:\n\n\n\n\n\n\nJuice\n\n\n\n\n\n\n\n\n\n\nSnails\n\n\n\n\n\nWhat could be our Contradictions here?\n\nFresh Juice is good for health, but ads from juice companies wish to portray it as harmful, and they may not be able to sell!\nThe ground is hot for a snail, but above ground it is visible to predators."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#appreciate-the-situation",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#appreciate-the-situation",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Appreciate the Situation",
    "text": "Appreciate the Situation\nWe do not always contemplate only objects; indeed, as inventors, we want to be able to make objects or systems. What we more commonly contemplate is a situation. So how does one assess a situation? We might use what is called an Ishikawa Fishbone Diagram. This is shown below.\n\n\n\n\nIshikawa Fishbone Diagram\n\n\n\nThere are many versions of this diagram depending upon the DOMAIN it is applied in. This diagram is very helpful to us in assessing resources and processes, and watching how their interplay in a situation could lead to…a Contradiction."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#determining-the-administrative-contradiction",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#determining-the-administrative-contradiction",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Determining the Administrative Contradiction",
    "text": "Determining the Administrative Contradiction\nIn the Ishikawa diagram, each of the items listed is considered a TRIZ KNOB, which is either in our control or not. By turning these KNOBs in either direction we can change a specific PARAMETER that the KNOB affects. This change may improve the situation for us but we may find that something else typically gets worse. This is the source of our CONTRADICTION for Situations. When this happens, we can usually state a CONTRADICTION in simple English.\nFor example: We want to boil milk, but we get bored watching it boil.\nHere Time and the Milk would both have been listed in your Ishikawa as Resources. You can further document your analysis of the Object or Situation using the following questionnaire 5W+H format:\n\nWhat does the problem seem to be?\nWho has the problem?\nWhen does the problem occur? All the time? Under certain circumstances?\nWhere does the problem occur?\nWhy does the problem occur? (“Ask why 5 times” – W. Edwards Deming)\nHow does the problem occur?\n\nThis will help in “aiming” the solutions that TRIZ offers, in the right way.\nSTEP1: In TRIZ, this way of expressing a Problem as a simple CONTRADICTION is referred to as stating an ADMINISTRATIVE CONTRADICTION (AC). You should be able to state an Administrative Contradiction in the following (loose!) sentence structure — Items in &lt; &gt; come from the Ishikawa and your 5W + H questions).\nWhen we, as &lt; WHO / MANPOWER &gt;, attempt to perform &lt; HOW / METHOD &gt; during &lt;WHEN&gt; on &lt; WHERE / MACHINERY / KNOB&gt;, we improve &lt;EFFECT&gt;, but lose out on &lt;negative EFFECT / KNOB&gt;"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#stating-a-technical-contradiction-tc",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#stating-a-technical-contradiction-tc",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Stating a Technical Contradiction (TC)",
    "text": "Stating a Technical Contradiction (TC)\nNow that we know how problems can be stated as simple Administrative Contradictions (AC), we need to take the next step and make what TRIZ calls Technical Contradictions (TC). Altshuller found that problems across domains could be expressed in a “TRIZ Language,” a set of metaphoric phrases that are an integral part of (classical) TRIZ. We will call these the 48 TRIZ Parameters. Some examples of TRIZ Parameters: Weight of a stationary Object, Loss of Substance, and Temperature.\nEvery problem could be described as a contradiction using some pair of these 48 parameters. These metaphoric phrases are simple enough and provide rich troves for imaginative problem solving. Expressing our specific problem in this way allowed us to see the similarity it has with problems in other domains and helps us to leverage solutions from there.\nSTEP 2: Take the AC and state it in terms of these 48 TRIZ Parameters, for example: Improve Loss of Substance and not worsen Weight of a Stationary Object ; Improve Loss of Information and not worsen Power"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#using-the-triz-contradiction-matrix",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#using-the-triz-contradiction-matrix",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Using the TRIZ Contradiction Matrix",
    "text": "Using the TRIZ Contradiction Matrix\nArmed with our TCs, we plug them into the TRIZ Contradiction Matrix. The TRIZ Matrix is a 48 X 48 structure, with every possible TRIZ Parameter being paired with every other TRIZ Parameter.\nSTEP3: One chooses one TRIZ Parameter from the TC as the ROW and the other as the COLUMN in the TRIZ Matrix. At their intersection lies a single cell which contains one or more TRIZ Inventive Principles. These Inventive Principles have been derived as solutions from hundreds and thousands of patents.\nSTEP4: These Inventive Principles must now be applied into our Problem to solve it. This is the TRIZ Contradiction Matrix Workflow."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#a-complete-example",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#a-complete-example",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n A Complete Example",
    "text": "A Complete Example\nLet us consider the example of the pizza box that we saw at the start of this article. (We will not trouble to make the Ishikawa for this simple problem) Here is our AC:\nAC: The pizza needs to be hot but the steam it gives off must not make the lid collapse and ruin the pizza\nWe can convert this into a TC by choosing several pairs of TRIZ Parameters:\nTC: Improve 21(Temperature) while not worsening 22(Stability)!\nLooking up the TRIZ Contradiction Matrix(Row#21 Col#22), we get the TRIZ Inventive Principles: 24: Intermediary. 35: Parameter Change, 32: Colour Change, 3: Local Quality.\nLooking at IP 24 (Intermediary) we need to think of something between the pizza and the lid, and IP 3 ( Local Quality) suggests that it should we quite small, or “local” compared to the size of the pizza! What could that be? This!\n\n\n\n\nPizza Saver or Pizza Table"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#conclusion",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#conclusion",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Conclusion",
    "text": "Conclusion\nThe TRIZ process allows us to leverage solutions that have been obtained from a vast number of patents. The TRIZ language allows us to access these solutions by expressing our specific problem in terms of the TRIZ Parameters, and leads us to the relevant TRIZ Inventive Principles which can solve our problem!\nWorth mastering!!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#references",
    "href": "content/projects/Modules/TRIZ/2023-11-15-TRIZ-Intro/index.html#references",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n References",
    "text": "References\n\nOpen Source TRIZ. &lt;TRIZ PowerTools - Free downloads ebooks pdfs teaching materials (opensourcetriz.com)&gt;\nJack Hipple’s Webpage.https://innovation-triz.com/personnel/JHipple.html\nValeri Souchkov’s Webpage.http://www.xtriz.com/ValeriSouchkov.htm"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Famous Spotify Ad",
    "text": "The Famous Spotify Ad\nLet us watch the Spotify ad first, before analyzing it!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Young Man’s Problem",
    "text": "The Young Man’s Problem\nIn order to make a story out of this, I want make a Protagonist out the young man in the ad. It is he who has the problem and he who is going to apply TRIZ to solve it. I discuss the source of his Problem and give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles, that lead to the solution, which of course, is meant to unerringly include Spotify !\nFirst a philosophical digression:—\nSeveral authors have taken a Game View of life. James P Carse’s famous book titled Finite and Infinite Games speaks of Play, Types of Games, Rules, Winning and our own aims in the Game itself. A similar articulation is, in my opinion, that of Mihaly Csikszentlmihalyi in his concept of Flow, shown here below:\n\n\nFrom Sketchplanations\n\nWhen the Game presents very little Challenge, we are bored. When the Game demands extreme skills the challenge is too much for us and we experience anxiety. When the Challenge presented is just barely matched by our Skill, we are in the zone of Flow, or what I call Play.\nA good metaphoric image for this experience is as follows:— that we live in a space where the Floor of Boredom is always rising and would crush us against our Ceiling of Anxiety. One Way to deal with this is to develop more Skills and push the Ceiling away, effectively moving into the zone of Flow. Another Way of looking at this is what Carse suggests: When Play is no longer possible, change the Game.\nSo what does all this have to do with getting veggies?\nThe ad is, in my opinion, all about Boredom, and how to avoid it. And not offend anybody. The Young Man (hereinafter, “YM”) simply has to accompany his Mom, and be there while she gets the veggies. I will exaggerate his irritation and his boredom at the risk of offending young people likely to read this, and say that he would rather not be there but he does not want to hurt Mom.\nWe are now ready for the TRIZ based Analysis of this Problem!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "A TRIZ Analysis of a Visit to the Subzi Mandi",
    "text": "A TRIZ Analysis of a Visit to the Subzi Mandi\nFor a TRIZ workflow, we proceed as before:\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the YM goes to the market with Mom, he would most likely get bored, but would please Mom. If he doesn’t go, then he chills at home, but Mom is going to justifiably furious. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The YM wants to chill at home but Mom wants him to take her veggie shopping. He has to put up with the Waste of Time, and being bored, and Stress at being away from friends.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR: The YM must go to the Market and not be bored.\n\n\nNote again the impossible sounding way of expressing the IFR! One needs practice, like the Queen in Alice in Wonderland, who could think of Six Impossible Things before Breakfast ! Also note there could be other ways of specifying the IFR. See below, section Alternative Ideas for IFR.\nLet us take the AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze each Contradiction both ways1:\n\n- TC 1: Improve Loss of Time (26) and not worsen Effect of External Harmful Factors (30)\n- TC 2: Improve Increase Productivity (44) and not worsen Stress (19)\n\nHere we choose these Parameters based on our IFR that while going to the Market may be unavoidable, Boredom need not ensue. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Noise(29) as the “metaphoric thing” to avoid, but the current IFR doesn’t quite support that. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could also formulate a Physical Contradiction(PC)2:\n\n\nPC: The YM must be in the market and not be in the market at the same time.\n\n\nwhich is aimed squarely at one of the Assumptions in the Problem, that the YM simply has to go. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC. See below, section Alternative Ideas for IFR.\nIn a future post, we will deal with using the PC and the TRIZ Separation Principles to solve Problems."
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\n\nThe two squares for the TC1 have been circled in red, solving TC-1.\nThe Inventive Principles are:(TC1, TC2, both ways)\n\n1(Segmentation)\n35(Parameter Change)\n21(Skipping)\n18(Mechanical Vibration) (!!)\n2(Taking Out/Separation)\n10(Prior Action)\n\n36(Phase Transitions)\nand with TC2:\n\n3(Local Quality)\n14(Spheroidality/Curvature)\n9(Preliminary Anti-Action)\n37(Thermal Expansion)\n40(Composite Materials)\n25(Self Service)\n24(Intermediary)\n\nThat is a considerable list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n35(Parameter Change): Which Parameter to change? Location? No. Sound? Change the “Bargaining Talk” into what? Sweet Musical Lyrics!!🎵🤣\n\n18(Mechanical Vibration) : What, make noise of your own? Yes! Play Music !!🔉 🤣\n\n14(Spheroidality): Wear “spherical” headphones!!🎧! Create a “sound sphere”! This is a long shot!!\n\n3(Local Quality): also indicates the creation of a “local” cocoon around the YM, but needs to be combined with 18(Mechanical Vibration) to truly arrive at the musical solution!\n\nOne could make decent interpretations of 2(Taking Out/Separation), and 24(Intermediary), but we are already there! The rest are perhaps (at least to me!) not very evocative, unless 37(Thermal Expansion) means “throw a temper tantrum at Mom”? Never! So there you have it! The Cinderella song played on Spotify becomes not just a noise canceller but actually seems to substitute the very conversation between Mom and the vendor. And the YM has successfully attained Flow ! And the IFR too, since with the music in his head, he is effectively “in the marketplace and not in the marketplace at the same time!\nAnd I attained Flow in writing this!!"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Alternative Ideas for IFR",
    "text": "Alternative Ideas for IFR\nWe note in passing that there is more than one way of formulating the Ideal Final Result. Here are two more examples:\n\n\nIFR2: The veggies should arrive without (the YM) going to the Market\nIFR3: Food should be prepared without having to go buy veggies.\n\n\nClearly these are at least as good as the one we have chosen, sounding nicely “impossible” in their own right! The point is that in the analysis of the Problem, we do need to ask Who has the Problem, as we did, and the IFR needs to stem from there. These alternative IFRs could well be the Voice of (another) Customer.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "References",
    "text": "References\n\nJames P Carse, Finite and Infinite Games, Free Press, 1986. ISBN: 0-02-905980-1\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "href": "content/projects/Modules/TRIZ/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/Modules/SMI/fsp-doe/index.html",
    "href": "content/projects/Modules/SMI/fsp-doe/index.html",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/Modules/SMI/fsp-doe/index.html#slides-and-code-links",
    "href": "content/projects/Modules/SMI/fsp-doe/index.html#slides-and-code-links",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/Modules/SMI/fsp-doe/index.html#introduction",
    "href": "content/projects/Modules/SMI/fsp-doe/index.html#introduction",
    "title": "A Design of Experiments Class",
    "section": " Introduction",
    "text": "Introduction\nThis is a brief description and analysis of a Design of Experiments module conducted as a part of the Order and Chaos course, in the Foundation Studies Program (FSP 2021-2022) at SMI, MAHE, Bangalore."
  },
  {
    "objectID": "content/projects/Modules/SMI/fsp-doe/index.html#context",
    "href": "content/projects/Modules/SMI/fsp-doe/index.html#context",
    "title": "A Design of Experiments Class",
    "section": "Context",
    "text": "Context\nA Short Term Memory(STM) Test was the investigative tool used to verify several Hypotheses that were documented on the subject of STM.\nThis article describes the statistical analysis that was done with the readings. In particular, Permutations Tests were used to verify the effect size for each of three parameters that were hypothesized.\nFor more information, please click on the icon above to look at the Lab document."
  },
  {
    "objectID": "content/projects/Modules/SMI/fsp-doe/index.html#references",
    "href": "content/projects/Modules/SMI/fsp-doe/index.html#references",
    "title": "A Design of Experiments Class",
    "section": "References",
    "text": "References\n\nLawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364.\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html."
  },
  {
    "objectID": "content/projects/Modules/R/2024-28-5-Mondrian/index.html",
    "href": "content/projects/Modules/R/2024-28-5-Mondrian/index.html",
    "title": "Recreating Mondrian",
    "section": "",
    "text": "library(gsubfn)\nlibrary(tidyverse)\n\n\n# Number of symbols in rule\ns &lt;- sample(15:26, 1)\n# Extract s symbols from c(\"F\", \"+\", \"-\") randomly\nv1 &lt;- sample(c(\"F\", \"+\", \"-\"), size = s, replace = TRUE, prob = c(10, 12, 12))\n# Add 3 pairs of brackets\nv2 &lt;- sample(\"[]\", 3, replace = TRUE) %&gt;%\n  str_extract_all(\"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;%\n  unlist()\n# Where to insert brackets\nv3 &lt;- sample(1:(s + 1), size = length(v2)) %&gt;% sort()\n# Insert them correctly\nfor (i in 1:length(v3)) {\n  c(v1[1:(v3[i] + i - 1)], v2[i], v1[(v3[i] + i - 1):length(v1)]) -&gt; v1\n}\n\n\n# All pictures start with the same axiom\naxiom &lt;- \"F-F-F-F\"\n# Rule to substitute F, as generated previously\nrules &lt;- list(\"F\" = paste(v1, collapse = \"\"))\n# Turning angle\nangle &lt;- 90\n# How many times to apply the rule\ndepth &lt;- sample(3:4, 1)\n# Longitude (factor) of the segments\nds &lt;- jitter(1)\n# Substitute axiom depth times\nfor (i in 1:depth) axiom &lt;- gsubfn(\".\", rules, axiom)\n# Actions that will generate the drawing\nactions &lt;- str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|G|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist()\n\n\n# These vars store the current position, angle and longitude factor of the point\nx_current &lt;- 0\ny_current &lt;- 0\na_current &lt;- 0\nd_current &lt;- 0\n\n# To store point position, angle and longitude\nstatus &lt;- tibble(\n  x = x_current,\n  y = y_current,\n  alfa = a_current,\n  depth = d_current\n)\n# To store segments\nlines &lt;- data.frame(\n  x = numeric(),\n  y = numeric(),\n  xend = numeric(),\n  yend = numeric()\n)\n\n\n# This loop reads actions and generates the drawing depending on the concrete action\n#   F -&gt; draw forward\n#   + -&gt; turn right\n#   - -&gt; turn left\n#   [ -&gt; save the current status of point\n#   ] -&gt; restore the last current status of point and remove from stack\nfor (action in actions)\n{\n  if (action == \"F\") {\n    lines &lt;- lines %&gt;% add_row(\n      x = x_current,\n      y = y_current,\n      xend = x_current + (ds^d_current) * cos(a_current * pi / 180),\n      yend = y_current + (ds^d_current) * sin(a_current * pi / 180)\n    )\n    x_current &lt;- x_current + (ds^d_current) * cos(a_current * pi / 180)\n    y_current &lt;- y_current + (ds^d_current) * sin(a_current * pi / 180)\n    d_current &lt;- d_current + 1\n  }\n  if (action == \"+\") {\n    a_current &lt;- a_current - angle\n  }\n  if (action == \"-\") {\n    a_current &lt;- a_current + angle\n  }\n  if (action == \"[\") {\n    status &lt;- status %&gt;% add_row(\n      x = x_current,\n      y = y_current,\n      alfa = a_current,\n      depth = d_current\n    )\n  }\n  if (action == \"]\") {\n    x_current &lt;- tail(status, 1) %&gt;% pull(x)\n    y_current &lt;- tail(status, 1) %&gt;% pull(y)\n    a_current &lt;- tail(status, 1) %&gt;% pull(alfa)\n    d_current &lt;- tail(status, 1) %&gt;% pull(depth)\n    status &lt;- head(status, -1)\n  }\n}\n\nlines %&gt;%\n  mutate(\n    x = round(x, 1),\n    y = round(y, 1),\n    xend = round(xend, 1),\n    yend = round(yend, 1)\n  ) %&gt;%\n  distinct(x, y, xend, yend) -&gt; lines\n\nselect(lines, x3 = x, y3 = y) %&gt;%\n  bind_rows(select(lines, x3 = xend, y3 = yend)) %&gt;%\n  distinct(x3, y3) -&gt; points\n\n\n# Let's find squares to fill inside the drawing\n# Since this operation maybe hard to compute, I divide points into\n# 10 pieces to process them separately\nn &lt;- 10\n\nsplit(points, rep(1:ceiling(nrow(points) / n),\n  each = n,\n  length.out = nrow(points)\n)) -&gt; points_divided\n\n# Squares1: add X3, y3 to current segments and filter to find\n# right angles\nlapply(points_divided, function(sub) {\n  sub %&gt;%\n    crossing(lines) %&gt;%\n    filter(x == x3 | y == y3 | xend == x3 | yend == y3) %&gt;%\n    filter(x != x3 | y != y3, xend != x3 | yend != y3) %&gt;%\n    mutate(id = row_number())\n}) %&gt;% bind_rows() -&gt; squares1\n\n\n# Squares2:  keep those squares where some of new sides exist in lines\nbind_rows(\n  squares1 %&gt;%\n    inner_join(lines, c(\n      \"x\" = \"x\",\n      \"y\" = \"y\",\n      \"x3\" = \"xend\",\n      \"y3\" = \"yend\"\n    )),\n  squares1 %&gt;%\n    inner_join(lines, c(\n      \"xend\" = \"x\",\n      \"yend\" = \"y\",\n      \"x3\" = \"xend\",\n      \"y3\" = \"yend\"\n    )),\n  squares1 %&gt;%\n    inner_join(lines, c(\n      \"x3\" = \"x\",\n      \"y3\" = \"y\",\n      \"x\" = \"xend\",\n      \"y\" = \"yend\"\n    )),\n  squares1 %&gt;%\n    inner_join(lines, c(\n      \"x3\" = \"x\",\n      \"y3\" = \"y\",\n      \"xend\" = \"xend\",\n      \"yend\" = \"yend\"\n    ))\n) %&gt;%\n  distinct(x, y, xend, yend, x3, y3, id) -&gt; squares2\n\n# Remove those whose sides form a straight line\nsquares2 %&gt;%\n  anti_join(squares2 %&gt;% filter(x == xend, xend == x3),\n    by = c(\"x\", \"y\", \"xend\", \"yend\", \"x3\", \"y3\", \"id\")\n  ) -&gt; squares2\n\nsquares2 %&gt;%\n  anti_join(squares2 %&gt;% filter(y == yend, yend == y3),\n    by = c(\"x\", \"y\", \"xend\", \"yend\", \"x3\", \"y3\", \"id\")\n  ) -&gt; squares2\n\n# We leave squares2 prepared for geom_rect\nsquares2 %&gt;%\n  mutate(\n    xmax = pmax(x, xend, x3),\n    xmin = pmin(x, xend, x3),\n    ymax = pmax(y, yend, y3),\n    ymin = pmin(y, yend, y3)\n  ) %&gt;%\n  mutate(A = (xmax - xmin) * (ymax - ymin) / 2) -&gt; squares\n\n\n# Piet mondrian's palette\ncolors &lt;- c(\"#FEFFFA\", \"#000002\", \"#F60201\", \"#FDED01\", \"#1F7FC9\")\n\n# To remove very small squares I calculate quantiles from its area\nqnts &lt;- quantile(squares$A,\n  probs = seq(0, 1, 0.05),\n  na.rm = FALSE,\n  names = TRUE,\n  type = 7\n)\n\n# Here comes the magic of ggplot\nggplot() +\n  geom_rect(\n    aes(\n      xmax = xmax,\n      xmin = xmin,\n      ymax = ymax,\n      ymin = ymin,\n      fill = id %% length(colors) %&gt;% jitter(amount = .025)\n    ),\n    data = squares %&gt;% filter(A &gt;= qnts[1]), # remove small squares\n    lwd = 2,\n    color = \"white\"\n  ) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend),\n    data = lines,\n    lwd = .65,\n    lineend = \"square\",\n    color = \"#000002\"\n  ) +\n  scale_fill_gradientn(colors = colors) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  coord_equal() -&gt; plot\nplot\n\n\n\n\n\n\n# Calculate dimensions of the picture for ggsave\nwidth &lt;- max(points$x3) - min(points$x3)\nheight &lt;- max(points$y3) - min(points$y3)\n\nwhmax &lt;- 8\nif (width &gt;= height) {\n  w &lt;- whmax\n  h &lt;- whmax * height / width\n} else {\n  h &lt;- whmax\n  w &lt;- whmax * width / height\n}\n\n\n# Save the drawing with a random name\nname &lt;- paste(sample(letters, 6), collapse = \"\")\nggsave(paste0(\"./new/\", name, \".png\"), plot, width = w, height = h)\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/Modules/R/Rmd_Project/index.html",
    "href": "content/projects/Modules/R/Rmd_Project/index.html",
    "title": "Rmd Project for Quarto Website",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/projects/Modules/R/Rmd_Project/index.html#r-markdown",
    "href": "content/projects/Modules/R/Rmd_Project/index.html#r-markdown",
    "title": "Rmd Project for Quarto Website",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/projects/Modules/R/Rmd_Project/index.html#including-plots",
    "href": "content/projects/Modules/R/Rmd_Project/index.html#including-plots",
    "title": "Rmd Project for Quarto Website",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/projects/Modules/R/20204-28-5-DrawingWithPurrr/index.html",
    "href": "content/projects/Modules/R/20204-28-5-DrawingWithPurrr/index.html",
    "title": "Drawing with purrr",
    "section": "",
    "text": "library(tidyverse)\n\n\n# This function creates the segments of the original polygon\npolygon &lt;- function(n) {\n  tibble(\n    x    = accumulate(1:(n - 1), ~ .x + cos(.y * 2 * pi / n), .init = 0),\n    y    = accumulate(1:(n - 1), ~ .x + sin(.y * 2 * pi / n), .init = 0),\n    xend = accumulate(2:n, ~ .x + cos(.y * 2 * pi / n), .init = cos(2 * pi / n)),\n    yend = accumulate(2:n, ~ .x + sin(.y * 2 * pi / n), .init = sin(2 * pi / n))\n  )\n}\n\n\n# This function creates segments from some mid-point of the edges\nmid_points &lt;- function(d, p, a, i, FUN = ratio_f) {\n  d %&gt;%\n    mutate(\n      angle = atan2(yend - y, xend - x) + a,\n      radius = FUN(i),\n      x = p * x + (1 - p) * xend,\n      y = p * y + (1 - p) * yend,\n      xend = x + radius * cos(angle),\n      yend = y + radius * sin(angle)\n    ) %&gt;%\n    select(x, y, xend, yend)\n}\n\n\n# This function connect the ending points of mid-segments\ncon_points &lt;- function(d) {\n  d %&gt;%\n    mutate(\n      x = xend,\n      y = yend,\n      xend = lead(x, default = first(x)),\n      yend = lead(y, default = first(y))\n    ) %&gt;%\n    select(x, y, xend, yend)\n}\n\n\nedges &lt;- 3 # Number of edges of the original polygon\nniter &lt;- 250 # Number of iterations\npond &lt;- 0.24 # Weight to calculate the point on the middle of each edge\nstep &lt;- 13 # Number of times to draw mid-segments before connect ending points\nalph &lt;- 0.25 # transparency of curves in geom_curve\nangle &lt;- 0.6 # angle of mid-segment with the edge\ncurv &lt;- 0.1 # Curvature of curves\nline_color &lt;- \"black\" # Color of curves in geom_curve\nback_color &lt;- \"white\" # Background of the ggplot\nratio_f &lt;- function(x) {\n  sin(x)\n} # To calculate the longitude of mid-segments\n\n\n# Generation on the fly of the dataset\naccumulate(\n  .f = function(old, y) {\n    if (y %% step != 0) mid_points(old, pond, angle, y) else con_points(old)\n  }, 1:niter,\n  .init = polygon(edges)\n) %&gt;% bind_rows() -&gt; df\n\n\n# Plot\nggplot(df) +\n  geom_curve(aes(x = x, y = y, xend = xend, yend = yend),\n    curvature = curv,\n    color = line_color,\n    alpha = alph\n  ) +\n  coord_equal() +\n  theme(\n    legend.position = \"none\",\n    panel.background = element_rect(fill = back_color),\n    plot.background = element_rect(fill = back_color),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/Modules/R/UsingTachyons/index.html",
    "href": "content/projects/Modules/R/UsingTachyons/index.html",
    "title": "Using Tachyons",
    "section": "",
    "text": "Discipline in typography is a prime virtue. Individuality must be secured by means that are rational. Distinction needs to be won by simplicity and restraint. It is equally true that these qualities need to be infused with a certain spirit and vitality, or they degenerate into dullness and mediocrity.\n— Stanley Morison\n\n\nAnd What Will Happen Here?\n\nWhat is this thing going to do?\n\n\n\n\n\nTable with Tachyons\n\n\nCol1 {.w-25}\nCol2{.w-25}\nCol3{.w-25}"
  },
  {
    "objectID": "content/projects/Modules/R/UsingTachyons/index.html#trying-various-tachyons",
    "href": "content/projects/Modules/R/UsingTachyons/index.html#trying-various-tachyons",
    "title": "Using Tachyons",
    "section": "",
    "text": "Discipline in typography is a prime virtue. Individuality must be secured by means that are rational. Distinction needs to be won by simplicity and restraint. It is equally true that these qualities need to be infused with a certain spirit and vitality, or they degenerate into dullness and mediocrity.\n— Stanley Morison\n\n\nAnd What Will Happen Here?\n\nWhat is this thing going to do?\n\n\n\n\n\nTable with Tachyons\n\n\nCol1 {.w-25}\nCol2{.w-25}\nCol3{.w-25}"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#agenda",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#agenda",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Agenda!",
    "text": "Agenda!\n\nOrange? What is this Orange stuff, anyhow?\nThrowing it All Away with Brad Pitt: Data Summaries\nCounting Letters with Sherlock Holmes: Bar Charts\nNursery Rhymes with Ben Affleck: Line Charts\nBeing a Mermaid with Katie Ledecky: Box Plots\nJack and Rose lived happily ever after: Mosaic Plots\nThe Art of Surprise with Gabbar Singh: Permutation Tests\n\n\ndatasaurus_dozen %&gt;% download_this(output_name = “datasaurus”, output_extension = “.csv”, button_label = “DataSaurus Dirty Dozen”, button_type = “default”, icon = “fa fa-save”, class = “hvr-sweep-to-left”)"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#orange-what-is-this-orange-stuff-anyhow",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#orange-what-is-this-orange-stuff-anyhow",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Orange? What is this Orange stuff, anyhow?",
    "text": "Orange? What is this Orange stuff, anyhow?\n\n\nOrange is a visual drag-and-drop tool for\n\nData visualization\n\nStatistical Tests\nMachine Learning\n\nData mining\n\nand much more. You can download and install Orange from here:\nhttps://orangedatamining.com/download/\n\n\n\n\n\n\n\nFigure 1: Orange Data Mining GUI"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#basic-usage-of-orange",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#basic-usage-of-orange",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#widgets-and-channels",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#widgets-and-channels",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-orange-visual-interface",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-orange-visual-interface",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Orange Visual Interface",
    "text": "The Orange Visual Interface\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#what-does-data-look-like",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#what-does-data-look-like",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "What does Data Look like?",
    "text": "What does Data Look like?\n\n\n\n\n\nVariable Types\n\n\n\n\n\n\nTidy Data"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Orange Practice Session#1",
    "text": "Orange Practice Session#1\n\n\n\nLet’s use the Datasets widget\nClick on it to select one of the built-in CSV files : Auto MPG\nLet’s look at the Data using the Data Table widget\nAnd create a Scatter Plot with the Scatter Plot widget (Horsepower vs Displacement)\nTry the menu options on the left side to see how they alter the plot\n\n\n\n\n\nLook at the data table to see the variable names, and the data types."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Brad Pitt: Throwing it All Away",
    "text": "Brad Pitt: Throwing it All Away"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Brad Pitt: Throwing it All Away",
    "text": "Brad Pitt: Throwing it All Away\n\n\n\n\nWhat was Brad Pitt throwing away? He was throwing away the individuality of the measures, subsuming them to one summary. This is a big idea in statistics, and it is called aggregation."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#a-pillar-of-statistical-wisdom",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#a-pillar-of-statistical-wisdom",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "A Pillar of Statistical Wisdom",
    "text": "A Pillar of Statistical Wisdom\n\n\nSteven Stigler (2016) in “The Seven Pillars of Statistical Wisdom”:\n\nOne of the Big Ideas in Statistics is: Aggregation\nHow is it revolutionary?\nBy stipulating that, given a number of observations, you can actually gain information by throwing information away\nIn taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#brad-pitt-throwing-it-all-away-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Brad Pitt: Throwing it All Away",
    "text": "Brad Pitt: Throwing it All Away\nWhat was he throwing away?\n\n\n\n\n\n\n\n\nAll the “Variables”\n\n\n\nAge\nPrevious Seasons\nWaist Size\nTreadmill Test Score\nBat Speed?\nSmoke Weed?\nGirlfriend?\nGirlfriend Looks Rating?\nWaddles like a Duck?\nLooks Weird?\n\n\n\n\n\n\n\n\n\n\n\nAnd he was looking ONLY at…\n\n\n\n\n\nOBP Data"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#how-do-we-throw-away-data",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#how-do-we-throw-away-data",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " How do we throw away data?",
    "text": "How do we throw away data?\n\n  \n\nBefore we plot a single chart, it is wise to take a look at several numbers that summarize the dataset under consideration. What might these be? Some obviously useful numbers are:\n\nDataset length: How many rows/observations?\nDataset breadth: How many columns/variables?\nHow many Quant variables?\nHow many Qual variables?\nQuant variables: min, max, mean, median, sd\nQual variables: levels, counts per level\nBoth: means, medians for each level of a Qual variable…"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Orange Practice Session #2",
    "text": "Orange Practice Session #2\n\n\n\nSo what do we throw away now? And how?\n\n\n\nLet’s look at the existing dataset, Auto MPG\nPull in the Feature Statistics widget\nConnect the Datasets widget to the Feature Statistics widget\nAppreciate the Summary Statistics that are generated"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Wait, But Why?",
    "text": "Wait, But Why?\n\n\n\nTo summarize is to understand.\nAdd to that the fact that our Working Memories can hold maybe 7 items, so it means information retention too.\nBorges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of “Funes the Memorious,”, there were only details.”\nBrad Pitt aka Billy Beane was throwing away the details, and looking at the aggregated picture to pick his future Oakland A’s team."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#counting-letters-with-sherlock-holmes",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#counting-letters-with-sherlock-holmes",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Counting Letters with Sherlock Holmes",
    "text": "Counting Letters with Sherlock Holmes"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#sherlock-holmes-the-adventure-of-the-dancing-men",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#sherlock-holmes-the-adventure-of-the-dancing-men",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Sherlock Holmes: The Adventure of the Dancing Men",
    "text": "Sherlock Holmes: The Adventure of the Dancing Men\n\n\nIn the Sherlock Holmes story, The Adventure of the Dancing Men, a criminal known to one of the characters communicates with her using a childish/child-like drawing which looks like this:\n\n\n\n\nAm Here, Abe Slaney\n\n\nHow would Holmes decipher this message?"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#sherlock-holmes-the-adventure-of-the-dancing-men-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#sherlock-holmes-the-adventure-of-the-dancing-men-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Sherlock Holmes: The Adventure of the Dancing Men",
    "text": "Sherlock Holmes: The Adventure of the Dancing Men\n\n\n\nUsing Conjectures:\nSymbols -&gt; Letters\nBased on well-known Counts of letters (Zipf’s Law)\nHolmes deduces that the most common letter in the message is “E”\nHe then deduces that the second most common letter is “T”\n\n\n\n\n\nZipf’s Law"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#what-charts-work-for-counting",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#what-charts-work-for-counting",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "What Charts work for counting?",
    "text": "What Charts work for counting?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\n\n\nBar charts are used to show “counts” and “tallies” with respect to Qual variables.\nFor instance, in a survey, how many people vs Gender?\nIn a Target Audience survey on Weekly Consumption, how many low, medium, or high expenditure people?\nNOTE: we count with Qual variables! (Why? We count labels, like letters.)\nWhich is why the Bars are not touching each other."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session-3",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session-3",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Orange Practice Session #3",
    "text": "Orange Practice Session #3\nBanned Books!!!\n\n\nOK, Let’s get some data to count:\n\n\n Banned Books data\n\n\nAnd let’s for now use a pre-set Workflow in Orange\n Download the Orange Barchart Workflow \n\n\nWe will look at the data\nMake a Data dictionary\nIdentify the Qual and Quant variables\nPrepare Counts and Bar Charts wrt Qual variables\nIn Orange! Point, Click, and See!"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Data Dictionary",
    "text": "Data Dictionary\nQualitative Variables\n\nAuthor: Author of the book (Qual)\nTitle: Title of the book (Qual)\nOrigin: Origin of the Challenge (Qual)\nType of Ban: Type of ban on the book (Qual)\nState: State in which the book was banned (Qual)\nDistrict: District in which the book was banned (Qual)\nOrigin: Origin of the Challenge (Qual)\n\nQuantitative Variables\n\nNone"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#counting-our-data",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#counting-our-data",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Counting our Data",
    "text": "Counting our Data\n\n\n\n\n\n\n\n\nResearch Question\n\n\nDo some States ban more books than some others?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the Story Here?\n\n\n\nTexas is the worst at book banning!\nTexas, Florida, Oklahoma, Kansas, Indiana,..are next in line\nIs there a “Bible Belt” story here?\n\n\n\n\n\n\n\nFigure 2: Bible Belt"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#counting-our-data-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#counting-our-data-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Counting our Data",
    "text": "Counting our Data\n\n\n\n\n\n\n\n\nResearch Question\n\n\nWhat are the kinds of bans that are being imposed on books? How many books banned by each type of ban?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the Story Here?\n\n\n\nFour reasons for banning books\n“Investigation” is the commonest kind of ban\nHow does one “investigate” a book???"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Wait, But Why?",
    "text": "Wait, But Why?\n\nCounts first give you an absolute sense of how much data you have.\nCounts by different Qual variables give you a sense of the combinations you have in your data: (State) * (District) * (Ban)\nCounts then give an idea whether your data is lop-sided\nSince the X-axis in bar charts is Qualitative (the bars don’t touch, remember!) it is possible to sort the bars at will."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#nursery-rhymes-with-ben-affleck",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#nursery-rhymes-with-ben-affleck",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Nursery Rhymes with Ben Affleck",
    "text": "Nursery Rhymes with Ben Affleck"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#nursery-rhymes-with-ben-affleck-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#nursery-rhymes-with-ben-affleck-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Nursery Rhymes with Ben Affleck",
    "text": "Nursery Rhymes with Ben Affleck\n\n\n\nIn “The Accountant,” Christian Wolff is heard reciting “Solomon Grundy,”\nThe nursery rhyme tells the life and death of a man named Solomon Grundy, all within a single week.\nIt was innocently used to help children learn their days of the week.\nHowever, when we look into the fact that Thursday through Sunday detail the tragic end of Mr. Grundy due to an unspecified illness…\nit’s hard to ignore the dark undertones."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#what-is-the-data-here-and-the-chart",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#what-is-the-data-here-and-the-chart",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "What is the Data here? And the Chart?",
    "text": "What is the Data here? And the Chart?\n\n\n\nThe data is the days of the week.\nThe data is the number of events that happen on each day.\nThe y-variable is a Quant variable, a number\nThe x-variable is also Quant variable, a time variable\n\n\n\n\n\n\n\nNote\n\n\nTourist: Any famous people born around here? Guide: No sir, best we can do is babies.\n\n\n\n\n\n\n\nLine Chart for Time Series"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#timing-our-data",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#timing-our-data",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Timing our Data",
    "text": "Timing our Data\n\n\nOK, Let’s get some data to chart:\n\n\n Born in the USA data\n\n\nAnd let’s for now use a pre-set Workflow in Orange\n Download Orange Time Series Workflow \n\n\nWe will look at the data\nMake a Data dictionary\nIdentify the Qual and Quant variables\nPrepare Time Series Charts with Quant variables, and Qual variables"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\n\nQualitative Variables\n\n\n\nyear: Year of birth (Qual)\nmonth: Month of the year (Qual)\nday_of_month: Day of the month (Qual)\nday_of_week: Day of the week (Qual)\n\n\n\n\n\n\n\n\n\n\nQuantitative Variables\n\n\n\nbirths: Number of births on that day (Quant)\n\n\n\n\n\n\n\n# A tibble: 6 × 5\n   year month date_of_month day_of_week births\n  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1  2000     1             1           6   9083\n2  2000     1             2           7   8006\n3  2000     1             3           1  11363\n4  2000     1             4           2  13032\n5  2000     1             5           3  12558\n6  2000     1             6           4  12466"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session4",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#orange-practice-session4",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Orange Practice Session#4",
    "text": "Orange Practice Session#4\n\n\n\nThe data is the number of births in the USA, by day, month, and year\nLet us use the Group By widget to group by day_of_week\nAND compute the mean(births) in the same widget\nWe plot the mean(births) vs month, and colour by day_of_week\n\n\n\n\n\nMean Births Time Series"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Wait, But Why?",
    "text": "Wait, But Why?\n\n\n\nTime series data is a special kind of Quantitative data, where the x-variable is a time variable.\nThe y-variable is a Quant variable, a number.\nThe x-variable is a Quant variable, a time variable.\nBy colouring by day_of_week, we can see how the number of births varies by day of the week.\n\n\nWhat is the Story Here?\n\nCould there be a staffing shortage at hospitals on weekends?\nIs this a “revealed preference” thing?\nOr should we watch Grey’s Anatomy, or even House?"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#being-a-mermaid-with-katie-ledecky",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#being-a-mermaid-with-katie-ledecky",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Being a Mermaid with Katie Ledecky",
    "text": "Being a Mermaid with Katie Ledecky"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#being-a-mermaid-with-katie-ledecky-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#being-a-mermaid-with-katie-ledecky-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Being a Mermaid with Katie Ledecky",
    "text": "Being a Mermaid with Katie Ledecky\n\n\n\n\n\nKatie Ledecky is a swimmer, and a mermaid.\nShe has won 7 Olympic gold medals, and 15 World Championship gold medals.\nShe is the world record holder in the 400, 800, and 1500 meter freestyle events, and in the 4x100 meter freestyle relay, and the 4x200 meter freestyle relay.\nWhat does that make her? An Outlier…"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#so-how-do-we-find-and-show-outliers",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#so-how-do-we-find-and-show-outliers",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "So how do we find, and show, outliers?",
    "text": "So how do we find, and show, outliers?\n\n\n\nOutliers are data points that are significantly different from the rest of the data.\nThey can be identified using box plots, which show the distribution of the data.\nBox plots show the median, quartiles, and outliers of the data.\nOf course, Ledecky was in the water! Well in.\n\n\n\n\n\n\n\n\n\n\nFigure 3: Box Plot Definitions"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#being-an-outlier-with-katie-ledecky",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#being-an-outlier-with-katie-ledecky",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Being an Outlier with Katie Ledecky",
    "text": "Being an Outlier with Katie Ledecky\n\n\n\nLet’s get some data to plot:\n\n\n\n Academic Salaries data\n\n\n\n\nAnd let’s use a pre-set Workflow in Orange  Download the Orange Box Plot Workflow"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#data-dictionary-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\n\nQualitative Variables\n\n\n\nrank: Rank of the academic (Qual)\ndiscipline: Discipline of the academic (Qual)\nsex: Male / Female\n\n\n\n\n\n\n\n\n\n\nQuantitative Variables\n\n\n\nyrs.since.phd: Years since PhD (Quant). Can be Qual??\nyrs.service`: Years of service (Quant)\nsalary: Salary of the academic (Quant)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Salaries Data Table"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#research-question1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#research-question1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Research Question#1",
    "text": "Research Question#1\n\n\n\n\n\n\nQuestion\n\n\nQ1. What is the distribution of salary? If we split by sex?\n\n\n\n\n\n\n\n\n\nFigure 5: Salaries Box Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Salaries Box Plot by Sex"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#research-question2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#research-question2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Research Question#2",
    "text": "Research Question#2\n\n\n\n\n\n\nQuestion\n\n\nQ2. What is the distribution of salary, when we split by other Qual variables, such as rank?\n\n\n\n\n\n\nFigure 7: Salaries Box Plot by Rank"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-3",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-3",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Wait, But Why?",
    "text": "Wait, But Why?\n\n\n\nBox Plots tell us distributions of Quant variables, and show us outliers.\nThey show us the median, quartiles, and outliers of the data.\nThey are useful for comparing distributions of Quant variables across Qual variables. ( E.g Sex or Rank)\n\n\nWhat is the Story Here?\n\nCould there be a systemic bias in salaries?\nA statistical t-test / ANOVA would tell us if that is true.\nLook in the figures for a t-test and ANOVA report at the bottom."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Jack and Rose lived happily ever after",
    "text": "Jack and Rose lived happily ever after"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Jack and Rose lived happily ever after?",
    "text": "Jack and Rose lived happily ever after?\n\n\n\nThe Titanic sank on 15 April 1912, after hitting an iceberg.\nWhat are the chances that Jack survived too?\nWhat did his chances depend on?"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#jack-and-rose-lived-happily-ever-after-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Jack and Rose lived happily ever after?",
    "text": "Jack and Rose lived happily ever after?\n\nLet’s get the titanic data, using the Datasets widget in Orange.\nThere were 2201 passengers, as per this dataset.\nAnd let’s use a pre-set Workflow in Orange\n\n Download the Orange Mosaic Chart Workflow"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\n\nQual\nQual\nPies, Mosaic Charts\n\n\n\n\nHere, area \\sim count, so the area of the tile is proportional to the count of observations in that tile."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#what-are-these-residuals-anyhow",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#what-are-these-residuals-anyhow",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "What are these Residuals anyhow?",
    "text": "What are these Residuals anyhow?\nWhen differences between the actual and expected counts are large, we deduce that one Qual variable has an effect on the other Qual variable. (speaking counts-wise or ratio-wise)\n\n\n\n\n\n\nActual Counts\n\n\n\n\n\n\n\nExpected Counts!!\n\n\n\n\n\n\n\nTile-Wise Differences = Residuals"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-4",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#wait-but-why-4",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Wait, But Why?",
    "text": "Wait, But Why?\n\nMosaic Charts are used to show the relationship between two Qual variables.\nThey show the counts of observations in each combination of the two Qual variables.\nThe area of each tile is proportional to the count of observations in that tile\nThe colour of the tile shows the residuals, which are the differences between the actual and expected counts."
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-1",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-1",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-2",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-2",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh\n\n\n\nWhat was the “data” Gabbar was looking at?\n\n\n\n\nChamber\nBullet\n\n\n\n\n1\nY / N\n\n\n2\nY / N\n\n\n..\n…\n\n\n6\nY / N\n\n\n\n\n\nThe number of people in the village, and the number of people who were armed. (Quant)\nThe number of bullets in the gun (Quant)\nThe ID of the pistol chamber which contains a bullet (Qual)"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-3",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-3",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh\nAnd Gabbar’s Hypothesis?\n\nThat three bullets would never line up just ready to be fired, especially after he had fired three off!!\nSo he could claim “ignorance”!\n“Ignorance” == “Fairness” == “Justice” !\nBut he was pretend-surprised when the three bullets were lined up, and he could fire them all off at once!\nBut no one could accuse him of anything, because he was “ignorant” of the fact that the bullets were lined up!"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#gabbars-gun-chamber-permutations",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#gabbars-gun-chamber-permutations",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Gabbar’s Gun Chamber Permutations",
    "text": "Gabbar’s Gun Chamber Permutations"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#so-how-do-we-become-gabbar-singh",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#so-how-do-we-become-gabbar-singh",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "So how do we become Gabbar Singh?",
    "text": "So how do we become Gabbar Singh?\n\n\n\n\n\n\n\n\nAre Emily and Greg more Employable than Lakisha and Jamal?\n\n\nIs ethnicity (as revealed by first names) a basis for racial discrimination, in the US?\nThis dataset was generated as part of a landmark research study done by Marianne Bertrand and Senthil Mullainathan.\nRead the description therein to really understand how you can prove causality with a well-crafted research experiment.\n\n\n\n\n\n\n# A tibble: 6 × 3\n  name    ethnicity call \n  &lt;chr&gt;   &lt;chr&gt;     &lt;fct&gt;\n1 Allison cauc      no   \n2 Kristen cauc      no   \n3 Lakisha afam      no   \n4 Latonya afam      no   \n5 Carrie  cauc      no   \n6 Jay     cauc      no   \n\n\n# A tibble: 4 × 3\n# Groups:   ethnicity [2]\n  ethnicity call      n\n  &lt;chr&gt;     &lt;fct&gt; &lt;int&gt;\n1 afam      yes     157\n2 afam      no     2278\n3 cauc      yes     235\n4 cauc      no     2200\n\n\n# A tibble: 2 × 2\n  ethnicity call_prop\n  &lt;chr&gt;         &lt;dbl&gt;\n1 afam           6.45\n2 cauc           9.65\n\n\ndiffprop \n0.108199"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-4",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-4",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh\n\n\n\nSo it appears the call percentage is different for the two ethnicities, afam and cauc\nBut is it statistically significant? Would Gabbar be surprised?\nLet us pretend ethnicity does not matter and spin the revolver!!\nWe mess with the ethnicity variable, some 5000 times\n\n\n\n\n[1] \"diffprop\""
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-5",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#the-art-of-surprise-with-gabbar-singh-5",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "The Art of Surprise with Gabbar Singh",
    "text": "The Art of Surprise with Gabbar Singh\n\n\n\nWe are not able to mimic Mother Nature aka Reality\nThe red line is the observed difference in proportions, and it is way out of the null distribution.\nSo we can reject the NULL Hypothesis that ethnicity does not matter.\nHence we infer that there was bias in the hiring process, and that afam candidates were discriminated against.\n\n\n\n\n[1] \"diffprop\""
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#gabbar-viv-a-vis-a-stats-teacher",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#gabbar-viv-a-vis-a-stats-teacher",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": " Gabbar viv-a-vis a Stats Teacher",
    "text": "Gabbar viv-a-vis a Stats Teacher\n\n\n\n\n\n\n\nGabbar\nStats Teacher\n\n\n\n\n“Kitne aadmi thay?”\nHow many observations do you have? n &lt; 30 is a joke.\n\n\nKya Samajh kar aaye thay? Gabbar khus hoga? Sabaasi dega kya?\nWhat are the levels in your Factors? Are they binary? Don’t do ANOVA just yet!\n\n\n(Fires off three rounds ) Haan, ab theek hai!\nYes, now the dataset is balanced wrt the factor (Treatment and Control).\n\n\nIs pistol mein teen zindagi aur teen maut bandh hai. Dekhte hain kisko kya milega.\nThis is our Research Question, for which we will Design an Experiment.\n\n\nHume kuchh nahi pataa!\nLet us perform a non-parametric Permutation Test for this Factor!\n\n\nKamaal ho gaya!\nFantastic! Our p-value is so small that we can reject the NULL Hypothesis!!"
  },
  {
    "objectID": "content/projects/Modules/Talks/VizChitra25/index.html#thank-you",
    "href": "content/projects/Modules/Talks/VizChitra25/index.html#thank-you",
    "title": "Data, DataViz, and Stats with the Stars",
    "section": "Thank You!!",
    "text": "Thank You!!\nQuestions? Comments? Suggestions?\narvind.venkatadri@gmail.com\nThis Presentation: https://av-quarto.netlify.app/content/projects/modules/talks/vizchitra25/\nData and Workflow files are available at https://www.dropbox.com/scl/fo/15ha6pmtqd3t17c39idxi/AEviMggAcido-xQHiD82dH4?rlkey=h9xj09pdtx3st9basdfljwwyw&st=06dvqnbb&dl=0"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "href": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "title": "The Nature of Data",
    "section": "What makes Human Experience?",
    "text": "What makes Human Experience?\n\n\n\n\n\nHow would we begin to describe this experience?\n\n\n\nWhere / When?\nWho?\nHow?\nHow Big? How small? How frequent? How sudden?\n\n\n\n\n\nAnd….How Surprising ! How Shocking! How sad…How Wonderful !!!\nSo: Our Questions, and our Surprise lead us to creating Human Experiences."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "href": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "title": "The Nature of Data",
    "section": "Does this Surprise you?",
    "text": "Does this Surprise you?\n\n\nNeeds to be celebrated. Spotted in a men’s washroom @BLRAirport - a diaper change station\nChildcare is not just a woman’s responsibility.\npic/twitter.com/Za4CG9jZfR\n-Sukhada(@appadappajapa), June 27, 2022"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "href": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "title": "The Nature of Data",
    "section": "The Element of Surprise?",
    "text": "The Element of Surprise?\n\n\n\n\n\nJane Austen knew a lot about human information processing as these snippets from Pride and Prejudice (published in 1813 – over 200 years ago)1 show :\n\nShe was a woman of mean understanding, little information , and uncertain temper.\nCatherine and Lydia had information for them of a different sort.\nWhen this information was given, and they had all taken their seats, Mr. Collins was at leisure to look around him and admire,…\nYou could not have met with a person more capable of giving you certain information on that head than myself, for I have been connected with his family in a particular manner from my infancy.\nThis information made Elizabeth smile, as she thought of poor Miss Bingley.\nThis information, however, startled Mrs. Bennet …\n\n\n\nhttps://www.cs.bham.ac.uk/research/projects/cogaff/misc/austen-info.html"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\n\n\n\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is….Data??",
    "text": "Human Experience is….Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\n\n\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nOutputs: We obtain data about the things that happen\nInputs: What makes things happen?\nProcess: How?\nFactors: When?\nEffect Size: How much “output” is caused by how much “input”?\n\n\nAll Experiments stem from\n- Human Curiosity,\n- a Hypothesis, and  - a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors – failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale’s Data",
    "text": "Florence Nightingale’s Data\n\n\n   Month Year Disease.rate Wounds.rate Other.rate\n1    Apr 1854          1.4         0.0        7.0\n2    May 1854          6.2         0.0        4.6\n3    Jun 1854          4.7         0.0        2.5\n4    Jul 1854        150.0         0.0        9.6\n5    Aug 1854        328.5         0.4       11.9\n6    Sep 1854        312.2        32.1       27.7\n7    Oct 1854        197.0        51.7       50.1\n8    Nov 1854        340.6       115.8       42.8\n9    Dec 1854        631.5        41.7       48.0\n10   Jan 1855       1022.8        30.7      120.0"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\nTypes of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\nUsing Interrogative Pronouns\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-right",
    "href": "content/slides/r-slides/networks/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-left",
    "href": "content/slides/r-slides/networks/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#section",
    "href": "content/slides/r-slides/networks/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "href": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#video-slide-title",
    "href": "content/slides/r-slides/networks/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#slide-title",
    "href": "content/slides/r-slides/networks/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#quarto",
    "href": "content/slides/r-slides/networks/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#bullets",
    "href": "content/slides/r-slides/networks/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#code",
    "href": "content/slides/r-slides/networks/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\n```{r setup}\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sf)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\nlibrary(osmplotr) # Creating maps with OSM data in R\n# library(OpenStreetMap) # Raster Data\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#add-shapes-to-a-map",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "Add Shapes to a Map",
    "text": "Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\nAdd Markers with popups\n\n```{r adding_markers}\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n# Click on the Marker for the popup to appear\n```\n\n\n\n\n\nThis uses the default pin shape as the Marker.\n\n\nAdding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\n```{r popups}\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n```\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n\nAdding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\n```{r labels}\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n```\n\n\n\n\n\n\n\nAdding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n```{r drawing_circles_on_a_map}\n#| message: false\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n```\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n\nAdding Rectangles to a Map\n\n```{r}\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n```\n\n\n\n\n\n\n\nAdd Polygons to a Map\n\n```{r}\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n```\n\n\n\n\n\n\n\nAdd PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\n```{r}\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n```\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#point-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Point Data Sources for leaflet",
    "text": "Point Data Sources for leaflet\nPoint data for markers can come from a variety of sources:\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\nPoints using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n```{r data.world_leaflet_example}\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nindia_airports %&gt;% head()\n```\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\n```{r}\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n```\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n```{r airports_with_popups}\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n```\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n```{r itpl}\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n```\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n```{r}\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n```\n\n\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\n```{r,message=FALSE}\ndata(metro, package = \"tmap\")\nmetro\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n```\n\n\n  \n\n\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data&lt;https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\n```{r}\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\nlocations &lt;- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nlocations &lt;- locations %&gt;% \n  dplyr::filter(cuisine == \"indian\")\nlocations %&gt;% head()\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n```\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\n\n\n  \n\n\n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\n```{r fontawesome_workaround}\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n```\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\n```{r matrix_point_data}\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")\n```\n\n         [,1]     [,2]\n[1,] 76.64400 12.31891\n[2,] 76.65291 12.31729\n[3,] 76.65451 12.31965\n[4,] 76.64680 12.31829\n[5,] 76.65230 12.32089"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Polygons, Lines, and Polylines Data Sources for leaflet",
    "text": "Polygons, Lines, and Polylines Data Sources for leaflet\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\n```{r, cache=TRUE}\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n# Run the lines below ONE TIME in your CONSOLE!\n# \n# colleges &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                            key = \"amenity\",\n#                                            value = \"college\",\n#                                            return_type = \"polygon\" )\n# parks &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                            key = \"park\",\n#                                            return_type = \"polygon\" )\n# roads &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                        key = \"highway\",\n#                                        value = \"primary\",\n#                                        return_type = \"line\")\n# cyclelanes &lt;-\n#   osmplotr::extract_osm_objects(bbox,\n#                                 key = \"cycleway\",\n#                                 value =  \"lane\",\n#                                 return_type = \"line\")\n# st_write(colleges, \n#          dsn = \"colleges.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(parks, \n#          dsn = \"parks.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(roads, \n#          dsn = \"roads.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(cyclelanes, \n#          dsn = \"cyclelanes.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n```\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\n\n\n```{r}\ncolleges &lt;- st_read(\"./colleges.gpkg\")\nparks &lt;- st_read(\"./parks.gpkg\")\ncyclelanes &lt;- st_read(\"./cyclelanes.gpkg\")\nroads &lt;- st_read(\"./roads.gpkg\")\n```\n\nReading layer `colleges' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\colleges.gpkg' \n  using driver `GPKG'\nSimple feature collection with 21 features and 38 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 4.827297 ymin: 52.27956 xmax: 4.971438 ymax: 52.3907\nGeodetic CRS:  WGS 84\nReading layer `parks' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\parks.gpkg' \n  using driver `GPKG'\nSimple feature collection with 384 features and 52 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 4.74452 ymin: 52.27908 xmax: 5.070239 ymax: 52.43409\nGeodetic CRS:  WGS 84\nReading layer `cyclelanes' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\cyclelanes.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1031 features and 163 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 4.717813 ymin: 52.27064 xmax: 5.061863 ymax: 52.43169\nGeodetic CRS:  WGS 84\nReading layer `roads' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1845 features and 136 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 4.72487 ymin: 52.27799 xmax: 5.082486 ymax: 52.43236\nGeodetic CRS:  WGS 84\n\n\nWe have 21 colleges in our data and 384 parks in our data.\n\n```{r}\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, color= \"yellow\",\n              popup = ~colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "Chapter 3: Using Raster Data in leaflet",
    "text": "Chapter 3: Using Raster Data in leaflet\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\n```{r raster_data_in_leaflet}\nlibrary(terra)\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash...\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#adding-legendswork-in-progress",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "Adding Legends[Work in Progress!]",
    "text": "Adding Legends[Work in Progress!]\n\n```{r}\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n```"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/LICENSE.html",
    "href": "content/slides/projects-slides/portfolio/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/ML4Artists/listing.html",
    "href": "content/courses/ML4Artists/listing.html",
    "title": "Machine Learning for Artists and Managers",
    "section": "",
    "text": "🐉 Intro to Orange\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Regression\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Classification\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nClassification using Support Vector Machines\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nML - Clustering\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n🕔 Modelling Time Series\n\n\nWe will look at the basic models for Time Series\n\n\n\n\n\nNov 19, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#introduction",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling Time Series",
    "section": "Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative here !! A multiplicative time series can be converted to additive by taking a log of the time series."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies,the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval.( i.e. self-similar and fractal)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;%\n  pivot_wider(.,\n    id_cols = c(Date),\n    names_from = Dept,\n    values_from = Weekly_Sales,\n    names_prefix = \"Sales_\"\n  )\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;%\n  select(Sales_95) %&gt;%\n  ts(start = c(2010, 1), end = c(2012, 52), frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast,\n  fitted = TRUE,\n  timetk_idx = TRUE\n)\n\nsales95_forecast_tidy\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;-\n  sales95_forecast_tidy %&gt;%\n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95),\n    fill = \"#3182bd\", color = NA\n  ) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key),\n    fill = \"#deebf7\", color = NA, alpha = 0.8\n  ) +\n  geom_line(size = 1) +\n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  # coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_time_series(.data = sales95_forecast_tidy, .date_var = index, .value = value, .color_var = key, .smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling Time Series",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#references",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#a-childhood-game",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#a-childhood-game",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n2009\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n2008\n\n\nGentoo\nBiscoe\n50.0\n16.3\n230\n5700\nmale\n2007\n\n\nAdelie\nBiscoe\n35.9\n19.2\n189\n3800\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.3\n220\n5550\nmale\n2007\n\n\nAdelie\nTorgersen\n35.2\n15.9\n186\n3050\nfemale\n2009\n\n\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmale\n2007\n\n\nChinstrap\nDream\n51.3\n18.2\n197\n3750\nmale\n2007\n\n\nGentoo\nBiscoe\n45.8\n14.2\n219\n4700\nfemale\n2008\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.1\n15.1\n209\n5500\nmale\n2009\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: &lt; 50% diameter narrowing\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Decision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#references",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html",
    "title": "Classification using Support Vector Machines",
    "section": "",
    "text": "Show the Codepenguins %&gt;% gf_point(body_mass_g ~ flipper_length_mm, colour = ~species)\n\n\n\n\n\n\n\n\nlibrary(p5)\nlibrary(tibble)\n\n# Create drawings from data frames\n\nsquares &lt;- data_frame(\n  x = c(100, 100, 200, 200),\n  y = c(50, 150, 50, 150),\n  w = rep(40, 4),\n  h = rep(40, 4)\n)\n\nsquares %&gt;%\n  p5() %&gt;%\n  createCanvas(300, 200) %&gt;%\n  background(\"#002d72\") %&gt;%\n  rect()\n\n\n\n\n\n\n# Draw complex shapes\n\np5() %&gt;%\n  createCanvas(200, 200) %&gt;%\n  background(\"#DCDCDC\") %&gt;%\n  arc(50, 55, 50, 50, 0, ~HALF_PI) %&gt;%\n  noFill() %&gt;%\n  arc(50, 55, 60, 60, ~HALF_PI, ~PI) %&gt;%\n  arc(50, 55, 70, 70, ~PI, ~ PI + QUARTER_PI) %&gt;%\n  arc(50, 55, 80, 80, ~ PI + QUARTER_PI, ~TWO_PI)\n\n\n\n\n\n\n# Create a sketch piece-by-piece\n\nsquares %&gt;%\n  draw() %&gt;%\n  fill(\"#808080\") %&gt;%\n  rect() %&gt;%\n  sketch(\n    draw = .,\n    setup = setup() %&gt;% createCanvas(300, 200)\n  )\n\n\n\n\n\n\n# Create interactions\n\ndraw() %&gt;%\n  background(\"#F4F8FC\") %&gt;%\n  line(~mouseX, 0, ~mouseX, 200) %&gt;%\n  sketch(\n    draw = .,\n    setup = setup() %&gt;% createCanvas(300, 200)\n  )\n\n\n\n\n\n\n# Let users draw\n\np5() %&gt;%\n  createCanvas(400, 300) %&gt;%\n  background(\"#F4F8FC\") %&gt;%\n  ellipse(~mouseX, ~mouseY, 30, 30)\n\n\n\n\n# Click to change the brush\n\nsetup_ &lt;- setup() %&gt;%\n  createCanvas(640, 380, ~WEBGL)\ndraw_ &lt;- draw() %&gt;%\n  js(\"\n      if (mouseIsPressed) {\n        fill(0);\n      } else {\n        fill(255);\n      }\n  \") %&gt;%\n  ellipse(~mouseX, ~mouseY, 80, 80)\n\nsketch(setup = setup_, draw = draw_)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html#introduction",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html#introduction",
    "title": "Classification using Support Vector Machines",
    "section": "",
    "text": "Show the Codepenguins %&gt;% gf_point(body_mass_g ~ flipper_length_mm, colour = ~species)\n\n\n\n\n\n\n\n\nlibrary(p5)\nlibrary(tibble)\n\n# Create drawings from data frames\n\nsquares &lt;- data_frame(\n  x = c(100, 100, 200, 200),\n  y = c(50, 150, 50, 150),\n  w = rep(40, 4),\n  h = rep(40, 4)\n)\n\nsquares %&gt;%\n  p5() %&gt;%\n  createCanvas(300, 200) %&gt;%\n  background(\"#002d72\") %&gt;%\n  rect()\n\n\n\n\n\n\n# Draw complex shapes\n\np5() %&gt;%\n  createCanvas(200, 200) %&gt;%\n  background(\"#DCDCDC\") %&gt;%\n  arc(50, 55, 50, 50, 0, ~HALF_PI) %&gt;%\n  noFill() %&gt;%\n  arc(50, 55, 60, 60, ~HALF_PI, ~PI) %&gt;%\n  arc(50, 55, 70, 70, ~PI, ~ PI + QUARTER_PI) %&gt;%\n  arc(50, 55, 80, 80, ~ PI + QUARTER_PI, ~TWO_PI)\n\n\n\n\n\n\n# Create a sketch piece-by-piece\n\nsquares %&gt;%\n  draw() %&gt;%\n  fill(\"#808080\") %&gt;%\n  rect() %&gt;%\n  sketch(\n    draw = .,\n    setup = setup() %&gt;% createCanvas(300, 200)\n  )\n\n\n\n\n\n\n# Create interactions\n\ndraw() %&gt;%\n  background(\"#F4F8FC\") %&gt;%\n  line(~mouseX, 0, ~mouseX, 200) %&gt;%\n  sketch(\n    draw = .,\n    setup = setup() %&gt;% createCanvas(300, 200)\n  )\n\n\n\n\n\n\n# Let users draw\n\np5() %&gt;%\n  createCanvas(400, 300) %&gt;%\n  background(\"#F4F8FC\") %&gt;%\n  ellipse(~mouseX, ~mouseY, 30, 30)\n\n\n\n\n# Click to change the brush\n\nsetup_ &lt;- setup() %&gt;%\n  createCanvas(640, 380, ~WEBGL)\ndraw_ &lt;- draw() %&gt;%\n  js(\"\n      if (mouseIsPressed) {\n        fill(0);\n      } else {\n        fill(255);\n      }\n  \") %&gt;%\n  ellipse(~mouseX, ~mouseY, 80, 80)\n\nsketch(setup = setup_, draw = draw_)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html#what-is-classification-again",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html#what-is-classification-again",
    "title": "Classification using Support Vector Machines",
    "section": "What is Classification, again?",
    "text": "What is Classification, again?\nClassification is a Machine Learning method that predicts the class or level of a target Qualitative variable, based on several Qualitative and Quantitative predictors."
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html#how-does-one-classify",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html#how-does-one-classify",
    "title": "Classification using Support Vector Machines",
    "section": "How does one Classify?",
    "text": "How does one Classify?"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html#what-is-a-hyperplane",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html#what-is-a-hyperplane",
    "title": "Classification using Support Vector Machines",
    "section": "What is a Hyperplane?",
    "text": "What is a Hyperplane?\nConsider a dataset that contains a pair of Quantitative predictor variables and a target Qualitative variable.\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Hyperplane"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/35-SVM/index.html#references",
    "href": "content/courses/ML4Artists/Modules/35-SVM/index.html#references",
    "title": "Classification using Support Vector Machines",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\n\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\n\n\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\n\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#abstract",
    "href": "content/courses/NoCode/listing.html#abstract",
    "title": "Data Science with No Code!",
    "section": "Abstract",
    "text": "Abstract\n\nIt is a truth universally acknowledged, that a Srishti Art and Design student, in possession of a good Mac, must be terrified of coding.\n\n-Code and Prejudice, Jane Austen, 1813\n\n\nThis Unit takes SMI peasants students on a journey of using data to tell stories, make decisions, and maybe startle a few people. Without doing an iota of work writing one byte of code. Bah.\nDatasets from various domains of human enterprise and activity are introduced. The human contexts from these datasets are used to appreciate the specifics of data formats, the nature of variables within the data, and their significance in Art and Design.\nThe stuff within the datasets are motivated from the point of view of the Types of Information they contain: Quantities, Attributes, Changes, Portions, Rankings, Relations, Hierarchies, and related to Space, and Time, for example. Student will relate the data variables to Data/Information Visualizations, making decisions on how geometric shapes and other aspects of different Data Types and Visualizations can be metaphorically matched to the contexts. These information-to-geometry metaphors will lead us to Insights, Questions, and eventually to Stories and good Decisions.\nStatistical tests will be introduced at appropriate moments to help appreciate how large and significant our insights are.\nStudents will then be prompted to work in groups, or as a whole, to conduct (one or more) complete data gathering experiments on campus, visualize the data, and tell a Story that pertains to their immediate surroundings."
  },
  {
    "objectID": "content/courses/NoCode/listing.html#what-you-will-learn-the-recipe",
    "href": "content/courses/NoCode/listing.html#what-you-will-learn-the-recipe",
    "title": "Data Science with No Code!",
    "section": "What you will learn: The Recipe",
    "text": "What you will learn: The Recipe\n\nGet the Ingredients Together:\n\nLearn to shop for data\nAppreciate how it was grown in the first place\nWhat does data look like and why should we care?\nHow to Spot a good vegetable Variable\n\nWash, Clean, Peel, Grate, Fry:\n\nRapidly make different kinds of Charts and Tables\nMasala Colour? Large-sized Pieces? Filtering?\nCopy first, then Innovate\n\nSample and Taste\n\nAsk Questions: How does your Chart taste?\nTry to Answer your Questions with Graphs, Tables, or Plain Numbers\nDoes it need more salt, or do you want to bin the whole thing and start over?\n\nThe Element of Surprise:\n\nUse your Sense of Surprise: Is a different chart what you need?\nDevelop intuition that matches data and chart types\n\nDoes it Matter?:\n\nHow significant is my insight?\nDoes turmeric really add flavour to pineapple-pizza?\n\nLay the Table:\n\nTell the Stories: What is the Big Idea?\nAnnotate Graphs with text and insights\nExport these to create crisp and readable documents that you can share\nDecide what you are going to do next\n\nReplicate and Reproduce:\n\nUse the Recipe Again"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#introduction",
    "href": "content/courses/NoCode/listing.html#introduction",
    "title": "Data Science with No Code!",
    "section": "Introduction",
    "text": "Introduction\nTake a look at the graph visualization below:\n\n\n\n\nWhat information does the graph convey? How ?\nWhat aspects of the Visual convey “human” information, such as Number and Relation?\nWhat could the sloping dotted line in the picture depict?\n\nWe will form our intuition about shapes and data and learn to create some evocative information graphics that tell stories."
  },
  {
    "objectID": "content/courses/NoCode/listing.html#readings-and-references",
    "href": "content/courses/NoCode/listing.html#readings-and-references",
    "title": "Data Science with No Code!",
    "section": "Readings and References",
    "text": "Readings and References\n\nThe Elevate DataViz Blog: The Genres of Data Stories https://blog.elevatedataviz.com/the-types-of-data-stories/\n\nDear Data Science. https://users.dimi.uniud.it/~massimo.franceschet/ds/syllabus/syllabus.html\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free Online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free Online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art:An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361.\nCole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253.\nStat Literacy. http://www.statlit.org"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#quick-lookup",
    "href": "content/courses/NoCode/listing.html#quick-lookup",
    "title": "Data Science with No Code!",
    "section": "Quick Lookup",
    "text": "Quick Lookup\n\nCharts and Data\n\nData Vis Project https://datavizproject.com/ Allows you to match data types and data-vis types!! Perfect!!\nData Viz Catalogue https://datavizcatalogue.com/ Another good place to look for graphs that match your data!\nData-to-Viz https://www.data-to-viz.com/#explore\nFinancial Times Visual Vocabulary Chart. A great chart to match data to data-viz. PDF here and Web version https://ft-interactive.github.io/visual-vocabulary/\nGramener Blog. 72 types of Visualization for Data Stories. https://blog.gramener.com/types-of-data-visualization-for-data-stories/\n\n\n\nCharting in R\n\nR Charts https://r-charts.com/\nR Graph Gallery https://r-graph-gallery.com/index.html"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#shopping-for-free-data",
    "href": "content/courses/NoCode/listing.html#shopping-for-free-data",
    "title": "Data Science with No Code!",
    "section": "Shopping for Free Data",
    "text": "Shopping for Free Data\n\nA wide variety of graphics and datasets on global issues at Our World in Data https://ourworldindata.org/\nDatasets at calmcode.io https://calmcode.io/datasets.html. Simple datasets that you should begin with.\nData.World https://data.world. A very well organized easily searchable database of datasets and visualizations!\nThe Harvard Dataverse https://dataverse.harvard.edu/. A very large searchable database of datasets on a very wode set of topics.\nIPUMS https://www.ipums.org/ The Integrated Public Use Microdata Series (IPUMS) is the world’s largest individual-level population database. IPUMS consists of microdata samples from United States (IPUMS-USA) and international (IPUMS-International) census records, as well as data from U.S. and international surveys. Data provided is integrated across time and space. Health, Economics, Higher Education, Historical Data and much more.\nKaggle Datasets https://www.kaggle.com/datasets E.g. Netflix Shows\nData Is Plural https://www.data-is-plural.com/. This a weekly newsletter of useful/curious datasets by Jeremy Singer-Vine. And a web archive of datasets too.\nInformation is Beautiful https://informationisbeautiful.net/ David McCandless’ terrific information visualization site. All datasets used here are also available for download.\nIndia Data by Sector https://data.gov.in/sector\nThe FBI’s Crime Data Explorer (very US-centric) https://crime-data-explorer.app.cloud.gov/pages/home\nDatasets at Nate Silver’s website 538 ( very US-centric) https://data.fivethirtyeight.com/\nOpen Data Network ( again very US-centric) https://www.opendatanetwork.com/\n311-data.org https://www.311-data.org/. Data about 311 calls in different parts of the US. (#311 is a complaints service that deals with non-crime / non-emergency related neighbourhood issues in the US)\nGoogle Dataset Search https://datasetsearch.research.google.com/\nGithub dataset search https://github.com/search?q=datasets\nWorld Inequality Database, https://wid.world/. Global data on income and wealth inequality. India specific data also available.\nWorld Bank Open Data https://data.worldbank.org/. A global collection of economic development data .\nJonathan Schwabish’s PolicyViz DataViz Catalogue. https://policyviz.com/resources/policyviz-data-visualization-catalog/ This is a spreadsheet that has links to data and images of visualizations that have been achieved with each of the datasets. Over 800 entries…see the embedded table below! (US centric, but very inspirational visualizations!).\n\n\n\n\n\n\nWork With Data. https://www.workwithdata.com/data A good selection of datasets on a wide set of topics. Check the neat network diagram there!\nVincent Arel-Bundock’s RDatasets webpage: https://vincentarelbundock.github.io/Rdatasets/index.html\nhttps://www.city-data.com\nOECD Data Explorer. https://data-explorer.oecd.org"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#our-tools",
    "href": "content/courses/NoCode/listing.html#our-tools",
    "title": "Data Science with No Code!",
    "section": "Our Tools",
    "text": "Our Tools\n\nChart Creation and Export\n\nOrange Data Mining https://orangedatamining.com/ Free software. Very intuitive, point-and-click, goes all the way from simple data-viz to ML!\nDatawrapper https://academy.datawrapper.de/ A free browser-based tool, requires registration and login.\nRAWGraphs https://app.rawgraphs.io/ Another Free browser-based tool, no registration, no login. Simple interface too.\n\n\n\nStory Telling with Charts\n\nObservable Plot: https://observablehq.com/plot/ The JavaScript library for exploratory data visualization that can create expressive charts with concise (Javascript) code. There is also a #NoCode method available there. Ugh.\nFlourish Studio https://flourish.studio/ Beautiful and easy data visualization and storytelling\nInfogram https://infogram.com/ Create engaging infographics and reports in minutes\nVisme https://www.visme.co/ Yet another…"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#data-viz-courses-elsewhere",
    "href": "content/courses/NoCode/listing.html#data-viz-courses-elsewhere",
    "title": "Data Science with No Code!",
    "section": "Data viz Courses Elsewhere",
    "text": "Data viz Courses Elsewhere\n\nhttps://shancarter.github.io/ucb-dataviz-fall-2013/\n\nAnd of course, there is this:\nhttps://openai.com/index/improvements-to-data-analysis-in-chatgpt/"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#data-quirkiness",
    "href": "content/courses/NoCode/listing.html#data-quirkiness",
    "title": "Data Science with No Code!",
    "section": "Data Quirkiness",
    "text": "Data Quirkiness\n\nData@Urban blog.Bringing Data to Life: Physical Data Projects at Urban. https://urban-institute.medium.com/bringing-data-to-life-physical-data-projects-at-urban-2afe193bd038"
  },
  {
    "objectID": "content/courses/NoCode/listing.html#modules",
    "href": "content/courses/NoCode/listing.html#modules",
    "title": "Data Science with No Code!",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html",
    "title": "\n Quantity",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#what-graphs-will-we-see-today",
    "title": "\n Quantity",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Quantity",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#inspiration",
    "title": "\n Quantity",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Golf Drive Distance over the years\n\n\nWhat do we see here? In about two-and-a-half decades, golf drive distances have increased, on the average, by 35 yards. The maximum distance has also gone up by 30 yards, and the minimum is now at 250 yards, which was close to average in 1983! What was a decent average in 1983 is just the bare minimum in 2017!!\nIs it the dimples that the golf balls have? But these have been around a long time…or is it the clubs, and the swing technique invented by more recent players?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#how-do-these-charts-work",
    "title": "\n Quantity",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nHistograms are best to show the distribution of values of a quantitative variable. A distribution shows how often the variable in question lies within specific value ranges. We plot the histogram by displaying the how often vs defined ranges, often called buckets or bins. For example, in 2017, 8.5% of all drive distances were at the then average distance of 292.1 yards. One can create histogram buckets from Quant variables, such as 0-5, 6-10, 11-15…etc.\n\n\n\n\n\n\nImportantHistograms vs Bar/Column Charts\n\n\n\nAs we will see shortly, Bar/Column charts show categorical data, such as the number of apples, bananas, carrots, etc. Visually speaking, histograms do not usually show spaces between buckets because these are continuous values, while column charts must show spaces to separate each category. More later."
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#plotting-a-histograms",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#plotting-a-histograms",
    "title": "\n Quantity",
    "section": "\n Plotting a Histograms",
    "text": "Plotting a Histograms\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us rapidly make some histograms in Orange, so that we know how the tool works here. We start with the iris dataset: Download this Orange workflow file and open it in Orange.\n Download the Histogram Workflow \nYou can see the effect of modifying the bin widths, and of fitting a standard distribution for comparison.\n\n\nRAWgraphs does not appear to have a histogram plotting tool…\n\n\nhttps://academy.datawrapper.de/article/136-histogram-min-max-median-mean\nDataWrapper also does not offer a separate histogram-making tool. Histograms in DataWrapper are available as a part of the data-inspection part of the work flow, as a small thumbnail-sized plot."
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#dataset-netflix-original-series",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#dataset-netflix-original-series",
    "title": "\n Quantity",
    "section": "\n Dataset: Netflix Original Series",
    "text": "Dataset: Netflix Original Series\nWe are now ready for a more detailed example. Here is a look at this data on Netflix Original Series. Download it to your machine by clicking on the button below.\n Download the Netflix Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 2: Netflix Data Table\n\n\nFigure 2 states that there are 109 movies, 6 variables in the dataset.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nPremiere_Year: (int) Year the movie premiered\n\nSeasons: (int) No. of Seasons\n\nEpisodes: (int) No. of Episodes\n\nIMDB_Rating: (int) IMDB Rating!!\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nGenere: (chr) types of Genres\n\nTitle: (chr) 109 titles\n\nSubgenre: (chr) types of sub-Genres\n\nStatus: (chr) status on Netflix\n\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Histograms.\n\n\n\n\n\n\nNote\n\n\n\nQ1. What is the distribution of IMDB_Rating? If we split/colour by movie Genere?\n\n\n\n\n\n\n\n\n\n(a) IMDB Ratings Histogram\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Genere\n\n\n\n\n\n\nFigure 3: Netflix Data Histograms\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. Are IMDB_Rating affected by the number of Seasons or Episodes?\n\n\n\n\n\n\n\n\n\n(a) Reformatting “Seasons”\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Seasons\n\n\n\n\n\n\nFigure 4: Plotting with Seasons\n\n\nWe first need to reformat the Seasons variable from N to C in the data file view. This converts it to Qual. Then we split the IMDB histogram by this new variable.\n\n\n\n What is the Story Here?\nMost movies have decent IMDB scores; the distribution is left-skewed. Some of course have been trashed!! Splitting IMDBRating by Genere is not too illuminating…\nNot much wisdom to be gleaned either from splitting IMDBRating by Seasons…"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "title": "\n Quantity",
    "section": "\n Dataset: the Old Faithful geyser in the USA",
    "text": "Dataset: the Old Faithful geyser in the USA\nHere is a dataset about the eruption durations, and wait times between eruptions of the Old Faithful geyser in Yellowstone National Park, USA.\nDownload this data to your machine and import it into Orange.\n Download the Geyser Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 5: Old Faithful Data Table\n\n\nFigure 5 states that we have 272 data points, and three variables. All variables are Quantitative!\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\neruptions: (dbl Duration Times of Eruptions\n\nwaiting: (dbl) Waiting Times between Eruptions\n\ndensity: (dbl) (Ignore this for now)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\nNo Qual variables!!\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. How are eruptions (durations) and waiting (times) distributed?\n\n\n\n\n\n\n\n\n\n(a) Eruption Durations Histogram\n\n\n\n\n\n\n\n\n\n(b) Waiting Times Histogram\n\n\n\n\n\n\nFigure 6: Old Faithful Data Histograms\n\n\n\n\n\n What is the Story Here?\n\nBoth durations have a “double-humped” distribution…\nThere are therefore two distinct ranges for both durations.\nAre there two different mechanisms at work in the geyser, that randomly kick in?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#your-turn",
    "title": "\n Quantity",
    "section": "\n Your Turn",
    "text": "Your Turn\nTry your hand at these datasets. Look at the data table, state the data dictionary, contemplate a few Research Questions and answer them with graphs in Orange!\n\n\n\n\n\n\nNoteAirbnb Price Data on the French Riviera\n\n\n\n\n\n Airbnb data\n\n\n\n\n\n\n\n\n\n\nNoteWage and Education Data from Canada\n\n\n\n Download the Wages/Education Dataset \n\n\n\n\n\n\n\n\nNoteTime taken to Open or Close Packages\n\n\n\nSome HCD peasants tested Elderly people, some with and some without hand pain, and observed how long they took to open or close typical packages for milk, cheese, bottles etc.\n Download the Package Opening Times xlsx \n Download the Package Closing Times xlsx \n\n\n\n\n\n\n\n\nNote\n\n\n\nOrange can handle xlsx files directly. Try! How might you disregard the different package types and concentrate on “Opening/Closing Times” vs “Hand Pain or no Hand Pain”?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#wait-but-why",
    "title": "\n Quantity",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nHistograms are used to study the distribution of one or a few Quant variables.\n\nChecking the distribution of your variables one by one is probably the first task you should do when you get a new dataset.\n\nIt delivers a good quantity of information about spread, how frequent the observations are, and if there are some outlandish ones.\n\nComparing histograms side-by-side helps to provide insight about whether a Quant measurement varies with situation (a Qual variable). We will see this properly in a statistical way soon.\n\n\n Pareto, Power Laws, and Fat Tailed Distributions\nCity Populations, Sales across product categories, Salaries, Instagram connections, number of customers vs Companies, net worth / valuation of Companies, extreme events on stock markets….all of these could have highly skewed distributions. In such a case, the standard statistics of mean/median/sd may not convey too much information. With such distributions, one additional observation on say net worth, like say Mr Gates’, will change these measures completely.\nSince very large observations are indeed possible, if not highly probable, one needs to look at the result of such an observation and its impact on a situation rather than its (mere) probability. Classical statistical measures and analysis cannot apply with long-tailed distributions. More on this later when we discuss Statistical Inference, but for now, here is a video that talks in detail about fat-tailed distributions, and how one should use them and get used to them:\n\nSeveral distribution shapes exist, here is an illustration of the 6 most common ones:\n\n\n\n\n\n\n\n\nWhat insights could you develop based on these distribution shapes?\n\n\nBimodal: Maybe two different systems or phenomena or regimes under which the data unfolds. Like our geyser above. Or a machine that works differently when cold and when hot. Intermittent faulty behaviour…\n\n\nComb: Some specific Observations occur predominantly, in an otherwise even spread or observations. In a survey many respondents round off numbers to nearest 100 or 1000. Check the distribution of carat values for this diamonds dataset which are suspiciously integer numbers in too many cases.\n\n\nEdge Peak: Could even be a data entry artifact!! All unknown / unrecorded observations are recorded as \\(999\\) !!🙀\n\n\nNormal: Just what it says! Course Marks in a Univ cohort…\n\n\nSkewed: Income, or friends count in a set of people. Do UI/UX peasants have more followers on Insta than say CAP people?\n\n\nUniform: The World is not flat. Anything can happen within a range. But not much happens outside! Sharp limits…\n\nIn your Design-Project-related research, you will collect data from or about your target audience. The Quantitative parts of that data may obtain with any of these distributions. Inspecting these may give you an insight into the population of your target audience, something that may likely be true, a hunch, which you could verify and convert into …design opportunity."
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-Quantity/index.html#readings",
    "href": "content/courses/NoCode/Modules/22-Quantity/index.html#readings",
    "title": "\n Quantity",
    "section": "\n Readings",
    "text": "Readings\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\nhttps://www.data-to-viz.com/graph/histogram.html"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html",
    "title": "\n Counts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nNone\nBar Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#what-graphs-will-we-see-today",
    "title": "\n Counts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nNone\nBar Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Counts",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#inspiration",
    "title": "\n Counts",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Capital Cities\n\n\nHow much does the (financial) capital of a country contribute to its GDP? Which would be India’s city? What would be the reduction in percentage?\nAnd these Germans are crazy.(Toc, toc, toc.toc!)"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#how-do-these-charts-work",
    "title": "\n Counts",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBar are used to show “counts” and “tallies” with respect to Qual variables. For instance, in a survey, how many people vs Gender? In a Target Audience survey on Weekly Consumption, how many low, medium, or high expenditure people?\nEach Qual variable potentially has many levels as we saw in the Nature of Data. For instance, in the above example on Weekly Expenditure, low, medium and high were levels for the Qual variable Expenditure. Bar charts perform internal counts for each level of the Qual variable under consideration. The Bar Plot is then a set of disjoint bars representing these counts; see the icon above, and then that for histograms!! The X-axis is the set of levels in the Qual variable, and the Y-axis represents the counts for each level.\n\n\n\n\n\n\nNoteBar Charts and Column Charts\n\n\n\nAnd Column charts just plot numbers over categories. No internal counting. As you can see in the Figure 1 above.\nThough in many places, these two names are used interchangeably! But be aware of what the tool may be doing!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#plotting-a-bar-chart",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#plotting-a-bar-chart",
    "title": "\n Counts",
    "section": "\n Plotting a Bar Chart",
    "text": "Plotting a Bar Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe Bar Plot widget in Orange is described here. https://orangedatamining.com/widget-catalog/visualize/barplot/\n Download the Banned Books data \nAnd download the Bar Chart workflow file for this data:\n Download the Bar Chart Workflow \n\n\n\n\n\nhttps://academy.datawrapper.de/category/74-bar-charts"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#dataset-banned-books-in-the-usa",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#dataset-banned-books-in-the-usa",
    "title": "\n Counts",
    "section": "\n Dataset: Banned Books in the USA",
    "text": "Dataset: Banned Books in the USA\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\nDownload this data to your machine and use it in Orange.\n Download the Banned Books data \n\n Examine the Data\n\n\n\n\n\nFigure 2: Banned Books Data Table\n\n\n\n\n\n\n\nFigure 3: Banned Books Data Summary\n\n\nFigure 2 states that we have 1586 rows, 7 columns. So 1586 banned books are on this list! 🙀 🙀 🙀\nThe Figure 3 already has a thumbnail-like bar chart. We will still make a “proper” one with the appropriate widget.\n\n\n\n\n\n\nWarning\n\n\n\nIn the workflow below, note how it is still the Distributions widget that gives the Bar Chart. This is unfortunate, since we have been at pains to state how a Bar Chart and the Histogram deal with different types of variables (Qual and Quant respectively). Just one of those things we need to get used to!!\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nDate of Challenge: Date the book was (selected to be?) banned\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nAuthor: (text) Meta Data. Can be treated as Qual\n\nTitle: (text) Meta Data. Can be treated as Qual\n\nState: (text) Qual factor\n\nDistrict: (text) Qual factor\n\nType of Ban: (text) Qual factor\n\nOrigin of Challenge: (text) Who requested the Ban?\n\nHow many levels in each?? Find out in Orange!!\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. Which is the US state that bans the most? Which state is least involved in banning books? What can you say of the “geography of book banning” based on your understanding of the US of A? 😆\n\n\n\n\n\nFigure 4: Banned Books Count by State\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. Create Bar charts of the count of banned books by Reason for Banning!!\nTry!!\n\n\n\n What is the Story Here?\n\n\nFigure 4 says that Texas is the worst at book banning!\nTexas, Florida, Oklahoma, Kansas, Indiana,..are next in line\nIs there a “Bible Belt” story here?\n\n\n\n\n\n\nFigure 5: Bible Belt\n\n\n\nAnd what, Californians are too busy making money to care about book-banning!!! The state does not even show up in the chart! 🤣\nWhat does the second bar chart say?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#your-turn",
    "title": "\n Counts",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nAiRbnb Price Data on the French Riviera:\n\n\n\n AiRbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n\nFertility: This rather large and interesting Fertility related dataset from https://vincentarelbundock.github.io/Rdatasets/csv/AER/Fertility.csv\n\n\n Download the Fertility Data"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#wait-but-why",
    "title": "\n Counts",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nAlways count your chickens count your data before you model or infer!\nCounts first give you an absolute sense of how much data you have.\nCounts by different Qual variables give you a sense of the combinations you have in your data: \\((Male/Female) * (Income-Status) * (Old/Young) * (Urban/Rural)\\) (Say 2 * 3 * 2 * 2 = 24 combinations of data)\nCounts then give an idea whether your data is lop-sided: do you have too many observations of one category(level) and too few of another category(level) in a given Qual variable?\nBalance is important in order to draw decent inferences\nAnd for ML algorithms, to train them properly.\nSince the X-axis in bar charts is Qualitative (the bars don’t touch, remember!) it is possible to sort the bars at will, based on the levels within the Qualitative variables. See the approx Zipf’s Law distribution for the English alphabet below:\n\n\n\n\n\n\nFigure 6: Zipf’s Law\n\n\nIn Figure 6, the letters of the alphabet are “levels” within a Qualitative variable, and these levels have been sorted based on the frequency or count!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Counts/index.html#readings",
    "href": "content/courses/NoCode/Modules/20-Counts/index.html#readings",
    "title": "\n Counts",
    "section": "\n Readings",
    "text": "Readings"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html",
    "title": "🕶 Happy Data are all Alike",
    "section": "",
    "text": "Some of the very basic and commonly used plots for data are:\n- Bar and Column Charts\n- Histograms and Frequency Distributions\n- Scatter Plots (if there is more than one quant variable) and\n- 2D Hexbins Plots and 2D Frequency Distributions (horrors!!)\n\nHistograms are best to show the distribution of raw quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins.\nAlthough histograms may look similar to bar/column charts, the two are different. First, histograms show continuous data, and usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc.\nBy contrast, bar/column charts show categorical data, such as the number of apples, bananas, carrots, etc. Second, histograms do not usually show spaces between buckets because these are continuous values, while column charts show spaces to separate each category."
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#what-graphs-will-we-see-today",
    "title": "🕶 Happy Data are all Alike",
    "section": "",
    "text": "Some of the very basic and commonly used plots for data are:\n- Bar and Column Charts\n- Histograms and Frequency Distributions\n- Scatter Plots (if there is more than one quant variable) and\n- 2D Hexbins Plots and 2D Frequency Distributions (horrors!!)\n\nHistograms are best to show the distribution of raw quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins.\nAlthough histograms may look similar to bar/column charts, the two are different. First, histograms show continuous data, and usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc.\nBy contrast, bar/column charts show categorical data, such as the number of apples, bananas, carrots, etc. Second, histograms do not usually show spaces between buckets because these are continuous values, while column charts show spaces to separate each category."
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#bar-and-column-charts-in-rawgraphs",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#bar-and-column-charts-in-rawgraphs",
    "title": "🕶 Happy Data are all Alike",
    "section": "Bar and Column Charts in RAWgraphs",
    "text": "Bar and Column Charts in RAWgraphs"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#dataset-netflix-original-series",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#dataset-netflix-original-series",
    "title": "🕶 Happy Data are all Alike",
    "section": "\n Dataset: Netflix Original Series",
    "text": "Dataset: Netflix Original Series\nHere is a look at this data. It is readily available as on the in-bult datasets at RAWgraphs.\n\n Examine the Data\n\n\n\n  \n\n\n\n\n\n\nname\nclass\nlevels\nn\nmissing\ndistribution\n\n\n\nGenere\ncharacter\n10\n109\n0\nFamily Animation (29.4%) ...\n\n\nTitle\ncharacter\n109\n109\n0\n0.03 (0.9%), 13 Reasons Why (0.9%) ...\n\n\nSubgenre\ncharacter\n42\n109\n0\nAnimation (30.3%), Comedy (7.3%) ...\n\n\nStatus\ncharacter\n4\n106\n3\nPending (41.5%), Renewed (34.9%) ...\n\n\n\n\n\n\n\n\n\nname\nclass\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\nPremiere_Year\nnumeric\n2013\n2015\n2016\n2016\n2017\n2015.697248\n1.101384\n109\n0\n\n\nSeasons\nnumeric\n1\n1\n1\n2\n5\n1.642202\n1.041037\n109\n0\n\n\nEpisodes\nnumeric\n3\n8\n13\n21\n90\n17.871560\n15.671454\n109\n0\n\n\nIMDB_Rating\nnumeric\n0\n70\n77\n84\n96\n73.559633\n16.966978\n109\n0\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nPremiere_Year:  Year the movie premiered\n\n\nSeasons:  No. of Seasons\n\n\nEpisodes:  No. of Episodes\n\n\nIMDB_Rating:  IMDB Rating!!\n\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nGenere:  10 types of Genres\n\n\nTitle:  109 titles\n\n\nSubgenre:  42 types of sub-Genres\n\n\nStatus:  4 levels, status on Netflix\n\n\n\n\nResearch Questions\nLet’s try a few questions and see if they are answerable with Bar Charts. Recall that Bar Charts show counts of Qualitative variables!\nQ1. How many movies of each Genere? Sort them by Genere! Q2. Which Genere has the highest average IMDB_Rating? Some grouping + aggregating needed here!\n\n Plotting a Bar Chart\nLet us create this figure:\nWhat is the Story Here?\n\nTalk Shows?? What??"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#dataset-banned-books-in-the-usa",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#dataset-banned-books-in-the-usa",
    "title": "🕶 Happy Data are all Alike",
    "section": "\n Dataset: Banned Books in the USA",
    "text": "Dataset: Banned Books in the USA\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n\n\n\n  \n\n\n\nDownload this data to your machine and use it on RAWGraphs.\n\n\n Banned Books data\n\n\nWhat is the Story Here?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#frequency-distributions",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#frequency-distributions",
    "title": "🕶 Happy Data are all Alike",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#d-frequency-distributions-and-hexbin-plots",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#d-frequency-distributions-and-hexbin-plots",
    "title": "🕶 Happy Data are all Alike",
    "section": "2D Frequency Distributions and Hexbin plots",
    "text": "2D Frequency Distributions and Hexbin plots\nWhat is the Story here?\nTBD"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#an-example-frequency-density",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#an-example-frequency-density",
    "title": "🕶 Happy Data are all Alike",
    "section": "An Example: Frequency Density",
    "text": "An Example: Frequency Density\nHow does this work?\nLet us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.\n\n\n\nHow many are rich and how many are poor? from Gapminder on Vimeo.\n\n\n\n\n\nHow could you explore?\nTBD. Add 2D contour plots and link up to hexbin plots.\nWhat is the Story here?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#your-turn",
    "title": "🕶 Happy Data are all Alike",
    "section": "Your Turn",
    "text": "Your Turn\n\nRbnb Price Data on the French Riviera:\n\n\n\n Rbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n(Try a Scatter Plot too, since we have two Quant variables)\n\nRbnb Price Data on the French Riviera:\n\n\n\n Rbnb data\n\n\n\nIndia\nOld Faithful Data\nIncome data\nDiamonds Data from R\ncalmcode.io dataset"
  },
  {
    "objectID": "content/courses/NoCode/Modules/24-Densities/index.html#fun-stuff",
    "href": "content/courses/NoCode/Modules/24-Densities/index.html#fun-stuff",
    "title": "🕶 Happy Data are all Alike",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html",
    "title": "\n Flow",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nSankey Plot\n\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#what-graphs-will-we-see-today",
    "title": "\n Flow",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nSankey Plot\n\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#inspiration",
    "title": "\n Flow",
    "section": "\n Inspiration",
    "text": "Inspiration"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#how-do-these-charts-work",
    "title": "\n Flow",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nSometimes Qual data can itself vary over, or depend upon, over a bunch of independent Qual data categories. For instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nAt such times, the Mosaic Chart becomes a limited and unwieldy, and we need to turn to a new visualization.\nAs can be surmised, the independent categories can be interpreted both as time ( e.g semesters / cycles / years) and space (teams / courses / departments). And we can chart another Quant or Qual variable that moves across levels of the first chosen Qual variable.\n\n\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students going through multiple courses during a semester of study.\nHere is an example of a Sankey Diagram for the Titanic dataset:\n\n\n\n\n\n\n\nFigure 1: Titanic Sankey Plot\n\n\n\n\nIt is seen from Figure 1 that the x-axis has Qual variables stages shown as “pillars” and these are split into nodes based on the levels for each Qual variable stage respectively. Flows with variable thickness connect one node at one stage to another node and another stage.\n\n\n\n\n\n\nNoteSankey, Parallel sets, and Alluvial Charts\n\n\n\nHere is what Thomas Lin Pedersen says:\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables.\nIf the variables have an intrinsic order the representation can be thought of as a Sankey Diagram.\nIf each variable is a point in time it will resemble an Alluvial diagram."
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#creating-sankey-plots",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#creating-sankey-plots",
    "title": "\n Flow",
    "section": "\n Creating Sankey Plots",
    "text": "Creating Sankey Plots\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nIt does not seem possible to create an Alluvial Diagram in Orange. 😢\n\n\n\n Download the RAWgraphs Alluvial Tutorial File \nDownload this file and upload to https://app.rawgraphs.io/.\nLet us however examine this data in Orange:\n Download the Hate Crimes dataset \n\n\n\n\n\nFigure 2: Hate Crimes in NY 2020\n\n\nFrom the Figure 2, We see that the data is all Qualitative, except for Age. The Precinct, while apparently an integer, is really a Qual variable! Why?\nAnd here is the Sankey Diagram:\n\n\n\n\n\nFigure 3: Hate Crime Sankey Diagram\n\n\nIn the Figure 3, we have three Qual variables along the x-axis: Gender, Race and Bias-Motivation. The chart counts the crime episodes at each stage/node and portrays them as flows with varying thickness leading to the next stage/node.\nWhat trends do you detect from this diagram?\n\n\nIt does not seem possible to create an Alluvial Diagram in DataWrapper."
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#dataset-course-allocations",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#dataset-course-allocations",
    "title": "\n Flow",
    "section": "\n Dataset: Course Allocations",
    "text": "Dataset: Course Allocations\nLet us try one more dataset:\n Download the FSP Course Allocation data \n\n Examine the Data\nLet us still import into Orange and see the data anyway!\n\n\n\n\n\nFigure 4: Course Allocation Data\n\n\nFrom Figure 4, we see that we have 300 students and their course allocations over one Foundation year at SMI. (The data is anonymized but accurate; no staff or students were harmed in the collection of this data, which is a pity of course)!\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\nNone!!\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nMajor(chr): Student Major\nAll other columns: Courses they were allocated to during the course of the year.\nMM = Mark Making; DM = Digital Making; FM = Form Making;\n\nDTT = Digital Thinking Tools; VTT = Visual Thinking Tools;\n\nO&C = Order and Chaos; P&I = Play and Invent; S&P = Space and Place; C&P = Communities and Practices; F&S = Form and Structure; B&C = Body and Context; L&L = Layers and Lenses; E&C = Everything’s Connected; P&P = Patterns and Paradigms; F&F = Faculty and Fools (Oh all right, all right.)\n\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Sankey/Alluvial Plots.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ1. Do all DMA/Film/CAP students take at least one B&C course?\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ2. Do all IADP, HCD, and PSD students take one P&I course?\n\n\n\n What is the Story Here?\nWrite in!!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#your-turn",
    "title": "\n Flow",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package from R, are two datasets, majors and vaccinations. Plot alluvial charts for both of these, and write their stories.\n\n\n\n Majors data\n\n\n\n\n\n Vaccinations data\n\n\n\n\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module.\nTry this from Vincent Arel-Bundock’s website: Cybersecurity breaches reported to the US Department of Health and Human Services. The dataset is downloadable here CSV. NOTE: data may require some cleaning beforehand in Excel!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#wait-but-why",
    "title": "\n Flow",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nMosaic Charts gave us a view of counts of data across level-combinations of Qual variables.\nSankey/Alluvial Charts give us a sense of flow: how did different observations flow from one Qual variable to another? So in a sense, a Sankey/Alluvial is a concatenation of several mosaics: at each axis pillar in a Sankey/Alluvial, we can flatten that out into a Mosaic.\nThis is very valuable if these Qual variables and their levels have a natural sequence. E.g. Choices made in purchases, Attitudes over time and situation, Affiliations and Friendships over time etc.\nThe sequence may even be conceptualized as a consequence, provided you have adequate insight into the situations involved.\nYou get a sense of the sub-populations in each combo of Qual variables and can decide what to do about both plenitude and rarity!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-Flows/index.html#readings",
    "href": "content/courses/NoCode/Modules/40-Flows/index.html#readings",
    "title": "\n Flow",
    "section": "\n Readings",
    "text": "Readings\n\nA good pictorial introduction to different parts of a Sankey Chart. https://github.com/davidsjoberg/ggsankey\nMinard’s famous Alluvial Plot of Napoleon’s Invasion of Russia. https://www.andrewheiss.com/blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/?utm_content=buffer70e4b&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\nMinard Revisited. https://www.masswerk.at/nowgobang/2020/minard-revisited\n100+ years of the Titanic data. https://www.datavis.ca/papers/titanic/"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html",
    "title": "\n Surveys",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nLikert Plots"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#what-graphs-will-we-see-today",
    "title": "\n Surveys",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nLikert Plots"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Surveys",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#how-do-these-charts-work",
    "title": "\n Surveys",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn many design project situations, we perform say target audience surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example.\nSome examples of Likert Scales are shown below.\n\n\n\n\n\nFigure 1: Likert Scale Questionnaire Samples\n\n\nAs seen, we can use Likert Scale based questionnaire for a variety of aspects in our survey instruments.\n\n\n\n\n\n\nNoteVariable Labels and Value Labels\n\n\n\nVariable label is human readable description of the variable. R supports rather long variable names and these names can contain even spaces and punctuation but short variables names make coding easier. Variable label can give a nice, long description of variable. With this description it is easier to remember what those variable names refer to.Value labels are similar to variable labels, but value labels are descriptions of the values a variable can take. Labeling values means we don’t have to remember if 1=Extremely poor and 7=Excellent or vice-versa. We can easily get dataset description and variables summary with info function."
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#plotting-likert-charts",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#plotting-likert-charts",
    "title": "\n Surveys",
    "section": "\n Plotting Likert Charts",
    "text": "Plotting Likert Charts\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe description of the Orange widget for mosaic charts is here.\nLet us take a very sadly famous data set (no, not iris again 🙀), but titanic and examine it in Orange.\n\n\nNot a mosaic plot, but a Matrix Plot.\n\nDownload this RAWGraphs workflow file and import there and see.\n\n\nDoes not seem to have a mosaic diagram capability."
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-caregivers",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-caregivers",
    "title": "\n Surveys",
    "section": "\n Dataset: CareGivers",
    "text": "Dataset: CareGivers\nHere is another example of Likert data from the healthcare industry.\nefc is a German data set from a European study titled EUROFAM study, on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e. caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. The research questions in this EUROFAM study were:\n\n\nTo what extent do family carers of older people use support services or receive financial allowances across Europe? What kind of supports and allowances do they mainly use?\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\nIn order to improve support provision, what can be understood about the service characteristics considered crucial by carers, and how far are these needs met? and,\nWhich channels or actors can provide the greatest help in underpinning future policy efforts to improve access to services/supports?\n\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n```{r}\n#| label: efc_data\n#| layout-nrow: 2\n#| column: body-outset-right\ndata(efc, package = \"sjPlot\")\n\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  head(20)\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  str()\n```\n\n\n\n\n  \n\n\n\n\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. The labels are actually ( and perhaps usually ) the questions in the survey.\n\n Examine the Data\n\n\n\n\n\n\n\n\n\n(a) Titanic Data Table\n\n\n\n\n\n\n\n\n\n(b) Titanic Data Table\n\n\n\n\n\n\nFigure 2\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nNone.\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nsurvived: (chr) yes or no\n\nstatus: (chr) Class of Travel, else “crew”\n\nage: (chr) Adult, Child\n\nsex: (chr) Male / Female.\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1. What is the dependence of survived upon sex?\n\n\n\n\n\n\n\n\nFigure 3: Titanic Mosaic Chart\n\n\nNote the huge imbalance in survived with sex: men have clearly perished in larger numbers than women. Which is why the colouring by the Pearson Residuals show large positive residuals for men who died, and large negative residuals for women who died\n\n\n\n\n\n\n\n\nNoteQ.2. How does survived depend upon status?\n\n\n\n\n\n\n\n\nFigure 4: Titanic Mosaic Chart\n\n\nCrew has seen deaths in large numbers, as seen by the large negative residual for crew-survivals. First Class passengers have had speedy access to the boats and have survived in larger proportions than say second or third class. There is a large positive residual for first-class survivals.\n\n\n\n What is the Story Here?\nIn Figure 4, we have plotted sex vs status, and coloured by whether the (subset of) people survived or not. (Red is YES, Blue is NO!). As can be seen the areas are very dissimilar across both variables. More deaths occurred among the crew than among the passengers; and first class passengers have survived more than third class passengers. And of course, more men died than women.\nSo we can state that:\n\n\nStatus and Survived are not un-correlated\n\nSex and Survived are not un-correlated\nDoes ticking the Compare with Total box in Orange help to arrive at this inference? How so?\n\nIt remains to figure out just how serious this correlation is.\n\n\n\n\n\n\nImportantActual and “Expected” Counts\n\n\n\nThe mosaic chart is a visualization of the obtained count on which the tile is constructed.\nIt is also possible to compute a per-cell expected count, if the categorical variables are assumed independent, that is, not correlated. This is the NULL Hypothesis. The test for whether they are independent or not, as any inferential test, is based on comparing the observed counts with these expected counts under the null hypothesis. So, what might the expected frequency of a cell be in cross-tabulation table for cell \\(i,j\\) given no relationship between the variables of interest?\nRepresent the sum of row \\(i\\) with \\(n_{+i}\\), the sum of column \\(j\\) with \\(n_{j+}\\), and the grand total of all the observations with \\(n\\). And independence of variables means that their joint probability is the product of their probabilities. Therefore, the Expected Cell Frequency/Count is given by:\n\\[\n\\begin{array}{lcl} ~Expected~Count~ e_{i,j}  &=& \\frac{rowSum ~\\times~colSum}{n}\\\\\n&=& \\frac{(n_{+i})(n_{j+})}{n}\\\\\n\\end{array}\n\\]\nThe comparison of what occurred to what is expected is based on their difference, scaled by the square root of the expected, the Pearson Residual:\n\\[\n\\begin{array}{lcl} r_{i,j} &=& \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\\\\\n&=& \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\end{array}\n\\]\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows.\n\n\n\n\n\n\n\n\nNote χ2 For the Cat-egorically Curious\n\n\n\nFor the intrepid and insatiably curious, there is an intuitive explanation, and some hand-calculations and walk-through of the Contingency table and the χ2-test here."
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-who-does-the-housework",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-who-does-the-housework",
    "title": "\n Surveys",
    "section": "\n Dataset: Who Does the Housework?",
    "text": "Dataset: Who Does the Housework?\nLet us take this dataset on household tasks, and who does them. Download this dataset and import in into your Mosaic Chart workflow.\n\n Examine the Data\n\n\n\n\n\nFigure 5: Household Tasks Distribution Raw Data\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nFreq: (int) No of times a task was carried (in different ways)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nWho: (chr) Who carried out the task?\n\nTask: (chr) Task? Which task? Can’t you see I’m tired?\n\n\n\n\n\n\n\n\n\n\nFigure 6: Household Tasks Distribution Raw Data\n\n\nThis data looks fine all right, but the mosaic plot looks bewildering and of course is wrong. The reason for this is that the basic HouseTasks.csv data is pre-aggregated: we have a neat column of counts already in the Freq data. And why is this a problem? Orange expects data to be purely categorical and does it own counting, and is not able to sensibly use this Freq column. Orange simply counts categories, which are of course utterly symmetric and unique.\n\n\n\n\n\n\nNoteStat Figures and Stats\n\n\n\nMost, if not all, statistical graphs do some internal computation. For instance the bar chart performs counts vs Qual variables; a Histogram both bins the Quant variable, and counts for entries in each bin. This is a good thing, people, but it does mean that the data needs to be in specific format before using it for plots.\n\n\nSo now what? We need to (wait for it):\n\n\nuncount the data 🙀\nTake each combination of Quals Who and Task\n\nRepeat ( i.e copy-paste) that combo line as many times as the value in Freq\n\n(optionally) Deleting the Freq column, or at least not using it further\n\nAll this is (to the best of my ability) not possible in any of these trifling tools that we are using here, and can be done in a jiffy in R or Python. Didn’t I tell you coding was far far far far simpler? Peasants.\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1 Is there correlation between Who carries out the task, and the Task itself?\n\n\n\n\n\n\n\n\nFigure 7: Household Tasks Mosaic\n\n\n\n\n\n What is the Story Here?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#your-turn",
    "title": "\n Surveys",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\nClothing and Intelligence Rating of Children!! Are well-dressed actually smarter? Is that the exact reverse with SMI faculty?\n\n\nPre-marital Sex and Divorce"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#wait-but-why",
    "title": "\n Surveys",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-edible-insects",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#dataset-edible-insects",
    "title": "\n Surveys",
    "section": "Dataset: Edible Insects",
    "text": "Dataset: Edible Insects\nGBIF.org (26 April 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.texc32\n\nShelomi. (2022). Dataset for: Factors Affecting Willingness and Future Intention to Eat Insects in Students of an Edible Insect Course [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7379294"
  },
  {
    "objectID": "content/courses/NoCode/Modules/140-Surveys/index.html#references",
    "href": "content/courses/NoCode/Modules/140-Surveys/index.html#references",
    "title": "\n Surveys",
    "section": "References",
    "text": "References\n\nPiping Hot Data: Leveraging Labelled Data in R, https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/&gt;\nDataset: Edible Insects\n\nGBIF.org (26 April 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.texc32\n\nShelomi. (2022). Dataset for: Factors Affecting Willingness and Future Intention to Eat Insects in Students of an Edible Insect Course [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7379294"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html",
    "title": "\n Proportions",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nPies, and Mosaic Charts"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#what-graphs-will-we-see-today",
    "title": "\n Proportions",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nPies, and Mosaic Charts"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Proportions",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#inspiration",
    "title": "\n Proportions",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Obesity across the World\n\n\n\n\n\n\n\n\n\n(b) Covid Deaths https://datatopics.worldbank.org/sdgatlas/goal-3-good-health-and-well-being?lang=en\n\n\n\n\n\n\nFigure 1: Depicting Proportions\n\n\nFrom Figure 1 (a), it is seen that Egypt, Qatar, and the United States are the only countries with a population greater than 1 million on this list. Poor food habits are once again a factor, with some cultural differences. In Egypt, high food inflation has pushed residents to low-cost high-calorie meals. To combat food insecurity, the government subsidizes bread, wheat flour, sugar and cooking oil, many of which are the ingredients linked to weight gain. In Qatar, a country with one of the highest per capita GDPs in the world, a genetic predisposition towards obesity and sedentary lifestyles worsen the impact of rich diets. And in the U.S., bigger portions are one of the many reasons cited for rampant adult and child obesity. For example, Americans ate 20% more calories in the year 2000 than they did in 1983. They consume 195 lbs of meat annually compared to 138 lbs in 1953. And their grain intake has increased 45% since 1970.\nIt’s worth noting however that this dataset is based on BMI values, which do not fully account for body types with larger bone and muscle mass.\nFrom Figure 1 (b), according to World Bank, six countries (India, Russia, Indonesia, United States, Brazil, and Mexico) accounted for over 60 percent of the total additional deaths in the first two years of the pandemic."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#how-do-these-charts-work",
    "title": "\n Proportions",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nWe saw with Bar Charts that when we deal with single Qual variables, we perform counts for each level of the variable. For a single Qual variable, even with multiple levels ( e.g. Education Status: High school, College, Post-Graduate, PhD), we can count the observations as with Bar Charts and plot Pies.\nWhat if there are two Quals? Or even more?\nThe answer is to take them pair-wise, make all combinations of levels for both and calculate counts for these. This is called a Contingency Table. Then we plot that table. We’ll see."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-pies",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-pies",
    "title": "\n Proportions",
    "section": "\n Plotting Pies",
    "text": "Plotting Pies\nLet us deal with single Qual variables first.\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us the same dataset as is used in the RAWgraphs tutorial (to follow):\n Download the GDP dataset \nCan you find the Pie Chart Widget in Orange? Let us do this “live” in class and test our new-found Orange skills!\n\n\n\nDownload this RAWgraphs project workflow and open it in RAWgraphs.\n Download RAWgraphs Pie Chart Workflow \n\n\n\n\n\n\nNote\n\n\n\nNote the shape of data here: it is wide!\n\n\n\n\nhttps://academy.datawrapper.de/article/24-how-to-create-a-pie-chart\n\n\n\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in design and business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-nested-proportions",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-nested-proportions",
    "title": "\n Proportions",
    "section": "\n Plotting Nested Proportions",
    "text": "Plotting Nested Proportions\nWhen we want to visualize proportions based on Multiple Qual variables, we are looking at what Claus Wilke calls nested proportions: groups within groups. Making counts with combinations of levels for two Qual variables gives us a data structure called a Contingency Table, which we will use to build our plot for nested proportions\n\n What is a Contingency Table?\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts.\nMore precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\nThe Contingency Table is then plotted in a chart called the Mosaic Chart. Let us develop our intuition for a Contingency Table first, and arrive at the mosaic chart."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-general-social-survey-2002",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-general-social-survey-2002",
    "title": "\n Proportions",
    "section": "\n Dataset: General Social Survey 2002",
    "text": "Dataset: General Social Survey 2002\nLet us first construct a Contingency Table from this dataset, and then plot the mosaic chart for it.\n\n\n GSS data\n\n\nHere is the Orange workflow:\n Download the Orange Mosaic Chart Workflow \n\n Examine the Data\n\n\n\n\n\nFigure 2: GSS2002 Data Table\n\n\n\n\n\n\n\n\n\n\n\n(a) GSS2002 Data Summary #1\n\n\n\n\n\n\n\n\n\n(b) GSS2002 Data Summary #2\n\n\n\n\n\n\nFigure 3: General Social Survey 2002 Dataset\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nID is the only Quant data variable!\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n“ID” “Region” “Gender” “Race”\n“Education” “Marital” “Religion” “Happy”\n“Income” “PolParty” “Politics” “Marijuana”\n“DeathPenalty” “OwnGun” “GunLaw” “SpendMilitary” “SpendEduc” “SpendEnv” “SpendSci” “Pres00”\n“Postlife”\nare all Qual variables! Let us choose just two Qual variables from this dataset, DeathPenalty and Education.\n\n\nDeathPenalty: (chr) Opinion as to whether they favour or oppose the death penalty\n\nEducation: (chr) Education among respondents, 5 levels (Left HS, HS, Jr Col, Bachelors, Graduate).\n\n\n\nA Contingency table with these two Qual variables looks like Figure 4:\n\n\n\n\n\n    \n\n      \n\nDeathPenalty\n                Left HS\n                HS\n                Jr Col\n                Bachelors\n                Graduate\n                Sum\n              \n\n\nFavor\n                  117\n                  511\n                  71\n                  135\n                  64\n                  898\n                \n\nOppose\n                  72\n                  200\n                  16\n                  71\n                  50\n                  409\n                \n\nSum\n                  189\n                  711\n                  87\n                  206\n                  114\n                  1307\n                \n\n\n\n\n\n\nFigure 4: Contingency Table\n\n\n\nHow was this computed?\nSo \\(117\\) is the number of people who Left HS and Favor the death penalty, and \\(71\\) is the count for Bachelors who Oppose the death penalty. And so on.\nNow then, how does one plot a set of data that looks like this, a matrix? No column is a single variable, nor is each row a single observation, which is what we understand with the idea of tidy data.\nThe answer is provided in the very shape of the data: we plot this as a set of tiles, where \\[ \\pmb{area~of~tile \\sim count} \\] In this way we recursively partition off a (usually) square area into vertical and horizontal pieces whose area is proportional to the count at a specific combination of levels of the two Qual variables. So we might follow the process as shown below:\n\nTake the bottom row of per-column totals and create vertical rectangles with these widths\n\nTake the individual counts in the rows and partition each rectangle based in the counts in these rows. \n\nLet us do this step by step.\n\n Research Questions\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ1. Are Education and DeathPenalty associated?\nLet us plot the mosaic chart in two steps: we now choose Qual variables Education and DeathPenalty, in that order to plot the mosaic chart. Here are the two steps in the recursion:\n\n\n\n\n\n\n\n\n\n(a) GSS Mosaic Chart Step #1\n\n\n\n\n\n\n\n\n\n(b) GSS Mosaic Chart Step #2\n\n\n\n\n\n\nFigure 5: Mosaic Chart for GSS Data\n\n\nThe first split shows the various levels of Education and their counts as widths. Order is alphabetical! This splitting corresponds to the bottom ROW of the Figure 4. HS is clearly the largest subgroup in Education.\nIn the second step, the columns from Figure 5 (a) are sliced horizontally into tiles, in proportion to the number of people in each Education category/level who support/do not support DeathPenalty. This is done in proportion to all the entries in each COLUMN.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the order in which we choose the variables matters, since the mosaic plot is fundamentally asymmetric. More on this in a bit.\n\n\n\n\n\n\n\n\nImportantColouring by Pearson Residuals\n\n\n\nMosaic Charts generated by Orange can be coloured based on “Pearson Residuals”. What this means is that the mosaic plot calculates what might be the “expected counts” (see below) in the Contingency Table and calculates the differences (i.e. “residuals” ) between Observed/Actual and Expected values. If the errors are negative (Obs &lt; Exp) then the tile is coloured red. And blue if the error is positive (Obs &gt; Exp).\nIn Figure 5 (b) we see that there is a small positive and a small negative residual at two locations in the mosaic chart. By and large the chart is white, showing very little association between Education and DeathPenalty. However, we should verify this using a statistical “chi-square” \\(X^2\\) test.\nMore on “expected counts” and the “chi-square” \\(X^2\\) test below."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-mosaic-charts",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#plotting-mosaic-charts",
    "title": "\n Proportions",
    "section": "\n Plotting Mosaic Charts",
    "text": "Plotting Mosaic Charts\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe description of the Orange widget for mosaic charts is here.\nLet us take a very sadly famous data set (no, not iris again 🙀), but titanic and examine it in Orange.\nWe will reuse this workflow:\n Download the Orange Mosaic Chart Workflow \n\n\nNot a mosaic plot, but a Matrix Plot.\n\nDownload this RAWGraphs workflow file and import there and see.\n Download RAWgraphs Mosaic Chart Workflow \n\n\nDoes not seem to have a mosaic diagram capability."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-titanic",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-titanic",
    "title": "\n Proportions",
    "section": "\n Dataset: Titanic",
    "text": "Dataset: Titanic\nOk, let us see if we can rescue Jack also. Here is the titanic data. Use the Datasets widget in Orange to get it.\n\n Examine the Data\n\n\n\n\n\n\n\n\n\n(a) Titanic Data Table\n\n\n\n\n\n\n\n\n\n(b) Titanic Data Summary Stats\n\n\n\n\n\n\nFigure 6: Titanic Data\n\n\nThere were 2201 passengers, as per this dataset.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nNone.\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nsurvived: (chr) yes or no\n\nstatus: (chr) Class of Travel, else “crew”\n\nage: (chr) Adult, Child\n\nsex: (chr) Male / Female.\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1. What is the dependence of survived upon sex?\n\n\n\n\n\n\n\n\nFigure 7: Titanic Mosaic Chart #1\n\n\nNote the huge imbalance in survived with sex: men have clearly perished in larger numbers than women. Which is why the colouring by the Pearson Residuals show large positive residuals for men who died, and large negative residuals for women who died.\nSo sadly Jack is far more likely to have died than Rose.\n\n\n\n\n\n\n\n\nNoteQ.2. How does survived depend upon status?\n\n\n\n\n\n\n\n\nFigure 8: Titanic Mosaic Chart #2\n\n\nCrew has seen deaths in large numbers, as seen by the large negative residual for crew-survivals. First Class passengers have had speedy access to the boats and have survived in larger proportions than say second or third class. There is a large positive residual for first-class survivals.\nRose travelled first class and Jack was third class. So again the odds are stacked against him.\n\n\n\n What is the Story Here?\nIn Figure 8, we have plotted survived vs status. As can be seen the areas are very dissimilar across both variables. More deaths occurred among the crew than among the passengers; and more first class passengers have survived than third class passengers. And from Figure 7, more men died than women.\nSo we can state that:\n\n\nStatus and Survived are not un-associated\n\nSex and Survived are not un-associated\nDoes ticking the Compare with Total box in Orange help to arrive at this inference? How so? Or does it confuse?\n\n Are these differences in proportion significant?\nIt remains to figure out just how serious this association is, or whether these differences in proportion just happenned by chance. For that we need the statistical “chi-square” \\(X^2\\) test.\n\n\n\n\n\n\nImportantActual and “Expected” Counts\n\n\n\nThe mosaic chart is a visualization of the obtained counts, based on which the tiles are constructed.\nIt is also possible to compute a per-cell expected count, if the categorical variables are assumed independent, that is, not correlated. This is the NULL Hypothesis. The test for whether they are independent or not, as any inferential test, is based on comparing the observed counts with these expected counts under the null hypothesis. So, what might the expected frequency of a cell be in cross-tabulation table for cell \\(i,j\\) given no relationship between the variables of interest?\nRepresent the sum of row \\(i\\) with \\(n_{+i}\\), the sum of column \\(j\\) with \\(n_{j+}\\), and the grand total of all the observations with \\(n\\). And independence of variables means that their joint probability is the product of their probabilities. Therefore, the Expected Cell Frequency/Count is given by:\n\\[\n\\begin{array}{lcl} ~Expected~Count~ e_{i,j}  &=& \\frac{rowSum ~\\times~colSum}{n}\\\\\n&=& \\frac{(n_{+i})(n_{j+})}{n}\\\\\n\\end{array}\n\\]\nThe comparison of what occurred to what is expected is based on their difference, scaled by the square root of the expected, the Pearson Residual:\n\\[\n\\begin{array}{lcl} r_{i,j} &=& \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\\\\\n&=& \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\end{array}\n\\]\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows.\n\n\n\n\n\n\n\n\nNote χ2-Test for the Cat-egorically Curious\n\n\n\nFor the intrepid and insatiably curious, there is an intuitive explanation, and some hand-calculations and walk-through of the Contingency table and the χ2-test here."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-who-does-the-housework",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#dataset-who-does-the-housework",
    "title": "\n Proportions",
    "section": "\n Dataset: Who Does the Housework?",
    "text": "Dataset: Who Does the Housework?\nLet us take this dataset on household tasks, and who does them. Download this dataset and import in into your Mosaic Chart workflow.\n Download the Household Tasks Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 9: Household Tasks Distribution Raw Data\n\n\n52 observations.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nFreq: (int) No of times a task was carried out by specific people\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nWho: (chr) Who carried out the task?\n\nTask: (chr) Task? Which task? Can’t you see I’m tired?\n\n\n\nLet us plot the mosaic chart:\n\n\n\n\n\n\n\nFigure 10: Household Tasks Distribution Raw Data\n\n\nThis data looks fine all right, but this mosaic plot looks bewildering, utterly dumbfounding, and is of course wrong. The reason for this is that the basic HouseTasks.csv data is pre-aggregated: we have a neat column of counts already in the Freq data. Each combination of Qual factors has exactly one count/row/observation, hence all tiles are the same size.\nAnd why is this a problem? Orange expects data to be purely categorical for the Mosaic Chart and does it own counting internally. It is not able to sensibly use this Freq column. Orange simply counts categories here, which are of course utterly symmetric, unique, and of no use. Bah!\n\n\n\n\n\n\nNoteStat Figures and Stats\n\n\n\nMost, if not all, statistical graphs do some internal computation. For instance the bar chart performs counts vs Qual variables; a Histogram both bins the Quant variable, and counts for entries in each bin. This is a good thing, peasants, but it does mean that the data needs to be in specific format before using it for plots. Cultivate that.\n\n\nSo now what? We need to (wait for it):\n\n\nuncount the data 🙀 🙀 🙀\nHow? Take each combination of Quals Who and Task\n\nRepeat (i.e. copy-paste) that combo line as many times as the value in Freq\n\n(optionally) Deleting the Freq column, or at least not using it further\n\nAll this is (to the best of my ability) not possible in any of these trifling tools that we are using here, and can be done in a jiffy in R or Python. Didn’t I tell you coding was far far far far simpler? Oh, these peasants.\nSo following this ashtavakra procedure of jumping to another tool and coming back here, good things can be somehow made to happen, and so here is the “un-aggregated” data for you:\n\n\n Household data Cleaned up for Peasants\n\n\nImport this into Orange.\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1 Is there association between Who carries out the task, and the Task itself?\n\n\n\n\n\n\n\n\nFigure 11: Household Tasks Mosaic\n\n\n\n\n\n What is the Story Here?\nThe Mosaic plot in Figure 11 is seriously coloured, showing that there are Pearson Residuals/Errors in both directions (positive and negative). The χ2-value is large (not visible here, check in Orange) and the p-value is zero. This indicates that it is very very unlikely that this data happened by chance, assuming the two Qual variables are un-related. Hence, we are likely to conclude that our assumption that they are un-related can be rejected. (Note this complex wording here. We don’t say they are related.)\nWhy is this unsurprising? Men don’t do housework, it would seem.\nIn general, if you want to spot association, look for serious amounts of colour in the mosaic chart."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#your-turn",
    "title": "\n Proportions",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteClothing and Intelligence Rating of Children!!\n\n\n\nAre well-dressed students actually smarter? Isn’t that the exact reverse with SMI faculty? Or …?\n Download the Gilby Study dataset \n\n\n\n\n\n\n\n\nNotePre-marital Sex and Divorce.\n\n\n\n Download the pre- and extra-marital sex and divorce dataset \n\n\n\n\n\n\n\n\nNoteAre Emily and Greg More Employable Than Lakisha and Jamal?\n\n\n\nAre first names a basis for racial discrimination, in the US?\nThis dataset was generated as part of a landmark research study done by Marianne Bertrand and Senthil Mullainathan. Read the description therein to really understand how you can prove causality with a well-crafted research experiment.  Download the Resume Name dataset"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#wait-but-why",
    "title": "\n Proportions",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can detect correlation between Quant variables using the scatter plots and regression lines\n\nAnd we can detect association between Qual variables using mosaics and sieves (which we did not see here, but is possible in Orange)\nYour project primary research data may be pure Qualitative too, as with a Questionnaire / Survey instrument.\nOne such Qual variable therein will be your target variable\n\nYou will need to justify whether the target variable is dependent upon the other Quals, and then to decide what to do about that.\n\n\n\n\n\n\n\nNoteSurvey Data and Likert Plots\n\n\n\nOften times, the primary research questionnaire is in the form of Questions whose answer is on a Likert Scale data, where several respondents rate a product, or a service, on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. The data are again categorical; but a Contingency Table / Mosaic Chart would be quite complex to behold and understand. A Likert Plot is what is constructed at such times. Here is a sample Likert Plot for a fictitious app called “QuickEZ”:\n\nYeah, this is possible in R and Python. But not in these barbarian tools that we are using. There are some websites that offer free apps for these plots too.\nFor more tutorial information, head off to Visualizing Survey Data (in R)."
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#readings",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#readings",
    "title": "\n Proportions",
    "section": "Readings",
    "text": "Readings\n\nMichael friendly. A Brief History of the Mosaic Display. https://www.datavis.ca/papers/moshist.pdf\nDavid Meyer, Achim Zeileis, Kurt Hornik. Visualizing Contingency Tables. Some very clear and simple pictures at https://statmath.wu.ac.at/projects/vcd/\nNice Chi-square interactive story at https://statisticalstories.xyz/chi-square\nA different graph on Housework Inequality, but the same story! https://datatopics.worldbank.org/sdgatlas/goal-5-gender-equality?lang=en#c4"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-Proportions/index.html#references",
    "href": "content/courses/NoCode/Modules/35-Proportions/index.html#references",
    "title": "\n Proportions",
    "section": "References",
    "text": "References\n\nCATAAS: Cat as a service. https://cataas.com. Please bookmark this.\nYet another Shipwreck story.\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html",
    "href": "content/courses/NoCode/Modules/80-Time/index.html",
    "title": "\n Time",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nTime Series Line Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#what-graphs-will-we-see-today",
    "title": "\n Time",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nTime Series Line Chart"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Time",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#inspiration",
    "title": "\n Time",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends?\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1: Solomon Grundy, born on Monday… was an Accountant?\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\nAnd why more births in September? That should be a no-brainer!! 😆"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#how-do-these-charts-work",
    "title": "\n Time",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…).\nThe X-axis is mapped to a temporal variable (i.e. representing time). The Y-axis is mapped to one or more Quant variables. We can easily get a sense of rhythm, season, cyclical variations, peaks, troughs….and whether something is getting seriously out of hand over time. Sadly\nWe can also take averages of the Y-axis Quant variable, over periods of X-axis variable; e.g. weekly or monthly averages. This is called smoothing of the data."
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#plotting-a-time-series-line-chart",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#plotting-a-time-series-line-chart",
    "title": "\n Time",
    "section": "\n Plotting a Time Series Line Chart",
    "text": "Plotting a Time Series Line Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe Time Series Line Chart widget in Orange is described here. https://orangedatamining.com/widget-catalog/time-series/line_chart/\nLet us take some Births related data and plot it in Orange.\n Download the US Births data \nAnd download the Line Chart workflow file for this data:\n Download the Time Series Line Chart Workflow \nNote how we have two widgets for the Line Charts. More shortly.\n\n\n\n\n\nWe can use a built-in dataset to create a line chart for browser usage: \nNote that DataWrapper again requires/uses data in wide format to create its Line Charts!!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#dataset-born-in-the-usa",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#dataset-born-in-the-usa",
    "title": "\n Time",
    "section": "\n Dataset: Born in the USA",
    "text": "Dataset: Born in the USA\n\nTourist: Any famous people born around here?\nGuide: No sir, best we can do is babies.\n\n\n Examine the Data\n\n\n\n\n\nFigure 2: Born in the USA\n\n\n\n\n\n\n\nFigure 3: Births Summary Table\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nyear, month, date_of_month: (int) Columns giving time information\n\nday-of_week: (int) Additional Time information\n\nbirths: (int) Total live births across the USA that day\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\nNone. Though we might covert day_of_week and month into Qual variables later.\n\n\nEvenly spread year, month, date_of_month and day_of_week variables…the bumps are curious though, no? day_of_week is of course neat. births are numerical data and have a good spread with a bimodal distribution distribution. Some numbers in the mid-range hardly occur at all… So a premonition of some two-valued phenomenon here already.\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. What does the births data look like over the years?\n Hmmm…very busy graph. The overall trend is a slight bump in births around 2007 and then a slow reduction in births. Large variations otherwise, which we need to see in finer detail on a magnified scale, a folded scale, or by averaging.\nConverting month or day_of_week to categorical in the File Menu does not provide us with a way of separating the time series by month or weekday…sad.. We will be able to average over month, day_of_week to see what happens.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. What do births look like averaged over month?\n\nThis is good! We have converted the dataset to a timeseries, of course, and then added a moving transform widget, that allows us to take averages of births over weeks, months, or years. Play with this setting in the moving transform widget.\nWe see that averaging i.e. \\(aggregating\\) by Month of year clearly shows September as the month for the most number of births.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. What do births look like averaged over day_of_week?\nHere too with the moving transform widget, choosing Day of Week as the aggregating parameter, we see a dip in births over weekends.\n\n\n\n\n\n\n\n\nImportantFolded Scale?\n\n\n\nLook at the figure below.\n\n\n\n\n\nFigure 4: Aggregate over Days of Week\n\n\nIt should be apparent that the line chart shows averages based on “Week of Year”. What does that mean?\nImagine a carpenter’s folding footrule: \nImagine the entire time series stretched out and then folded over itself at intervals of a week. There will of course be overlapping data that represent data points for the same week year after year. THAT is what goes into the averaging!\nSo we see that the weeks in September show the highest average birth numbers, which seems right!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#other-plots",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#other-plots",
    "title": "\n Time",
    "section": "\n Other Plots",
    "text": "Other Plots\nImagine that we follow this overlap routine and get the data by same-week-of-year, as before. We need not necessarily average that data; we can simply plot each (repeated) week’s worth of data as a box plot. This results in an array of boxplots, one per week, and is called a candlestick plot. Clearly we can do this for months, weeks, and even days of the week. Here is what it looks like; it does not seem possible to create these with any of the tools we are currently using.\nAs before, the medians are the black lines across each boxplot, which is one for each month. Note that since the medians are towards the upper end of the boxplots, we can guess that the per-month distribution must be skewed to the left (lower than median values are less frequent).\nIf the Quantities that vary over time are not continuous but discrete values such as high, medium, and low,, a time-series heatmap is also a possibility.\n Very arbitrarily slicing the birth numbers into three bins titled high, fine, and low, we can plot a heatmap like this. Orange does have a heatmap widget, however it seems suited to Machine Learning methods such as Clustering. We need to investigate its possibilities for time series."
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#your-turn",
    "title": "\n Time",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nArctic and Antarctic Sea Ice coverage over time. Is global warming affecting ice coverage at the poles?\n\n Download the Sea Ice Data \nThis data is in wide form, and you may have to massage it into long form before pulling it into Orange!\n\n\nIn the Air Tonight: Head over to Purple Rain Purple Air and download air quality data from community based air quality sensors. Plot these as time series, and try getting historical data, or data on festivals or important occasions in specific cities."
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#wait-but-why",
    "title": "\n Time",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe encounter many things that vary over time: weather, wealth, No. of users or downloads of an app, hits to a webpage, customers at a supermarket, or population of animals or plants in a region.\nThese are best represented by Line Charts\nAs humans, we are also deeply interested in patterns of recurrence over time, and in forecasting for the future, using tech, and using say Oracles.\nBoth these purposes are amply served by Line Charts."
  },
  {
    "objectID": "content/courses/NoCode/Modules/80-Time/index.html#references",
    "href": "content/courses/NoCode/Modules/80-Time/index.html#references",
    "title": "\n Time",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition).available online\n\n\nTime Series Analysis at Our Coding Club\n\n\nThe Nuclear Threat—The Shadow Peace, part 1\n\n\n11 Ways to Visualize Changes Over Time – A Guide"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html",
    "title": " Structure",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\n\nNone\nQual\nTrees and Dendrograms"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#what-graphs-will-we-see-today",
    "title": " Structure",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\n\nNone\nQual\nTrees and Dendrograms"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#what-kind-of-data-variables-will-we-choose",
    "title": " Structure",
    "section": " What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#inspiration",
    "title": " Structure",
    "section": " Inspiration",
    "text": "Inspiration"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#how-do-these-charts-work",
    "title": " Structure",
    "section": " How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#plotting-a-tree-diagram",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#plotting-a-tree-diagram",
    "title": " Structure",
    "section": " Plotting a Tree Diagram",
    "text": "Plotting a Tree Diagram\n\nUsing OrangeUsing RAWgraphsUsing DataWrapper\n\n\n\n\n\n\n\n\n\n\n\n\n\n Examine the Data\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ1.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ1.\n\n\n\n\n What is the Story Here?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#your-turn",
    "title": " Structure",
    "section": " Your Turn",
    "text": "Your Turn"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#wait-but-why",
    "title": " Structure",
    "section": " Wait, But Why?",
    "text": "Wait, But Why?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/54-Structures/index.html#readings",
    "href": "content/courses/NoCode/Modules/54-Structures/index.html#readings",
    "title": " Structure",
    "section": " Readings",
    "text": "Readings"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html",
    "title": "\n Summaries",
    "section": "",
    "text": "First, some baseball:\n\nAnd then, an example from a more sombre story:\n\n\n\n\n\nUS Population: Reading and Numeracy Levels\n\nYear\nBelow Level #1\nLevel #1\nLevel #2\nLevel #3\nLevels #4 and #5\n\n\n\nNumber in millions (2012/2014)\n8.35\n26.49\n65.10\n71.41\n26.57\n\n\nNumber in millions (2017)\n7.59\n29.23\n66.07\n68.81\n26.75\n\n\n\n\nNote: \n\n\n\n\n\n\n\n\n SOURCE: U.S. Department of Education, National Center for Education Statistics, Program for the International Assessment of Adult Competencies (PIAAC), U.S. PIAAC 2017, U.S. PIAAC 2012/2014.\n\n\n\n\n\n\n\n\n\n\n\nTable 1: US Population: Reading and Numeracy Levels\n\n\n\nThis ghastly-looking Table 1 examines U.S. adults with low English literacy and numeracy skills—or low-skilled adults—at two points in the 2010s, in the years 2012/20141 and 2017, using data from the Program for the International Assessment of Adult Competencies (PIAAC). As can be seen the summary table is quite surprising in absolute terms, for a developed country like the US, and the numbers have increased from 2012/2014 to 2017!\nSo why do we need to summarise data? Summarization is an act of throwing away data to make more sense, as stated by (Stigler 2016) and also in the movie by Brad Pitt aka Billy Beane. To summarize is to understand. Add to that the fact that our Working Memories can hold maybe 7 items, so it means information retention too.\nAnd if we don’t summarise? Jorge Luis Borges, in a fantasy short story published in 1942, titled “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes, there were only details.” (emphasis mine)\nAggregation can yield great gains above the individual components in data. Funes was big data without Statistics."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#inspirations",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#inspirations",
    "title": "\n Summaries",
    "section": "",
    "text": "First, some baseball:\n\nAnd then, an example from a more sombre story:\n\n\n\n\n\nUS Population: Reading and Numeracy Levels\n\nYear\nBelow Level #1\nLevel #1\nLevel #2\nLevel #3\nLevels #4 and #5\n\n\n\nNumber in millions (2012/2014)\n8.35\n26.49\n65.10\n71.41\n26.57\n\n\nNumber in millions (2017)\n7.59\n29.23\n66.07\n68.81\n26.75\n\n\n\n\nNote: \n\n\n\n\n\n\n\n\n SOURCE: U.S. Department of Education, National Center for Education Statistics, Program for the International Assessment of Adult Competencies (PIAAC), U.S. PIAAC 2017, U.S. PIAAC 2012/2014.\n\n\n\n\n\n\n\n\n\n\n\nTable 1: US Population: Reading and Numeracy Levels\n\n\n\nThis ghastly-looking Table 1 examines U.S. adults with low English literacy and numeracy skills—or low-skilled adults—at two points in the 2010s, in the years 2012/20141 and 2017, using data from the Program for the International Assessment of Adult Competencies (PIAAC). As can be seen the summary table is quite surprising in absolute terms, for a developed country like the US, and the numbers have increased from 2012/2014 to 2017!\nSo why do we need to summarise data? Summarization is an act of throwing away data to make more sense, as stated by (Stigler 2016) and also in the movie by Brad Pitt aka Billy Beane. To summarize is to understand. Add to that the fact that our Working Memories can hold maybe 7 items, so it means information retention too.\nAnd if we don’t summarise? Jorge Luis Borges, in a fantasy short story published in 1942, titled “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes, there were only details.” (emphasis mine)\nAggregation can yield great gains above the individual components in data. Funes was big data without Statistics."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-graphs-numbers-will-we-see-today",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-graphs-numbers-will-we-see-today",
    "title": "\n Summaries",
    "section": "\n What graphs / numbers will we see today?",
    "text": "What graphs / numbers will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\n“Chart Shape”\n\n\nAll\nAll\nTables and Stat Measures\n\n\n\n\n\nBefore we plot a single chart, it is wise to take a look at several numbers that summarize the dataset under consideration. What might these be? Some obviously useful numbers are:\n\nDataset length: How many rows/observations?\nDataset breadth: How many columns/variables?\nHow many Quant variables?\nHow many Qual variables?\nQuant variables: min, max, mean, median, sd\nQual variables: levels, counts per level\nBoth: means, medians for each level of a Qual variable…"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Summaries",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nWe will obviously choose all variables in the dataset, unless they are unrelated ones such as row number or ID which (we think) may not contribute any information and we can disregard."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#how-do-these-summaries-work",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#how-do-these-summaries-work",
    "title": "\n Summaries",
    "section": "\n How do these Summaries Work?",
    "text": "How do these Summaries Work?\nQuant variables: Inspecting the min, max,mean, median and sd of each of the Quant variables tells us straightaway what the ranges of the variables are, and if there are some outliers, which could be normal, or maybe due to data entry error! Comparing two Quant variables for their ranges also tells us that we may have to \\(scale/normalize\\) them for computational ease, if one variable has large numbers and the other has very small ones.\nQual variables: With Qual variables, we understand the levels within each, and understand the total number of combinations of the levels across these. Counts across levels, and across combinations of levels tells us whether the data has sufficient readings for graphing, inference, and decision-making, of if certain levels/classes of data are under or over represented.\nTogether?: We can use Quant and Qual together, to develop the above summaries (min, max,mean, median and sd) for Quant variables, again across levels, and across combinations of levels of single or multiple Quals, along with counts if we are interested in that.\nFor both types of variables, we need to keep an eye open for data entries that are missing! This may point to data gathering errors, which may be fixable. Or we will have to take a decision to let go of that entire observation (i.e. a row). Or even do what is called imputation to fill in values that are based on the other values in the same column, which sounds like we are making up data, but isn’t so really.\nAnd this may also tell us if we are witnessing a Simpson’s Paradox situation. You may have to decide on what to do with this data sparseness, or just check your biases!"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#obtaining-quant-summaries",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#obtaining-quant-summaries",
    "title": "\n Summaries",
    "section": "\n Obtaining Quant Summaries",
    "text": "Obtaining Quant Summaries\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us examine a healthcare-related dataset in Orange, on heart disease. Download the Orange workflow by clicking on the icon below, and open it in Orange.\n Download the Orange Workflow \n\n\n\n\n\nFigure 1: Grouped Summaries\n\n\nIn Figure 1, we see two sub-windows: on the upper right, we see the output of “Group By” where we have selected gender. We can also in the window choose what summary statistics we wish to see for each of the other variables in the dataset. To the lower left, we see the output of the Grouped Summaries Data Table, which shows just two rows: one for gender::female, and another for gender::male. All other variables have been summarised as desired.\nPlay with the summary output settings, and also with choosing which variable to Group By. Can you Group By more than one Qual variable?\n\n\n\n\n\n\nNote\n\n\n\nDoes Group By with a Quant variable make sense?\n\n\n\n\n\n\n\n\nNoteGrouping By Multiple Variables\n\n\n\nThere does not appear to be a way in which one can choose to Group By multiple variables as an input to summary…which does not seem possible in Orange, but is a breeze with R or Python or…stuff which you peasants will not touch. Use CMD and the Windows key respectively to select two or more Group By variables. Hmph!\n\n\n\n\nhttps://www.rawgraphs.io\n\n\nhttps://www.datawrapper.de"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-penguins",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-penguins",
    "title": "\n Summaries",
    "section": "\n Dataset: Penguins",
    "text": "Dataset: Penguins\nLet us use a (now) well-known data set on penguins. Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n\n Penguins data\n\n\nDownload this data into your Orange work folder and then use it in Orange.\n\n Examine the Data\nHere is the Data Table for the penguins data:\n\n\n\n\n\nFigure 2: Penguins Data Table\n\n\nWe see from Figure 2 that there are 344 observations (i.e. individual penguins) and 6 variables. There is some missing data but not too much!\n\n Data Dictionary\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nsex: male and female penguins\n\nisland: they have islands to themselves!!\n\nspecies: Three adorable types!\n\n\n\n\n\n\n\n\n\n\nFigure 3: Penguin Species\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nbill_length_mm: The length of the penguins’ bills\n\nbill_depth_mm: See the picture!!\n\nflipper_length_mm: Flippers! Penguins have “hands”!!\n\nbody_mass_gm: Grams? Grams??? Why, these penguins are like human babies!!❤️\n\n\n\n\n\n\n\n\n\n\nFigure 4: Penguin Features\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Summary Figures and Tables.\n\n\n\n\n\n\nNote\n\n\n\nQ1. What are the mean weights of the penguins, for each species? In Orange, we do a Group By with the species variable, and select mean for the summary function for the variable `body_mass_gm.\n\n\n\n\n\n\nNote\n\n\n\nFor now, disable summaries for everything else to avoid clutter!\n\n\n\n\n\n\n\n\n\n\n\n(a) Group By Species\n\n\n\n\n\n\n\n\n\n(b) Mass and Count by Species\n\n\n\n\n\n\nFigure 5: Penguins’ Mass and Counts by Species\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. What if we group by species and sex? And also look at flipper_length_mm?\n\n\n\n\n\n\n\n\n\n(a) Group by Species and Sex\n\n\n\n\n\n\n\n\n\n(b) Mass and Flipper Length by Species and Sex\n\n\n\n\n\n\nFigure 6: Summary over two Qual variables\n\n\n\n\n\n What is the Story Here?\n\nFrom Figure 5, clearly Gentoo penguins are the big brothers/sisters here, with a mean body mass higher by around 1250 grams!!😀\n\nFrom Figure 6:\n\nHmm..Chinstrap penguins are fewer in number, compared to the other two species.\n\nflipper_lengths_mm are pretty much ballpark same across all species and sex combinations; Gentoo still dominates the body_mass_gm across both.\n\nfemale Gentoo are heavier (on average) than other male-s even!!💪 (Not necessarily on individual basis!!)."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#your-turn",
    "title": "\n Summaries",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry adding more summary functions to the summary table? Which might you choose? Why?\nTry your hand at these datasets. Look at the data table, state the data dictionary, contemplate a few Research Questions and answer them with Summaries and Tables in Orange!\n\n\n\n\n\n\n\nNoteStar Trek Books\n\n\n\n\n\n Start Trek Book data\n\n\nWhich would be the Group By variables here? And what would you summarize? With which function?\n\n\n\n\n\n\n\n\nNoteMath Anxiety! Hah!\n\n\n\n\n\n Math Anxiety data"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#wait-but-why",
    "title": "\n Summaries",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nData Summaries give you the essentials, without getting bogged down in the details(just yet).\n\nSummaries help you “live with your data”; this is an important step in understanding it, and deciding what to do with it.\n\nSummaries help evoke Questions and Hypotheses, which may lead to inquiries, analysis, and insights"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#readings",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#readings",
    "title": "\n Summaries",
    "section": "\n Readings",
    "text": "Readings"
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html",
    "title": "\n Experiments",
    "section": "",
    "text": "Important Srishti kids are predominantly introverted\n\n\n\n\n\n\nWhat are we looking at, data-wise? A proportion, which if more than 50% would justify our hunch. So we do an MBTI on some unsuspecting sample of people, and try to generalize that result to the population"
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-1-i-am-an-intj",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-1-i-am-an-intj",
    "title": "\n Experiments",
    "section": "",
    "text": "Important Srishti kids are predominantly introverted\n\n\n\n\n\n\nWhat are we looking at, data-wise? A proportion, which if more than 50% would justify our hunch. So we do an MBTI on some unsuspecting sample of people, and try to generalize that result to the population"
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-2-lets-go-to-chefstouch",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-2-lets-go-to-chefstouch",
    "title": "\n Experiments",
    "section": "\n Free Hunch #2: Let’s Go to ChefsTouch(?)",
    "text": "Free Hunch #2: Let’s Go to ChefsTouch(?)\n\n\n\n\n\n\nImportant Most people think the food in the cafeteria is ordinary.\n\n\n\n\n\n\nAgain, a survey of a sample. Opinions, yes or no. A Proportion for the sample, and an extension to the population. A proportion test."
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-3-i-will-eat-my-tip-thank-you.",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-3-i-will-eat-my-tip-thank-you.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #3: I will eat my tip, thank you.",
    "text": "Free Hunch #3: I will eat my tip, thank you.\n\n\n\n\n\n\nImportant The average tip people give is higher for people who are non-vegetarians. Regardless of whether you are going Dutch or not.\n\n\n\n\n\n\nAre vegetarians more kanjoos? Or it is the meat-eaters?\nSo Swiggy/Zomato/Dining Out bills. For both sets of people. And then the t-t-t-t-t-test…"
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-3.-art-design-and-vocation-are-all-diff-different.",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#free-hunch-3.-art-design-and-vocation-are-all-diff-different.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #3. Art, Design, and Vocation are all diff-different.",
    "text": "Free Hunch #3. Art, Design, and Vocation are all diff-different.\n\n\n\n\n\n\nImportant Grades are very different between B.Voc, B.Cra, and B.Des folks.\n\n\n\n\n\n\nSo? Grades of course, for a good sample from all three groups of people..and then? ANOVA of course."
  },
  {
    "objectID": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#references",
    "href": "content/courses/NoCode/Modules/150-NoFreeHunch/index.html#references",
    "title": "\n Experiments",
    "section": "References",
    "text": "References\n\nFacing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/listing.html",
    "href": "content/courses/TRIZ4ProbSolving/listing.html",
    "title": "TRIZ for Problem Solvers",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\nI am Water\n\n\n2 min\n\n\n\n\n\n\n\n\n\nI am What I yam\n\n\n2 min\n\n\n\n\n\n\n\n\n\nBirds of Different Feathers\n\n\n2 min\n\n\n\n\n\n\n\n\n\nI Connect therefore I am\n\n\n5 min\n\n\n\n\n\n\n\n\n\nI Think, Therefore I am\n\n\n1 min\n\n\n\n\n\n\n\n\n\nThe Art of Parallel Thinking\n\n\n2 min\n\n\n\n\n\n\n\n\n\nA Year of Metaphoric Thinking\n\n\n5 min\n\n\n\n\n\n\n\n\n\nTRIZ - Problems and Contradictions\n\n\n6 min\n\n\n\n\n\n\n\n\n\nTRIZ - The Unreasonable Effectiveness of Available Resources\n\n\n6 min\n\n\n\n\n\n\n\n\n\nTRIZ - The Ideal Final Result\n\n\n2 min\n\n\n\n\n\n\n\n\n\nTRIZ - A Contradictory Language\n\n\n13 min\n\n\n\n\n\n\n\n\n\nTRIZ - The Contradiction Matrix Workflow\n\n\n2 min\n\n\n\n\n\n\n\n\n\nTRIZ - The Laws of Evolution\n\n\n2 min\n\n\n\n\n\n\n\n\n\nTRIZ - Substance Field Analysis, and ARIZ\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#introduction",
    "title": "TRIZ - The Laws of Evolution",
    "section": "Introduction",
    "text": "Introduction\n\nLaws of Product Evolution\n8 directions\nRadar Chart in R\nFind Opportunities based on Radar Chart",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#laws-of-product-evolution",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#laws-of-product-evolution",
    "title": "TRIZ - The Laws of Evolution",
    "section": "Laws of Product Evolution",
    "text": "Laws of Product Evolution\n\nLaw of completeness of the system: Systems derive from synthesis of separate parts into a functional system.\nLaw of energy transfer in the system: Shaft, gears, magnetic fields, charged particles, which are the heart of many inventive problems.\nLaw of increasing ideality: Function is created with minimum complexity, which can be considered a ratio of system usefulness to its harmful effects. The ideal system has the desired outputs with no harmful effects, i.e., no machine, just the function(s).\nLaw of harmonization: Transferring energy more efficiently.\nLaw of uneven development of parts: Not all parts evolve at the same pace. The least will limit the overall system\nLaw of transition to a super system: Solution system becomes subsystem of larger system.\nLaw of Transition from Macro to Micro: Using physically smaller solutions, e.g., electronic tubes to chips.\nLaw of Increasing Substance-Field Involvement: Viewing and modeling systems as composed of two substances interacting through a field.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#how-would-you-redesign-this-product",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#how-would-you-redesign-this-product",
    "title": "TRIZ - The Laws of Evolution",
    "section": "How Would you Redesign this Product",
    "text": "How Would you Redesign this Product\nTBD. Plot a set of Radar charts showing different stages of evolution of well known product lines. Obsolete products are even more interesting.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#references",
    "title": "TRIZ - The Laws of Evolution",
    "section": "References",
    "text": "References\n\nProject TETRIS: Chapter 2: Laws of System Evolution (PDF)\nProject TETRIS: Chapter 5: Techniques to Resolve Contradictions / Resources / Effects (PDF)\nProject TETRIS: Examples of inventive problems: Example 1-5 (RAR Archive File)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#ideal-final-result-game",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#ideal-final-result-game",
    "title": "TRIZ - The Ideal Final Result",
    "section": "Ideal Final Result: Game",
    "text": "Ideal Final Result: Game\nTBD",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#discussion",
    "title": "TRIZ - The Ideal Final Result",
    "section": "Discussion",
    "text": "Discussion\nLike the Queen in Alice in Wonderland, it is important to conceptualize the Ideal Final Result as one of the six impossible things before breakfast. As Jack Hipple says in his book The Ideal Result ( Reference 3), it’s when something performs its function, and does not exist.\nIn this sense, TRIZ is a foretaste of what was said much later after TRIZ was invented, that the best Technologies are those that disappear.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#references",
    "title": "TRIZ - The Ideal Final Result",
    "section": "References",
    "text": "References\n\nTitanic Game (PDF)\nStan Kaplan, An Introduction to TRIZ(PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achieve it”, Springer, 2012.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html",
    "title": "I Think, Therefore I am",
    "section": "",
    "text": "In this Module, we will acquaint ourselves with a few of our Cognitive Biases, in gamefied fashion. We will then discuss how these biases inform our efforts in the practice of Creativity and Innovation and discuss ways of overcoming them."
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-1-cognitive-miserliness",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-1-cognitive-miserliness",
    "title": "I Think, Therefore I am",
    "section": "Bias #1: Cognitive Miserliness",
    "text": "Bias #1: Cognitive Miserliness\nGame\nLet us start with a set of relatively easy games. Here PDF are some questions for you. Please write or call out the answers as you see them.\nInterpretation\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-2-halo-effect",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-2-halo-effect",
    "title": "I Think, Therefore I am",
    "section": "Bias #2 Halo Effect",
    "text": "Bias #2 Halo Effect\nGame\nLook at the set of words given here PDF. We have a set of words arranged in a series; each word-set describing a person. Please look at these and then write a brief description of that person.\nLet us now compare what different people wrote about the persons described the respective word-sets.\nInterpretation\nWhat we saw was that the word-set are the same but in reverse order. The SEQUENCE in which we encounter them colours our opinion of the person whom they describe. So first impressions do seem to be last impressions! This is a Cognitive Bias the Halo Effect at work. We formulate decisions based on first impressions. What can we do to over come this bias?\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-3-priming",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-3-priming",
    "title": "I Think, Therefore I am",
    "section": "Bias #3 Priming",
    "text": "Bias #3 Priming\nGame\nYou will be given a small questionnaire PDF, with a single multiple-choice question. Please enter your answer at the appropriate location.\nIntepretation\nThe Marvels of Priming\nAs is common in science, the first big breakthrough in our understanding of the mechanism of association was an improvement in a method of measurement. Until a few decades ago, the only way to study associations was to ask many people questions such as, “What is the first word that comes to your mind when you hear the word DAY?” The researchers tallied the frequency of responses, such as “night,” “sunny,” or “long.” In the 1980s, psychologists discovered that exposure to a word causes immediate and measurable changes in the ease with which many related words can be evoked. If you have recently seen or heard the word EAT, you are temporarily more likely to complete the word fragment SO_P as SOUP than as SOAP. The opposite would happen, of course, if you had just seen WASH. We call this a priming effect and say that the idea of EAT primes the idea of SOUP, and that WASH primes SOAP.\nPriming effects take many forms. If the idea of EAT is currently on your mind (whether or not you are conscious of it), you will be quicker than usual to recognize the word SOUP when it is spoken in a whisper or presented in a blurry font. And of course you are primed not only for the idea of soup but also for a multitude of food-related ideas, including fork, hungry, fat, diet, and cookie. If for your most recent meal you sat at a wobbly restaurant table, you will be primed for wobbly as well. Furthermore, the primed ideas have some ability to prime other ideas, although more weakly. Like ripples on a pond, activation spreads through a small part of the vast network of associated ideas. The mapping of these ripples is now one of the most exciting pursuits in psychological research.\nThe Ideomotor Effect Another major advance in our understanding of memory was the discovery that priming is not restricted to concepts and words. You cannot know this from conscious experience, of course, but you must accept the alien idea that your actions and your emotions can be primed by events of which you are not even aware.\nIn an experiment that became an instant classic, the psychologist John Bargh and his collaborators asked students at New York University — most aged eighteen to twentytwo — to assemble four-word sentences from a set of five words (for example, “finds he it yellow instantly”). For one group of students, half the scrambled sentences contained words associated with the elderly, such as Florida, forgetful, bald, gray, or wrinkle. When they had completed that task, the young participants were sent out to do another experiment in an office down the hall. That short walk was what the experiment was about. The researchers unobtrusively measured the time it took people to get from one end of the corridor to the other. As Bargh had predicted, the young people who had fashioned a sentence from words with an elderly theme walked down the hallway significantly more slowly than the others.\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-4",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-4",
    "title": "I Think, Therefore I am",
    "section": "Bias #4",
    "text": "Bias #4\nGame\nIntepretation\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#conclusions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#conclusions",
    "title": "I Think, Therefore I am",
    "section": "Conclusions",
    "text": "Conclusions\nSo what should we do to try to overcome these ( and other ) Cognitive Biases?\n\nBe aware: Awareness is the first step towards overcoming errors in judgement. Keeping in mind the potentially harmful consequences of first impressions is helpful when meeting new people.\n\n\nSlow down: The second step is to deliberately slow down your judgement and any subsequent decisions. For example, never make a recruitment choice straight after the interview.\n\n\nBe systematic: Finally, try to engage your analytical reasoning skills by taking a systematic approach. This sounds trickier than it is. In the context of interviewing, you could prepare a list of essential criteria and force yourself to consider each one carefully before making a choice.\n\nThis helps us to not arrive at flawed ideas to pursue but ones that we are reasonably sure we can justify to….our Field, as conceptualized by Mihaly Csikszentmihalyi."
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#introduction",
    "title": "I am What I yam",
    "section": "Introduction",
    "text": "Introduction\nWe will understand Pierre Bourdieu’s Concept of Cultural Capital(CC).\nCC affects our view of the world and helps us identify Problem Situations and Contradictions in Life.\n\nWhat is Cultural Capital? https://culturallearningalliance.org.uk/what-is-cultural-capital\n\nCultural Capital",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#activities",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#activities",
    "title": "I am What I yam",
    "section": "Activities",
    "text": "Activities\nSharing\n\nSharing of CC: we will each of us take turns to reveal some aspect of Cultural Capital that we directly possess: Objectified, Institutional, and Embodied\n\n\n\nWhat is something that replicates from generation to generation, biologically?\nIs there something that replicates “non-biologically”?\nHow would you measure that? In what units, so to speak?\nRe-Inventing your Own Game\n\nYou will be divided into Teams. Each Team needs to chose a wellknown parlour/household or board game from your childhood. Set it up and play it.\nThen, you will be given a piece of Electronic Hardware called a Makey-Makey. You will need to include this hardware in your Game and extend the Game in a certain way.\n\n\n\n\n\n\n\nNoteTRIZ Concept: Su-Field Analysis\n\n\n\nWe will later tie this activity to the TRIZ idea of Substance-Field Analysis. Perhaps you have a premonitory idea about this already?\n\n\nInventing your City\n\nMaking a Street Intersection Logo\n\nLet us take inspiration from this website: https://www.cartogrammar.com/blog/maps-make-the-best-logos/\n\nHere are some instructions:\n\n\n\nVisit your home town on Google Maps\nSelect a specific STREET INTERSECTION there.\nCopy paste the Shape of the Street Intersection into your favourite image making software\nCreate A LOGO for your city, using the Shape, such that you show off some interesting aspects of your City, based on your own CULTURAL CAPITAL.\nThree sentences to describe your Logo.\nSee examples at the link above for inspiration\n\n\n\n\n\n\n\nNoteSerendipity\n\n\n\nThis course is about nothing if not about serendipity. https://www.dictionary.com/browse/serendipity. We will connect this logo building idea with the idea of Metaphors in TRIZ.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#references",
    "title": "I am What I yam",
    "section": "References",
    "text": "References\n\nWeb Apps for your Makey-Makey. https://makeymakey.com/blogs/how-to-instructions/apps-for-plug-and-play\nMake your own game with a Makey-Makey. https://www.instructables.com/Using-Scratch-and-Makey-Makey-to-make-your-own-gam\nRichard Dawkins. The Selfish Gene. https://richarddawkins.net/2014/02/whats-in-a-meme/",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#the-creativity-systems-model",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#the-creativity-systems-model",
    "title": "I Connect therefore I am",
    "section": "The Creativity Systems Model",
    "text": "The Creativity Systems Model\n\n\n\n\n\n\n\nMihaly Csikszentmihalyi\n\n\n\nMihaly Csikszentmihalyi created what he called a Systems Model for Creativity, shown in the Figure below.\n\n\n\n\n\n\n\n\nHere are some brief extracts from his book[Reference 1]\n\nMany of these books and articles have tried to answer what has been thought to be the most fundamental question: What is creativity? But no one has raised the simple question that should precede attempts at defining, measuring, or enhancing, namely: Where is creativity? On hearing this question, most people would answer “Why, in the creative person’s head, of course”.\n\n\nAfter studying creativity on and off for almost a quarter of a century, I have come to the reluctant conclusion that this is not the case. We cannot study creativity by isolating individuals and their works from the social and historical milieu in which their actions are carried out.\n\n\nThis is because what we call creative is never the result of individual action alone; it is the product of three main shaping forces:\n\na set of social institutions, or field, that selects from the variations produced by individuals those that are worth preserving;\n\na stable cultural domain that will preserve and transmit the selected new ideas or forms to the following generations;\n\nand finally the individual, who brings about some change in the domain, a change that the field will consider to be creative.\n\n\n\nSo the question “Where is creativity?” cannot be answered solely with reference to the person and the person’s work. Creativity is a phenomenon that results from interaction between these three systems. Without a culturally defined domain of action in which innovation is possible, the person cannot even get started. And without a group of peers to evaluate and confirm the adaptiveness of the innovation, it is impossible to differentiate what is creative from what is simply statistically improbable or bizarre.\n\n\nIf we think about it, the reason we believe that Leonardo or Einstein was creative is that we have read that that is the case, we have been told it is true; our opinions about who is creative and why ultimately are based on faith. We have faith in the domains of art and science, and we trust the judgment of the field, that is, of the artistic and scientific establishments. There is nothing wrong with this, because it is an inevitable situation. But by recognizing it, we must also accept some of its consequences, namely, that any attribution of creativity must be relative, grounded only in social agreement.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#discussion",
    "title": "I Connect therefore I am",
    "section": "Discussion",
    "text": "Discussion\nWhat are the salient points in this model?\n\nA rather “societal” view of creativity! Rather than a psychic view!\n\nThe influence of environment and people around the individual\n\nA defined domain of work, with a defined language to discuss its ideas and concepts.\nThe “evaluation” of the creative act by field members, who act as gatekeepers\n\nIs this nonplussing? Contrary to what you believe? Outright disturbing?\nI think this is immensely reassuring and humbling at the same time: reassuring because a creative act can be performed by anybody, not some arbitrarily defined “gifted individual”, and humbling, because if field members don’t like your act, or if you don’t have access to the field, you might not make it. It is fair and unfair, but immensely real.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#why-this-model-matters-to-us",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#why-this-model-matters-to-us",
    "title": "I Connect therefore I am",
    "section": "Why this Model matters to us",
    "text": "Why this Model matters to us\nAs we will see later in this course, classical TRIZ was developed by Genrikh Altshuller based on an evaluation of more than 25000 patents. He found “design patterns” that had been repeatedly used across patents and classified them for our use. There is a strong sense of history and society right at the heart of TRIZ.\nFurther, the patents the study of which helped Altshuller create TRIZ in the first place belong with domains that we may not necessarily encounter. And yet, the Principles of TRIZ straddle domains and help “cross-domain” solutions to happen: the use of principles in one domain in another, unrelated, domain. (More when we enter TRIZ proper)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#some-examples-from-history",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#some-examples-from-history",
    "title": "I Connect therefore I am",
    "section": "Some Examples from History",
    "text": "Some Examples from History\n\nBotticelli was for centuries considered to be a coarse painter, and the women he painted “sickly” and “clumsy”. Only in the mid-nineteenth century did some critics begin to reevaluate his work and see in it creative anticipations of modern sensibility. To what extent was creativity contained in Botticelli’s canvases, and to what extent did it emerge from the interpretive efforts of critics like Ruskin?\n\n\n…our view of Mendel’s contribution to genetics is generally quite wrong. The impression we have is that Mendel made a series of epochal experiments in the genetic transmission of traits in the 1860s, but that his creativity was not recognized by the scientific community until about 40 years later. This view is radically mistaken in a subtle but essential respect. (Brannigan) argues that Mendel’s experiments were not and could not have been contributions to genetics at the time they were made. Their implications for the theory of variation and natural selection were discovered only in 1900 by William Bateson and other evolutionists looking for a mechanism that explained discontinuous inheritance. Within their theoretical framework, Mendel’s work suddenly acquired an importance that it had lacked before, even in the mind of Mendel himself.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#references",
    "title": "I Connect therefore I am",
    "section": "References",
    "text": "References\n\nMihaly Csikszentmihalyi, The Systems Model of Creativity: The Collected Works of Mihaly Csikszentmihalyi, ISBN:9789401790857, Springer, 2015.\nOliver Ding, “Platform Creativity: Domain, Field, and Person”, https://medium.com/call4/domain-8a22b6b486f4\nJessica Wolf,UCLA Newsroom, The truth about Galileo and his conflict with the Catholic Church, https://newsroom.ucla.edu/releases/the-truth-about-galileo-and-his-conflict-with-the-catholic-church",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#introduction",
    "title": "A Year of Metaphoric Thinking",
    "section": "Introduction",
    "text": "Introduction\n\nThe Year of Magical Thinking is Joan Didion’s account of the year following the death of her husband, writer John Gregory Dunne, and her attempts to make sense of her grief while tending to the severe illness of her adopted daughter, Quintana. As she tries to make sense of John’s death and her own changed identity, Didion discovers that grief is not what she expected it to be. Consumed by memories of the years they lived in Los Angeles, shortly after they married and adopted Quintana, Didion feels that she has entered a state of temporary insanity. Though cool and collected on the surface, she begins to believe that her wishes might have the power to bring John back. To this end, she refuses to give away his clothes and shoes, believing that her husband will need them when he returns to her. She calls this childlike belief that her thoughts and wishes can alter reality “magical thinking.”\n\nOf course we are not embarked on anything like this, but we do want to generate some “magic” in our thoughts! So taking some inspiration from her “childlike belief” that can “alter reality”, let us hark back to our childhoods and see what we can do with these objects below:",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#activities",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#activities",
    "title": "A Year of Metaphoric Thinking",
    "section": "Activities",
    "text": "Activities\nActivity-1: Some Objects to Contemplate\nHow many different uses can you imagine for each of these objects? Can you briefly describe and quickly sketch a few ideas?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity-2: Some Shapes to Contemplate\nWhere do you reckon you can “see” these shapes ? Can you briefly describe and quickly sketch a few ideas?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoring your Ideas\nScoring is comprised of four components:\n\n\nFluency - total. Just add up all the responses. In this example it is 6.\n\nFlexibility - or different categories of ideas or, as Csikszentmihalyi would have us say, DOMAINS. Is your brick a Toy? Can it be used as…a Horticultural support thing? That is two domains, so two points.\n\n\n\nFrom https://houseplantjoy.com/bonsai-styles-fascinating-facts-about-bonsai-forms/\n\n\n\nElaboration - amount of detail (for Example: “in a bonsai” = 0, whereas “in a bonsai to create a root-over-rock structure as an island” = 2 (one for root-over-rock, two for further detail about the island structure).\n\nOriginality - each response it compared to the total amount of responses from all of the people you gave the test to. Responses that were given by only 5% of your group are unusual (1 point), responses that were given by only 1% of your group are unique - 2 points). Total all the points. Higher scores indicate creativity*\n\n\n\n\n\n\n\nNote\n\n\n\nYou might have noticed that the higher fluency the higher the originality (if you did “good for you!”) This is a contamination problem and can be corrected by using a corrective calculation for originality (originality = originality/fluency).\n\n\nDiscussion\nBoth these activities are examples of exercises in divergent thinking. See the references for more information.\n\nDid you use the words “as” and “like” to describe your ideas?\nDid you not use these words to describe your ideas?\nWere there, in your opinion, any outrageous ideas presented? Why were they outrageous?\nAre metaphors more interesting when they are surprising?\nHow did the attributes of the objects ( shape , texture, size, weight, orientation…) get embedded in the ideas presented?\nWere these “embeddings” meaningful? How and why so? (Ask Bourdieu!)\nActivity-3: Gangs of Wasseypur\nWe will divide into two groups (four if necessary) and contemplate a brief description of the town of Wasseypur. There are 4 short questions / problems for you to consider at the end.\nActivity-4: Seymour Papert Constructionism Game\nhttps://arvindvenkatadri.com/courses/1-play-and-invent/modules/50-metaphoric-thinking/#activity-4-seymour-papert-constructionism-game-1\nActivity-5: C’est ne une Pipe\n\nWe will break up into groups of 4-5.\nEach group will be given a household object, perhaps an unusual one.\nYou need to imagine a use for it that is not what is the common known one.\nMarket it as a product that serves this new purpose. Make an ad.\nUse only Gen Z language in your ad.\nAd = Performance/Jingle + Poster\n\n(Articles: Book End made of Wood; Aristo Slide Rule; 80-year-old brass mortar and pestle; Node from Elephant Bamboo stem)\nActivity-6: Metaphorical Story re-Writing\n\nRead the story in Ref. 7 below.\nUnderstand the metaphors in the story:\n\nWhat is the story about?\nWhat “metaphorical language” has Joshua Ferris used to describe the characters, their actions, and the results of their actions?\nWhat is the domain of these metaphors in the story?\n\n\nNow, choose a (short!) story that you know really well, something that you may have encountered in school.\nRe-write this story using language, metaphors, and images from any domain (other than what may be in the original story).\n\nFor example, in Primo Levi’s lyrical story Carbon, a lot of metaphors from Chemistry and atomic physics are used to describe life, and connections with people.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#references",
    "title": "A Year of Metaphoric Thinking",
    "section": "References",
    "text": "References\n\nGuilford Test for Divergent Thinking: (Weblink)\nWallas-Kogan Test for Divergent Thinking: (Weblink)\nThibodeau, Paul & Boroditsky, Lera. (2011). Metaphors We Think With: The Role of Metaphor in Reasoning. PloS one. 6. e16782. 10.1371/journal.pone.0016782. (PDF)\nBobo Hjort. (2003) Drawing, Knowledge, and Intuitive Thinking: Drawing as a Way to Understand and Solve Complex Problems in Art and Complexity. J. Casti and A. Karlqvist (editors) © 2003 Published by Elsevier Science B.V.(PDF)\nDavid Chen, Creative Paradoxical Thinking and Its Implications for Teaching and Learning Motor Skills (PDF)\nHolyoak and Thagard, The Analogical Mind, (PDF)\nJoshua Ferris, The Market Value of my Father, a metaphorical Short Story. (Web Link)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#project-tetris-triz-videos",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#project-tetris-triz-videos",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "Project TETRIS TRIZ Videos",
    "text": "Project TETRIS TRIZ Videos\nLet us get an actual experience of TRIZ methods with these short videos:\n\n\nHistory of TRIZ\n \n\n\nTRIZ Tales: Nina at School\n \n\n\nTRIZ Tales: Nina at University\n\n\n\n\nTRIZ Tales: Nina at Work #1 \n\n\n\n\nTRIZ Tales: Nina at Work #2 \n\n\n\nHow to use the Contradiction Matrix:(Coursera Video)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#a-triz-workflow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#a-triz-workflow",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "A TRIZ Workflow",
    "text": "A TRIZ Workflow\nWe have seen\n\nHow to examine a Situation\n\nUse the Ishikawa Diagram to find Knobs and Settings\nFind one or more Administrative Contradictions\n\nState the Ideal Final Result\n\nConvert the AC into a TC with 39 TRIZ Parameters\n\n(Convert the AC into a PC)\n\nNow we will use the TC and find out solutions for it using what is called the Contradiction Matrix. Here is a workflow presentation below:https://arvindvenkatadri.com/slides/playandinvent/triz-basics/#1\"",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#triz-stories-and-case-studies",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#triz-stories-and-case-studies",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "TRIZ Stories and Case Studies!!",
    "text": "TRIZ Stories and Case Studies!!\nWant to create a good protest or strike? Or simply want to clean up the oceans?\nhttps://arvindvenkatadri.com/slides/playandinvent/triz-database/#1\"",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#the-triz-contradiction-matrix",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#the-triz-contradiction-matrix",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "The TRIZ Contradiction Matrix",
    "text": "The TRIZ Contradiction Matrix\n\nThe TRIZ Contradiction Matrix (2003) (Matrix)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#references",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "References",
    "text": "References\n\nPizza Box https://www.metodolog.ru/triz-journal/archives/2008/03/04/\nTRIZ for Dummies Cheat Sheet (weblink)\nTRIZ Inventive Principles in Pics @instagram: https://www.instagram.com/stories/highlights/17857566926043828/\nStan Kaplan, An Introduction to TRIZ(PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achive it”, Springer, 2012.\nProject TETRIS: Chapter 1: Introduction to Classical TRIZ (PDF)\nProject TETRIS: Chapter 5: Techniques to Resolve Contradictions / Resources / Effects (PDF)\nProject TETRIS: Examples of inventive problems: Example 1-5 (RAR Archive File)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solvers",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/listing.html",
    "title": "Media",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFourier Series\n\n\n11 min\n\n\n\n\n\n\nAdditive Sound Synthesis\n\n\n9 min\n\n\n\n\n\n\n Making Noise Predictably\n\n\n21 min\n\n\n\n\n\n\nThe Karplus-Strong Guitar Algorithm\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "",
    "text": "Here is the GUI for a guitar created in JavaScript (not p5.js).",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#inspiration",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "",
    "text": "Here is the GUI for a guitar created in JavaScript (not p5.js).",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-the-karplus-strong-algorithm",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-the-karplus-strong-algorithm",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is the Karplus-Strong Algorithm?",
    "text": "What is the Karplus-Strong Algorithm?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-a-delay-line",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-a-delay-line",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is a Delay Line?",
    "text": "What is a Delay Line?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-feedback-oscillation",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-feedback-oscillation",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is Feedback Oscillation?",
    "text": "What is Feedback Oscillation?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#karplus-strong-guitar-in-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#karplus-strong-guitar-in-code",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "Karplus-Strong Guitar in Code",
    "text": "Karplus-Strong Guitar in Code\nLet us code up our guitar and see how it works!\n\n\n p5.js\n R",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#references",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "References",
    "text": "References\n\nKarplus, K., & Strong, A. (1983). Digital Synthesis of Plucked-String and Drum Timbres. Computer Music Journal, 7(2), 43. doi:10.2307/3680062. https://sci-hub.se/https://doi.org/10.2307/3680062\n\nJulius O. Smith. PHYSICAL AUDIO SIGNAL PROCESSING FOR VIRTUAL MUSICAL INSTRUMENTS AND AUDIO EFFECTS. https://ccrma.stanford.edu/~jos/pasp/pasp.html\n\n\nEuphonics: The Science of Musical Instruments.https://euphonics.org/about/\n\nKarplus-Strong Guitar in PureData (pD). https://youtu.be/xEpbMWpz65E?si=Mkz-eQ4UTMrYR3Fu\n\nAmid Fish.(May 2017). Karplus Strong String Synthesis. http://amid.fish/karplus-strong\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html",
    "title": "Additive Sound Synthesis",
    "section": "",
    "text": "So we understand the Fourier Transform: we can express any waveform as a sum of sinusoids that are appropriately weighted and are at discrete multiples of a chosen “fundamental frequency”.\nHow do we use these ideas to synthesize sound?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#introduction",
    "title": "Additive Sound Synthesis",
    "section": "",
    "text": "So we understand the Fourier Transform: we can express any waveform as a sum of sinusoids that are appropriately weighted and are at discrete multiples of a chosen “fundamental frequency”.\nHow do we use these ideas to synthesize sound?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#inspiration",
    "title": "Additive Sound Synthesis",
    "section": "\n Inspiration",
    "text": "Inspiration\nTO BE ADDED (sic!)",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-additive-synthesis",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-additive-synthesis",
    "title": "Additive Sound Synthesis",
    "section": "What is Additive Synthesis?",
    "text": "What is Additive Synthesis?\nFirst we need to get used to the idea of an oscillator.\nAn oscillator is a source: it generates waveforms that we perceive as sound. Let us play with a few oscillator types here:\nhttps://musiclab.chromeexperiments.com/Oscillators/\nEach of these waveforms, by the Fourier series, is the sum of an ( infinite) number of sine wave outputs.\nIn Fourier series, we normally use just sine wave oscillators, and use many of them to add up to obtain the wave form we need. Now let us hear from Mr Shiffman again:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#the-math-of-waveform-addition",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#the-math-of-waveform-addition",
    "title": "Additive Sound Synthesis",
    "section": "The Math of Waveform Addition",
    "text": "The Math of Waveform Addition\nIn general, we can write a sum of sine/cos waves as:\n\\[\nf(\\theta) = \\frac{1}{a_0} + \\sum_{k=0}^{\\infty} a_k*sin(k\\theta) + b_k*cos(k\\theta)\n\\tag{1}\\]\n\\(f\\) is the desired time-waveform, the \\(\\theta = 2\\pi\\times fundamental~frequency \\times t\\), and the \\(a_k\\) and \\(b_k\\) are weights for the individual components that are to be designed.\nHow does this look like? Mr. Shiffman again:\n\nNow let us see how we can design something using the Additive Method.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#design-principles-for-additive-synthesis",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#design-principles-for-additive-synthesis",
    "title": "Additive Sound Synthesis",
    "section": "Design Principles for Additive Synthesis",
    "text": "Design Principles for Additive Synthesis\nHow do we do this with intent? We will follow the development in Farnell and Risset and Mathews, (Risset and Mathews 1969) and Moorer.\n\nThe idea is to take an original sound, analyze that using the Fourier Series, and then use those coefficients to synthesize the sound with code.\nThe coefficients, or parameters, need to be manipulated and transformed with time, in order for the synthesized sound to have a “live” feel.\nThe number of such parameters and their control over time could pose a formidable data management challenge. This leads to the idea of data reduction in order to have a manageable number of these, and generate the sound in its essentials.\nOne essential part of this is to use envelopes around the amplitudes of several sine waves, what is called the ADSR method. This could also lead to several oscillators being turned on or off based on need.\nSo one needs to break down the sound into “principal components” that are harmonically related ( as with the Fourier series) and then fill in inharmonic tones using additional oscillators.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-adsr",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-adsr",
    "title": "Additive Sound Synthesis",
    "section": "What is ADSR?",
    "text": "What is ADSR?\nADSR stands for “Attack Decay Sustain Release”. These related to the way a note of music varies over time in a typical piece of music.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#additive-synthesis-with-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#additive-synthesis-with-code",
    "title": "Additive Sound Synthesis",
    "section": "Additive Synthesis with Code",
    "text": "Additive Synthesis with Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\n\nShow the Codemusic &lt;-\n  Music() +\n  Meter(4, 4) +\n  Line(c(\"C5\", \"D5\", \"E5\", \"F5\"))\n\nshow(music)",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#wait-but-why",
    "title": "Additive Sound Synthesis",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nTo be Written Up.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#references",
    "title": "Additive Sound Synthesis",
    "section": "\n References",
    "text": "References\n\nJames Moorer. (Nov 1976) The Synthesis of Complex Audio Spectra by Means of Discrete Summation Formulae. Journal of the Audio Society. PDF\n\nJean-Claude Risset, Max V. Matthews. (Feb 1969). Analysis of Musical Instrument Tones. Physics Today. https://sci-hub.se/https://doi.org/10.1063/1.3035399\n\np5.Sound Tutorial.https://pdm.lsupathways.org/6_resources/7_soundandmusic/p5.sound/\n\nSound in p5.js Playlist. https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aFcVjlDAkkGIixw70s7jpW\n\n\nSounds with Tone.js. https://pdm.lsupathways.org/3_audio/\n\nMister Bomb. p5.Sound project tutorials https://www.youtube.com/playlist?list=PLIsdHp2z9wFl7A1wWb2VmQUUojEGsKELE\n\nhttps://www.cs.cmu.edu/~tcortina/15104-f20/lectures/24-MoreSound.pdf\n\nR package gm: the grammar of Music. https://cran.r-project.org/web/packages/gm/vignettes/gm.html\n\nPhil Burk,Larry Polansky, Douglas Repetto, Mary Roberts Dan Rockmore. Music and Computers: A Theoretical and Historical Approach https://musicandcomputersbook.com\n\nJulius O. Smith. PHYSICAL AUDIO SIGNAL PROCESSING FOR VIRTUAL MUSICAL INSTRUMENTS AND AUDIO EFFECTS https://ccrma.stanford.edu/~jos/pasp/pasp.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\ngm\n2.0.0\nMao (2024)\n\n\n\n\n\nMao, Renfei. 2024. gm: Create Music with Ease. https://doi.org/10.32614/CRAN.package.gm.\n\n\nRisset, Jean-Claude, and Max V. Mathews. 1969. “Analysis of Musical-Instrument Tones.” Physics Today 22 (2): 23–30. https://doi.org/10.1063/1.3035399.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html",
    "title": "Fourier Series",
    "section": "",
    "text": "Can Circles do more for us than draw these lovely patterns? Can they give us an alphabet, a universal way of generating and representing many forms of interest? Can we treat them like a bunch of kitchen ingredients, that we throw into a recipe to conjure up new dishes that look different?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#introduction",
    "title": "Fourier Series",
    "section": "",
    "text": "Can Circles do more for us than draw these lovely patterns? Can they give us an alphabet, a universal way of generating and representing many forms of interest? Can we treat them like a bunch of kitchen ingredients, that we throw into a recipe to conjure up new dishes that look different?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#inspiration",
    "title": "Fourier Series",
    "section": "\n Inspiration",
    "text": "Inspiration\nTake a look at these paintings:\n  \nAlso see: https://x.com/jagarikin/status/962449509782495232",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#what-is-the-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#what-is-the-fourier-series",
    "title": "Fourier Series",
    "section": "What is the Fourier Series?",
    "text": "What is the Fourier Series?\n\n\n\n\n\n\nImportant\n\n\n\nA Fourier Series is a way of composing/decomposing a complex waveform into a set of harmonically related sine Oscillations, which are summed up to create the original waveform.\n\n\nIn Circles, we saw how we could make symmetric patterns from rotating circles. We did not have a pattern in mind, except for the symmetry order. So, when we chose number of circles \\(M\\) and their complex amplitudes \\(a_j\\), \\(j={1..M}\\) relying on our (hopefully growing) intuition, we could systematically generate symmetric patterns based on the idea of rolling circles. By trial and error, we can design both the value of \\(M\\) and the values for \\(a_j\\), \\(j={1..M}\\). So far, so good.\nBut how about the other way around? What if we had a pattern in mind, and wanted to compute the circles, their number and amplitudes, that would generate that pattern? This is where the Fourier Series comes in.\nThe best way to form this intuition is to play some of the Wave Game that is available on the University of Colorado PHET Simulations website:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#rolling-circles-and-the-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#rolling-circles-and-the-fourier-series",
    "title": "Fourier Series",
    "section": "Rolling Circles and the Fourier Series",
    "text": "Rolling Circles and the Fourier Series\nBy sliding the amplitudes of various sine Oscillators (whose number you could choose), you were hopefully able to visually create a waveform that looked very close the one on the screen. This was a way of doing waveform synthesis. How did you know, visually speaking, how to set the amplitude?\nA. Correlation of Time waveforms: We adjusted the slider on each sine wave when the selected sine Oscillation that you were manipulating had the best possible correlation with the target waveform!! But how does this correlation work here, with waveforms, instead of data variables?\nWe all know what Pearson Correlations are: we take the product of two (scaled and centered) quantitative variables, value by value, and take the average of these products. With waveforms, we can intuitively do the same thing to determine the coefficient of each component of the Fourier Series:\n\\[\ncoeff~for~sin(\\omega_c*t) = Average~Product \\Big(sin(\\omega_c*t) * target.waveform\\Big)\n\\]\n\\[\n= \\frac{1}{Waveform~Period} * \\displaystyle{\\int}_{0}^{Waveform~Period} sin(\\omega_c*t)*target.waveform * dt\n\\tag{1}\\]\nOK, but how does one make use of these time-waveform correlations?\nB. Orthogonal Waveforms: We need one more concept here: that of “orthogonal waveforms”: these are waveforms whose correlations, as defined above, are zero! But which are these? Our good old sine and cosine waves!!\nWhen we take sine/cosine waves whose frequencies are integer multiples of some base frequency, then all such waveforms are orthogonal.\n\\[\n\\frac{1}{Waveform~Period} * \\int sin(m*\\omega_c*t)*sin(n*\\omega_c*t)* dt = 0\\\\\n\\] \\[\n\\text{where m and n are multiples of some base frequency}\n\\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor more on Orthogonality, see here: https://qr.ae/pATe4W\n\n\nC: “Base Frequency”: So what is this base frequency we have been assuming? It is determined by the target waveform:\n\\[\nBase~ Frequency = \\frac{1}{Period~of~Target~Waveform}\n\\tag{3}\\]",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#how-does-the-fourier-series-compute",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#how-does-the-fourier-series-compute",
    "title": "Fourier Series",
    "section": "How does the Fourier Series Compute?",
    "text": "How does the Fourier Series Compute?\nSo now we are ready to define the steps in computing the Fourier Series:\n\nCompute the base-time-period \\(T\\) of the target waveform, and calculate the base frequency \\(f_c = \\frac{1}{T}\\) using Equation 3.\nTake say \\(M\\) integer multiples of this base frequency (\\(n = 1.....M\\)) and create sine waves with these. These are called harmonics.\nCompute the correlations of each harmonic with the target waveform, as indicated in Equation 1. These are the coefficients (i.e. amplitudes) for each of these harmonics.\nWrite the Fourier Series for the target waveform as:\n\n\\[\ntarget~waveform \\sim \\sum_{i=1}^{M} corr(i)*sin/cos(2\\pi*i*f_c*t)\n\\]",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#videos",
    "title": "Fourier Series",
    "section": "Videos",
    "text": "Videos\nLet us now hear from Dan Schiffman, and also from 3Blue1Brown!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#fourier-series-in-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#fourier-series-in-code",
    "title": "Fourier Series",
    "section": "Fourier Series in Code",
    "text": "Fourier Series in Code\nHow if we just enter a series of numbers, representing our waveform, or pick up sounds off the micrphone, and then make up a Fourier Series for that? We will use pretty much the techique used in creating the rolling circles for the drawing that we saw at first.\n\n\n p5.js\n R",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#wait-but-why",
    "title": "Fourier Series",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nThink of the Fourier Series as a set of sinewaves that are derived by decomposing an original waveform\nHow are these components related? As integer multiples of a fundamental frequency.\nHow are their amplitudes calculated? By taking a correlation between the original waveform and the given sinewave component (unit amplitude)\nHow is this accurate? By minimizing a “least square error” between the original waveform and the sum of sinusoids.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#a-sound-vocabulary",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#a-sound-vocabulary",
    "title": "Fourier Series",
    "section": "\n A Sound Vocabulary",
    "text": "A Sound Vocabulary\nSome terms will show up repeatedly in our work and we should be clear what they mean:\n\n\nOscillation: Any periodic change in amplitude. https://natureofcode.com/oscillation/\n\n\nSinusoid: A Sine Wave Oscillation, created typically with p5.Oscillator\n\n\nWaveform: A graph of amplitude vs time\n\nFrequency: The rate of the oscillation, in cycles per second. Look for a repeating pattern, and measure its time period. \\(1/time.period\\) will give you frequency in Hertz(Hz)\n\nAmplitude: The height, or scaling factor of the oscillation. Easiest to decipher for a simple repeating pattern like sine, square, or triangle.\n\nPhase: The instantaneous angle-position of a rotating vector which generates the wave: Remember the Euler’s Formula. Also the instantaneous angle-value of a repeating wave at a certain amplitude.\n\nHarmonic: A (usually) Sine Oscillation that is at some integer multiple frequency of a reference Sine Oscillation. 2X = octave; 10X = decade.\n\nIn-harmonic: TBW\n\nPartials: TBW\n\nTransient: TBW\n\nAlias: TBW",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#references",
    "title": "Fourier Series",
    "section": "\n References",
    "text": "References\n\nJez Swanson. An Interactive Introduction to Fourier Transforms https://www.jezzamon.com/fourier/index.html\n\nAlex Miller. (2018). Fourier Series and Spinning Circles. https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\n\nBetter Explained. An Interactive Guide to the Fourier Transform. http://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\n\nAatish Bhatia (November 6, 2013). The Math Trick Behind MP3s, JPEGs, and Homer Simpson’s Face. https://nautil.us/the-math-trick-behind-mp3s-jpegs-and-homer-simpsons-face-234629/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#resources",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#resources",
    "title": "Fourier Series",
    "section": "Resources",
    "text": "Resources\n\nhttps://mathlets.org/mathlets/fourier-coefficients/\nWorking with Audio in p5.js. https://pdm.lsupathways.org/3_audio/\n\nViolet Whitney. (Sep 28, 2023) Sounds: Working with sounds and speech in P5.js. https://medium.spatialpixel.com/sounds-bd05429aba38\n\nMister Bomb. p5.Sound project tutorials. https://www.youtube.com/playlist?list=PLIsdHp2z9wFl7A1wWb2VmQUUojEGsKELE\n\nhttps://musiclab.chromeexperiments.com/oscillators\nhttps://www.electronicbeats.net/the-feed/excel-drum-machine/\nhttps://junshern.github.io/algorithmic-music-tutorial/\nhttps://blackwhiskercult.com/visual-music-in-p5-js-i/\nJason Sigal.Visualizing Music with p5.js https://therewasaguy.github.io/p5-music-viz/\n\nDoga Kurkcuoglu. https://bilimneguzellan.net/en/?s=Fourier\n\n\nOther tools to explore\n\nStrudel REPL https://strudel.cc\n\nIntroducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing a tool for everyone to explore the generated samples, as well as the model and code: https://openai.com/index/jukebox/ (OpenAI, April 30, 2020,via Twitter https://twitter.com/OpenAI)\nhttps://algorithmicpattern.org/2023/05/15/strudel-live-coding-patterns-on-the-web/\nhttps://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/\nFreesound: Find Any Sound you Like. https://freesound.org\n\nWebSpeech API. https://developer.chrome.com/blog/voice-driven-web-apps-introduction-to-the-web-speech-api/\n\nhttps://dogbotic.com\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n Right! On to our first little fallibility!\n\n\n Test: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#can-you-see-straight",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#can-you-see-straight",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n Right! On to our first little fallibility!\n\n\n Test: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#discussion",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#discussion",
    "title": "Working With Thoughts",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.\nBias on TV\nLet’s find some of these ideas in our favourite Episode of one Season of your favourite show and tell everybody with a poster!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#bayesian-estimation",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#bayesian-estimation",
    "title": "Working With Thoughts",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTaxicab Accident problem\nDisease Problem\nBaseball score prediction in R ( David Robinson)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#references",
    "title": "Working With Thoughts",
    "section": "References",
    "text": "References\n\nThe Halo Effect, https://explorable.com/halo-effect\nNisbett, R. E., & Wilson, T. D. (1977). The halo effect: Evidence for unconscious alteration of judgments. Journal of Personality and Social Psychology, 35(4), 250–256. https://doi.org/10.1037/0022-3514.35.4.250 Download PDF\nBayesian Thinking Tutorial https://arbital.com/p/bayes_frequency_diagram/?l=55z&pathId=86923\nhttp://ndl.ethernet.edu.et/bitstream/123456789/37455/1/Max_Marchi.pdf"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/listing.html",
    "title": "Uncertainty",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\nWorking with Chance\n\n\n1 min\n\n\n\n\n\n\n\n\n\nWorking With Thoughts\n\n\n6 min\n\n\n\n\n\n\n\n\n\nWorking With Chaos\n\n\n2 min\n\n\n\n\n\n\n\n\n\nWorking with Agents\n\n\n1 min\n\n\n\n\n\n\n\n\n\nWorking with Time\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/20-Chaos/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/20-Chaos/index.html#references",
    "title": "Working With Chaos",
    "section": "References",
    "text": "References\nWebsites\n\nhttps://openprocessing.org\nhttps://timrodenbroeker.de/processing-or-p5/\nCode References\n\nAnimations, coding, interactives in this video by Jonny Hyman 🙌 Try the code yourself: https://github.com/jonnyhyman/Chaos\n\nPapers\n\nMay, R. Simple mathematical models with very complicated dynamics. Nature 261, 459–467 (1976). https://doi.org/10.1038/261459a0\nRobert Shaw, The Dripping Faucet as a Model Chaotic System https://archive.org/details/ShawRober…\nCrevier DW, Meister M. Synchronous period-doubling in flicker vision of salamander and man. J Neurophysiol. 1998 Apr;79(4):1869-78.\nBing Jia, Huaguang Gu, Li Li, Xiaoyan Zhao. Dynamics of period-doubling bifurcation to chaos in the spontaneous neural firing patterns Cogn Neurodyn (2012) 6:89–106 DOI 10.1007/s11571-011-9184-7\nA Garfinkel, ML Spano, WL Ditto, JN Weiss. Controlling cardiac chaos Science 28 Aug 1992: Vol. 257, Issue 5074, pp. 1230-1235 DOI: 10.1126/science.1519060\nR. M. May, D. M. G. Wishart, J. Bray and R. L. Smith Chaos and the Dynamics of Biological Populations Source: Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 413, No. 1844, Dynamical Chaos (Sep. 8, 1987), pp. 27-44\nChialvo, D., Gilmour Jr, R. & Jalife, J. Low dimensional chaos in cardiac tissue. Nature 343, 653–657 (1990). https://doi.org/10.1038/343653a0\nXujun Ye, Kenshi Sakai. A new modified resource budget model for nonlinear dynamics in citrus production. Chaos, Solitons and Fractals 87 (2016) 51–60\nLibchaber, A. & Laroche, C. & Fauve, Stephan. (1982). Period doubling cascade in mercury, a quantitative measurement. http://dx.doi.org/10.1051/jphyslet:01…. 43. 10.1051/jphyslet:01982004307021100."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "We obtained the backpropagated error for each layer:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nAnd the matrix form:\n\\[\ne^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}\n\\tag{1}\\]\nNow what? How do we use all these errors, from the output right up to those backpropagated backwards up to the first (\\(l=1\\)) layer? To adapt the weights of the NN using these backpropagated errors, here are the steps:\n\n\nPer-Weight Cost Gradient: We are looking for something like \\(\\large{\\pmb{\\color{red}{\\frac{dC}{W_{jk}}}}}\\) for all possible combos of \\(jk\\).\n\nLearn: Adapt the Weights in the opposite direction to its Cost-Gradient. (Why?)\n\nAre you ready? ;-D Let us do this !\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{2}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * a^{l}_k * [1- a^{l}_k}]\n\\end{align}\n\\tag{3}\\]\n\nEquation corrected by Adit Joshi and Ananya Krishnan, April 2025\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{4}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 3, and each weight thusly adapted using Equation 4.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#learning-adapting-the-weights",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#learning-adapting-the-weights",
    "title": "Gradient Descent",
    "section": "",
    "text": "We obtained the backpropagated error for each layer:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nAnd the matrix form:\n\\[\ne^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}\n\\tag{1}\\]\nNow what? How do we use all these errors, from the output right up to those backpropagated backwards up to the first (\\(l=1\\)) layer? To adapt the weights of the NN using these backpropagated errors, here are the steps:\n\n\nPer-Weight Cost Gradient: We are looking for something like \\(\\large{\\pmb{\\color{red}{\\frac{dC}{W_{jk}}}}}\\) for all possible combos of \\(jk\\).\n\nLearn: Adapt the Weights in the opposite direction to its Cost-Gradient. (Why?)\n\nAre you ready? ;-D Let us do this !\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{2}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * a^{l}_k * [1- a^{l}_k}]\n\\end{align}\n\\tag{3}\\]\n\nEquation corrected by Adit Joshi and Ananya Krishnan, April 2025\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{4}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 3, and each weight thusly adapted using Equation 4.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#here-comes-the-rain-maths-again",
    "title": "Gradient Descent",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.k.a. 3Blue1Brown.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#gradient-descent-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#gradient-descent-in-code",
    "title": "Gradient Descent",
    "section": "Gradient Descent in Code",
    "text": "Gradient Descent in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#references",
    "title": "Gradient Descent",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html",
    "title": "\n Working with Neural Nets",
    "section": "",
    "text": "(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1: Bharat Natyam Poses",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#introduction",
    "title": "\n Working with Neural Nets",
    "section": "Introduction",
    "text": "Introduction\nOne of our aims with Creative Coding is to of course make things interactive. Here we will apply the ml5.js library in p5.js to use an ML/DL algorithm called Classification to detect human poses in front of the camera. The code can then create unique experiences based on pose-detection with ML, and the subsequent code that responds to the user.\nWe will be following the ideas from here:\nAdavu Detection\nhttps://docs.ml5js.org/#/reference/bodypose\nMudra Detection\nhttps://docs.ml5js.org/#/reference/handpose\nBhava Detection\nhttps://docs.ml5js.org/#/reference/facemesh",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#but-wait-how-does-classification-work",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#but-wait-how-does-classification-work",
    "title": "\n Working with Neural Nets",
    "section": "But Wait! How does Classification Work?",
    "text": "But Wait! How does Classification Work?\nAh, peasants. Isn’t it enough that you can dance?\nSo, we can perform Classification based on Machine Learning (ML) structured and algorithms such as:\n\nRandom Forests. Also see Google Decision Forests. We will try to get an intuition into bootstrapping of variables in data, creating decision trees, and making random selections of variables from a dataset to create random forests.\n\nAnd there are Deep Learning (DL) structured and algorithms that allow us to do the same things, perhaps in a more “black-box” manner. We will peep into:\n\nThe Perceptron\nThe Multilayer Perceptron\nBackpropagation\nGradient Descent\n\nConvolutional Neural Nets (in a later course)\n\n\n\nHere, we will also try to build an intuitive sense of some of the technical terminology involved: convolution, regression, activation, weighting…and such terms that generally elude peasants.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#wait-but-why",
    "title": "\n Working with Neural Nets",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nUnderstanding the underlying math inside of Neural Nets can help us appreciate better how to apply them design with them, and even keep them as simple as needed.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#how-to-train-your-dragon-neural-network",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#how-to-train-your-dragon-neural-network",
    "title": "\n Working with Neural Nets",
    "section": "How to Train your Dragon Neural Network",
    "text": "How to Train your Dragon Neural Network",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#references",
    "title": "\n Working with Neural Nets",
    "section": "\n References",
    "text": "References\n\nColah’s Blog.(Apr 6, 2014). Neural Networks, Manifolds, and Topology. https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/. Very simple and readable article.\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\nhttps://developers.google.com/machine-learning.https://developers.google.com/machine-learning\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\n\nConvolutional Neural Networks\n\n\nDigit Recognition with CNNs. Interactive! https://transcranial.github.io/keras-js/#/mnist-cnn\n\n\nCNN Convoluter. https://pwwang.github.io/cnn-convoluter/\n\n\nCNN Explainer: Learn Convolutional Neural Network (CNN) in your browser!. https://poloclub.github.io/cnn-explainer/\n\nDeep Lizard. Understanding Convolution Operations in Neural Networks. https://deeplizard.com/resource/pavq7noze2\n\nAndrej Karpathy. ConvNetJS: Deep Learning in your browser.https://cs.stanford.edu/people/karpathy/convnetjs/\n\nAdit Deshpande. A Beginner’s Guide to CNNs. https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\nNeural Networks Visual with vcubingx\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\n\nLLMs\n\nBrendan Bycroft.Visualizing LLMs. https://bbycroft.net/llm\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nAI-powered reporting and annotation for radiology. https://www.md.ai\n\nUsing R for DL\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nTorch Interactive Tutorial. https://mlverse.shinyapps.io/torch-tour\n\nGeeks for Geeks. Convolutional Neural Nets in R. https://www.geeksforgeeks.org/convolutional-neural-networks-cnns-in-r/\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nAnder Fernandez Jauregui. https://anderfernandez.com/en/blog/how-to-create-neural-networks-with-torch-in-r/\n\nhttps://f0nzie.github.io/rtorch-minimal-book/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#textbooks",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#textbooks",
    "title": "\n Working with Neural Nets",
    "section": "Textbooks",
    "text": "Textbooks\n\nMichael Nielsen. Neural Networks and Deep Learning. Available Online\n\nThe Little Book of Deep Learning. Available Online\n\nSimone Scardapane. Alice’s Adventures in Diffferentiable WonderLand: A Primer on Designing Neural Networks. https://www.sscardapane.it/alice-book/\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nZhang, Lipton, Li, Smola. Dive into Deep Learning. https://www.d2l.ai/\n\nSigrid Keydana. Deep Learning and Scientific Computing with R torch https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\nAllaire and Chollet (2024)\n\n\nsafetensors\n0.1.2\nFalbel (2023)\n\n\ntensorflow\n2.16.0.9000\nAllaire and Tang (2025)\n\n\ntorch\n0.15.1\nFalbel and Luraschi (2025)\n\n\n\n\n\n\nAllaire, JJ, and François Chollet. 2024. keras: R Interface to “Keras”. https://doi.org/10.32614/CRAN.package.keras.\n\n\nAllaire, JJ, and Yuan Tang. 2025. tensorflow: R Interface to “TensorFlow”. https://github.com/rstudio/tensorflow.\n\n\nFalbel, Daniel. 2023. safetensors: Safetensors File Format. https://doi.org/10.32614/CRAN.package.safetensors.\n\n\nFalbel, Daniel, and Javier Luraschi. 2025. torch: Tensors and Neural Networks with “GPU” Acceleration. https://doi.org/10.32614/CRAN.package.torch.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/listing.html",
    "title": "AI",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n  Working with Neural Nets\n\n\n9 min\n\n\n\n\n\n\nThe Perceptron\n\n\n12 min\n\n\n\n\n\n\nThe Multilayer Perceptron\n\n\n17 min\n\n\n\n\n\n\nMLPs and Backpropagation\n\n\n16 min\n\n\n\n\n\n\nGradient Descent\n\n\n12 min\n\n\n\n\n\n\nAI by Hand\n\n\n12 min\n\n\n\n\n\n\nConvolutional Neural Nets\n\n\n9 min\n\n\n\n\n\n\nLarge Language Models\n\n\n7 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "AI"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html",
    "title": "AI by Hand",
    "section": "",
    "text": "Let us head off to this website and build a small Neural network by hand.\nhttps://student.desmos.com/join/6x8gme\nEvery time we give the network a new “sketch” to train with, potentially all network connections are updated, using backpropagation.\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{1}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_k} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\n\\end{align}\n\\tag{2}\\]\n\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{3}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 2, and each weight thusly adapted using Equation 3."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#training-the-neural-network",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#training-the-neural-network",
    "title": "AI by Hand",
    "section": "",
    "text": "Let us head off to this website and build a small Neural network by hand.\nhttps://student.desmos.com/join/6x8gme\nEvery time we give the network a new “sketch” to train with, potentially all network connections are updated, using backpropagation.\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{1}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_k} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\n\\end{align}\n\\tag{2}\\]\n\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{3}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 2, and each weight thusly adapted using Equation 3."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#here-comes-the-rain-maths-again",
    "title": "AI by Hand",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.k.a. 3Blue1Brown."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#gradient-descent-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#gradient-descent-in-code",
    "title": "AI by Hand",
    "section": "Gradient Descent in Code",
    "text": "Gradient Descent in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#references",
    "title": "AI by Hand",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-networks",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-networks",
    "title": "Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nWhat is Convolution?\nConsider that you life in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nYou might consider that a CNN has several such echo mechanisms operating. Each pixel value (if you are dealing with image input) goes through a series of such delayed weightings which multiply the pixel input. The output of each pixel contributes in such fashion to the activation of that unit/layer of the CNN.\n\nHow does a CNN Structure use “Convolution”?\nLet us contemplate the structure of a CNN."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#videos",
    "title": "Convolutional Neural Nets",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-nets-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-nets-in-code",
    "title": "Convolutional Neural Nets",
    "section": "Convolutional Neural Nets in Code",
    "text": "Convolutional Neural Nets in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#wait-but-why",
    "title": "Convolutional Neural Nets",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#references",
    "title": "Convolutional Neural Nets",
    "section": "\n References",
    "text": "References\n\n\nDigit Recognition with CNNs. Interactive! https://transcranial.github.io/keras-js/#/mnist-cnn\n\n\nCNN Convoluter. https://pwwang.github.io/cnn-convoluter/\n\n\nCNN Explainer: Learn Convolutional Neural Network (CNN) in your browser!. https://poloclub.github.io/cnn-explainer/\n\nDeep Lizard. Understanding Convolution Operations in Neural Networks. https://deeplizard.com/resource/pavq7noze2\n\nColah’s Blog.(Apr 6, 2014). Neural Networks, Manifolds, and Topology. https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/. Very simple and readable article.\nAndrej Karpathy. ConvNetJS:Deep Learning in your browser.https://cs.stanford.edu/people/karpathy/convnetjs/\n\nhttps://developers.google.com/machine-learning.https://developers.google.com/machine-learning\n\nAdit Deshpande. A Beginner’s Guide to CNNs. https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\nNeural Networks Visual with vcubingx\n\n\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nNeural Network Playground. https://playground.tensorflow.org/\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. https://neuralnetworksanddeeplearning.com/index.html\n\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland. https://www.sscardapane.it/alice-book/\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nThe Little Book of Deep Learning. Available Online\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nZhang, Lipton, Li, Smola. Dive into Deep Learning. https://www.d2l.ai/\n\nLLMs\n\nBrendan Bycroft.Visualizing LLMs. https://bbycroft.net/llm\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nAI-powered reporting and annotation for radiology. https://www.md.ai\n\nCarl T. Bergstrom and Jevin D. West. Modern-Day Oracles or Bullshit Machines? https://thebullshitmachines.com/index.html\n\nUsing R for DL\n\nGeeks for Geeks. Convolutional Neural Nets in R. https://www.geeksforgeeks.org/convolutional-neural-networks-cnns-in-r/\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nAnder Fernandez Jauregui. https://anderfernandez.com/en/blog/how-to-create-neural-networks-with-torch-in-r/\n\nhttps://f0nzie.github.io/rtorch-minimal-book/\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\n@keras\n\n\nsafetensors\n0.1.2\n@safetensors\n\n\ntensorflow\n2.16.0.9000\n@tensorflow"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html",
    "title": "What is Convolution?",
    "section": "",
    "text": "Consider that you live in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered/weighted form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nWhat you hear is the overlapping of multiple, weighted copies of the sound emitted by the ambulance. As long as you have direct, i.e. non-reflected path from the ambulance to your ears, the echoes are relatively subdued. Once the vehicle gets right into your building complex and you lose the direct line-of-sight path, the echoes take over and the sound becomes a very confused mass that is barely recognizable.\n\nAll right, what does this have to do with convolution? Let us make some definitions first:\n\n\n\n\n\n\nImportantChannel\n\n\n\nThe free-space medium plus the buildings and other things that reflect sound in our environment, are called the “Channel”. The channel ascts as a conduit between a source (transmitter) and a receiver.\n\n\n\n\n\n\n\n\nImportantImpulse Response of the Channel\n\n\n\nThe geometry of the echoes that connect transmitter to receiver, including the bounces of the walls, the resulting path-delays, and weighting are together denoted as the impulse response of the channel. This is what the channel would put out at the receiver if the source transmitter were to emit a very-short-duration signal, like the squeak of a mouse.\n\n\nNow, most signals emitted by a source are usually not “squeak-like”: the ambulance has a siren that continuously emits the wellknown sound. Such a continuous signal is capable of mathematically decomposed into a series of “squeak-like” signals, which we call impulses.\nSo finally:\n\n\n\n\n\n\nImportantWhat is Convolution?\n\n\n\nEach impulse undergoes the same geometry path-delays and path-weightings posed by the channel impulse response. This is diagrammatically shown below:\n\n\n\n\n\nFigure 1: Convolution\n\n\nWe see that impulses in the input waveform that arrive later undergo wieghting by the earliest of path-delays and path-weightings. This should give you an intuition, that mathematically, this is like taking a weighted average but with the sequence of weights inverted in time!!!\nIf \\(in(t)\\) is the emitted sound waveform, and \\(f(t)\\) is the channel impulse response, we write the output of the channel as:\n\\[\n\\Large{out(t) = \\int_{-\\infty}^{\\infty} in(t) * f(t-\\tau) *d\\tau}\n\\tag{1}\\]\nNote that we are integrating wrt delay \\(\\tau\\); and \\(f\\) uses negative \\(\\tau\\) as its variable. Hence it is hence inverted in time, as shown in the bottom left of the Figure 1."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#inspiration",
    "title": "What is Convolution?",
    "section": "",
    "text": "Consider that you live in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered/weighted form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nWhat you hear is the overlapping of multiple, weighted copies of the sound emitted by the ambulance. As long as you have direct, i.e. non-reflected path from the ambulance to your ears, the echoes are relatively subdued. Once the vehicle gets right into your building complex and you lose the direct line-of-sight path, the echoes take over and the sound becomes a very confused mass that is barely recognizable.\n\nAll right, what does this have to do with convolution? Let us make some definitions first:\n\n\n\n\n\n\nImportantChannel\n\n\n\nThe free-space medium plus the buildings and other things that reflect sound in our environment, are called the “Channel”. The channel ascts as a conduit between a source (transmitter) and a receiver.\n\n\n\n\n\n\n\n\nImportantImpulse Response of the Channel\n\n\n\nThe geometry of the echoes that connect transmitter to receiver, including the bounces of the walls, the resulting path-delays, and weighting are together denoted as the impulse response of the channel. This is what the channel would put out at the receiver if the source transmitter were to emit a very-short-duration signal, like the squeak of a mouse.\n\n\nNow, most signals emitted by a source are usually not “squeak-like”: the ambulance has a siren that continuously emits the wellknown sound. Such a continuous signal is capable of mathematically decomposed into a series of “squeak-like” signals, which we call impulses.\nSo finally:\n\n\n\n\n\n\nImportantWhat is Convolution?\n\n\n\nEach impulse undergoes the same geometry path-delays and path-weightings posed by the channel impulse response. This is diagrammatically shown below:\n\n\n\n\n\nFigure 1: Convolution\n\n\nWe see that impulses in the input waveform that arrive later undergo wieghting by the earliest of path-delays and path-weightings. This should give you an intuition, that mathematically, this is like taking a weighted average but with the sequence of weights inverted in time!!!\nIf \\(in(t)\\) is the emitted sound waveform, and \\(f(t)\\) is the channel impulse response, we write the output of the channel as:\n\\[\n\\Large{out(t) = \\int_{-\\infty}^{\\infty} in(t) * f(t-\\tau) *d\\tau}\n\\tag{1}\\]\nNote that we are integrating wrt delay \\(\\tau\\); and \\(f\\) uses negative \\(\\tau\\) as its variable. Hence it is hence inverted in time, as shown in the bottom left of the Figure 1."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#convolution-in-code",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#convolution-in-code",
    "title": "What is Convolution?",
    "section": "Convolution in Code",
    "text": "Convolution in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nWe’ll see"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#wait-but-why",
    "title": "What is Convolution?",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\n\nPerceptrons are a standard convolution operation.\nConvolution is an operation that is crucial to the operation of Convolutional Neural Networks. The early (spatial) filter layers in a CNN implement a convolution with impulse responses that learn to look for edges, curves and similar canonical pieces in an input image.\nWhen we generate guitar-like sounds using the Karplus-Strong Guitar Algorithm, we are using a set of filters (with low-pass/band-pass impulse responses) in the feedback loop of a delay-line primed with random noise.\nConvolution can be seen as a series of Vector Dot Products between two vectors sliding past each other."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#references",
    "title": "What is Convolution?",
    "section": "\n References",
    "text": "References\nTo be Written Up.\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\n@keras\n\n\nsafetensors\n0.1.2\n@safetensors\n\n\ntensorflow\n2.16.0.9000\n@tensorflow"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/listing.html",
    "title": "Connections",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\nWorking with Networks\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/listing.html",
    "title": "Physics",
    "section": "",
    "text": "No matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Here are projects that we will contemplate creating at the end of the course. All the techniques, software, packages, theory, hardware…will be pieces aimed the realization of these projects. So we begin this course with the end in mind.\nThe projects below have been grouped into two groups, one for the junior students and the other senior students, who are both paradoxically in the same class. But there is an opinion that prerequisites are irrelevant.\n\n\nDo not start with fundamentals. This is an awful approach to learning. Start with so-called \"advanced\" topics and ask questions until every term/concept is understood. This is the correct, rigorous, scientific way to learn, because the advanced topics are embedded in larger,…\n\n— Sean McClure (@sean_a_mcclure) February 27, 2025",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#introduction",
    "title": "Projects",
    "section": "",
    "text": "Here are projects that we will contemplate creating at the end of the course. All the techniques, software, packages, theory, hardware…will be pieces aimed the realization of these projects. So we begin this course with the end in mind.\nThe projects below have been grouped into two groups, one for the junior students and the other senior students, who are both paradoxically in the same class. But there is an opinion that prerequisites are irrelevant.\n\n\nDo not start with fundamentals. This is an awful approach to learning. Start with so-called \"advanced\" topics and ask questions until every term/concept is understood. This is the correct, rigorous, scientific way to learn, because the advanced topics are embedded in larger,…\n\n— Sean McClure (@sean_a_mcclure) February 27, 2025",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j0-make-an-ad-for-a-wellknown-logo",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j0-make-an-ad-for-a-wellknown-logo",
    "title": "Projects",
    "section": "Project #J0: Make an ad for a wellknown logo",
    "text": "Project #J0: Make an ad for a wellknown logo",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j1",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j1",
    "title": "Projects",
    "section": "Project #J1:",
    "text": "Project #J1:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-1-kingqueen-of-the-mountain",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-1-kingqueen-of-the-mountain",
    "title": "Projects",
    "section": "Project #1: King/Queen of the Mountain",
    "text": "Project #1: King/Queen of the Mountain\n\nDescription\n\nEach participant in the exhibit chooses whether they want to be on a mountain or in a valley, or in a grassland/meadow. More choices if you are up to it…volcano, anyone?\nTheir position within the space will be sensed. And they can move.\nA terrain map will be projected, either on the ceiling, or the screen, or on the participants themselves, with each individual being located on a specific terrain feature type that they chose at the start.\n\n\n\nPieces\n\nProject #1\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nTerrain Generation\n\np5.js/ Perlin Noise\n\n\nTerrain!\n\n\n\nLocation Sensing\n\nOSC protocol?\nArduino sensors?\nLink p5.js to Arduino, need libraries for interfacing and data transfer\n\n\nGive seed locations to Perlin Noise generator\nDefines preferences of terrain feature based in identity\n\n\n\nProjection\n\nProject terrain map\nDrive Projector using p5.js\nDo we need projection mapping libraries?\n\n\nWatch participants move and terrain follows them",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-2-sound-relationships",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-2-sound-relationships",
    "title": "Projects",
    "section": "Project #2: Sound Relationships",
    "text": "Project #2: Sound Relationships\n\nDescription\n\nThe floor is divided into footprint-sized areas. Each area is mapped to a particular note in a musical scale, that can be selected. Multi-octave.\nParticipants are told of the mapping and given a smartphone. And they can play hopscotch or dance or move, in sync with other participants to create a jam session.\nCan they make music / rhythms?\nThey are given headphones?\n\n\n\nPieces\n\nProject #2\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nSound Generation\n\np5.js with Sound/Music Libraries\nDo any of these SuperCollider? PureData? Strudel? JukeBox AI? have a library for p5.js?\n\n\nInstruments\nVocals?\n\n\n\nLocation Sensing\n\nOSC protocol?\nArduino sensors?\nLink p5.js to Arduino, need libraries for interfacing and data transfer\n\n\nJust location sensing\n\n\n\nProjection\n\nProject Network and musical Note?\nDrive Projector using p5.js\nDo we need projection mapping libraries?\n\n\nNetwork links must decay when participants are too far apart, or are silent…pauses in music",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-3a-and-3b-the-100-acre-wood-with-winnie-the-pooh",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-3a-and-3b-the-100-acre-wood-with-winnie-the-pooh",
    "title": "Projects",
    "section": "Project #3A and #3B: The 100-Acre Wood with Winnie the Pooh",
    "text": "Project #3A and #3B: The 100-Acre Wood with Winnie the Pooh\n\nDescription\n\nCreate natural shapes / images / illustrations / characters based on kolams/ lu-sona / fractals / L-Systems and graphs.\nA: Use grids, smoke, and mirrors to create laser diagrams in a table-top maze.\n\nPointer laser? Smoke? Agarbatti?\nMovable mirrors ( 0/90 degrees only so black paper clips should work.)\nSingle Point of entry for laser.\nLook from above through the smoke.\n\nB: Viewers can draw their illustration on a large paper canvas to create a Hundred Acre Wood\n\nPaper and Pencil / Colour / etc\nCheck with Code and replicate by hand.\nCatalogue of Shapes is essential\nGrid, Kolam Choice for shape, changes, innovation are possible\n\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nShape Generation\n\np5.js\nGrids + Mirror locations\nL Systems with random user-chosen seeds - Catalogue of shapes to choose from? In software? Poster also.\nNot too much code\n\n\nViewers can match their personality/choice and what is on the garden-wall already to decide on their Creature\nLaser + Smoke to check the shape if it is a Kolam / Lu-Sona\n\n\n\nHardware\n\nSmoke and Mirrors\nPointer Laser",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-kagazi-hai-pairhan",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-kagazi-hai-pairhan",
    "title": "Projects",
    "section": "Project #4: Kagazi hai Pairhan",
    "text": "Project #4: Kagazi hai Pairhan\n\n\n“Naqsh fariyādī hai kis kī shoḳhi-e-tahrīr kā Kāġhzī hai pairahan har paikar-e-tasvīr kā”\n— Ghalib\n\n\n\nDescription\n\nDress/Clothing design with paper\nParticipants will turn and pirouette/turn about in front of a camera\nMeasurements will be taken and modelled using non-Euclidean geometry\nPaper cutout diagrams which can be fabricated to “dress up” the viewer in paper.\nPaper can have their favourite theorem or piece of text printed upon it!\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nMeasurements of Fixed Points on/from Camera Image\n\nml5.js Pose Detection?\nMust get hold of Pose Detection Data and convert into “printed dress” pieces\nChoice of Clothing??\n\n\nClothing Design\n\n\n\nMath Model of Connecting Points with Hyperbolic Geometry Functions\n\nOther ways than ml5.js??\nImage to Geometry libraries?\n\n\nAllow for fit and drape\nShould not be too tight\n“Extra” Cloth to stitch with\nAll paper must be A4?\nEntire costume must be possible to carry in a file\nCan be “Assembled any time with tape/ pins/clips",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-my-face-is-a-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-my-face-is-a-fourier-series",
    "title": "Projects",
    "section": "Project #4: My Face is a Fourier Series",
    "text": "Project #4: My Face is a Fourier Series\nSee this:\nhttps://gofigure.impara.ai/drawing/5711774133256192\n\nPeople stand in front of camera\nDetailed picture is taken. Profile preferable!\nThe code will generate a Fourier Series of their profile and present an animated version on the screen.\nReset and Play should be possible\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nMeasurements of Fixed Points on/from Camera Image\n\nml5.js?\nProfile Detection and conversion to waveform\n\n\nCapturing Profile “waveform”\n\n\n\nGenerating Fourier Series with Waveform + Animation\n\nMath Library\nShould be easily available\n\n\nAnimation",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-5-music-by-hand-waving",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-5-music-by-hand-waving",
    "title": "Projects",
    "section": "Project #5: Music by Hand-Waving",
    "text": "Project #5: Music by Hand-Waving",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#other-project-ideas",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#other-project-ideas",
    "title": "Projects",
    "section": "Other Project Ideas",
    "text": "Other Project Ideas\n\nLink IFTTT app to p5.js and make things happen over WiFi, from other classes or buildings",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html",
    "title": "Kolams and Lusona",
    "section": "",
    "text": "Look at this fabric map of Africa:\n\n\n\n\n\nFigure 1: Africa Fabric Map: What’s your next T-Shirt?\n\n\nAnd look at this map of Nobel prize winners!\n\n\n\n\n\nFigure 2: Network of Nobel Prize Winners\n\n\nWould there be anything in common between these two 😮 ??!! How???!!!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration",
    "title": "Kolams and Lusona",
    "section": "",
    "text": "Look at this fabric map of Africa:\n\n\n\n\n\nFigure 1: Africa Fabric Map: What’s your next T-Shirt?\n\n\nAnd look at this map of Nobel prize winners!\n\n\n\n\n\nFigure 2: Network of Nobel Prize Winners\n\n\nWould there be anything in common between these two 😮 ??!! How???!!!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#introduction",
    "title": "Kolams and Lusona",
    "section": "Introduction",
    "text": "Introduction\nThe South Indian tradition of Kolam, and the Angolan tradition of Lusona art have quite a few things in common. Both are also strongly linked to finite grammars and networks.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration-1",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration-1",
    "title": "Kolams and Lusona",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\nClick to generate a fresh Kolam!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#creating-kolams",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#creating-kolams",
    "title": "Kolams and Lusona",
    "section": "Creating Kolams",
    "text": "Creating Kolams\nHow do we create these Kolam Patterns? Let us do this two ways: first pretending we are a South-Indian Woman adorning her doorstep in the morning. And then with two other methods, that lend themselves to computation / iteration. So, first by hand!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#by-hand",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#by-hand",
    "title": "Kolams and Lusona",
    "section": "By Hand!",
    "text": "By Hand!\nSee if you can manually mimic some of the moves here! As an exercise, try to anchor your elbow and forearm to the table, and draw the pattern by rotating the paper! What are your observations?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod #1: Using Canonical Pieces\n\nWhat does canonical piece mean? These are pieces like the alphabet: pieces that can be repeatedly used to create a vast variety of patterns? Sounds familiar again?\nCheck the Polypad: https://polypad.amplify.com/p#patterns\nHere we use “pieces of Kolam” that are standard: by repeated usage of combinations of these pieces, ( I believe ) any kolam can be produced. Here is a video showing kolams with a few canonical pieces:\n\nWhich are the canonical pieces here?\nThis is also the idea embedded in this toy called Kolam Tiles. See this YT Playlist on Kolam Tiles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod #2: Using Mirrors and Light\nThis follows the development of Paul Gerdes.\nFirst let us get a printable grid to make our Kolams manually, since making grids can become tedious when you are making a lot of Kolams. We can use the grid to place “pulli-s” on the grid to make our Kolam. Head off to: https://editor.p5js.org/arvindv/sketches/UuHApkvqd and open it in your p5.js web-editor. Print out a few samples of the .svg grid file that is generated.\nNow consider that each of your Kolam “lines” or “trajectories” is made of light. And place some single horizontal or vertical mirrors, at some locations midway between adjacent pulli-s. See the figure below:\n\n\n\n\n\nFigure 3: Kolam with Mirrors\n\n\nThe black lines here are to be imagined as “made of light”. Whenever they hit a mirror, a “curved reflection” occurs. Note how the arrangement of mirrors is symmetric here. Can we take computational liberties here and make asymmetric mirror arrangements? Can the grid also be non-square? Try?\nFor more inspiration, see here. This is a multipage article with many different grid+mirror arrangements! There is also an intriguing technique shown therein of colouring the squares in the grid alternatively white and black, to generate very symmetric shaded patterns!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#kolams-with-code",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#kolams-with-code",
    "title": "Kolams and Lusona",
    "section": "Kolams with Code",
    "text": "Kolams with Code\nWork in (slow….) Progress!!!\n\n\nUsing p5.js\nUsing R\n\n\n\n\n&lt;iframe width=“780px” height=“600px”",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#wait-but-why",
    "title": "Kolams and Lusona",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nKolams and Sona are powerful metaphors for graphs and networks. These ideas show up in a variety of situations, such as tranportation networks, supply-chain, friendship networks, tracing literary and artistic influence, and so on.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#references",
    "title": "Kolams and Lusona",
    "section": "\n References",
    "text": "References\n\nSome kolam like patterns for inspiration. https://www.pinterest.com/gbenainous/p5js-teaching-ideas/\n\nDr. Gift Siromoney’s webpage. https://www.cmi.ac.in/gift/Kolam.htm\n\nMirror Designs and Mirror Curves: Comparing Kolam and Tchokwe Art. https://www.mi.sanu.ac.rs/vismath/paulus/pg1.htm\n\nYANAGISAWA, Kiwamu & Nagata, Shojiro. (2007). Fundamental Study on Design System of Kolam Pattern.https://www.researchgate.net/publication/237442288_Fundamental_Study_on_Design_System_of_Kolam_Pattern)\nPaulus Gerdes. Lunda Geometry: Mirror Curves, Designs, Knots, Polyominoes, Patterns, Symmetries. https://www.sahistory.org.za/sites/default/files/archive-files3/paulus_gerdes_lunda_geometry_mirror_curves_desbook4you.pdf\n\nVisual Mathematics. Mathematical Institute of the Serbian Academy of Sciences and Arts. Editor: Ljiljana Radovic. ISSN: 1821-1437. https://www.mi.sanu.ac.rs/vismath/\n\nImaginary.Org. Frozen Light App. https://www.imaginary.org/program/frozenlight\n\nhttps://kolamtiles.com\nAnu Reddy and Alex McLean.(March 2024). Drawing Kolam Patterns in Stitches and Code. https://alpaca.pubpub.org/pub/eljjyi80/release/6\n\nhttps://algorithmicpattern.org\nAscher, M. (2002). The Kolam Tradition: A tradition of figure-drawing in southern India expresses mathematical ideas and has attracted the attention of computer science. American Scientist, 90(1), 56+. https://link.gale.com/apps/doc/A81528419/AONE?u=anon\\~274c1208&sid=googleScholar&xid=4105e718",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html",
    "title": "\n Circles",
    "section": "",
    "text": "Let us start with an investigation into rolling circles! Circles have been with us since our childhood toys and to our older (and more silly!) aspirations for wheels (ahem!). Let us understand their mathematics and see what we can make with them.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#introduction",
    "title": "\n Circles",
    "section": "",
    "text": "Let us start with an investigation into rolling circles! Circles have been with us since our childhood toys and to our older (and more silly!) aspirations for wheels (ahem!). Let us understand their mathematics and see what we can make with them.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#inspiration",
    "title": "\n Circles",
    "section": "\n Inspiration",
    "text": "Inspiration\n\nShow the Codet &lt;- seq(0, 2 * pi, by = 0.001)\nx &lt;- t\ndata &lt;- tibble::tibble(t,\n  x = cos(t) + cos(6 * t) / 2 + sin(14 * t) / 3,\n  y = sin(t) + sin(6 * t) / 2 + cos(14 * t) / 3\n)\n\ndata %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\nFigure 1: Mystery Curve",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#what-is-a-parametric-equation",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#what-is-a-parametric-equation",
    "title": "\n Circles",
    "section": "What is a Parametric Equation?",
    "text": "What is a Parametric Equation?\nHow was this curve created? The equation for the curve is given by a pair of parametric equations, one for \\(x\\) and one for \\(y\\):\n\\[\n\\begin{eqnarray}\nx &= cos(t) + cos(6t)/2 + sin(14t)/3\\\\\ny &= sin(t) + sin(6t)/2 + cos(14t)/3\n\\end{eqnarray}\n\\tag{1}\\]\nThis form is especially suited for a computational depiction of the curve, since we can have the parameter \\(t\\) go from \\(0~ -&gt;\\infty\\) and let the \\(x\\) and \\(y\\) be computed and plotted. All right, whatever…but what does this have to do with circles?? For that we need to turn to the famous Euler formula relating complex vectors and circles.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#how-about-the-euler-formula",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#how-about-the-euler-formula",
    "title": "\n Circles",
    "section": "How about the Euler Formula?",
    "text": "How about the Euler Formula?\nWhat is the equation of a circle? Most likely you will say:\n\\[\n\\begin{eqnarray}\nx^2 + y^2 &= 1\\\\\nor ~ perhaps\\\\\n(𝑥 − ℎ)^2 + (𝑦 − 𝑘)^2 &= 𝑅^2\\\\\n\\end{eqnarray}\n\\tag{2}\\]\nfor a circle with center \\(C(h,k)\\) and radius R.\nAs Frank Farris says, this is fine, but it represents a static view of a circle, which is not the simplest way to direct the drawing of one. The simplest way to instruct a machine to draw a circle uses a parametric form discussed above, also known as a vector-valued function:\n\\[\n\\gamma(t) = (cos(t), sin(t))\n\\] for the unit circle and\n\\[\n𝛾(𝑡) = (h + 𝑅 cos(𝑡), k + 𝑅 sin(𝑡))\n\\] for a more general one.\nNow, if we were to use complex numbers as our notation, then the function for our circle becomes:\n\\[\n\\begin{eqnarray}\n\\gamma(t) &=& (cos(t), sin(t))\\\\\n&=& e^{it}\n\\end{eqnarray}\n\\tag{3}\\]\nwhere of course, \\(i = \\sqrt{-1}\\).\nThis is the famous Euler Formula that connects complex numbers with trigonometry.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#the-mystery-curve",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#the-mystery-curve",
    "title": "\n Circles",
    "section": "The Mystery Curve",
    "text": "The Mystery Curve\nUsing this formula, our parametric function \\(\\mu(t)\\) for our mystery curve becomes a family of three circles, of different sizes and rotating at different speeds:\n\\[\n\\begin{eqnarray}\nx &= cos(t) + cos(6t)/2 + sin(14t)/3\\\\\ny &= sin(t) + sin(6t)/2 + cos(14t)/3\\\\\n\\end{eqnarray}\n\\]\nand\n\\[\n\\mu(t) = {\\large{\\color{hotpink}{1}} * {e^{\\color{Blue}{\\Large\\pmb{1it}}}}} +\n{\\large{\\color{hotpink}{\\frac{1}{2}}} * {e^{\\color{Blue}{\\Large{\\pmb{6it}}}}}} +\n{\\large{\\color{hotpink}{\\frac{i}{3}}} * {e^{\\color{Blue}{\\Large{\\pmb{-14it}}}}}}\n\\tag{4}\\]\nwhich gives us three rotating vectors with amplitudes given by {1, 1/2, 1/3} and with rotation speeds in the ratio {1 : 6: -14}. The first two rotate counter-clockwise; the third vector rotates in the clockwise direction since we have a negative coefficient for \\(t\\)). The tips of these rotating vectors course trace out the individual circles. We can picture the pattern as the vector sum of the vectors, or as three circles where each subsequent circle rotates and rolls on the circumference of the earlier one, like meshed gears.\n\n\nFrom The Math Less Travelled Blog\n\nHow do we go from the parametric form in Equation 1 to the complex exponential form in Equation 4? The first two terms are direct combinings of the respective cos and sin terms into exponentials; the third term may need a bit of understanding.\nHere the \\(sin\\) and \\(cos\\) terms are “interchanged” between x and y, so we need multiply by \\(i\\) (rotate by \\(\\pi/2\\)) to swap them, which means that the third circle starts from a 90 degree angle compared to the other two. Multiplying by \\(i\\) however makes the \\(sin\\) term negative, so we need to negate t as well, since \\(-sin(-t) = sin(t)\\). This means that the third exponential rotates in the opposite direction compared to the first two. See the expansion / explanation in the margin. We discuss this more in the following.\n\n\n\\[\n\\begin{eqnarray}\n\\frac{i}{3}*e^{-i14t} &=& \\frac{i}{3} \\Big\\{cos(-14t) + i(sin(-14t) \\Big\\}\\\\\n&=& \\frac{i}{3} \\Big\\{cos(14t) - i*sin(14t)\\Big\\}\\\\\n&=& \\frac{1}{3} \\Big\\{i*cos(14t) + sin(14t)\\Big\\}\\\\\n&=& \\frac{1}{3} \\Big\\{sin(14t) + i*cos(14t)\\Big\\}\\\\\n\\end{eqnarray}\n\\]\nwhich are respectively the desired x and y parametric functions for the third term.\n\n\n\n\n\n\nNoteReverse Rotating Vectors and Complex Amplitudes!!\n\n\n\nSigh.\n\n\n\n\n\n\n\n\n\n\n\nAmplitude\nRotation\nExample\nOperation in Parametric Equation\nWhat does it mean, really?\n\n\n\nReal\nPositive/CCW\n\\(2*e^{3it}\\)\n\n\\(x = 2*cos(3t)\\); \\(y=2*sin(3t)\\)\n\nVector starts from x-axis, goes CCW\n\n\nReal\nNegative/CW\n\\(2*e^{-3it}\\)\n\n\\(x = 2*cos(3t)\\); \\(y=-2*sin(3t)\\)\n\nVector starts from x-axis, goes CW\n\n\nComplex\nPositive/CCW\n\\(2*i*e^{3it}\\)\n\n\\(x = -2*sin(3t)\\); \\(y=2*cos(3t)\\)\n\nVector starts at \\(\\pi/2\\), goes CCW\n\n\nComplex\nNegative/CW\n\\(2*i*e^{-3it}\\)\n\n\\(x = 2*sin(3t)\\); \\(y=2*cos(3t)\\)\n\nVector starts at \\(\\pi/2\\), goes CW\n\n\n\n\n\nTable 1: Rotating Complex Exponentials and their Complex Amplitudes\n\n\n\n\nDo think of what might happen when the amplitude has an overall negative sign, like \\(-2*e^{-3it}\\) or \\(-2i*e^{-3it}\\)! (Gasp!! Swoon…). Just flip the vector on its head and rotate the same way as stated.\nPlotting with Complex Exponentials in R\nWe can use the rules in the above table to directly plot using complex vectors in R:\n\nShow the Codef_mystery1 &lt;- function(x) {\n  (exp((0 + 1i) * x) +\n    0.5 * exp((0 + 6i) * x) +\n    1 / 3 * 1i * exp((0 - 14i) * x)) # Note the \"1i\"!\n}\n\ndata_mystery_1 &lt;- tibble(t, pattern = f_mystery1(t))\n\ndata_mystery_1 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#rotational-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#rotational-symmetry",
    "title": "\n Circles",
    "section": "Rotational Symmetry",
    "text": "Rotational Symmetry\nWe notice a pattern in Figure 1, our inspiration example: the shape has a five-fold symmetry: If we rotate the entire figure by \\(\\frac{2\\pi}{5}\\), it will overlap exactly with the original. Further, we suspect that the curve has 5 “pieces”, that repeat every \\(\\frac{2\\pi}{5}\\). If we chop up the parametric variable \\(t\\) into 5 sections, we might obtain each individual piece, rotated by that angle. What properties does the generating function Equation 4 have that causes this symmetry?\nFollowing the development in Frank Farris’ book, let us record our ideas/suspicions of symmetry as:\n\\[\n\\begin{eqnarray}\n\\mu(t) &= \\mu(t + \\color{Blue}{\\Large\\pmb{2\\pi/5}})\\\\\n&= e^{\\color{Blue}{\\huge\\pmb{2\\pi *i/5}}} * \\mu(t)\n\\end{eqnarray}\n\\tag{5}\\]\nDoes this work out? Let’s see:\n\\[\n\\begin{eqnarray}\n\\mu(t + 2\\pi/5) &=& \\Big\\{ e^{i(t + 2\\pi/5)} + \\frac{1}{2}*e^{i6(t + 2\\pi/5)} + \\frac{1}{3}*i*e^{-i14(t + 2\\pi/5)}\\Big\\}\\\\\n&=& \\Big\\{e^{2\\pi i/5} *e^{it} + \\frac{1}{2}*e^{12\\pi i/5} *e^{i6t} + \\frac{1}{3}*e^{-28\\pi i/5} *e^{-i14t}\\Big\\}\\\\\n&=& \\Big\\{e^{2\\pi i/5}*e^{it} + \\frac{1}{2}*e^{(10+2)\\pi i/5}*e^{i6t} +  \\frac{1}{3}*e^{(-30 +2)\\pi i/5} *e^{-i14t}\\Big\\}\\\\\n&=& e^{2\\pi i/5}* \\Big\\{ e^{it} + \\frac{1}{2}*e^{i6t} + \\frac{1}{3}*i*e^{-i14t}\\Big\\}\\\\\n\\end{eqnarray}\n\\tag{6}\\]\nSo if we shift time by \\(t = 2\\pi/5\\), we get the same pattern rotated by \\(2\\pi/5\\) radians. Because the frequencies 1, 6, and −14 are all congruent to 1 modulo 5, shifting time by \\(2\\pi/5\\) causes the equation to add on a complex rotation term of \\(e^{2\\pi*i/5}\\).\nTime shifts are Angle Shifts. And our mystery curve hence meets the symmetry condition in Equation 5.\nThe Symmetry Condition Theorem\nSuppose that \\(m\\) and \\(k\\) are integers and that all the frequency numbers \\(n_j\\), \\(j={1..M}\\) in the finite sum:\n\\[\nf(t) = \\sum_{i=1}^M(a_1*e^{in_1t} + a_2*e^{in_2t}...+ a_M*e^{in_Mt})\n\\] have \\(n_j = k(mod~𝑚)\\).\nThen, for any choice of the complex coefficients \\(a_j\\), \\(f(t)\\) satisfies the symmetry condition:\n\\[\nf(t + \\frac{2\\pi}{m}) = e{\\frac{2k*\\pi*i}{m}} * f(t)\\\\\nfor~ all~ t\n\\]\nWhat a mouthful! What does that mean?\nIf we take a set of \\(M\\) integer frequencies, such that they have the same remainder \\(k\\) when divided by another integer \\(m\\), then these frequencies when attached to rotating circles will give us \\(m\\)-fold symmetry. E.g: M = 5, m = 7, k = 1 implies the frequencies are -14+1, -7+1, 1, 7+1, 14+1.\n\n\n\n\n\n\nNoteComplex Coefficients?\n\n\n\nQuestion: How do we handle \\(n_j\\), \\(j={1..M}\\) being complex, at least some of them?\nLook back at the table Table 1.\nConsider a \\(term = i/3 * e^{i6t}\\). We can expand this as:\n\\[\n\\begin{eqnarray}\nterm &=& i/3 * e^{i6t}\\\\\n&=& i/3 * \\big[cos(6t) + i*sin(6t)\\big]\\\\\n&=& -1/3*sin(6t) + i/3*cos(t)\\\\\n\\end{eqnarray}\n\\]\nWe can view this as a rotation by \\(\\pi/2\\) in the counter-clockwise direction. Other angles will contribute to rotations of the coefficients in the same way. Complex Coefficients will alter the nature of the pattern of course, but not the symmetry!\n\n\n\n\n\n\n\n\nNoteMutually Prime?\n\n\n\nQuestion: What happens when \\(k\\) is a factor of \\(m\\)? E.g: \\(k=3\\) and \\(m=9\\): what happens? Find out!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#design-principles-for-rotational-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#design-principles-for-rotational-symmetry",
    "title": "\n Circles",
    "section": "Design Principles for Rotational Symmetry",
    "text": "Design Principles for Rotational Symmetry\nHow do we capture all of the above in a set of design principles for symmetric rotation-based patterns? The design parameters for us are:\n\nNumber of frequencies / rolling circles: M\nThe Frequency values for each rolling circle: \\(n_j\\), \\(j={1..M}\\)\n\nThe (complex) Amplitudes \\(a_j\\), \\(j={1..M}\\)\n\n\nLarger values of \\(M\\) give a more fine grain structure to the pattern, especially when combined with diminishing amplitudes of \\(a_j\\), an idea that we will encounter again in Making Noise.\nLet us randomly create an equation, using the following parameters:\n\nM = 5 (Number of rotating circles)\nm = 7 (Prime Modulus) i.e. Order of Symmetry\nk = 2 (The remainder of \\(n*m~mod~k\\)) i.e. Type of Symmetry\n\nHere is the plot of the frequency components:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nset.seed(42)\nM &lt;- 5 # Number of circles\nm &lt;- 7 # Prime Modulus\nk &lt;- 2 # Type of Symmetry\ntibble(\n  index = seq(-floor((M - 1) / 2), floor((M - 1) / 2), 1),\n  prime_multiple = m * index,\n  remainder = rep(k, length(index)),\n  frequency = prime_multiple + remainder\n) %&gt;%\n  mutate(amplitude = if_else(frequency == k, 1, k / frequency)) %&gt;%\n  # scaling amplitudes\n  # mutate(y0 = rep(0, length(index)),\n  #        z0 = rep(0, length(index))) %&gt;%\n  dplyr::select(prime_multiple, frequency, amplitude) -&gt; circles\ncircles\n\n\n  \n\n\nShow the Code##\ncircles %&gt;%\n  gf_hline(yintercept = 0, colour = \"grey\") %&gt;%\n  gf_segment(rep(0, length(prime_multiple)) + rep(0, length(prime_multiple)) ~ prime_multiple + frequency,\n    arrow = arrow(\n      angle = 20,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_segment(\n    rep(0, length(prime_multiple)) + amplitude ~\n      frequency + frequency,\n    data = circles, linewidth = 2,\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.1, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_point(rep(0, length(prime_multiple)) ~ prime_multiple,\n    colour = \"red\", size = 3\n  ) %&gt;%\n  gf_point(rep(0, length(prime_multiple)) ~ frequency,\n    xlab = \"Frequency Component\",\n    ylab = \"Amplitude\", data = circles\n  ) %&gt;%\n  gf_labs(\n    title = \"Rotating Vectors Frquencies and Amplitudes\",\n    subtitle = \"Negative Frequency components rotate counterclockwise\", caption = \"Red Dots: Prime Modulus Multiples\"\n  ) %&gt;%\n  gf_refine(annotate(\n    x = circles$frequency + 1.75,\n    y = circles$amplitude,\n    geom = \"text\",\n    label = as.character(round(circles$amplitude, 4), nsmall = 3), size = 3.5\n  )) %&gt;%\n  gf_refine(scale_x_continuous(breaks = c(-28, -21, -14, -7, -5, 0, 2, 7, 9, 14, 16))) %&gt;%\n  gf_theme(theme_custom())\n\n\n\n\n\n\n\nThe function for this curve would be:\n\\[\n\\begin{multline}\n\\mu(t) = {\\large{\\color{hotpink}{1}} * {e^{\\color{Blue}{\\large{\\pmb{2it}}}}}}+\n\\\\\n+{\\large{\\color{hotpink}{0.2222}} * {e^{\\color{Blue}{\\large{\\pmb{9it}}}}}} +\n{\\large{\\color{hotpink}{0.125}} *\n{e^{\\color{Blue}{\\large{\\pmb{16it}}}}}}\n\\\\\\ -\n{\\large{\\color{hotpink}{0.4}} * {e^{\\color{Blue}{\\large{\\pmb{-5it}}}}}}\n-{\\large{\\color{hotpink}{0.1667}} * {e^{\\color{Blue}{\\large{\\pmb{-12it}}}}}}\n\\\\\\\n\\end{multline}\n\\tag{7}\\]\nLet us now plot this:\nShow the Code# remainder = 2 from 7\n# frequencies: 2, 7+2, 14+2, -7+2, -14+2\nf_mystery2 &lt;- function(x) {\n  1.0 * (exp((0 + 2i) * x) +\n    0.2222 * exp((0 + 9i) * x) +\n    0.125 * exp((0 + 16i) * x) -\n    0.4 * exp((0 - 5i) * x) -\n    0.1667 * exp((0 - 12i) * x))\n}\ndata_mystery_2 &lt;- tibble(t, pattern = f_mystery2(t))\ndata_mystery_2 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern), title = \"Complex Exponential Rendering\n           \") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata2 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(2 * t) + 0.2222 * cos(9 * t) + 0.125 * cos(16 * t) - 0.4 * cos(5 * t) - 0.1667 * cos(12 * t),\n  y = sin(2 * t) + 0.2222 * sin(9 * t) + 0.125 * sin(16 * t) + 0.4 * sin(5 * t) + 0.1667 * sin(12 * t)\n)\ndata2 %&gt;%\n  gf_point(y ~ x, title = \"Parametric Equation Rendering\") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere, we have designed a pattern with seven-fold rotational symmetry. Can you make this in p5.js? Can you try for other orders and types of symmetry?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#mirror-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#mirror-symmetry",
    "title": "\n Circles",
    "section": "\n Mirror Symmetry",
    "text": "Mirror Symmetry\nThe coordinate system defines a positive increase in angle as the counterclockwise direction. So an increase in the parameter \\(t\\), increases the angle for each frequency component in that direction, if their coefficient is positive, and the other way of their coefficient is negative. So far so good.\nConsider a small modification to our original Figure 1:\nShow the Code## Original Mystery Curve\n# ## remainder = +1 from 5\n# ## frequencies 1, 5+1, -15+1\ndata1 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(t) + cos(6 * t) / 2 + sin(14 * t) / 3,\n  y = sin(t) + sin(6 * t) / 2 + cos(14 * t) / 3\n)\n\n# Mystery Curve\ndata1 %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n# Derived from Mystery\n# Remainder = 2 from 5\n# Frequencies: 2, 5+2, -15+2\n# Coeffs: 1,1,1\ndata3 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(t) + cos(6 * t) + cos(14 * t),\n  y = sin(t) + sin(6 * t) + sin(14 * t)\n)\ndata3 %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be immediately clear that the second pattern above is the same above and below the horizontal line; it exhibits horizontal mirror symmetry, \\(f(-t) = f(t)\\).\nUnder what conditions would a pattern be symmetric about an arbitrarily-tilted mirror, a mirror at angle \\(\\alpha\\) say?\nFrom Farris:\n\n\n\n\n\n\nNoteMirror at Angle \\(\\alpha\\)\n\n\n\nWhen every coefficient is a real multiple of \\(e^{i\\alpha}\\), the curve satisfies \\(f(-t) = e^{2\\alpha*i}f(t)\\).\nThe right-hand side is the correct expression for reflection across the line through the origin inclined at angle \\(\\alpha\\)(Check!!). If one wants curves with slanted mirrors, simply finds a curve symmetric about the x-axis satisfying \\(f(-t) = f(t)\\), and tilts it by \\(\\alpha\\).",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#fun-extras-to-try",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#fun-extras-to-try",
    "title": "\n Circles",
    "section": "Fun Extras to Try",
    "text": "Fun Extras to Try\nIt would be cool to simply develop the equations for any pattern in complex notation as in Equation 4 and throw that into code, without the tedious conversions into sines and cosines. Can we try that?\nHere is an example in R:\nShow the Codet &lt;- seq(0, 2 * pi, by = 0.001)\nx &lt;- t\n## NOTE: need the minus sign here inside the exponential!!\n## AND Absolutely need the \"1\" here before the solitary \"i\"!!\n## Need to figure these out\nf1 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.25 * exp(-(0 + 6i) * x) +\n    0.2 * exp(-(0 + 11i) * x))\n}\nplot(f1(x), asp = 1)\n##\nf2 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    0.2222 * exp(-(0 + 9i) * x) +\n    0.125 * exp(-(0 + 16i) * x) -\n    0.4 * exp(-(0 - 5i) * x) -\n    0.1667 * exp(-(0 - 12i) * x))\n}\nplot(f2(x), asp = 1)\n## Plotting with Exponential Functions\nf3 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.5 * exp(-(0 + 6i) * x) +\n    1 / 3 * exp(-(0 - 14i) * x)\n  )\n}\nplot(f3(x), asp = 1)\n##\nf4 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.5 * exp(-(0 + 6i) * x) +\n    1 / 3 * exp(-(0 + 14i) * x)\n  )\n}\nplot(f4(x), asp = 1)\n##\n# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, 1, 1\nf5 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    1.0 * exp(-(0 + 7i) * x) +\n    1.0 * exp(-(0 + 13i) * x)\n  )\n}\nplot(f5(x), asp = 1)\n##\n# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, -1/2, -i/3 ( Note!!!)\nf6 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    -0.5 * exp(-(0 + 7i) * x) +\n    -1 / 3 * exp(pi / 2 * 1i) * exp(-(0 + 13i) * x)\n  )\n}\nplot(f6(x), asp = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the Code# t &lt;- seq(0, 2 * pi, by = 0.001) # Already computed\ndata1 &lt;- tibble(t, pattern = f1(t))\ndata2 &lt;- tibble(t, pattern = f2(t))\ndata3 &lt;- tibble(t, pattern = f3(t))\ndata4 &lt;- tibble(t, pattern = f4(t))\ndata5 &lt;- tibble(t, pattern = f5(t))\ndata6 &lt;- tibble(t, pattern = f6(t))\n\ndata1 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata2 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata3 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata4 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#comparing-exponential-and-trigonometric-functions",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#comparing-exponential-and-trigonometric-functions",
    "title": "\n Circles",
    "section": "Comparing Exponential and Trigonometric Functions",
    "text": "Comparing Exponential and Trigonometric Functions\nJust for practice, let us once more be clear between the complex exponential notation, and the parametric trigonometric functions.\nShow the Code# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, 1, 1\nf7 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) + exp(-(0 + 7i) * x) + exp(-(0 + 13i) * x))\n}\n\n### Parametric Coordinates Tibble\ndata7a &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos((2) * t) + cos(7 * t) + cos(13 * t),\n  y = sin(2 * t) + sin(7 * t) + sin(13 * t)\n)\n### Complex Exponential Tibble\ndata7b &lt;- tibble(t, pattern = f7(t))\n### Plots\nplot(f7(x),\n  asp = 1, cex = 0.2,\n  main = \"Base R: Exponential Function Plot\"\n)\n###\ndata7a %&gt;%\n  gf_point(y ~ x,\n    title = \"ggFormula: Trigonometric Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata7b %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern),\n    title = \"ggFormula: Exponential Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the Code# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, -1/2, -i/3 ( Note!!!)\nf8 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) - 0.5 * exp(-(0 + 7i) * x) +\n    1i / 3 * exp((0 + 13i) * x))\n}\n\ndata8a &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(2 * t) - 0.5 * cos(7 * t) +\n    0.3 * cos(-13 * t + pi / 2),\n  y = sin(2 * t) - 0.5 * sin(7 * t) +\n    0.3 * sin(-13 * t + pi / 2)\n)\ndata8b &lt;- tibble(t, pattern = f8(t))\n###\nplot(f8(x),\n  asp = 1, cex = 0.2,\n  main = \"Base R: Exponential Function Plot\"\n)\n###\ndata8a %&gt;%\n  gf_point(y ~ x, title = \"ggFormula: Trigonometric Function Plot\") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata8b %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern),\n    title = \"ggFormula: Exponential Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#wait-but-why",
    "title": "\n Circles",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nSums of Complex exponentials are very common in mathematics and show up in many places: here, with symmetry, with Fourier Series, with Sound synthesis and Analysis.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#conclusion",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#conclusion",
    "title": "\n Circles",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen the close relationship between complex rotating exponentials and their trigonometric decompositions, embodied in the Euler Formula.\nWe also saw how multiple such exponentials can be used to combine using complex weighting to create symmetric patterns.\nAnd how symmetry depends upon the frequencies of the exponentials having a very specific relationship using modulo arithmetic.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#your-turn",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#your-turn",
    "title": "\n Circles",
    "section": "Your Turn",
    "text": "Your Turn\nCan you reverse engineer these curves, in R or in p5.js?\n\n\nFrom The Math Less Travelled Blog",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#references",
    "title": "\n Circles",
    "section": "\n References",
    "text": "References\n\nFrank Farris. Creating Symmetry: The Artful Mathematics of Wallpaper Patterns. Princeton University Press (2 June 2015).\nDoga Kurkcuoglu. https://bilimneguzellan.net/en/follow-up-to-fourier-series-2/. Look at some very cool animations here!\nGorilla Sun Blog. https://www.gorillasun.de/blog/parametric-functions-and-particles/\n\nCrateCode: Complex Generative Art with p5.js. https://cratecode.com/info/p5js-generative-art-complex-functions\n\nGorilla Sun Blog. https://www.gorillasun.de/blog/parametric-functions-and-particles/\n\nBrent Yorgey.(2015). The Math Less Travelled Blog. Random Cylic Curves. https://mathlesstraveled.wordpress.com/2015/06/04/random-cyclic-curves-5/\n\nUniversity of New South wales. Exponential Sums Page. https://www.unsw.edu.au/science/our-schools/maths/our-school/spotlight-on-our-people/history-school/glimpses-mathematics-and-statistics/exponential-sums\n\nJohn Myles White. Complex Numbers in R. https://www.johnmyleswhite.com/notebook/2009/12/18/using-complex-numbers-in-r/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html",
    "title": "\n Complex Numbers",
    "section": "",
    "text": "One of the most basic kinds of numbers we will need are, of course, Complex Numbers. But what are they?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#introduction",
    "title": "\n Complex Numbers",
    "section": "",
    "text": "One of the most basic kinds of numbers we will need are, of course, Complex Numbers. But what are they?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#complex-planes-what-am-i",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#complex-planes-what-am-i",
    "title": "\n Complex Numbers",
    "section": "Complex Planes: What am \\(i\\)?",
    "text": "Complex Planes: What am \\(i\\)?\nDude, what’s the square root of -1?? 🙀 🙀 🙀!! And, what can you do repeatedly to arrive at \\([-1, 0]\\)?",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#wait-but-why",
    "title": "\n Complex Numbers",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nComplex Numbers are very useful in handling “2D data” in a compact fashion\nComplex Numbers help us to intuitively visualize, and implement, ideas such as rotation, scaling, and shadows, and projections.\nThe duality between rotating vectors and complex numbers is a very important concept.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#references",
    "title": "\n Complex Numbers",
    "section": "\n References",
    "text": "References\n\nWorking with Shadows. https://www.wikiwand.com/en/Map_projection\n\nWorking with Fourier Series and Epicyles http://www.jezzamon.com/fourier/index.html and https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\n\nhttps://twitter.com/i/status/962449509782495232 https://codegolf.stackexchange.com/questions/36374/redraw-an-image-with-just-one-closed-curve",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html",
    "title": "Fractals",
    "section": "",
    "text": "What is in us that must reach the top, that longs to look down upon the world as if a god? Don’t we know that in this infinite space the same rocks at the seashore know the secret of each peak?\nUnderneath the surface are caverns, caves soaring cathedrals the earth has made. What arias does she sing to dripping water, bats and other seekers of wisdom? What prayers echo while the ceilings reach slowly to the floor?\nThe open window houses everything: a cat lounging in the sunlight, the call of neighbors, the breath of possibility.\n- Robin Walthery Allen",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#inspiration-a-geometric-jewellery-store",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#inspiration-a-geometric-jewellery-store",
    "title": "Fractals",
    "section": "\n Inspiration: A Geometric Jewellery Store",
    "text": "Inspiration: A Geometric Jewellery Store\n\n\nPhoto by Mykola/Kolya Korzh on Unsplash\n\nExplore some jewels here: https://math.hws.edu/eck/js/mandelbrot/MB.html",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#an-introduction-to-fractals",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#an-introduction-to-fractals",
    "title": "Fractals",
    "section": "An Introduction to Fractals",
    "text": "An Introduction to Fractals\nLet us listen to the late great Benoit Mandelbrot, on the Art of Roughness.\n\nJulia, and Julia, and still more Julia….\nLet us head off to https://mathigon.org/course/fractals/mandelbrot and play with some iterated functions in the Complex Plane. This will lead us into an intuitive understanding of Julia and Mandelbrot Fractals.\nNow we are ready(?) to understand this video on Julia fractals!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#designing-with-juliamandelbrot-fractals",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#designing-with-juliamandelbrot-fractals",
    "title": "Fractals",
    "section": "Designing with Julia/Mandelbrot fractals",
    "text": "Designing with Julia/Mandelbrot fractals\nLet us play with this interactive Mandelbrot-Julia combination diagram!\n\n\n\nWe now that the Mandelbrot fractal uses the Iterated Function \\(z \\leftarrow ~ z^2 + C\\), where \\(C\\) is a starting complex constant.\nThe Mandelbrot (at left) is plotted in a coordinate space for \\(C\\).\nThe Julia(s) (at right) are plotted in a coordinate space for \\(z\\).\nDepending upon the starting value of \\(C\\) in the Mandelbrot we get a different choice of Julia fractal in the right side diagram.\nIn this “single-colour” Mandelbrot, we see that for those values of \\(C\\) where the corresponding Julia is connected, the \\(C\\) point is coloured black in the Mandelbrot. Else it is coloured white and appears to be “outside”.\nPlace your cursor inside the Julia diagram. A connected Julia always shows an iterative trace that lies within itself. A dis-connected Julia has points going off to infinity…\nVarying \\(C\\) we see a systematic variation of the attainable Julia fractals.\n\nJewellery Shopping with Mandelbrot\nEver gone shopping for jewellery? How is the place organized? That is a good metaphor for how Mandelbrot Set is generated!! Here is an image to complete the Jewellery Store metaphor:\n\n\n\n\n\nFigure 1: Mandelbrot and Julia Mapping",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#fractals-with-code",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#fractals-with-code",
    "title": "Fractals",
    "section": "Fractals with Code",
    "text": "Fractals with Code\n\n\nUsing p5.js\nUsing XaOS\n\n\n\n\n\nAnd here is the Julia set:\n\n\nSee also this sketch for a static Julia, set by a user choice of \\(C\\). https://editor.p5js.org/AhmadMoussa/full/nDrd9EfHr\n\n\nLet us use the XaOS software to make different kinds of fractals. A sample screen to explore the Mandelbrot fractal is here:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#wait-but-why",
    "title": "Fractals",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nFractal shapes are all around us in nature\nThese mathematically created fractal shapes can have uses that are limited only by your imagination: want to design footwear that does not slip?\nDifferent parts of the Mandelbrot fractal allow us to contemplate a variety of fractal shapes\nAnd if we choose a different IFS function (rather than \\(z \\rightarrow z^2 + C\\)), we can think of entire families of shapes!",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#references",
    "title": "Fractals",
    "section": "\n References",
    "text": "References\n\nAn Intrepid Tour of the Complex Fractal World using Dark Heart Package 2.2.0 for Mac. https://dhushara.com/DarkHeart/#Anchor-Introduction-35326\n\nGary William Flake.(28 August 1988) The Computational Beauty of Nature. ISBN: 978-0262062008. MIT Press.\nBatty, M. and Longley, P. A. (1994) Fractal Cities: A Geometry of Form and Function, London: Academic Press, 1994.\nWang H, Luo S, Luo T. Fractal characteristics of urban surface transit and road networks: Case study of Strasbourg, France. Advances in Mechanical Engineering. 2017;9(2). doi:10.1177/1687814017692289\n\nLarry Riddle.Classic Iterated Function Systems. https://larryriddle.agnesscott.org/ifs/ifs.htm\n\nTurtle Graphics in R: https://cran.r-project.org/web/packages/TurtleGraphics/vignettes/TurtleGraphics.pdf\n\nhttps://www.reddit.com/r/CitiesSkylines/comments/9r0y4e/grid_idea_im_working_on_fractal_squares/\nMenger Sponge: https://blogs.scientificamerican.com/roots-of-unity/a-few-of-my-favorite-spaces-the-menger-sponge/\n\nhttp://fractalfoundation.org/resources/fractal-software/",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html",
    "title": "Adding Libraries to p5.js",
    "section": "",
    "text": "When we wish to use some feature that is not directly available in p5.js, say for example creating Voronoi Tesselations using the c2.js library, then we need to import that JavaScript library into our p5.js session that allows us to do that.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#introduction",
    "title": "Adding Libraries to p5.js",
    "section": "",
    "text": "When we wish to use some feature that is not directly available in p5.js, say for example creating Voronoi Tesselations using the c2.js library, then we need to import that JavaScript library into our p5.js session that allows us to do that.",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#simple-instructions",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#simple-instructions",
    "title": "Adding Libraries to p5.js",
    "section": "Simple Instructions!",
    "text": "Simple Instructions!\n\nOn your p5.js web editor, click the arrow on the left side and access the index.html file.\n\n\n\n\n\n\n\nFigure 1: Accessing index.html on p5js web editor\n\n\n\n\nGo to either jsDeliver or cdnjs and get the &lt;script&gt; for your chosen library:\n\n\n\n\n\n\n\nFigure 2: Script for tonejs @ cdnjs website\n\n\n\n\nPaste that &lt;script&gt; tag and its contents into your index.html.\nThe library will now be available in your sketch.js and you can use the commands and features from that library therein.\n\nHere is a shortish video that tells us how to do this:",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#references",
    "title": "Adding Libraries to p5.js",
    "section": "References",
    "text": "References\n\np5.js Documentation: How to use a JS library in your p5.js sketch. https://archive.p5js.org/libraries/\np5.js Community Libraries. https://p5js.org/libraries/\njsDeliver https://www.jsdelivr.com/?query=p5play\ncdnJS.https://cdnjs.com",
    "crumbs": [
      "Teaching",
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/listing.html",
    "href": "content/courses/R4Artists/listing.html",
    "title": "R for Artists and Managers",
    "section": "",
    "text": "🕶 Lab-1: Science, Human Experience, Experiments, and Data\n\n\n\n\n\nNov 1, 2021\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nLab-2: Down the R-abbit Hole…\n\n\n\n\n\nJul 9, 2021\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nLab-3: Drink Me!\n\n\n\n\n\nMar 10, 2023\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nLab-4: I say what I mean and I mean what I say\n\n\n\n\n\nJul 12, 2022\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nLab-5: Twas brillig, and the slithy toves…\n\n\n\n\n\nNov 22, 2022\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nLab-6: These Roses have been Painted !!\n\n\n\n\n\nAug 21, 2022\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nLab-7: The Lobster Quadrille\n\n\n\n\n\nJul 21, 2021\n\n39 min\n\n\n\n\n\n\n\n\n\n\n\nLab-8: Did you ever see such a thing as a drawing of a muchness?\n\n\n\n\n\nJul 10, 2022\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nLab-9: If you please sir…which way to the Secret Garden?\n\n\n\n\n\nJul 10, 2022\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nLab-10: An Invitation from the Queen…to play Croquet\n\n\n\n\n\nJun 6, 2022\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nLab-11: The Queen of Hearts, She Made some Tarts\n\n\n\n\n\nInvalid Date\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nLab-12: Time is a Him!!\n\n\n\n\n\nFeb 14, 2022\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nIteration: Learning to purrr\n\n\n\n\n\nJun 14, 2023\n\n17 min\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "title": "The Grammar of Diagrams",
    "section": "",
    "text": "There are many presentation and drawing tools out there. And these allow the user full control over the diagram so generally result in prettier diagrams that can convey more information to the audience at that point in time.\nBut that point in time passes, and pretty pictures can quickly become out-of-date and, ironically, misinforming if they don’t match the reality of the system they are describing. This is especially so if one team is drawing the pretty pictures, and another team is writing the software/implementing the system.\nHaving diagrams as code that can live beside the system design/code, that the stakeholders are equally comfortable editing and viewing,reduces the gap i.e. “Where system diagrams meet system reality”.\nWe will “explore” two packages to do this: DiagrammeR and nomnoml. Each of these follows a specific grammar so that sets of “sentences” will morph into very different kinds of diagrams."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n\n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry...\n        Arvind -&gt;&gt; Anamika: Ok write it today\n\n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry...\n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n\n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n\n      node[shape = rectangle, style = filled]\n\n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n\n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n\n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n\n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n\n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n\n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n\n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n\n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n\n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n  \"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;\nstyle B fill:#87AB51;\nstyle C fill:#3C8937;\nstyle D fill:#23772C;\nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCodegrViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "title": "The Grammar of Diagrams",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "title": "The Grammar of Diagrams",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "title": "The Grammar of Diagrams",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "The Grammar of Diagrams",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\nCode[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\nCode[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\nCode[&lt;table&gt; table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "title": "The Grammar of Diagrams",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "title": "The Grammar of Diagrams",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "title": "The Grammar of Diagrams",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "title": "The Grammar of Diagrams",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\nCode[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#slides-and-tutorials",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#introduction",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": " Introduction",
    "text": "Introduction\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives, figures of speech..) get metaphorized into the R World!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Introduction",
    "text": "Introduction\nWe will add icing and froth to our vanilla ggplots: fonts, annotations, highlights and even pictures!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#goals",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#goals",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Goals",
    "text": "Goals\n\nAppreciate that a publication-worth graphic takes a lot of work!!\nAdding annotations, pictures and references to graphs is necessary for good understanding\nJudicious use of colour and scales can enhance comprehension."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#pedagogical-note",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#pedagogical-note",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nSetting up R Packages\nLet’s load up a few packages that we need to start:\n\n## packages\nlibrary(tidyverse) ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)\nlibrary(paletteer) ## scico  and many other colour palettes palettes(http://www.fabiocrameri.ch/colourmaps.php) in R\nlibrary(ggtext) ## add improved text rendering to ggplot2\nlibrary(ggforce) ## add missing functionality to ggplot2\nlibrary(concaveman) ## Needed by ggforce for plot annotation hulls\nlibrary(ggdist) ## add uncertainty visualizations to ggplot2\nlibrary(ggformula) ## Formula interface to ggplot\nlibrary(magick) ## load images into R\nlibrary(patchwork) ## combine outputs from ggplot2\nlibrary(palmerpenguins)\n\nlibrary(showtext) ## add google fonts to plots\n\nknitr::opts_chunk$set(\n  error = TRUE,\n  comment = NA,\n  warning = FALSE,\n  errors = FALSE,\n  message = FALSE,\n  tidy = FALSE,\n  cache = FALSE,\n  echo = TRUE,\n  warning = FALSE,\n  # from the vignette for the showtext package\n  fig.showtext = TRUE,\n  fig.retina = 1,\n  fig.path = \"figs/\"\n  # fig.height = 3.09,\n  # fig.width = 5\n)\n\nPlot Fonts and Theme\n\nShow the Code```{r}\n#| label: plot-theme\n#| code-fold: true\n#| messages: false\n#| warning: false\n\nlibrary(sysfonts)\nlibrary(showtext)\nfont_add(family = \"Alegreya\", regular = \"../../../gfonts/fonts/Alegreya/Alegreya-Regular.ttf\")\nfont_add(family = \"Roboto Condensed\", regular = \"../../../gfonts/fonts/RobotoCondensed-Regular.ttf\")\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n```\n\n\n\nShow the Code## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n\nUsing Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package (which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nlibrary(sysfonts)\nlibrary(showtext)\n\nsysfonts::font_add_google(\"Roboto Condensed\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n# set the google fonts as default\nshowtext::showtext_auto(enable = TRUE)\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nTipggformula and ggplot worlds do intersect!\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!! But it may not be a good idea to mix these up…"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na()\n# remove data containing missing data\npenguins"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;- gf_point(bill_depth_mm ~ bill_length_mm,\n  data = penguins,\n  alpha = 0.6, size = 3.5\n)\ngf\n\n\n\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(alpha = .6, size = 3.5)\ngg"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\n\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\nmy_theme &lt;- theme_set(theme_classic(\n  base_size = 12,\n  base_family = \"roboto\"\n)) +\n\n  ## modify plot elements globally (for all following plots)\n  theme_update(\n    text = element_text(family = \"roboto\"),\n    axis.ticks = element_line(color = \"grey92\"),\n    axis.ticks.length = unit(.5, \"lines\"),\n    panel.grid.minor = element_blank(),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(color = \"grey30\"),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 12, color = \"grey30\"),\n    plot.caption = element_text(size = 9, margin = margin(t = 15))\n  )\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf1 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n\n    # colour by continuous variable\n    color = ~body_mass_g,\n    alpha = .6, size = 3.5\n  ) %&gt;%\n  ## custom axes scaling\n  gf_refine(\n    scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n    scale_y_continuous(\n      breaks = seq(12.5, 22.5, by = 2.5),\n      limits = c(12.5, 22.5)\n    ),\n\n    ## custom colors from the scico package\n    ## using the paletteer super package\n    paletteer::scale_color_paletteer_c(`\"scico::bamako\"`,\n      direction = -1\n    ),\n\n    ## custom labels\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)\",\n      subtitle = \"A scatter plot of bill depth versus bill length.\",\n      caption = \"Data: Gorman, Williams & Fraser (2014) PLoS ONE\",\n      x = \"Bill Length (mm)\",\n      y = \"Bill Depth (mm)\",\n      ## See this!\n      color = \"Body mass (g)\"\n    )\n  )\n\ngf1\n\n\n\n\n\n\n\nNote this neat way of naming a scale and the legend in the labs command above!\n\n\n\ntheme_set(my_theme)\n\n\ngg1 &lt;- penguins %&gt;%\n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm),\n    alpha = .6\n  ) +\n  geom_point(aes(colour = body_mass_g), size = 3.5) +\n\n\n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n\n  ## custom colors from the scico package\n  paletteer::scale_color_paletteer_c(`\"scico::bamako\"`,\n    direction = -1\n  ) +\n\n  ## custom labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) PLoS ONE\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )\ngg1"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Using {ggtext}",
    "text": "Using {ggtext}\nFrom Claus Wilke’s website → www.wilkelab.org/ggtext\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\n\n\n\n\n\n\nCautionWorking with ggtext\n\n\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: install.packages(remotes) remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n\nUsing element_markdown()\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf2 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~body_mass_g,\n    alpha = 0.6, size = 3.5\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n    scale_y_continuous(\n      breaks = seq(12.5, 22.5, by = 2.5),\n      limits = c(12.5, 22.5)\n    ),\n\n    ## custom colors from the scico package\n    paletteer::scale_color_paletteer_c(\"scico::bamako\",\n      direction = -1\n    ),\n\n    ## custom labels using element_markdown()\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = \"A scatter plot of bill depth versus bill length.\",\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"**Bill Length** (mm)\",\n      y = \"**Bill Depth** (mm)\",\n      color = \"Body mass (g)\"\n    )\n  ) %&gt;%\n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\ngf2\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  paletteer::scale_color_paletteer_c(`\"scico::bamako\"`,\n    direction = -1\n  ) +\n\n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2\n\n\n\n\n\n\n\n\n\n\n\nelement_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to change text color\n\ngf2 %&gt;%\n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;')\n## use HTML syntax to change font and text size\ngf2 %&gt;%\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 15))\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;%\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt; img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to add images to text elements\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\n\n\n\nAnnotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a tibble to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\n\nDesign the labels\nUsing ggformula\nUsing ggplot\n\n\n\n\n# Create a label tibble\n# Three rich text labels,\n# so three sets of locations x and y, and angle of rotation\n\nlabels &lt;- tibble(\n  x = c(34, 56, 54),\n  y = c(20, 18.5, 14.5),\n  angle = c(12, 20, 335),\n  species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n  lab = c(\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n    \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n  )\n)\nlabels\n\n\n  \n\n\n\n\n\n\ntheme_set(my_theme)\n\ngf_rich &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~species,\n    alpha = 0.6, size = 3.5, data = penguins\n  ) +\n\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(\n    palette = `\"rcartocolor::Bold\"`,\n    guide = \"none\"\n  ) +\n\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\n\ntheme_set(my_theme)\n\n\ngg_rich &lt;- ggplot(penguins, aes(\n  x = bill_length_mm,\n  y = bill_depth_mm\n)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_rich\n\n\n\n\n\n\n\n\n\n\nFormatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf_box &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~species,\n    alpha = 0.6, size = 3.5, data = penguins\n  ) +\n\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(\n    palette = `\"rcartocolor::Bold\"`,\n    guide = \"none\"\n  ) +\n\n\n  # Now for the Plot Titles and Labels, as before\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 28,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg_box &lt;- ggplot(\n  penguins,\n  aes(x = bill_length_mm, y = bill_depth_mm)\n) +\n  geom_point(aes(color = species),\n    alpha = .6, size = 3.5\n  ) +\n\n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle),\n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  # show.legend = FALSE else we get some unusual legends!\n  # fill = NA makes the labels' fill transparent\n\n\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(\n    breaks = seq(12.5, 22.5, by = 2.5),\n    limits = c(12.5, 22.5)\n  ) +\n  scale_colour_paletteer_d(\n    palette = `\"rcartocolor::Bold\"`,\n    guide = \"none\"\n  ) +\n\n\n  # Now for the Plot Titles and Labels, as before\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\"\n  ) +\n\n  # Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 28,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box\n\n\n\n\n\n\n\n\n\n\nUsing geom_texbox() for formatted text boxes with word wrapping\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y,\n      label = label\n    ),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\",\n    colour = \"black\",\n    # This is ESSENTIAL !!!\n    # It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n    # So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    # box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\nUsing {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcomings” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n\nUsing ggformula and ggforce\nUsing ggplot and ggforce\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;%\n  gf_point(bill_depth_mm ~ bill_length_mm,\n    color = ~body_mass_g,\n    alpha = .6,\n    size = 3.5\n  ) +\n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n\n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`,\n    direction = -1\n  ) +\n\n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    color = \"black\",\n    # This is good to include\n    # Else ellipses get coloured too\n\n    alpha = .15,\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(\n  penguins,\n  aes(x = bill_length_mm, y = bill_depth_mm)\n) +\n  geom_point(aes(color = body_mass_g),\n    alpha = .6,\n    size = 3.5\n  ) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n\n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`,\n    direction = -1\n  ) +\n\n\n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    alpha = .15,\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species, label = species,\n      filter = species == \"Gentoo\"\n    ),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(\n      fill = species, label = species,\n      filter = species == \"Gentoo\"\n    ),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(\n      fill = species, label = species,\n      filter = species == \"Gentoo\"\n    ),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(\n      fill = species, label = species,\n      filter = species == \"Gentoo\"\n    ),\n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n\n\nggplot tricks\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg0 &lt;-\n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  ggforce::geom_mark_ellipse(\n    aes(\n      fill = species,\n      label = species\n    ),\n    alpha = .15,\n    show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  scale_x_continuous(\n    breaks = seq(25, 65, by = 5),\n    limits = c(25, 65)\n  ) +\n  scale_y_continuous(\n    breaks = seq(12, 24, by = 2),\n    limits = c(12, 24)\n  ) +\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  labs(\n    title = \"**Bill Dimensions of Brush-Tailed Penguins** (*Pygoscelis*)\",\n    subtitle = \"A scatter plot of bill depth versus bill length.\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"**Body mass** (g)\"\n  )\ngg0\n\n\n\n\n\n\n\nLeft-Aligned Title\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg1b &lt;- gg1 + theme(plot.caption.position = \"plot\")\ngg1b\n\n\n\n\n\n\n\nLegend Design\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg1b + theme(legend.position = \"top\")\n# ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b +\n  theme(legend.position = \"top\") +\n  guides(\n    color = guide_colorbar(\n      # title.position = \"top\",\n      # title.hjust = .5,\n      legend.key.width = unit(20, \"lines\"),\n      legend.bar.height = unit(.5, \"lines\")\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nAdd Images\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## read PNG file from web\npng &lt;- magick::image_read(\"images/culmen_depth.png\")\n\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;-\n  gg1 +\n  annotation_custom(img,\n    ymin = 22, ymax = 28,\n    xmin = 65, xmax = 80\n  ) +\n  labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\") +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\ngg5\n\n\n\n\n\n\n\nUsing {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrarily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\ntheme_set(new = my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;%\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;%\n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;-\n  ggplot(\n    penguins_stats,\n    aes(\n      y = bill_ratio,\n      x = species,\n      fill = species,\n      color = species\n    )\n  ) +\n  geom_violin() +\n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(linewidth = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## combine both plots\ngg5 | (gg6 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg5 / (gg6 + labs(\n  title = \"Bill Ratios of Brush-Tailed Penguins\",\n  subtitle = \"Violin Plots of Bill Ration versus species\"\n)) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "References",
    "text": "References\n\n\n1. Thomas Lin Pedersen, https://www.data-imaginist.com/. The creator of ggforce, and patchwork packages.\n2. Claus Wilke, cowplot – Streamlined plot theme and plot annotations for ggplot2, https://wilkelab.org/cowplot/index.html\n3. Claus Wilke, Spruce up your ggplot2 visualizations with formatted text, https://clauswilke.com/talk/rstudio_conf_2020/. Slides, Code, and Video !\n4. Robert Kabacoff, ggplot theme cheatsheet, https://rkabacoff.github.io/datavis/modifyingthemes.pdf\n5. Zuguang Gu, Circular Visualization in R, https://jokergoo.github.io/circlize_book/book/\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggdist\n3.3.3\n\n@ggdist2024; @ggdist2025\n\n\n\nggforce\n0.5.0\n@ggforce\n\n\nggtext\n0.1.2\n@ggtext\n\n\ngrid\n4.5.1\n@grid\n\n\nmagick\n2.8.7\n@magick\n\n\npaletteer\n1.6.0\n@paletteer\n\n\npatchwork\n1.3.1\n@patchwork\n\n\nscico\n1.5.0\n@scico\n\n\nshowtext\n0.9.7\n@showtext\n\n\nsysfonts\n0.8.9\n@sysfonts\n\n\nsystemfonts\n1.2.3\n@systemfonts"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "Introduction",
    "text": "Introduction\nWe will spend a little time wrapping up everything that we have learnt so far, in R.\nWe will take one large dataset that has numerical, spatial and network type data, and see how we can make multiple visual depictions of it. We will use most of the packages that we have encountered so far and try to make a polished info visualization with all the graphs, text, and descriptions put together! But first let us hear from the great Hadley Wickham, and see how all the things we have been doing fit into “The Whole Game”."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "References",
    "text": "References\n\nAlex Cookson, Great Data Sets github repo, https://github.com/tacookson/data"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "Introduction",
    "text": "Introduction\nLet’s make a website in RStudio to show off our data viz portfolio, and to share with friends, parents, prospective employers…\nWe will encounter a new package called blogdown and use workflows with github and a free web hosting service called Netlify to create a website where all our RMarkdowns become individual blog posts, complete with Titles, Sections, Text, Diagrams and Links!\nNOTE: Need to update this to Quarto!!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "References",
    "text": "References\n\nJenny Bryan, Tim Hester, et al. https://happygitwithr.com"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#slides-and-tutorials",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Slides and Tutorials",
    "text": "Slides and Tutorials"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#introduction",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Introduction",
    "text": "Introduction\nNetwork Diagrams are important in data visualization to bring out relationships between diverse entities. They are used in ecology, biology, transportation, and even history!\nAnd hey, whom did Jon Snow marry?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#references",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#references",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " References",
    "text": "References\n\nMichael Gastner, Data Analysis and Visualisation with R, Chapter 23: Networks\nDavid Schoch, Network Visualizations in R using ggraph and graphlayouts\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\n\n\n\nLearning to purrr"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#setting-up-r-packages",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\n\n\n\nLearning to purrr"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#introduction",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#introduction",
    "title": "Iteration: Learning to purrr",
    "section": "\n Introduction",
    "text": "Introduction\nOften we want to perform the same operation on several different sets of data. Rather than repeat the operation for each instance of data, it is faster, more intuitive, and less error-prone if we create a data structure that holds all the data, and use the map-* series functions from the purrr package to perform all the repeated operations in one shot.\nThis requires getting used to. We need to understand:\n\nthe data structure\nthe iteration mechanism using map functions\nthe form of the results"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "title": "Iteration: Learning to purrr",
    "section": "\n Case Study #1: Multiple Models for Life Expectancy with gapminder\n",
    "text": "Case Study #1: Multiple Models for Life Expectancy with gapminder\n\nWe will start with a complete case study and then work backwards to understand the various pieces of code that make it up.\nLet us look at the gapminder dataset:\n\nskimr::skim(gapminder)\n\n\nData summary\n\n\nName\ngapminder\n\n\nNumber of rows\n1704\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncountry\n0\n1\nFALSE\n142\nAfg: 12, Alb: 12, Alg: 12, Ang: 12\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 624, Asi: 396, Eur: 360, Ame: 300\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n1979.50\n17.27\n1952.00\n1965.75\n1979.50\n1993.25\n2007.0\n▇▅▅▅▇\n\n\nlifeExp\n0\n1\n59.47\n12.92\n23.60\n48.20\n60.71\n70.85\n82.6\n▁▆▇▇▇\n\n\npop\n0\n1\n29601212.32\n106157896.74\n60011.00\n2793664.00\n7023595.50\n19585221.75\n1318683096.0\n▇▁▁▁▁\n\n\ngdpPercap\n0\n1\n7215.33\n9857.45\n241.17\n1202.06\n3531.85\n9325.46\n113523.1\n▇▁▁▁▁\n\n\n\n\n\nWe have lifeExp, gdpPerCap, and pop as Quant variables over time (year) for each country in the world. Suppose now that we wish to create Linear Regression Models predicting lifeExp using year, for each country. ( We will leave out gdpPercap and pop for now) The straightforward by laborious and naive way would be to use the lm command after filtering the dataset for each country, creating 140+ Linear Models manually! This would be horribly tedious!\nThere is a better way with purrr, and also more recently, with dplyr itself. Let us see both methods, the established purrr method first, and the new dplyr based method thereafter.\n\n {{}} EDA Plots\nWe can first plot lifeExp over year, grouped by country:\nggplot(gapminder, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line(show.legend = FALSE) +\n  theme_classic()\nggplot(gapminder, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line(show.legend = FALSE) +\n  facet_wrap(~continent) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nBy and large we see positive slopes, but some countries do show non-linear behaviour.\nConstructing a Linear Model\nLet us take \\(1950\\) as a baseline year for all countries. Then we model lifeExp using year1950 across all countries together:\n\ngapminder &lt;- gapminder %&gt;%\n  mutate(year1950 = year - 1950) # baseline year\nmodel &lt;- lm(lifeExp ~ year1950, data = gapminder)\nsummary(model)\n\n\nCall:\nlm(formula = lifeExp ~ year1950, data = gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.949  -9.651   1.697  10.335  22.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.86028    0.55792   89.37   &lt;2e-16 ***\nyear1950     0.32590    0.01632   19.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.63 on 1702 degrees of freedom\nMultiple R-squared:  0.1898,    Adjusted R-squared:  0.1893 \nF-statistic: 398.6 on 1 and 1702 DF,  p-value: &lt; 2.2e-16\n\n\nmodel %&gt;% broom::tidy() # Parameters of the Model\nmodel %&gt;% broom::glance() # Statistics of the Model\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSince the slope 0.3259038 is positive, life expectancy has been increasing over the years, across all countries. (But r.squared 0.1897571 is low, so this model does not explain much).\nHow do we do this for each country? We need to use the split-apply-combine method to achieve this. The combination of group_by and summarise is a example of the split &gt; apply &gt; combine method. For example, we could (split) the data by country, calculate the linear model each group (apply), and (combine) the results in a data frame.\nHowever, this first-attempt code for a per-country linear model does not work:\n\n```{r}\n#| eval: false\ngapminder %&gt;%\n  group_by(country) %&gt;%\n  summarise(linmod = lm(lifeExp ~ year1950, data = .))\n```\n\nThis is because the linmod variable is a list variable and cannot be accommodated in a simple column, which is what summarize will try to create. So we need to be able to create “list” columns in a data frame…how do we do that? Before we contemplate that, let us understand the capabilities of the purrr package in R.\nThe purrr package\nThe purrr package contains a new class of functions, that can take vectors/tibbles/lists as input, and perform an identical function over each component of these, and generate vectors/tibbles/lists as output. These are the map_* functions that are part of the purrr package. The * in the map_* function defines what kind of output (vector/tibble/list) the function generates.\nLet us look at a few short examples.\nUsing map_* functions from purrr\n\nThe basic structure of the map_* functions is:\n\n```{r}\n#| eval: false\nmap_typeOfResult(\n  .x = what_to_iterate_with,\n  .f = function_to_apply\n)\n\nmap_typeOfResult(\n  .x = what_to_iterate_with,\n  .f = \\(x) function_to_apply(x, additional_parameters)\n)\n```\n\nTwo examples:\n\n# Example 1: Input: vector, Output: vector\ndiamonds %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # We need dbl-type numbers in output **vector**\n  map_dbl(\n    .x = .,\n    .f = mean\n  )\n\n       carat        depth        table        price            x            y \n   0.7979397   61.7494049   57.4571839 3932.7997219    5.7311572    5.7345260 \n           z \n   3.5387338 \n\n# Example 2: Input: vector, Output: tibble\ndiamonds %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # We need dbl-type numbers in output **vector**\n  map_df(\n    .x = .,\n    .f = mean\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote map_dbl outputs a (numeric) vector, and map_df outputs a tibble.\nIn each of the above examples, each vector in the diamonds dataset was passed to the respective map_* function as the parameter.x.\n\n\nSometimes the function .f may need some additional parameters to be specified, and these may not come from the input .x:\n\n# Example 3, with additional parameters to .f\npalmerpenguins::penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  map_dbl(\n    .x = .,\n    .f = \\(x) mean(x, na.rm = TRUE)\n  )\n\n   bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g \n         43.92193          17.15117         200.91520        4201.75439 \n             year \n       2008.02907 \n\n# penguins has two rows of NA entries which need to be dropped\n# Hence this additional parameter for the `mean` function\n\n\n# Example 4: if we want a tibble output\npalmerpenguins::penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  map_df(\n    .x = .,\n    .f = \\(x) mean(x, na.rm = TRUE)\n  )\n\n\n  \n\n\n\nThe .f function can be anything, even a ggformula plot command; in this case the output will not be a vector or a tibble, but a list:\n# library(ggformula)\npalmerpenguins::penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-year) %&gt;%\n  drop_na() %&gt;%\n  # `map` gives a list output\n  map(\n    .x = .,\n    .f = \\(x) gf_histogram(~x, bins = 30) %&gt;%\n      gf_theme(theme_classic())\n  )\n\n\n\n$bill_length_mm\n\n\n\n\n\n\n\n\n$bill_depth_mm\n\n\n\n\n\n\n\n\n$flipper_length_mm\n\n\n\n\n\n\n\n\n$body_mass_g\n\n\n\n\n\n\nNote: we need to do just a bit of extra pre-work to get the variable names on the x-axis of the histograms. There is a possibility to store all the plots in a separate column\nOK, so we can get vectors/tibbles/lists as output using vectors as inputs. Why would it be desirable to provide tibble/list as an input to a map_* function?\n\n Using purrr to create multiple models\nNow that we have some handle on purrr’s map functions, we can see how to develop a linear regression model for every country in the gapminder dataset. It should be clear from the command for a linear model:\n\n```{r}\n#| eval: false\nmodel &lt;- lm(target ~ predictor(s),\n  data = tibble_containing_target_and_predictors_columns\n)\n```\n\nthat we need to specify three things: target, predictors, and the data tibble for the development of a linear model. To do this for each country in gapminder, here is the process:\n\nGroup the gapminder data by country (and continent)\nCreate a column containing unique per-country data for each country. This column would hence contain a tibble in each cell. This is a list column!\nUse map which would take country and the data columns created above to create an lm object for each country (in another list column)\nUse map again with broom::tidy as the function to give us clean columns for the model per country.\nUse that multi-model tibble to plot graphs for each country.\n\nLet us do this now!\n\ngapminder_models &lt;- gapminder %&gt;%\n  group_by(continent, country) %&gt;%\n  # Create a per-country tibble in a new column called \"data_list\"\n  nest(.key = \"data_list\")\ngapminder_models\n\n\n  \n\n\ngapminder_models &lt;- gapminder_models %&gt;%\n  # We use mutate + map to add a list column containing linear models\n  mutate(model = map(\n    .x = data_list,\n\n    # One column .x to iterate over\n    # The .x list column contains data frames\n    # So we access individual columns for target and predictors\n    # within these individual data frames\n    .f = \\(.x) lm(lifeExp ~ year1950, data = .x)\n  )) %&gt;%\n  # Use mutate + map again to expose the columns of the models\n  # Use broom:: tidy, broom::glance(), and\n  # Use broom::augment for separate columns\n  mutate(\n    model_params = map(\n      .x = model,\n      .f = \\(.x) broom::tidy(.x,\n        conf.int = TRUE,\n        conf.lvel = 0.95\n      )\n    ),\n    model_metrics = map(\n      .x = model,\n      .f = \\(.x) broom::glance(.x)\n    ),\n    model_augment = map(\n      .x = model,\n      .f = \\(.x) broom::augment(.x)\n    )\n  )\ngapminder_models\n\n\n  \n\n\n\nWe can now take this tibble with multiple models and use broom to tidy, to glance at, and to augment the models:\n\nparams &lt;- gapminder_models %&gt;%\n  select(continent, country, model_params, model_metrics) %&gt;%\n  ungroup() %&gt;%\n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_params)\nparams\n\n\n  \n\n\n###\nmetrics &lt;- gapminder_models %&gt;%\n  select(continent, country, model_metrics) %&gt;%\n  ungroup() %&gt;%\n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_metrics)\nmetrics\n\n\n  \n\n\n###\naugments &lt;- gapminder_models %&gt;%\n  select(continent, country, model_augment) %&gt;%\n  ungroup() %&gt;%\n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_augment)\naugments\n\n\n  \n\n\n\n\n Model Visualization\nWe can now plot these models and their uncertainty (i.e Confidence Intervals). We can select a few of the countries and plot:\nparams_filtered &lt;- params %&gt;%\n  filter(\n    country %in% c(\"India\", \"United States\", \"Brazil\", \"China\"),\n    term == \"year1950\"\n  ) %&gt;%\n  select(country, estimate, conf.low, conf.high, p.value) %&gt;%\n  arrange(estimate)\nparams_filtered\n###\nparams_filtered %&gt;%\n  gf_errorbar(conf.high + conf.low ~ reorder(country, estimate),\n    linewidth = ~ -log10(p.value), width = 0.3,\n    ylab = \"Effect Size\",\n    xlab = \"Country\",\n    title = \"Effect of years on Life Expectancy\",\n    caption = \"Significance = - log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(country, estimate),\n    colour = \"black\", size = 4\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_refine(\n    coord_flip(),\n    scale_linewidth_continuous(\"Significance\",\n      range = c(0.2, 3)\n    )\n  ) %&gt;%\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    theme(axis.text.x = element_text(angle = 30, hjust = 1))\n  )\n\n\n\n\n  \n\n\n\n\n\n\n\nBut we can do better: visualize all models at once. What we will do is to plot the r.squared on the x-axis and the model term year1950 on the y-axis. We will need to combine params and metrics to do this:\nparams_combo &lt;- params %&gt;%\n  select(continent, country, term, estimate) %&gt;%\n  filter(term == \"year1950\") %&gt;%\n  left_join(metrics %&gt;% select(continent, country, r.squared))\nparams_combo\n###\nparams_combo %&gt;%\n  gf_point(reorder(country, r.squared) ~ r.squared,\n    color = \"grey70\"\n  ) %&gt;%\n  gf_point(reorder(country, r.squared) ~ r.squared,\n    data = params_combo %&gt;% filter(continent == \"Africa\"),\n    shape = 21, size = 3,\n    fill = \"salmon\",\n    ylab = \"Country\",\n    title = \"African Countries are Hard to Model\"\n  ) %&gt;%\n  gf_label(60 ~ 0.25,\n    label = \"African Countries\",\n    fill = \"salmon\",\n    color = \"black\",\n    inherit = FALSE\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_refine(theme(axis.text.y = element_text(size = 3, face = \"bold\")))\n###\nparams_combo %&gt;%\n  gf_point(estimate ~ r.squared, color = \"grey70\") %&gt;%\n  gf_point(estimate ~ r.squared,\n    data = params_combo %&gt;%\n      filter(continent == \"Africa\"),\n    shape = 21, size = 3,\n    fill = \"salmon\",\n    ylab = \"Slope Estimate for Linear Model\",\n    title = \"African Countries are Hard to Model\",\n    show.legend = FALSE\n  ) %&gt;%\n  gf_label(0.3 ~ 0.25,\n    label = \"African Countries\",\n    fill = \"salmon\",\n    color = \"black\",\n    inherit = FALSE\n  ) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAs can be seen, there are many models with low values of r.squared and these are sadly all about countries in Africa. The linear model fares badly for these countries, since there are other factors (not just year) that affects lifeExp in these countries.\nWe can look at the model metrics and see for which (African) countries the model fares the worst. We will reverse sort on r.squared and choose the 5 worst models:\n\nmetrics %&gt;% slice_min(order_by = r.squared, n = 5)\n\n\n  \n\n\n\nThere are of course reasons for this: genocide in Rwanda, and hyper-inflation in Zimbabwe, and of course the HIV-AIDS pandemic. These reasons are not captured in the original gapminder data!\nOne last plot! We can plot the model intercept on the x-axis and the slope year term on the y-axis to see where countries were in the beginning (1950) and at what rate they have improved in lifeExp:\n\nparams %&gt;%\n  select(continent, country, term, estimate) %&gt;%\n  pivot_wider(\n    id_cols = c(continent, country),\n    names_from = term,\n    values_from = estimate\n  ) %&gt;%\n  left_join(metrics %&gt;% select(continent, country, r.squared)) %&gt;%\n  gf_point(\n    year1950 ~ `(Intercept)`,\n    color = ~continent,\n    size = ~r.squared,\n    xlab = \"Baseline at 1950\",\n    ylab = \"Rate of Improvement\",\n    title = \"Asian Countries Show Improvement in Life Expectancy\",\n    subtitle = \"African Countries still struggling\",\n    caption = \"Data from Gapminder\"\n  ) %&gt;%\n  gf_refine(\n    scale_size(range = c(0.1, 4)),\n    scale_color_manual(\n      values =\n        c(\n          \"Africa\" = \"salmon\",\n          \"Asia\" = \"limegreen\",\n          \"Americas\" = \"grey90\",\n          \"Europe\" = \"grey90\",\n          \"Oceania\" = \"grey90\"\n        )\n    )\n  ) %&gt;%\n  gf_refine(guides(size = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nMany Asian countries were low in lifeExp in 1950 and have shown good rates of improvement; r.squared is also decent. Sadly African countries had low lifeExp in 1950 and have not shown good rates of improvement."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#recent-developments-in-dplyr",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#recent-developments-in-dplyr",
    "title": "Iteration: Learning to purrr",
    "section": "\n Recent developments in dplyr\n",
    "text": "Recent developments in dplyr\n\nIn recent times, the familiar dplyr package also has experimental functions that are syntactically easier and offer pretty much purrr-like capability, and without introducing the complexity of the list columns or list output.\nLook the code below and decipher how it works:\n\n# Using group_modify\ngapminder_model_dplyr &lt;- gapminder %&gt;%\n  group_by(continent, country) %&gt;%\n  # Here is the new function in dplyr!\n  # No need to use `mutate`\n  dplyr::group_modify(\n    .data = .,\n\n    # .f MUST generate a tibble here and *not* a list\n    # Hence broom::tidy is essential!\n    # glance/tidy is part of the group_map's .f variable.\n    # Applies to each model\n\n    .f = ~ lm(lifeExp ~ year, data = .) %&gt;%\n      broom::glance(\n        conf.int = TRUE, # try `tidy()` and `augment()`\n        conf.lvel = 0.95\n      )\n  ) %&gt;%\n  # We already have a grouped tibble from `group_modify()`\n  # So just ungroup()\n  ungroup()\n\ngapminder_model_dplyr\n\n\n  \n\n\n\nThere is no nesting and un-nesting; the data is the familiar tibble throughout! This seems like a simple and elegant method.\n\n\n\n\n\n\nTipUsing dplyr::group_modify\n\n\n\nNote: group_modify is new experimental function in dplyr (June 2023), as are group_map, list_cbind and list_rbind. group_modify requires that the operation in .fgenerates a tibble, not a list, and we can retain the grouping variable easily too. We can remove the groups with ungroup.\ngroup_modify() looks very clear and crisp, in my opinion. And very learner-friendly!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#conclusion",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#conclusion",
    "title": "Iteration: Learning to purrr",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen how purrr simplifies the application of functions iteratively to large groups of data, in a faster, replicable, and less error-prone manner. The basic idea (see video below) is:\n- Use tidyr::nest to create a grouped data frame with a nested list column\n- Use purrr::map_* to create a model for each of these data frames in the list column. The model will also be a column(usually) containing a list\n- Use broom::tidy to convert the list model-column into a data frame for visualization"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#references",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#references",
    "title": "Iteration: Learning to purrr",
    "section": "\n References",
    "text": "References\n\n\nRebecca Barter, Learn to purrr. https://www.rebeccabarter.com/blog/2019-08-19_purrr\nEmorie Beck, Introduction to purrr. https://emoriebeck.github.io/R-tutorials/purrr/#\nSander Wuyts, purrr Tutorial. https://sanderwuyts.com/en/blog/purrr-tutorial/\nJared Wilber, Using the tidyverse for Machine Learning. https://www.jwilber.me/nest/\nDan Ovando,Data Wrangling and Model Fitting using purrr\nCormac Nolan, Modelling with Nested Data frames. https://github.com/cormac85/modelling_practice/blob/master/nested_data_frames.Rmd"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#slides-and-tutorials",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#introduction",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Introduction",
    "text": "Introduction\nAh…ggplot ! All those wonderful pictures and graphs, that Alice might have relished!\nMetaphors, aesthetics, geometries…and pictures !! ggplot seems to equate ravens to writing desks in its syntax…and out come graphs!!\nAnd colours: Wes Anderson! Tim Burton! The Economist… and many others!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#references",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#references",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " References",
    "text": "References\n\nGeorge Lakoff and Mark Johnson, Metaphors We Live By, https://www.youtube.com/watch?v=lYcQcwUfo8c\nWickham and Grolemund, R for Data Science, ggplot chapter: https://r4ds.had.co.nz/data-visualisation.html\nCMDLineTips, 10 Tips to Customize Text Color, Font, Size in ggplot2 with element_text(), https://cmdlinetips.com/2021/05/tips-to-customize-text-color-font-size-in-ggplot2-with-element_text/\nCMDLineTips, How to write a simple custom ggplot theme from scratch, https://cmdlinetips.com/2022/05/how-to-write-a-simple-custom-ggplot-theme-from-scratch/\nAsha Hill @ mode.com, 12 Extensions to ggplot2 for More Powerful R Visualizations, https://mode.com/blog/r-ggplot-extension-packages/\nEmil Hvitfeldt, ggplot Trial and Error, https://www.emilhvitfeldt.com/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#fun-stuff",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Fun Stuff",
    "text": "Fun Stuff\n\nYihan Wu, Mapping ggplot geoms and aesthetic parameters, ( An interactive view of which aesthetic parameters work with which ggplot geom!! ) https://www.yihanwu.ca/post/geoms-and-aesthetic-parameters/\nhttps://www.theartstory.org/artist/kandinsky-wassily/"
  },
  {
    "objectID": "content/courses/Tech4Ed/listing.html#introduction",
    "href": "content/courses/Tech4Ed/listing.html#introduction",
    "title": "🧭 Tech Tools for Creative Education",
    "section": "Introduction",
    "text": "Introduction\nThis is my exploration of diverse tools to help explain things better. I will look at :\n\nIdyll Language: https://idyll-lang.org\nObservable JavaScript: https://observablehq.com/\nKinetic Graphs: https://kineticgraphs.org/\nApparatus: http://aprt.us/\nJoy.js: http://ncase.me/joy/\nTangle: http://worrydream.com/Tangle/\ng9: http://omrelli.ug/g9/gallery/\nLoopy: http://ncase.me/loopy/\nAll Explorable Tools: https://explorabl.es/tools/\nAPL: https://tryapl.org\np5.js: https://p5.js.org\nMath in Motion. https://math-in-motion.github.io/early-demo/\nReanimate: Build declarative animations with SVG and Haskell https://reanimate.github.io\nhttps://animejs.com\np5.teach.js. https://two-ticks.github.io/p5.teach.js/. Includes anime.js in a p5.js library.",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education"
    ]
  },
  {
    "objectID": "content/courses/Tech4Ed/listing.html#abstract-and-course-contents",
    "href": "content/courses/Tech4Ed/listing.html#abstract-and-course-contents",
    "title": "🧭 Tech Tools for Creative Education",
    "section": "Abstract and Course Contents",
    "text": "Abstract and Course Contents\nThe intent of this course is to provide a brief quick introduction to several different tools that help to create interactive diagrams, web pages, scrollies to explain ideas and concepts better.\nHopefully each module will also contain a simple but complete process on tools and how to install, integrate, and use them in a non-intimidating way.\nSince I am used to R + RStudio + Hugo/Quarto + Github + Netlify workflow, I will try my best to see if I can use any of these tools in that workflow. The best case would be:\n\nSimply invoke the specific tool in an RMarkdown or Quarto document. Today Observable is easily doable; Idyll also seems to be possible.\nPass data and variables seamlessly between multiple modules\nThey must not clash, these tools and give me hell like with htmlwidgets in R which was a miserable experience up to now. (Though it seems to be changing with Quarto..hmm, time to revisit)\nAnother very promising workflow is to simply use p5.js. This allows inclusion of a huge collection of teaching oriented libraries, and p5.js sketches are easily embedded in Quarto. (This may end up being my default workflow, and will allow me to keep R things and js things separate!)",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education"
    ]
  },
  {
    "objectID": "content/courses/Tech4Ed/listing.html#references",
    "href": "content/courses/Tech4Ed/listing.html#references",
    "title": "🧭 Tech Tools for Creative Education",
    "section": "References",
    "text": "References\n\nhttps://michaelnielsen.org/reinventing_explanation/\nhttps://distill.pub/\nBret Victor http://worrydream.com/\nNaval Ravikant. Good Explanations are Acts of Creativity. https://nav.al/explanations 5.Aditya Siddheshwarp5.teach: Teaching Math through Animations and Simulations. https://medium.com/processing-foundation/p5-teach-teaching-math-through-animations-and-simulations-64b6159fef85",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education"
    ]
  },
  {
    "objectID": "content/courses/Tech4Ed/Modules/20-UsingApparatus/index.html",
    "href": "content/courses/Tech4Ed/Modules/20-UsingApparatus/index.html",
    "title": "🧭 Using Apparatus",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education",
      "🧭 Using Apparatus"
    ]
  },
  {
    "objectID": "content/courses/Tech4Ed/Modules/30-UsingG9/index.html",
    "href": "content/courses/Tech4Ed/Modules/30-UsingG9/index.html",
    "title": "🧭 Using g9.js",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Tech for Creative Education",
      "🧭 Using g9.js"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/280-Israel-RuthCalderon/index.html",
    "href": "content/courses/ISTW/Modules/280-Israel-RuthCalderon/index.html",
    "title": " Israel - Ruth Calderon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-israel\"></iconify-icon> Israel - Ruth Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html",
    "title": "\n Sweden - Lars Gustaffsson",
    "section": "",
    "text": "From https://larsgustafssonblog.blogspot.com:\n\nLars Gustafsson (born May 17, 1936) is a Swedish, poet, novelist and scholar. He was born in Västerås, completed his secondary education at the Västerås gymnasium and continued to Uppsala University; he received his Licentiate degree in 1960 and was awarded his Ph.D. in Theoretical Philosophy in 1978. He lived in Austin, Texas until 2003, and has recently returned to Sweden. He served as a professor at the University of Texas in Austin, Texas, where he taught Philosophy and Creative Writing, until May 2006, when he retired.\nGustafsson is one of the most prolific Swedish writers since August Strindberg. Since the late 1950s he has produced a voluminous flow of poetry, novels, short stories, critical essays, and editorials. He is also an example of a Swedish writer who has gained international recognition with literary awards such as the Prix International Charles Veillon des Essais in 1983, the Heinrich Steffens Preis in 1986, Una Vita per la Litteratura in 1989, a John Simon Guggenheim Memorial Foundation Fellowship for poetry in 1994, and several others.\n\n\nWe will read Greatness Strikes Where It Pleases.\n\n“Greatness Strikes Where It Pleases” by Swedish writer Lars Gustafsson, was first published in Sweden in 1981. Translated into English in 1986, it appeared in Stories of Happy People (Norton, 1986; in print). It can also be found in You’ve Got to Read This: Contemporary Writers Introduce Stories That Held Them in Awe, edited by Ron Hansen (New York, 1994).\n\n\n\nBeing “Different”\nSensorial Learning\nSiblings and Families\nSchools, Institutions, and Teaching Pedagogies\nNature Deprivation\nEpiphanies",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-sweden\"></iconify-icon> Sweden - Lars Gustaffsson"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#lars-gustafsson",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#lars-gustafsson",
    "title": "\n Sweden - Lars Gustaffsson",
    "section": "",
    "text": "From https://larsgustafssonblog.blogspot.com:\n\nLars Gustafsson (born May 17, 1936) is a Swedish, poet, novelist and scholar. He was born in Västerås, completed his secondary education at the Västerås gymnasium and continued to Uppsala University; he received his Licentiate degree in 1960 and was awarded his Ph.D. in Theoretical Philosophy in 1978. He lived in Austin, Texas until 2003, and has recently returned to Sweden. He served as a professor at the University of Texas in Austin, Texas, where he taught Philosophy and Creative Writing, until May 2006, when he retired.\nGustafsson is one of the most prolific Swedish writers since August Strindberg. Since the late 1950s he has produced a voluminous flow of poetry, novels, short stories, critical essays, and editorials. He is also an example of a Swedish writer who has gained international recognition with literary awards such as the Prix International Charles Veillon des Essais in 1983, the Heinrich Steffens Preis in 1986, Una Vita per la Litteratura in 1989, a John Simon Guggenheim Memorial Foundation Fellowship for poetry in 1994, and several others.\n\n\nWe will read Greatness Strikes Where It Pleases.\n\n“Greatness Strikes Where It Pleases” by Swedish writer Lars Gustafsson, was first published in Sweden in 1981. Translated into English in 1986, it appeared in Stories of Happy People (Norton, 1986; in print). It can also be found in You’ve Got to Read This: Contemporary Writers Introduce Stories That Held Them in Awe, edited by Ron Hansen (New York, 1994).\n\n\n\nBeing “Different”\nSensorial Learning\nSiblings and Families\nSchools, Institutions, and Teaching Pedagogies\nNature Deprivation\nEpiphanies",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-sweden\"></iconify-icon> Sweden - Lars Gustaffsson"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#notes-and-references",
    "title": "\n Sweden - Lars Gustaffsson",
    "section": "Notes and References",
    "text": "Notes and References\n\n\n“Greatness Strikes Where It Pleases.” Short Stories for Students. Retrieved March 18, 2024 from Encyclopedia.com: https://www.encyclopedia.com/education/news-wires-white-papers-and-books/greatness-strikes-where-it-pleases\n\nLouv, Richard. October 15, 2019. What is Nature-Deficit Disorder? https://richardlouv.com/blog/what-is-nature-deficit-disorder\n\nMcleod, Saul.February 1, 2024. Jerome Bruner’s Theory Of Learning And Cognitive Development. https://www.simplypsychology.org/bruner.html. Especially read about Bruner’s idea of the Spiral Curriculum.\n\nNeurodiversity and Free Speech. https://quillette.com/2017/07/18/neurodiversity-case-free-speech/\n\nAn INTJ Professor’s Suicide Note. Yes. https://willopines.wordpress.com/2017/04/19/punched-out/\n\n\nSong for the Story !!\nA terrific comment on education itself!!\nArtistes: Super Tramp ( Roger Hodgson )\nAlbum: Breakfast in America\nYear: 1979\nChart Position: #6(US); #1(Australia, Canada, Norway, France, ..)\nFrom https://genius.com/Supertramp-the-logical-song-lyrics\n\n“The Logical Song” was the lead single from Supertramp’s chart-topping album Breakfast In America. It was internationally successful, reaching the top 20 in several countries including a #6 peak in the US and a #7 peak in the UK – their best showing in both countries. In Canada, “The Logical Song” not only topped the Canada Singles Chart, but was the #1 song of the entire year. “The Logical Song,” not unlike others in that period (eg – Pink Floyd’s “Another Brick in the Wall”) is a scathing criticism of British school and education at the time. When Paul McCartney was asked his favorite song of 1979, he chose “The Logical Song”. The song also won (Roger) Hodgson an Ivor Novello award from The British Academy of Composers and Songwriters in 1980.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-sweden\"></iconify-icon> Sweden - Lars Gustaffsson"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#writing-prompts",
    "title": "\n Sweden - Lars Gustaffsson",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nI never let my Schooling interfere with my Education.(Mark Twain)\nDescribing a Concept metaphorically using words from an unrelated field. Inspiration: Primo Levi’s immortal story, “Carbon”.\n\nOn being the odd child in a group of siblings (no victim-mentality-based ranting, please!)",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"emojione-v1:flag-for-sweden\"></iconify-icon> Sweden - Lars Gustaffsson"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html",
    "title": "\n Zimbabwe - Petina Gappah",
    "section": "",
    "text": "Petina Gappah is an Geneva-based international trade lawyer who has written poignant, humane and funny collection of stories about her home country, Zimbabwe. Her first collection of stories, An Elegy for Easterly was a winner of the Guardian First Book Award in 2009. Gappah’s collection of 13 stories, An Elegy for Easterly, tells of the lives of people, rich and poor, caught up in events over which they have little control.\n\n\n\nThe Mupandawana Dancing Champion\nPetina Gappah reads her own story Weblink to audio\n\n\nPoverty and Unemployment\nInflation and Purchasing Power\nIncompetent and Corrupt Government\nSilliness of High-Ranking Officials\nMusic, Dance and pursuits of common people\nHumour as a Defence Mechanism",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-zimbabwe\"></iconify-icon> Zimbabwe - Petina Gappah"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#zimbabwe-petina-gappah",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#zimbabwe-petina-gappah",
    "title": "\n Zimbabwe - Petina Gappah",
    "section": "",
    "text": "Petina Gappah is an Geneva-based international trade lawyer who has written poignant, humane and funny collection of stories about her home country, Zimbabwe. Her first collection of stories, An Elegy for Easterly was a winner of the Guardian First Book Award in 2009. Gappah’s collection of 13 stories, An Elegy for Easterly, tells of the lives of people, rich and poor, caught up in events over which they have little control.\n\n\n\nThe Mupandawana Dancing Champion\nPetina Gappah reads her own story Weblink to audio\n\n\nPoverty and Unemployment\nInflation and Purchasing Power\nIncompetent and Corrupt Government\nSilliness of High-Ranking Officials\nMusic, Dance and pursuits of common people\nHumour as a Defence Mechanism",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-zimbabwe\"></iconify-icon> Zimbabwe - Petina Gappah"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#notes-and-references",
    "title": "\n Zimbabwe - Petina Gappah",
    "section": "Notes and References",
    "text": "Notes and References\nAdditional Material\nThe Guardian. 5 Sept 2015. Interview with Petina Gappah. I’ve written a very Zimbabwean story.\nSong for the Story!\nBilly Joel’s Allentown from his 1982 album The Nylon Curtain:\n&gt; About Allentown\n\n“Allentown” is a song by American singer Billy Joel, which was the lead track on Joel’s The Nylon Curtain (1982) album, accompanied by a conceptual music video. Upon its release, and especially in subsequent years, “Allentown” emerged as an anthem of blue-collar America, representing both the aspirations and frustrations of America’s working class in the late 20th century.\n\n\n  Well we’re living here in Allentown\nAnd they’re closing all the factories down\nOut in Bethlehem they’re killing time\nFilling out forms\nStanding in line\nWell our fathers fought the Second World War\nSpent their weekends on the Jersey Shore\nMet our mothers in the USO1\nAsked them to dance\nDanced with them slow\nAnd we’re living here in AllentownBut the restlessness was handed down\nAnd it’s getting very hard to stay\nWell we’re waiting here in Allentown\nFor the Pennsylvania we never found\nFor the promises our teachers gave\nIf we worked hard\nIf we behaved\nSo the graduations hang on the wall\nBut they never really helped us at all\nNo they never taught us what was real\nIron and coal\nAnd chromium steel\nAnd we’re waiting here in Allentown\nBut they’ve taken all the coal from the ground\nAnd the union people crawled away\nEvery child has a pretty good shot\nTo get at least as far as their old man got\nBut something happened on the way to that place\nThey threw an American flag in our face\nWell I’m living here in Allentown\nAnd it’s hard to keep a good man down\nBut I won’t be giving up today\nAnd we’re living here in Allentown",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-zimbabwe\"></iconify-icon> Zimbabwe - Petina Gappah"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#writing-prompts",
    "title": "\n Zimbabwe - Petina Gappah",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nHumour and Laughter as our “last refuge”\nAn encounter with a Government Department\nListening to a Politician’s speech\nParticipating in an HR function at your workspot\nOn a story told to you by an elderly relative, full of mother-tongue words",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-zimbabwe\"></iconify-icon> Zimbabwe - Petina Gappah"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#footnotes",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#footnotes",
    "title": "\n Zimbabwe - Petina Gappah",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://en.wikipedia.org/wiki/United_Service_Organizations↩︎",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"twemoji:flag-zimbabwe\"></iconify-icon> Zimbabwe - Petina Gappah"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "",
    "text": "From the book blurb:\n\n“More convincingly than any other woman writing in Arabic today, Alifa Rifaat lifts the veil on what it means to be a woman living within a traditional Muslim society.” So states the translator’s foreword to this collection of the Egyptian author’s best short stories. Rifaat (1930–1996) did not go to university, spoke only Arabic, and seldom traveled abroad. This virtual immunity from Western influence lends a special authenticity to her direct yet sincere accounts of death, sexual fulfillment, the lives of women in purdah, and the frustrations of everyday life in a male-dominated Islamic environment.\nTranslated from the Arabic by Denys Johnson-Davies the collection admits the reader into a hidden private world, regulated by the call of the mosque, but often full of profound anguish and personal isolation. Badriyya’s despairing anger at her deceitful husband, for example, or the haunting melancholy of “At the Time of the Jasmine,” are treated with a sensitivity to the discipline and order of Islam.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#alifa-rifaat",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#alifa-rifaat",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "",
    "text": "From the book blurb:\n\n“More convincingly than any other woman writing in Arabic today, Alifa Rifaat lifts the veil on what it means to be a woman living within a traditional Muslim society.” So states the translator’s foreword to this collection of the Egyptian author’s best short stories. Rifaat (1930–1996) did not go to university, spoke only Arabic, and seldom traveled abroad. This virtual immunity from Western influence lends a special authenticity to her direct yet sincere accounts of death, sexual fulfillment, the lives of women in purdah, and the frustrations of everyday life in a male-dominated Islamic environment.\nTranslated from the Arabic by Denys Johnson-Davies the collection admits the reader into a hidden private world, regulated by the call of the mosque, but often full of profound anguish and personal isolation. Badriyya’s despairing anger at her deceitful husband, for example, or the haunting melancholy of “At the Time of the Jasmine,” are treated with a sensitivity to the discipline and order of Islam.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#story",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#story",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "Story",
    "text": "Story\nWe will read Rifaat’s story Bahiyya’s Eyes from the Collection Distant View of a Minaret and Other Stories.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#themes",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#themes",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "Themes",
    "text": "Themes\n\nAge and Aging\nPoverty, Disease, and Healthcare\nWoman’s Place in Society\n“Eyes are the Gateway to the Soul” - Herman Melville\nReligion, Tradition, and Conservative Belief: Is Belief always “Blind”?\nThrough a Child’s Eyes: A Child’s Discoveries about the World\nChildren and Family",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#additional-material",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nبَهِيّة Bahiyyah is an Arabic name for girls that means “brilliant”, “beautiful”, “radiant”.\nAlifa Rifaat on the Dangerous Women Project https://dangerouswomenproject.org/2016/04/30/alifa-rifaat/\nBanipal Magazine of Modern Arabic Literature. http://www.banipal.co.uk\nhttps://www.arabicfiction.org/en\nDenys Johnson-Davies https://www.theguardian.com/world/2017/jun/18/denys-johnson-davies-obituary\nMalti-Douglas, Fedwa. Men, Women, and God(s): Nawal El Saadawi and Arab Feminist Poetics. Berkeley: University of California Press, c1995 1995. http://ark.cdlib.org/ark:/13030/ft8c6009n4/\nYa Baheya, the Woman Behind the Legend. http://www.shira.net/music/ya-baheya-background.htm\nOyun Baheya, the Song. https://youtu.be/sb4ZNdLGeIo\nFemale Genital Mutilation. https://www.who.int/news-room/fact-sheets/detail/female-genital-mutilation and https://data.unicef.org/topic/child-protection/female-genital-mutilation/\nSong for the Story\nSong: Do Naina aur Ek Kahani\nFilm: Masoom (1983)\nArtist: Arati Mukherjee\nMusic Director: R.D Burman\nLyricist: Gulzar\nStarring: Naseeruddin Shah, Shabana Azmi, Master Jugal, Baby Urmila, Saeed Jaffery\nDirector : Shekhar Kapoor\nhttps://youtu.be/qfrdkbQHw48\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nDo naina aur ek kahanee(2)\nTwo eyes and one story,\n\n\nthoda sa badal, thoda sa panee aur ek kahanee(2)\nsome clouds, some water, and a story\n\n\n——\n——\n\n\nChhotee see do jheelon me woh, bahatee rahatee hai(2)\nIn two small lakes, they keep floating/flowing\n\n\nKoi sune ya na sune, kahatee rahatee hai\nWhether anyone is listening or not, they keep telling the story,\n\n\nKuchh likh ke aur, kuchh zubanee\nSome Written, and Sometimes by speaking out\n\n\n——\n——\n\n\nThodee sai hain janee hui, thodee see nayee(2)\nA part of the story is known, but a small part of it is new\n\n\nJahaan ruke aansu, waheen puree ho gayee\nWhenever the tears stop, that is where the story stops too,\n\n\nHai toh nayee phir bhi, hain puranee\nIt is new but its old at the same time\n\n\n——\n——-\n\n\nEk khatm ho toh, dusaree raat aa jatee hai(2)\nAs one night passes, it is time for the other night,\n\n\nHothhon pe phir bhulee hui, baat aa jatee hai\nAnd those forgotten words cross my lips\n\n\nDo naino kee hain yeh kahanee\nThat is the story of the two eyes!\n\n\nthoda sa badal, thoda sa panee aur ek kahanee\nsome clouds, some water, and a story\n\n\nDo naina aur ek kahanee…\nTwo eyes and one story…..",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#writing-prompts",
    "title": "\n Egypt - Alifa Rifaat",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nContemplate your life, and demise, and write a Will. Dwell on relationships and not merely on (imaginary) opulence/ property.\nYour Friend is Blind.\nA Child discovers a piece of Family History.\nCompare the theme of blindness in this story with that in Raymond Carver’s story, Cathedral",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:egypt\"></iconify-icon> Egypt - Alifa Rifaat"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#story",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#story",
    "title": "\n Italy - Dino Buzzati",
    "section": "Story",
    "text": "Story\nWe will read Buzzati’s surreal story, The Falling Girl PDF",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Dino Buzzati"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#themes",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#themes",
    "title": "\n Italy - Dino Buzzati",
    "section": "Themes",
    "text": "Themes\n\nBeauty and Other Values in Society\nClasses in Society\nAmbition and Competition\nFear of Missing Out (#FOMO)\nSurreal\n“Inversion of Scale”",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Dino Buzzati"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#notes-and-references",
    "title": "\n Italy - Dino Buzzati",
    "section": "Notes and References",
    "text": "Notes and References\nSong for the Story !!\nWaheeda Rehman, the stunning “Falling Girl” in “Guide” ❤️ !! Waah! Dino Buzzati should have watched this!! He would have approved!!\n\n\n Lyrics and Translation: https://www.filmyquotes.com/songs/1245\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKaanton se kheench ke yeh aanchal\nAfter pulling my dress out of thorns\n\n\nTodke bandhan baandhi payal\nAfter breaking the anklets of vows\n\n\nHo koi na roko dil ki udaan ko\nPlease no one stop the flight of the heart\n\n\nDil woh chala aa aa aa\nThere goes my heart flying…\n\n\n—————\n—————\n\n\nAaj phir jeene ki tamanna hai(2)\nToday, I have a desire to live again\n\n\nAaj phir marne ka irada hai(2)\nToday, I have an intention to die again!\n\n\n—————-\n—————-\n\n\nApne hi bas mein nahi main(2)\nI don’t have any control over myself\n\n\nDil hai kahin toh hoon kahin main(2)\nMy heart is somewhere and I’m somewhere else\n\n\nHo jaane kya paake meri zindagi ne\nI don’t know what my life has attained\n\n\nHanskar kaha ha ha ha\nBut it says this laughingly\n\n\n—————–\n——————\n\n\nMain hoon gubaar ya toofan hoon(2)\nAm I intoxicated or am I a storm\n\n\nKoi bataye main kahan hoon(2)\nPlease someone tell me where am I\n\n\nHo dar hai safar mein kahin kho na jaaon main\nI’m scared that I might get lost in the journey\n\n\nRasta naya aa aa aa\nAs these roads are new for me\n\n\n—————-\n——————–\n\n\nKal ke andheron se nikal ke(2)\nAfter coming out from the darkness of yesterday\n\n\nDekha hai aankhen malte malte(2)\nI’ve seen this rubbing my eyes\n\n\nHo phool hi phool zindagi bahaar hai\nThere is spring and flowers everywhere\n\n\nTay kar liya aa aa aa\nHence I’ve decided that…\n\n\n—————–\n———————\n\n\nAaj phir jeene ki tamanna hai(2)\nToday, I have a desire to live again\n\n\nAaj phir marne ka irada hai(2)\nToday, I have an intention to die again!",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Dino Buzzati"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#writing-prompts",
    "title": "\n Italy - Dino Buzzati",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Comparing this Falling Girl with another very Famous One\n\nOn Looking in the Mirror 25 Years from Now OR A Letter from my 60-year-old self to me at 18-years-old\nOn saying “Anyways” and “My Bad”: Trying to Be Different in the Same Way\nFirst Copy, Then Innovate!",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:italy\"></iconify-icon> Italy - Dino Buzzati"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html",
    "title": "\n Brazil - Clarice Lispector",
    "section": "",
    "text": "A Woman, a Jew, born in Ukraine, and one of Brazil’s greatest writers…..Clarice Lispector.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#clarice-lispector",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#clarice-lispector",
    "title": "\n Brazil - Clarice Lispector",
    "section": "",
    "text": "A Woman, a Jew, born in Ukraine, and one of Brazil’s greatest writers…..Clarice Lispector.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#story",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#story",
    "title": "\n Brazil - Clarice Lispector",
    "section": "Story",
    "text": "Story\nWe will read her story, “Beauty and the Beast, or, The Wound too Great”, p. 291 in the Oxford Anthology of the Brazilian Short Story, edited by K. David Jackson.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#themes",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#themes",
    "title": "\n Brazil - Clarice Lispector",
    "section": "Themes",
    "text": "Themes\n\nBeauty and Ugliness\nBody as Power\nEphiphany\nWhere, When, and from Whom do We Learn?\nReligion as “Lived Life” (Practical Spirituality)\nMetaphors\nMaterialism…",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#additional-material",
    "title": "\n Brazil - Clarice Lispector",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nhttps://www.newyorker.com/books/page-turner/the-true-glamour-of-clarice-lispector\nSuryakant Tripathi “Nirala”. 1923. Bhikshuk. http://kavitakosh.org\n\nवह आता–\nदो टूक कलेजे को करता, पछताता पथ पर आता।\nपेट पीठ दोनों मिलकर हैं एक,\nचल रहा लकुटिया टेक,\nमुट्ठी भर दाने को —भूख मिटाने को\nमुँह फटी पुरानी झोली का फैलाता —\nदो टूक कलेजे के करता पछताता पथ पर आता।\nसाथ दो बच्चे भी हैं सदा हाथ फैलाए,\nबाएँ से वे मलते हुए पेट को चलते,\nऔर दाहिना दया दृष्टि-पाने की ओर बढ़ाए।\nभूख से सूख ओठ जब जाते\nदाता-भाग्य विधाता से क्या पाते?\nघूँट आँसुओं के पीकर रह जाते।\nचाट रहे जूठी पत्तल वे सभी सड़क पर खड़े हुए,\nऔर झपट लेने को उनसे कुत्ते भी हैं अड़े हुए !\nठहरो ! अहो मेरे हृदय में है अमृत, मैं सींच दूँगा\nअभिमन्यु जैसे हो सकोगे तुम\nतुम्हारे दुख मैं अपने हृदय में खींच लूँगा।\n-Suryakant Tripathi “Nirala”",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#song-for-the-story",
    "title": "\n Brazil - Clarice Lispector",
    "section": "Song for the Story",
    "text": "Song for the Story\nThe roles are the reversed in this song are they? But that is possibly what we saw in the story…who is the beggar?\n\n\n\nAnother not un-related song, is this one by England Dan Seals and John Ford Coley, especially if you think the lady had a boyfriend earlier.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#writing-prompts",
    "title": "\n Brazil - Clarice Lispector",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Growing Older by 5 years in 5 minutes\nOn How Others seem to Define Us\nExplain something using Religous Metaphors (any religion, any scripture) !!",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:brazil\"></iconify-icon> Brazil - Clarice Lispector"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html",
    "title": "\n Russia - Ivan Bunin",
    "section": "",
    "text": "IVAN BUNIN, the first Russian writer to win the Nobel Prize (1931), was born in 1870 to an aristocratic family in Vorornezh. After attending the University of Moscow briefly, he brought out his first book, a volume of verse. For this and his realistic accounts of the decay of the Russian nobility, he was awarded the Pushkin Prize for Literature and elected to the Russian Academy. He fled to western Europe, following the Revolution, and lived mainly in Paris, sometimes nearly destitute, until his death at the age of eightythree. His study of the dying patriarchy among Russian peasants raises him into the front rank of European novelists, but his present reputation rests on his short stories, in such collections as The Gentleman from San Francisco and The Grammar of Love. In many of his stories he contrasts the transitoriness of human life with the endurance of beauty and nature. Somerset Maugham has called “Sunstroke” one of the world’s best stories.\n\n\nWe will read Ivan Bunin’s short story, Sunstroke\n\n\nFirst Love\nShipboard Romances ( cliche )\nTime and Memory, Senses\n“River of Life” situation\nOne Night Stands?\nAdultery: Bad for Life but good for Literature?\nMen Don’t Cry?\n\nThe Map of the Story\n\n\nError in tmap_options(check.and.fix = TRUE): the following option does not exist: check.and.fix\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"c('double', 'numeric')\"\n\n\n\n\nError: object 'russian_rivers' not found\n\n\n\n\nIvan Bunin – Biographical. NobelPrize.org. Nobel Prize Outreach AB 2022. Sat. 5 Mar 2022. https://www.nobelprize.org/prizes/literature/1933/bunin/biographical/",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Ivan Bunin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#ivan-bunin",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#ivan-bunin",
    "title": "\n Russia - Ivan Bunin",
    "section": "",
    "text": "IVAN BUNIN, the first Russian writer to win the Nobel Prize (1931), was born in 1870 to an aristocratic family in Vorornezh. After attending the University of Moscow briefly, he brought out his first book, a volume of verse. For this and his realistic accounts of the decay of the Russian nobility, he was awarded the Pushkin Prize for Literature and elected to the Russian Academy. He fled to western Europe, following the Revolution, and lived mainly in Paris, sometimes nearly destitute, until his death at the age of eightythree. His study of the dying patriarchy among Russian peasants raises him into the front rank of European novelists, but his present reputation rests on his short stories, in such collections as The Gentleman from San Francisco and The Grammar of Love. In many of his stories he contrasts the transitoriness of human life with the endurance of beauty and nature. Somerset Maugham has called “Sunstroke” one of the world’s best stories.\n\n\nWe will read Ivan Bunin’s short story, Sunstroke\n\n\nFirst Love\nShipboard Romances ( cliche )\nTime and Memory, Senses\n“River of Life” situation\nOne Night Stands?\nAdultery: Bad for Life but good for Literature?\nMen Don’t Cry?\n\nThe Map of the Story\n\n\nError in tmap_options(check.and.fix = TRUE): the following option does not exist: check.and.fix\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"c('double', 'numeric')\"\n\n\n\n\nError: object 'russian_rivers' not found\n\n\n\n\nIvan Bunin – Biographical. NobelPrize.org. Nobel Prize Outreach AB 2022. Sat. 5 Mar 2022. https://www.nobelprize.org/prizes/literature/1933/bunin/biographical/",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Ivan Bunin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#notes-and-references",
    "title": "\n Russia - Ivan Bunin",
    "section": "Notes and References",
    "text": "Notes and References\n\nMahAkavi KAlidAsa, “raghuvaMsham” (Dynasty of Emperor Raghu, 8th chapter, 95 verses). https://sanskritdocuments.org/sites/giirvaani/giirvaani/rv/sargas/08_rv.htm\n\n\n\nThe Lament of Aja\n\n\nविललाप स बाष्पगद्गदम् सहजामप्यपहाय धीरताम्| अभितप्तमयोऽपि मार्दवम् भजते कैव कथा शरीरिषु॥ ८-४\n\n\nvilalāpa sa bāṣpagadgadam sahajāmapyapahāya dhīratām| abhitaptamayo’pi mārdavam bhajate kaiva kathā śarīriṣu || 8-43\n\n\n\nsaH sahajAm api dhIratAm apahAya = he, naturally though, firmness, on forgoing;\n\nbAShpa gadgadam vilalApa = with tears, stammer, bewailed;\n\nabhitaptam ayaH api mArdavam bhajate = when excessively heated, iron, even, softness, acquires;\n\nsharIriShu kaiva kathA = of those possessing bodies, what, can be said.\n\n\n\nHaving even given up his natural fortitude, Aja bewailed stammering on account of his being choked with tears. Even iron when excessively heated acquires softness; what then can be said in respect of those possessing bodies. [8-43]\n\n\nPorter, Richard N. “Bunin’s ‘A Sunstroke’ and Chekhov’s ‘The Lady with the Dog.’” South Atlantic Bulletin 42, no. 4 (1977): 51–56. https://doi.org/10.2307/3199025.\n\n\nChekhov’s “The Lady with the Dog”and Bunin’s “A Sunstroke” have much in common, are frequently mentioned in connection with each other, and lend themselves to comparison. By discerning what features of the stories are alike and unlike one can learn much about the overall similarities and differences of the authors.\nThe plots of both stories are familiar. “The Lady with the Dog” is about Dmitry Dmitrich Gurov, a banker from Moscow, not yet forty, married, and the father of three children, and Anna Sergeevna von Dideritz, who has married two years before and now lives in the provincial city of S. They meet in Yalta, where they are spending their vacations alone. Soon they have an affair. Despite qualms on Anna’s part, they are fairly happy, but Gurov is relieved when she goes. At home in Moscow, he is surprised find that he does not forget her quickly. Instead, he misses her more and more and decides to go to S. to see her. She is surprised but admits that she has thought of him often and arranges to visit him occasionally in Moscow. On her visits, they meet in her hotel room. Although they find some happiness, they realize that the most difficult part of their affair is just beginning.\n\n\n“A Sunstroke” is about a lieutenant and a young married woman, both of them anonymous, who meet on a Volga river boat. They are immediately drawn to each other and agree to get off at a small town, where they spend the night. When the woman leaves the next morning, the lieutenant does not mind her going; but later in the day he realizes that he misses her desperately. He cannot go after her because she has not told him her name. He tries unsuccessfully in various ways to overcome his sense of loss, and, when he takes the boat that evening, he feels that he has grown ten years older.\n\nSongs for the Story !!\nA torch ballad by Phil Collins!\n\n\n And an equally good lament by the lady: Vacation, by the Go-Gos.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Ivan Bunin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#writing-prompts",
    "title": "\n Russia - Ivan Bunin",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nDon’t be a Crybaby\n“Tere Bina Zindagi se Koi” story in English\n…",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Ivan Bunin"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html",
    "title": "\n Canada - John Cheever",
    "section": "",
    "text": "We will read Cheever’s dystopian tech story, The Enormous Radio.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:canada\"></iconify-icon> Canada - John Cheever"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#john-cheever",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#john-cheever",
    "title": "\n Canada - John Cheever",
    "section": "",
    "text": "We will read Cheever’s dystopian tech story, The Enormous Radio.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:canada\"></iconify-icon> Canada - John Cheever"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#themes",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#themes",
    "title": "\n Canada - John Cheever",
    "section": "Themes",
    "text": "Themes\n\nCommunications Technology: Boon and Bane\n“Sweet Old Tech”\nUses of Tech, and Used by Tech\n“We Wish to Be SomeWhere We are Not”\n“We Wish to Be SomeWhenTime We are Not”\n“We Wish to Be SomeOne We are Not”\n“We Wish to Be SomeThing We are Not”\nForbidden Knowledge\nLove Thy Neighbour\n\nAdditional Material\nNotes and References",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:canada\"></iconify-icon> Canada - John Cheever"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#song-for-the-story",
    "title": "\n Canada - John Cheever",
    "section": "Song for the Story !!",
    "text": "Song for the Story !!\nSong: Somebody’s Watching Me\nGroup: Rockwell and Michael Jackson\nRelease Date: Pongal / Makara Sankranthi, January 14, 1984\n\n“Somebody’s Watching Me” is a song recorded by American singer Rockwell (Kennedy Gordy), released by the Motown label in 1984, as the lead single from his debut studio album of the same name. Rockwell’s debut single release, the song features guest vocals by brothers Michael Jackson (in the chorus) and Jermaine Jackson (additional backing vocals).\n\n\n\nAlso listen to The Buggles - Video Killed the Radio Star and Queen - Radio Gaga.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:canada\"></iconify-icon> Canada - John Cheever"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#writing-prompts",
    "title": "\n Canada - John Cheever",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn being Average, or being thought to be Average\nAm I using [your favourite SM app] or is it using me?\nOn Human Dreams and Technology. Take inspiration from Robert Lucky New Communications Services –What does Society want?. Look especially on the Section VI. therein on Fulfilling Ancient Dreams.\nOn bugs in internet apps. Take inspiration from Vinton G. Cerf’s famous Internet document in poetry form, RFC968 https://datatracker.ietf.org/doc/html/rfc968.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:canada\"></iconify-icon> Canada - John Cheever"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/260-USA-KurtVonnegut/index.html",
    "href": "content/courses/ISTW/Modules/260-USA-KurtVonnegut/index.html",
    "title": " USA - Kurt Vonnegut",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:united-states\"></iconify-icon> USA - Kurt Vonnegut"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "",
    "text": "Ventura García Calderón (1886–1959) was a Peruvian man of letters and a diplomat who was at the center of the hispanophone community in Paris in the first half of the twentieth century. Known as a proponent of Spanish American literature, García Calderón achieved a global celebrity for his dramatic, colorful, and ironic short stories. These stories, published in both Spanish and French, feature a raw depiction of reality, a strong sense of retributive justice, and a sympathy for the marginalized people that characterize European Naturalism. García Calderón adapted this style to advance his goal of providing European readers with an authentic understanding of Peru and Spanish America, thus replacing the voyeuristic and patronizing notion of the “exotic” inherited from literary romanticism and nineteenth-century travel writers.\n\n\nThe construction of García Calderón’s stories was subversive and destabilized the widespread notion of Peru held by European critics and readers. While international critics during the author’s lifetime unanimously praised García Calderón’s fiction as well as his essays that theorize the transformation and renaissance of Spanish language and literature by americano writers, scholars since the 1960s have largely misunderstood his reformative project.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#ventura-garcia-calderon",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#ventura-garcia-calderon",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "",
    "text": "Ventura García Calderón (1886–1959) was a Peruvian man of letters and a diplomat who was at the center of the hispanophone community in Paris in the first half of the twentieth century. Known as a proponent of Spanish American literature, García Calderón achieved a global celebrity for his dramatic, colorful, and ironic short stories. These stories, published in both Spanish and French, feature a raw depiction of reality, a strong sense of retributive justice, and a sympathy for the marginalized people that characterize European Naturalism. García Calderón adapted this style to advance his goal of providing European readers with an authentic understanding of Peru and Spanish America, thus replacing the voyeuristic and patronizing notion of the “exotic” inherited from literary romanticism and nineteenth-century travel writers.\n\n\nThe construction of García Calderón’s stories was subversive and destabilized the widespread notion of Peru held by European critics and readers. While international critics during the author’s lifetime unanimously praised García Calderón’s fiction as well as his essays that theorize the transformation and renaissance of Spanish language and literature by americano writers, scholars since the 1960s have largely misunderstood his reformative project.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#story",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#story",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Story",
    "text": "Story\nWe will read Ventura Garcia Calderon’s short short story, The Lottery Ticket",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#themes",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#themes",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Themes",
    "text": "Themes\n\nMarginalized People\nColour\nWomen and Objectification\nMob Justice\nThe Art of Protest!",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#additional-material",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Additional Material",
    "text": "Additional Material\n\n\nKursi Nashin: A Certificate of Discrimination from British India\n\n\n\nReview of The Lottery Ticket https://caponomics.blogspot.com/2013/05/short-story-review-lottery-ticket-by.html\nVentural Garcia Calderon at Short Story Magic Tricks: https://shortstorymagictricks.com/2021/07/06/the-lottery-ticket-by-ventura-garcia-calderon/\nGoldberg, N. S. (2014). Rereading Ventura García Calderón. Hispania, 97(2), 220–232. http://www.jstor.org/stable/24368768",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#notes-and-references",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Notes and References",
    "text": "Notes and References\n\nRene Girard’s Mimetic Theory and the Scapegoat: https://violenceandreligion.com/mimetic-theory/\nFrear, G. L. (1992). René Girard on Mimesis, Scapegoats, and Ethics. The Annual of the Society of Christian Ethics, 12, 115–133. http://www.jstor.org/stable/23559770\nThe Beauty of Fat Women: Leonard Nimoy (“Mr Spock”) and his Full Body Project: https://www.theguardian.com/artanddesign/gallery/2015/mar/03/the-full-body-project-by-leonard-nimoy-in-pictures\n\n\nLeonard Nimoy, who died February 27,2015 at the age of 83, was beloved by fans for his distinctive portrayal of Mr. Spock on Star Trek. Those fans may not have known that Nimoy, through his work as a photographer, also championed women who did not conform to Hollywood’s ideal of physical perfection. [see also http://mashable.com/2015/02/26/body-positivity-get-involved In 2007, Nimoy published The Full Body Project, a collection of photos featuring nude women of many shapes and sizes. Nimoy’s previous book of photographs captured images of nude women as well, though the models’ slim bodies hewed closely to the conventional standards of beauty. The inspiration for The Fully Body Project struck when a full-figured woman approached Nimoy and asked if he might photograph her and her friends.\n\nListen to Leonard Nimoy discuss this project on NPR:\n\n\n\nIt Ain’t Over until the Fat Lady Sings: https://knowyourphrase.com/aint-over-until-the-fat-lady-sings\nIs it all about Hips? http://bollynatyam.com/books/\n\n\n\n\n\n\n\n\n\nhttps://www.youtube.com/embed/ByAbV-MKDgs?si=PU26bRl2sXOynWpL",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#songs-for-the-story",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#songs-for-the-story",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Songs for the Story !!",
    "text": "Songs for the Story !!\nSong for the Black Hero of this story\n\nTitle: What About Me?\n\nBand: Moving Pictures\n\nBand Location: Sydney, NSW, Australia\n\nAlbum: Days Of Innocence\n\nComposed By: Garry Frost, Frances Swan\n\nRelease Date: January, 1982\nChart Position:\nNo.1 (Australia)\n\nNo.29 (US Billboard Hot 100)\n\n\n\n\n\n\nSong for Cielito\n\nTitle: Bette Davis Eyes\nArtiste: Kim Carnes\nAlbum: Mistaken Identity\nComposed by: Donna Weiss and Jackie DeShannon\nYear: 1981\nChart Position:\nNo.1 (Australia)\nNo.1 (US Billboard Hot 100)\nNo. 10 (UK Singles Charts)\n\n\n\n\n\nThe “Lottery” Idea in Literature\n\nShirley Jackson, The Lottery. https://www.newyorker.com/magazine/1948/06/26/the-lottery\n\nMunshi Premchand. Lottery, short story in Hindi. https://en.wikipedia.org/wiki/Lottery_(short_story)\n\nAnton Chekhov, The Lottery Ticket. https://www.classicshorts.com/stories/lottery.html\n\nJose Luis Borges. The Lottery of Babylon.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#writing-prompt",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#writing-prompt",
    "title": "\n Peru - Ventura Garcia Calderon",
    "section": "Writing Prompt",
    "text": "Writing Prompt\n\nHow I went on Strike\nOn an ad for Fairness Cream\nOn a personal encounter with racism/bias\nDid I win the Lottery?\nCompare Cielito with the Prostitute in Guy de Maupassant’s story Boule de Suif.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"openmoji:flag-peru\"></iconify-icon> Peru - Ventura Garcia Calderon"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#themes",
    "href": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#themes",
    "title": "\n India - Bharati Mukherjee",
    "section": "Themes",
    "text": "Themes\nNotes and References",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:india\"></iconify-icon> India - Bharati Mukherjee"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#song-for-the-story",
    "title": "\n India - Bharati Mukherjee",
    "section": "Song for the Story",
    "text": "Song for the Story",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:india\"></iconify-icon> India - Bharati Mukherjee"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html",
    "title": "\n Russia - Maxim Gorky",
    "section": "",
    "text": "Maxim Gorky (born March 16, 1868, Nizhny Novgorod, Russia—died June 14, 1936) Russian short-story writer and novelist who first attracted attention with his naturalistic and sympathetic stories of tramps and social outcasts and later wrote other stories, novels, and plays, including his famous The Lower Depths.\n\n\n(He) was a Russian author considered the father of Soviet revolutionary literature and founder of the doctrine of socialist realism. After having a difficult childhood, he roamed across the Russian empire, frequently changing jobs for about fifteen years before he became a successful writer. The experiences he had during those fifteen years deeply influenced his writing. Initially, he wrote stories mainly based on the lives of tramps and social outcasts, and he became known for his naturalistic style of writing. One of his greatest works is ‘The Mother,’ which Lenin praised as “a very timely book.”\n\n\nGorky was deeply associated with fellow Russian writers, Anton Chekhov and Leo Tolstoy and later wrote memoirs on them. Gorky was not only a great writer but also an influential figure in the political thinking. He was active with the emerging Marxist social-democrat movement. Initially a Bolshevik supporter, he became a critic when Vladimir Lenin seized power. However, later Gorky served as a Soviet advocate and headed the Union of Soviet Writers. His life was marked with a number of politically forced and sometimes self-imposed exiles.\n\n\nHe was nominated five times for the Nobel Prize in Literature.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#maxim-gorky",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#maxim-gorky",
    "title": "\n Russia - Maxim Gorky",
    "section": "",
    "text": "Maxim Gorky (born March 16, 1868, Nizhny Novgorod, Russia—died June 14, 1936) Russian short-story writer and novelist who first attracted attention with his naturalistic and sympathetic stories of tramps and social outcasts and later wrote other stories, novels, and plays, including his famous The Lower Depths.\n\n\n(He) was a Russian author considered the father of Soviet revolutionary literature and founder of the doctrine of socialist realism. After having a difficult childhood, he roamed across the Russian empire, frequently changing jobs for about fifteen years before he became a successful writer. The experiences he had during those fifteen years deeply influenced his writing. Initially, he wrote stories mainly based on the lives of tramps and social outcasts, and he became known for his naturalistic style of writing. One of his greatest works is ‘The Mother,’ which Lenin praised as “a very timely book.”\n\n\nGorky was deeply associated with fellow Russian writers, Anton Chekhov and Leo Tolstoy and later wrote memoirs on them. Gorky was not only a great writer but also an influential figure in the political thinking. He was active with the emerging Marxist social-democrat movement. Initially a Bolshevik supporter, he became a critic when Vladimir Lenin seized power. However, later Gorky served as a Soviet advocate and headed the Union of Soviet Writers. His life was marked with a number of politically forced and sometimes self-imposed exiles.\n\n\nHe was nominated five times for the Nobel Prize in Literature.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#story",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#story",
    "title": "\n Russia - Maxim Gorky",
    "section": "Story",
    "text": "Story\nWe will read Gorky’s story The Clown.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#themes",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#themes",
    "title": "\n Russia - Maxim Gorky",
    "section": "Themes",
    "text": "Themes\n\nBeing Pretentious and Being Genuine\nHumour as way to examine Life1\n\nWhat does a Court Jester/Stand-Up Comedian do for a living2?\nA Joker in a pack of cards?\nStreet Events, The Bystander Effect, and Jane Jacobs’ idea “Eyes on the Street”\nSignalling and How to Speak without Talking\nRisk Taking for the Benefit of Society\nHating Oneself for Cowardice?\nEnvy / Jealousy and Anger",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#additional-material",
    "title": "\n Russia - Maxim Gorky",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nHingley, Ronald Francis. “Maxim Gorky”. Encyclopedia Britannica, 30 Nov. 2023. https://www.britannica.com/biography/Maxim-Gorky. Accessed 6 January 2024.\nGuzeva, Alexandra. Russia Beyond, March 28, 2018. 5 reasons why Soviet writer Maxim Gorky is so great. https://www.rbth.com/arts/327885-why-soviet-writer-gorky-great. Accessed 6 January 2024.\nJane Jacobs. 1961. “The Death and Life of Great American Cities”. PDF.\n\n\nThe Death and Life of Great American Cities is a 1961 book by writer and activist Jane Jacobs. The book is a critique of 1950s urban planning policy, which it holds responsible for the decline of many city neighborhoods in the United States. The book is Jacobs’ best-known and most influential work.\n\n\nRobert Kanigel’s biography of Jacobs is called Eyes on the Street, a phrase that Jacobs herself coined about the crucial importance of a vibrant street life to neighborhood safety and community.\n\n\n\n\nCarlheim-Gyllensköld, Monika. March 11, 2016. Lars Gustafsson’s Blog: An open letter to Roger Mogert\n\nOn Not Speaking Out and the Bystander Effect\n\n\n\n“First they came for the Communists\nAnd I did not speak out\nBecause I was not a Communist\nThen they came for the Socialists\nAnd I did not speak out\nBecause I was not a Socialist\nThen they came for the trade unionists\nAnd I did not speak out\nBecause I was not a trade unionist\nThen they came for the Jews\nAnd I did not speak out\nBecause I was not a Jew\nThen they came for me\nAnd there was no one left\nTo speak out for me”\n— PASTOR MARTIN NIEMÖLLER\n\n\n\nIs Signalling Costly? Penn DJ, Számadó S. The Handicap Principle: how an erroneous hypothesis became a scientific principle - PDF. Biol Rev Camb Philos Soc.2020;95(1):267-290. doi:10.1111/brv.12563",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#songs-for-the-story",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#songs-for-the-story",
    "title": "\n Russia - Maxim Gorky",
    "section": "Song(s) for the Story",
    "text": "Song(s) for the Story\n\nPerhaps, after listening to this Hindi song, we will understand why Raj Kapoor was so popular in Soviet Russia !!\n\nhttps://youtu.be/OXPmxcFgXvY\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\nSinger: Mukesh\n\nMusic Director: Shankar-Jaikishan\n\nFilm: Mera Naam Joker\n\nStarring: Raj Kapoor, Simi Garewal, Manoj Kumar, Rishi Kapoor, Dharmendra, Dara Singh, Padmini, Rajendra Kumar.\n\nYear: 1970\n\n\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKehta hai joker saara zamaana\nSays the Joker, that the whole World\n\n\nAadhi haqikat aadha fasana\n(is) Half Truth, Half Story\n\n\nChashma utaaro phir dekho yaaro\nRemove your specs, and then see, my friends\n\n\nduniya nayee hai, chehra purana\nThe World is New, the Face is Old\n\n\nkehta hai joker…\nSays the Joker…\n\n\n—–\n—–\n\n\n( Apne pe hans kar jag ko hansaya\nHe laughs at Himself to make the World laugh\n\n\nBan ke tamasha mele main aaya) (x2)\nBecoming a Show, he Comes to this gathering\n\n\nmele main aaya\nComes to this Gathering\n\n\n(hindu na muslim, poorab na pashchim) (x2)\nNot Hindu not Muslim, nor East nor West\n\n\nMazhub hai apna, hansna hansaana\nMy Religion is to Laugh and make others Laugh\n\n\nkehta hai joker …\nSays the Joker…\n\n\n——\n—–\n\n\n(Dhakke pe dhakka, rele pe rela\nPushing and Shoving all around\n\n\nHai bheed itni, par dil akela ) (x2)\nSuch a Crowd, but the Heart is Lonely\n\n\npar dil akela\nbut the Heart is Lonely\n\n\n(gam jab sataye, seeti bajaana ) (x2)\nWhen Sadness troubles you, Whistle!!\n\n\npar maskhare se dil na lagana\nBut never Become Attached to Joker / Humour!\n\n\nkehta hai joker …\nSays the Joker…\n\n\nchasma utaro …\nRemove your specs,….\n\n\nkehta hai joker …\nSays the Joker…",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#writing-prompts",
    "title": "\n Russia - Maxim Gorky",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn being funny at the “wrong time”\nOn being a bystander in a street crime / Actions Speak Louder than Words?\nOn the Design of an ad/campaign for the Police to prevent neighbourhood crime\nOn a cringe-worthy act on your part (autobiographical first person writing)\nA discussion of jobs and professions that seem to take risks for the benefit of society\nComparing this story with Peter Carey’s The Last Days of the Famous Mime.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#footnotes",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#footnotes",
    "title": "\n Russia - Maxim Gorky",
    "section": "Footnotes",
    "text": "Footnotes\n\nReference to the “circus life, we all need a clown” in Steve Perry/Journey’s classic rock song, Faithfully.↩︎\nReference to “….When the jester sang for the king and queen, In a coat he borrowed from James Dean….Oh, and while the king was looking down, The jester stole his thorny crown, The courtroom was adjourned” in Don McClean’s rock anthem, American Pie.↩︎",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:russia\"></iconify-icon> Russia - Maxim Gorky"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "",
    "text": "From https://encyclopedia.densho.org/Hisaye_Yamamoto/\n\nA Southern California Nisei writer of short stories, Hisaye Yamamoto (1921–2011) was among the first Japanese American writers to win national renown after World War II. Yamamoto’s upbringing in an immigrant farming community and her incarceration in a World War II U.S. government prison camp formed the basis for some of her best-known stories, notable for their sensitive portrayal of the emotionally and artistically constricted lives of Issei women and intergenerational family dynamics. Oblique, often deadpan in delivery and told with quiet humor and bracing candor, they reveal the love affairs, madness, psychic and physical brutality that lay beneath the placid surface of Issei and Nisei life. The subject matter, precision and grace of Yamamoto’s works have led critics to compare her to short story masters Katherine Mansfield, Flannery O’Connor, and Grace Paley.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#hisaye-yamamoto",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#hisaye-yamamoto",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "",
    "text": "From https://encyclopedia.densho.org/Hisaye_Yamamoto/\n\nA Southern California Nisei writer of short stories, Hisaye Yamamoto (1921–2011) was among the first Japanese American writers to win national renown after World War II. Yamamoto’s upbringing in an immigrant farming community and her incarceration in a World War II U.S. government prison camp formed the basis for some of her best-known stories, notable for their sensitive portrayal of the emotionally and artistically constricted lives of Issei women and intergenerational family dynamics. Oblique, often deadpan in delivery and told with quiet humor and bracing candor, they reveal the love affairs, madness, psychic and physical brutality that lay beneath the placid surface of Issei and Nisei life. The subject matter, precision and grace of Yamamoto’s works have led critics to compare her to short story masters Katherine Mansfield, Flannery O’Connor, and Grace Paley.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#story",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#story",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "Story",
    "text": "Story\nWe will read Yamamoto’s story Seventeen Syllables.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#themes",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#themes",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "Themes",
    "text": "Themes\n\nInter-generational Conflict\nLiving as an Expat\nFirst Love\nTeenage Romance\n“Arranged Marriage” (picture marriage)\nDeception…",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#additional-material",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "Additional Material",
    "text": "Additional Material\n\n\nNotes and References\n\nA Beautiful Scrolly Story about Yamamoto and her influence. https://artsandculture.google.com/story/hisaye-yamamoto-an-american-story-american-writers-museum/OAVRaqAwV3tpLA?hl=en\nReading Yamamoto. https://faculty.georgetown.edu/bassr/heath/syllabuild/iguide/yamamoto.html\nGangs of Wasseypur: “Permission Leni Chahiye, Na?” https://youtu.be/To54dv2jnJg",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#song-for-the-story",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "Song for the Story",
    "text": "Song for the Story\nA song from 70 years ago? For Teens!?? You must be joking!!! But just maybe it could work….so there goes!\nStarring: Bharat Bhushan & Madhubala & Pradeep Kumar\nArtist: Mohammed Rafi & Lata Mangeshkar\nLyrics: Rajendra Krishan\nComposed: Madan Mohan Kohli\nMovie/Album: Gateway Of India (1957)\n\n\nHere is the audio track alone:\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\nHindi (and some Urdu!) lyrics\nEnglish Translation\n\n\n\nDo ghadi wo jo paas aa baithe (2)\nFor Two Moments, when They sat beside me\n\n\nHam zamane se dur ja baithe(2)\nWe were far away from everybody\n\n\n—–\n—-\n\n\nBhul ki unka hamnashi ho ke (2)\n’Twas a mistake to share a drink\n\n\nRoyege dil ko umar bhar kho ke (2)\nI will cry all my life for my Lost Heart\n\n\nHaay kya chiz thi luta baithe\nAlas, what a Thing it was, That I have Lost…\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—-\n\n\nDil ko ek din zarur jana tha (2)\nThe Heart had to leave One Day\n\n\nVahi pahucha jaha thikana tha (2)\nThere It Reached, where it was Right\n\n\nDil vahi dil jo dil me ja baithe\nThat is a Heart, that Resides in a Heart\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—–\n\n\nEk dil hi tha gham gusaar (2)\nThe Heart was my One Solace\n\n\nmeharbaan khaas raazdaar apna (2)\nMy Patron, my Confidant…\n\n\nghair ka kyun use banaa baithe\nWhy Did I Give it Away to a Stranger !!\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—-\n\n\nGhair bhi to koi haseen hogaa (2)\nThe Stranger must also have been Lovely\n\n\nDil yoon hi de diya nahin hoga (2)\nYou would not have parted with your Heart Just Like That\n\n\nDekhkar kuchh to chot khaa baithe\nYou Saw Them, and were Wounded\n\n\nDo ghadi wo jo paas aa baithe\nTwo Moments…",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#writing-prompts",
    "title": "\nJapan - Hisaye Yamamoto",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nYour Mama’s “Arranged Marriage”\nA Rant in GenZ language about almost anything (please create a Glossary in an Appendix!)\nGetting “(ab)used” to Dad’s / Mom’s taste in Music\nThe communication between parents and child in “Seventeen Syllables” and in Grace Paley’s “The Loudest Voice”.",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World",
      "<iconify-icon icon=\"streamline-emojis:japan\"></iconify-icon>Japan - Hisaye Yamamoto"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/listing.html",
    "href": "content/courses/Analytics/Predictive/listing.html",
    "title": "Predictive Analytics",
    "section": "",
    "text": "🐉 Intro to Orange\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Regression\n\n\n\nLinear Regression\n\nTrend Line\n\nFrancis Galton\n\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Classification\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nML - Clustering\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(prettydoc)\nlibrary(corrplot)\nlibrary(ggformula)\nlibrary(palmerpenguins) # Allison Horst's `penguins` data.\n##\nlibrary(tidymodels)\nlibrary(dials)\nlibrary(modeldata)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(parsnip)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` (14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor()\ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\", tl.cex = 1.0, )\n\n\n\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negatively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n\n  \n\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;%\n  recipe(species ~ .) %&gt;%\n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;% # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;- penguin_train %&gt;%\n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;- penguin_test %&gt;%\n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;-\n  penguin_model %&gt;%\n  fit(species ~ ., penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.01%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        86         2      0  0.02272727\nChinstrap      2        38      0  0.05000000\nGentoo         0         0     71  0.00000000\n\n# iris_ranger &lt;-\n#   rand_forest(trees = 100) %&gt;%\n#   set_mode(\"classification\") %&gt;%\n#   set_engine(\"ranger\") %&gt;%\n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%\n  dplyr::bind_cols(penguin_test_baked) %&gt;%\n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Biscoe, Biscoe, Bis…\n$ bill_length_mm    &lt;dbl&gt; -1.3335592, -1.7541369, 0.3670377, -1.0592694, -0.62…\n$ bill_depth_mm     &lt;dbl&gt; 1.08424573, 0.62721557, 2.20143056, 0.47487218, 0.72…\n$ flipper_length_mm &lt;dbl&gt; -0.56842897, -1.21056301, -0.49708074, -1.13921478, …\n$ body_mass_g       &lt;dbl&gt; -0.940191505, -1.095429393, -0.008764181, -0.3192399…\n$ sex               &lt;fct&gt; female, female, male, male, male, male, female, fema…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%\n  dplyr::bind_cols(penguin_test_baked) %&gt;%\n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs &lt;-\n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      &lt;dbl&gt; 0.99, 0.99, 0.59, 1.00, 1.00, 1.00, 0.84, 0.95, 0.92…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.01, 0.01, 0.39, 0.00, 0.00, 0.00, 0.16, 0.05, 0.08…\n$ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Biscoe, Biscoe, Bis…\n$ bill_length_mm    &lt;dbl&gt; -1.3335592, -1.7541369, 0.3670377, -1.0592694, -0.62…\n$ bill_depth_mm     &lt;dbl&gt; 1.08424573, 0.62721557, 2.20143056, 0.47487218, 0.72…\n$ flipper_length_mm &lt;dbl&gt; -0.56842897, -1.21056301, -0.49708074, -1.13921478, …\n$ body_mass_g       &lt;dbl&gt; -0.940191505, -1.095429393, -0.008764181, -0.3192399…\n$ sex               &lt;fct&gt; female, female, male, male, male, male, female, fema…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %&gt;%\n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n# ROC Plot\npenguin_fit_probs %&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n\n  \n\n\npenguin_recipe %&gt;% broom::tidy()\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n# penguin_model %&gt;% tidy()\n# penguin_fit %&gt;% tidy()\npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n# set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;%\n  training() %&gt;%\n  glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 7.2, 5.1, 6.7, 6.7, 7.1, 5.0, 5.1, 5.2, 6.0, 5.9, 6.6, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.6, 3.8, 3.0, 3.1, 3.0, 3.5, 3.5, 2.7, 2.2, 3.0, 2.9, 3.…\n$ Petal.Length &lt;dbl&gt; 6.1, 1.5, 5.2, 4.4, 5.9, 1.6, 1.4, 3.9, 5.0, 4.2, 4.6, 1.…\n$ Petal.Width  &lt;dbl&gt; 2.5, 0.3, 2.3, 1.4, 2.1, 0.6, 0.2, 1.4, 1.5, 1.5, 1.3, 0.…\n$ Species      &lt;fct&gt; virginica, setosa, virginica, versicolor, virginica, seto…\n\niris_split %&gt;%\n  testing() %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 4.6, 5.4, 4.4, 4.8, 4.8, 5.7, 4.6, 4.8, 5.0, 5.2, 4.8, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.1, 3.9, 2.9, 3.4, 3.0, 3.8, 3.6, 3.4, 3.4, 3.4, 3.1, 3.…\n$ Petal.Length &lt;dbl&gt; 1.5, 1.7, 1.4, 1.6, 1.4, 1.7, 1.0, 1.9, 1.6, 1.4, 1.6, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.4, 0.2, 0.2, 0.1, 0.3, 0.2, 0.2, 0.4, 0.2, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;-\n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .) # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;%\n  step_corr(all_predictors()) %&gt;%\n  step_center(all_predictors(), -all_outcomes()) %&gt;%\n  step_scale(all_predictors(), -all_outcomes()) %&gt;%\n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;-\n  iris_split %&gt;%\n  training() %&gt;%\n  bake(iris_recipe, .)\niris_training_baked\n\n\n  \n\n\niris_testing_baked &lt;-\n  iris_split %&gt;%\n  testing() %&gt;%\n  bake(iris_recipe, .)\niris_testing_baked\n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest, quietly = TRUE)\niris_rf &lt;-\n  rand_forest(trees = 100) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -1.5852786, -0.5918925, -1.8336251, -1.3369321, -1.336932…\n$ Sepal.Width  &lt;dbl&gt; 0.05284097, 1.78218168, -0.37949421, 0.70134373, -0.16332…\n$ Petal.Width  &lt;dbl&gt; -1.3124100, -1.0448745, -1.3124100, -1.3124100, -1.446177…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;%\n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;-\n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.980329365, 0.980809524, 0.887333333, 0.964476190, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.01967063, 0.00900000, 0.10541667, 0.02385714, 0.014…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.010190476, 0.007250000, 0.011666667, 0…\n$ Sepal.Length     &lt;dbl&gt; -1.5852786, -0.5918925, -1.8336251, -1.3369321, -1.33…\n$ Sepal.Width      &lt;dbl&gt; 0.05284097, 1.78218168, -0.37949421, 0.70134373, -0.1…\n$ Petal.Width      &lt;dbl&gt; -1.3124100, -1.0448745, -1.3124100, -1.3124100, -1.44…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;-\n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 1.00, 1.00, 0.94, 0.99, 1.00, 0.87, 0.99, 0.99, 0.99,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.10, 0.00, 0.00, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.00, 0.03, 0.01, 0.01, 0.01,…\n$ Sepal.Length     &lt;dbl&gt; -1.5852786, -0.5918925, -1.8336251, -1.3369321, -1.33…\n$ Sepal.Width      &lt;dbl&gt; 0.05284097, 1.78218168, -0.37949421, 0.70134373, -0.1…\n$ Petal.Width      &lt;dbl&gt; -1.3124100, -1.0448745, -1.3124100, -1.3124100, -1.44…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.06 0.08 0.1 0.18 0.2 0.24 0.27 0.3 0.33 0.43 0.59 0.61 0.63 0.7 0.71 0.8 0.83 0.84 0.86 0.9 0.91 0.93 0.94  1\n                                                                                                                                            \n 16    3    1    4    1    1    4    2   3    1   1    1    1   2    1    1    2    1    1   1    1   1    2    1    1   1    1    2    1  1\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.03 0.05 0.06 0.07 0.08 0.09 0.14 0.16 0.17 0.19 0.29 0.37 0.41 0.57 0.67 0.69 0.7 0.73 0.75 0.79 0.82 0.9 0.92 0.94 0.95 0.97 0.99  1\n                                                                                                                                                \n 14    7    1    2    1    2    1    1    1    1    1    1    1    2    1    1    1    1   1    1    1    1    1   2    2    5    1    1    3  1\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.03 0.04 0.05 0.1 0.2 0.24 0.87 0.94 0.95 0.96 0.98 0.99  1\n                                                                     \n 23    9    2    1    1   1   2    1    1    2    1    1    1    5  9\n\n\nIris Classifier: Gain and ROC Curves\nWe can plot gain and ROC curves for each of these models\n\niris_ranger_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 145\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 1…\n$ .n_events       &lt;dbl&gt; 0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 1…\n$ .percent_tested &lt;dbl&gt; 0.000000, 1.666667, 5.000000, 6.666667, 10.000000, 11.…\n$ .percent_found  &lt;dbl&gt; 0, 5, 15, 20, 30, 35, 45, 50, 60, 65, 70, 75, 80, 85, …\n\niris_ranger_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\niris_ranger_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 148\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001111111, 0.002000000, 0.002361111, …\n$ specificity &lt;dbl&gt; 0.000, 0.000, 0.225, 0.275, 0.300, 0.325, 0.375, 0.500, 0.…\n$ sensitivity &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\niris_ranger_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\niris_rf_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 78\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 9, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 26, 28, …\n$ .n_events       &lt;dbl&gt; 0, 9, 14, 15, 16, 17, 19, 20, 20, 20, 20, 20, 20, 20, …\n$ .percent_tested &lt;dbl&gt; 0.000000, 15.000000, 23.333333, 25.000000, 26.666667, …\n$ .percent_found  &lt;dbl&gt; 0.000000, 45.000000, 70.000000, 75.000000, 80.000000, …\n\niris_rf_probs %&gt;%\n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\niris_rf_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  glimpse()\n\nRows: 81\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.03, 0.04, 0.05, 0.10, 0.20, 0.24, 0.87…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.5750000, 0.8000000, 0.8500000, 0.8…\n$ sensitivity &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00…\n\niris_rf_probs %&gt;%\n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  bind_cols(predict(iris_ranger, iris_testing_baked)) %&gt;%\n  bind_cols(select(iris_testing_baked, Species)) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.980329365, 0.980809524, 0.887333333, 0.964476190, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.01967063, 0.00900000, 0.10541667, 0.02385714, 0.014…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.010190476, 0.007250000, 0.011666667, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;%\n#   bind_cols(select(iris_testing_baked,Species)) %&gt;%\n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  bind_cols(predict(iris_ranger, iris_testing_baked)) %&gt;%\n  bind_cols(select(iris_testing_baked, Species)) %&gt;%\n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 1.00, 1.00, 0.94, 0.99, 1.00, 0.87, 0.99, 0.99, 0.99,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.10, 0.00, 0.00, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.00, 0.03, 0.01, 0.01, 0.01,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;%\n#   bind_cols(select(iris_testing_baked,Species)) %&gt;%\n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "References",
    "text": "References\n\nMachine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/listing.html",
    "href": "content/courses/Analytics/CaseStudies/listing.html",
    "title": "Case Studies",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n Demo:Product Packaging and Elderly People\n\n\n23 min\n\n\n\n\n\n\n\n\n Ikea Furniture\n\n\n8 min\n\n\n\n\n\n\n\n\n ANOVA - Tyre Brands and Mileage\n\n\n4 min\n\n\n\n\n\n\n\n\n Movie Profits\n\n\n8 min\n\n\n\n\n\n\n\n\n Gender at the Work Place\n\n\n8 min\n\n\n\n\n\n\n\n\n Heptathlon\n\n\n8 min\n\n\n\n\n\n\n\n\n School Scores\n\n\n10 min\n\n\n\n\n\n\n\n\n Children’s Games\n\n\n8 min\n\n\n\n\n\n\n\n\n Valentine’s Day Spending\n\n\n8 min\n\n\n\n\n\n\n\n\n Women Live Longer?\n\n\n8 min\n\n\n\n\n\n\n\n\n Hearing Loss in Children\n\n\n8 min\n\n\n\n\n\n\n\n\nGrain Transportation Cartels\n\n\n12 min\n\n\n\n\n\n\n\n\n California Transit Payments\n\n\n7 min\n\n\n\n\n\n\n\n\n Seaweed Nutrients\n\n\n10 min\n\n\n\n\n\n\n\n\n Coffee Flavours\n\n\n13 min\n\n\n\n\n\n\n\n\n Legionnaire’s Disease in the USA\n\n\n13 min\n\n\n\n\n\n\n\n\n Antarctic Sea ice\n\n\n10 min\n\n\n\n\n\n\n\n\n William Farr’s Observations on Cholera in London\n\n\n9 min\n\n\n\n\n\n\n\n\n Satisfaction with AI Tools\n\n\n8 min\n\n\n\n\n\n\n\n\n New York Dog Bites\n\n\n13 min\n\n\n\n\n\n\n\n\n Elizabeth Bennett says No\n\n\n8 min\n\n\n\n\n\n\n\n\n Seattle Bicycle Zones\n\n\n8 min\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(ggridges)\nlibrary(supernova)\n# devtools::install_github('cttobin/ggthemr')\nlibrary(ggthemr)\nlibrary(ggsci)\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nfont &lt;- \"Roboto Condensed\"\nmytheme &lt;- theme_classic(base_size = 14) + ### %+replace%    #replace elements we want to change\n\n  theme(\n    text = element_text(family = font),\n    panel.grid.minor = element_blank(),\n    # text elements\n    plot.title = element_text(\n      family = font,\n      face = \"bold\",\n      hjust = 0, # left align\n      # vjust = 2 #raise slightly\n      margin = margin(0, 0, 10, 0)\n    ),\n    plot.subtitle = element_text(\n      family = font,\n      hjust = 0,\n      margin = margin(2, 0, 5, 0)\n    ),\n    plot.caption = element_text(\n      family = font,\n      size = 8,\n      hjust = 1\n    ),\n    # right align\n\n    axis.title = element_text( # axis titles\n      family = font, # font family\n      size = 10\n    ), # font size\n    axis.text = element_text( # axis text\n      family = font, # axis family\n      size = 8\n    ) # font size\n  )\ntheme_av &lt;- list(\n  mytheme,\n  scale_colour_manual(values = my_bolds, aesthetics = c(\"colour\", \"fill\"))\n)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#setting-up-r-packages",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(ggridges)\nlibrary(supernova)\n# devtools::install_github('cttobin/ggthemr')\nlibrary(ggthemr)\nlibrary(ggsci)\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nfont &lt;- \"Roboto Condensed\"\nmytheme &lt;- theme_classic(base_size = 14) + ### %+replace%    #replace elements we want to change\n\n  theme(\n    text = element_text(family = font),\n    panel.grid.minor = element_blank(),\n    # text elements\n    plot.title = element_text(\n      family = font,\n      face = \"bold\",\n      hjust = 0, # left align\n      # vjust = 2 #raise slightly\n      margin = margin(0, 0, 10, 0)\n    ),\n    plot.subtitle = element_text(\n      family = font,\n      hjust = 0,\n      margin = margin(2, 0, 5, 0)\n    ),\n    plot.caption = element_text(\n      family = font,\n      size = 8,\n      hjust = 1\n    ),\n    # right align\n\n    axis.title = element_text( # axis titles\n      family = font, # font family\n      size = 10\n    ), # font size\n    axis.text = element_text( # axis text\n      family = font, # axis family\n      size = 8\n    ) # font size\n  )\ntheme_av &lt;- list(\n  mytheme,\n  scale_colour_manual(values = my_bolds, aesthetics = c(\"colour\", \"fill\"))\n)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#introduction",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to tyres from different companies and their lifetime mileages."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#download-the-modified-data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Tyre Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data-dictionary",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#plot-the-data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#task-and-discussion-anova",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#task-and-discussion-anova",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Task and Discussion: ANOVA",
    "text": "Task and Discussion: ANOVA\n\nComplete the pre-analysis steps for ANOVA\n\nWrite in.\nModel + Table\n\nCreate the ANOVA model\nCreate the ANOVA table using the supernova package\n\n\n\nCall:\n   aov(formula = Mileage ~ Brands, data = tyre)\n\nTerms:\n                  Brands Residuals\nSum of Squares  256.2908  266.6495\nDeg. of Freedom        3        56\n\nResidual standard error: 2.182108\nEstimated effects may be unbalanced\n\n\n Analysis of Variance Table (Type III SS)\n Model: Mileage ~ Brands\n\n                              SS df     MS      F   PRE     p\n ----- --------------- | ------- -- ------ ------ ----- -----\n Model (error reduced) | 256.291  3 85.430 17.942 .4901 .0000\n Error (from model)    | 266.649 56  4.762                   \n ----- --------------- | ------- -- ------ ------ ----- -----\n Total (empty model)   | 522.940 59  8.863                   \n\n\nPost-hoc Analysis and Plots\n\nCompute the post-hoc differences in means and plot the pair-wise difference plots\n\n\n\n\n  group_1     group_2       diff pooled_se      q    df  lower  upper  p_adj\n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bridgestone Apollo      -3.019     0.563 -5.358    56 -5.129 -0.909  .0021\n2 CEAT        Apollo      -0.038     0.563 -0.067    56 -2.148  2.072 1.0000\n3 Falken      Apollo       2.826     0.563  5.015    56  0.716  4.935  .0043\n4 CEAT        Bridgestone  2.981     0.563  5.291    56  0.871  5.091  .0024\n5 Falken      Bridgestone  5.845     0.563 10.373    56  3.735  7.954  .0000\n6 Falken      CEAT         2.863     0.563  5.082    56  0.754  4.973  .0037\n\n\n\n\n\n\n\n\nConclusion\n\nState a conclusion about the effect of Brands on Mileage.\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html",
    "title": "\n California Transit Payments",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n#\nlibrary(ggstats)\nlibrary(labelled)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#setting-up-r-packages",
    "title": "\n California Transit Payments",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n#\nlibrary(ggstats)\nlibrary(labelled)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#introduction",
    "title": "\n California Transit Payments",
    "section": "Introduction",
    "text": "Introduction\nThis dataset is the result of a research study on payment options for people using public transit in California.\nThe dataset is available on Dataset Dryad:\nPike, Susan (2022). Transit payment preferences of unbanked passengers. Dataset Dryad. https://doi.org/10.25338/B8R04T\nAnd a brief 2-pager on the research methodology is here.\nYes, peasants, you should read such stuff from other very different domains!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#read-the-data",
    "title": "\n California Transit Payments",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download the Cal Payment data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-dictionary",
    "title": "\n California Transit Payments",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-munging",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-munging",
    "title": "\n California Transit Payments",
    "section": "Data Munging",
    "text": "Data Munging\n\n\n Munged Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#summarize-and-prepare-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#summarize-and-prepare-the-data",
    "title": "\n California Transit Payments",
    "section": "Summarize and Prepare the Data",
    "text": "Summarize and Prepare the Data\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\nLet’s label the data variables…\n\n\ntibble [204 × 5] (S3: tbl_df/tbl/data.frame)\n $ phone.wifi    : num [1:204] 1 1 2 1 1 2 1 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Wi_Fi access?\"\n  .. ..- attr(*, \"names\")= chr \"phone.wifi\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.money   : num [1:204] 1 1 1 1 1 1 1 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Ways to add money?\"\n  .. ..- attr(*, \"names\")= chr \"phone.money\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.identity: num [1:204] 1 1 2 2 1 1 2 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Identity Concerns?\"\n  .. ..- attr(*, \"names\")= chr \"phone.identity\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.fees    : num [1:204] 1 2 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"label\")= Named chr \"Monthly Fees?\"\n  .. ..- attr(*, \"names\")= chr \"phone.fees\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.balance : num [1:204] 1 2 1 1 1 1 1 2 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Knowing the balance?\"\n  .. ..- attr(*, \"names\")= chr \"phone.balance\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#plot-the-data",
    "title": "\n California Transit Payments",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#task-and-discussion",
    "title": "\n California Transit Payments",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graph shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat would be your recommendation to the Transport Company?\nTo the Phone Companies?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html",
    "title": "\n Elizabeth Bennett says No",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#setting-up-r-packages",
    "title": "\n Elizabeth Bennett says No",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#introduction",
    "title": "\n Elizabeth Bennett says No",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#read-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#inspect-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#data-dictionary",
    "title": "\n Elizabeth Bennett says No",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#research-question",
    "title": "\n Elizabeth Bennett says No",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#analysetransform-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#plot-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#tasks-and-discussion",
    "title": "\n Elizabeth Bennett says No",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html",
    "title": "\n Seaweed Nutrients",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)\nlibrary(paletteer) # fancy colour palettes!\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#setting-up-r-packages",
    "title": "\n Seaweed Nutrients",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)\nlibrary(paletteer) # fancy colour palettes!\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#introduction",
    "title": "\n Seaweed Nutrients",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#read-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Read the Data",
    "text": "Read the Data\n Download the Seaweed data \n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R. NOTE: The sheet that contains our data of interest is titled “seaweed nutrition”, range = “A3:R13”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#inspect-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#data-dictionary",
    "title": "\n Seaweed Nutrients",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#research-question",
    "title": "\n Seaweed Nutrients",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! First look at the chart below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#analysetransform-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#plot-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#tasks-and-discussion",
    "title": "\n Seaweed Nutrients",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#references",
    "title": "\n Seaweed Nutrients",
    "section": "References",
    "text": "References\nOver 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton?\nHere are the Qualitative Palettes:\n\n\n\n\n\n\nAnd the Quantitative/Continuous palettes:\n\n\n\n\n\n\nUse the commands:\n\n## For Qual variable-&gt; colour/fill:\nscale_colour_paletteer_d(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)\n\n## For Quant variable-&gt; colour/fill:\nscale_colour_paletteer_c(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html",
    "title": "\n New York Dog Bites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#setting-up-r-packages",
    "title": "\n New York Dog Bites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#introduction",
    "title": "\n New York Dog Bites",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#read-the-data",
    "title": "\n New York Dog Bites",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#inspect-the-data",
    "title": "\n New York Dog Bites",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#data-dictionary",
    "title": "\n New York Dog Bites",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#research-question",
    "title": "\n New York Dog Bites",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#analysetransform-the-data",
    "title": "\n New York Dog Bites",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#plot-the-data",
    "title": "\n New York Dog Bites",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#tasks-and-discussion",
    "title": "\n New York Dog Bites",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html",
    "title": "\n Women Live Longer?",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#setting-up-r-packages",
    "title": "\n Women Live Longer?",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#introduction",
    "title": "\n Women Live Longer?",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to survival ages in different countries across the world. Women survival ages are compared to those of men.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#read-the-data",
    "title": "\n Women Live Longer?",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download the data\n\n\nRows: 18,408\nColumns: 7\n$ Entity                                                               &lt;chr&gt; \"…\n$ Code                                                                 &lt;chr&gt; \"…\n$ Year                                                                 &lt;dbl&gt; 2…\n$ `Life expectancy - Sex: female - Age: at birth - Variant: estimates` &lt;dbl&gt; N…\n$ `Life expectancy - Sex: male - Age: at birth - Variant: estimates`   &lt;dbl&gt; N…\n$ `Population - Sex: all - Age: all - Variant: estimates`              &lt;dbl&gt; N…\n$ Continent                                                            &lt;chr&gt; \"…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#data-dictionary",
    "title": "\n Women Live Longer?",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#analysetransform-the-data",
    "title": "\n Women Live Longer?",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#research-question",
    "title": "\n Women Live Longer?",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!! Look at the Chart!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#plot-the-data",
    "title": "\n Women Live Longer?",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#task-and-discussion",
    "title": "\n Women Live Longer?",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWhat might be the Hypothesis / Research Question, based on the Chart?\nWhat does the dashed line in the chart represent?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#reference",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#reference",
    "title": "\n Women Live Longer?",
    "section": "Reference",
    "text": "Reference\nIn order to obtain that floating text note slope = 1 in the chart, you need to use gf_refine(annotate(....)). Look at the vignette/help here. https://ggplot2.tidyverse.org/reference/annotate.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#setting-up-r-packages",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#introduction",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Introduction",
    "text": "Introduction\nJohn Snow’s contention that cholera was principally spread by water was not accepted in the 1850s by the medical elite. The consequence of rejection was that hundreds in the UK continued to die. William Farr, who founded the science of epidemiology, tried to examine if there were other causes that led to cholera. He had concluded that the available data not only supported miasma (spread via atmospheric vapours) but also indicated that there was an underlying ‘natural law’ linking infection with cholera inversely to elevation above high water. The data is available on Vincent Arel-Bundock’s website, and is part of the HistData package from Michael Friendly, UC Davis.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#read-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Read the Data",
    "text": "Read the Data\n\nCholera &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/HistData/Cholera.csv\")\nCholera",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#data-dictionary",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#research-question",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look at the charts below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#analysetransform-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#plot-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#tasks-and-discussion",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as needed.\nLook at Plot 1. Would you agree based on this chart that William Farr was right in believing that elevation was a good predictor for cholera deaths? Justify.\nWhat is the nature of the relationship between Cholera Deaths and Elevation?\nLook at Plot 2. What kind of plot is it? What is the relationship here between Elevation and Cholera Death Rate?\nBased on this graph, would you agree that Elevation is a predictor for Cholera Deaths? Justify.\nIs the relationship you found between Cholera Deaths and Elevation also found in Plot 1? Justify.\nLook at Plot 3. Would you guess that there could be another predictor for Cholera Deaths? What could that Predictor be? Justify.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html",
    "title": "\n Hearing Loss in Children",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#setting-up-r-packages",
    "title": "\n Hearing Loss in Children",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#introduction",
    "title": "\n Hearing Loss in Children",
    "section": "Introduction",
    "text": "Introduction\nChildren are monitored for OME (Otitis Media with Effusion, i.e. fluid in the middle ear) over time. It is believed that they later ( i.e. during aduldhood) suffer from “binaural hearing loss” (detecting sound amplitude and direction) after past episodes of OME during their childhood. The hearing-test is conducted multiple times, with a Test Signal embedded in noise played over audio loudspeakers. One loudspeaker has only Noise, and the other loudspeaker has the Test Signal in Noise. There are also two types of Test Signals: one is like noise itself and the other is distinct. In any test round, children are expected to orient themselves towards the appropriate loudspeaker and detect the presence of the Test Signal at varying levels of volume, with a passing success rate of 75% over multiple tests.\nThis dataset is available on Vincent Arel-Bundock’s dataset repository and is a part of the R package MASS.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#read-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Read the Data",
    "text": "Read the Data\n\nome &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/OME.csv\")\nglimpse(ome)\n\nRows: 1,097\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ ID       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3…\n$ Age      &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 60, 60, 60, 60, 60, 6…\n$ OME      &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"…\n$ Loud     &lt;dbl&gt; 35, 35, 40, 40, 45, 45, 50, 50, 55, 55, 35, 35, 40, 40, 45, 4…\n$ Noise    &lt;chr&gt; \"coherent\", \"incoherent\", \"coherent\", \"incoherent\", \"coherent…\n$ Correct  &lt;dbl&gt; 1, 4, 0, 1, 2, 2, 3, 4, 3, 2, 2, 3, 1, 1, 1, 5, 4, 2, 3, 4, 4…\n$ Trials   &lt;dbl&gt; 4, 5, 3, 1, 4, 2, 3, 4, 3, 2, 4, 4, 4, 1, 2, 5, 4, 2, 3, 4, 6…\n\nome",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#data-dictionary",
    "title": "\n Hearing Loss in Children",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#research-question",
    "title": "\n Hearing Loss in Children",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nIn hearing tests on people with varying levels of OME infection in their childhood, what is the effect of using distinct types of Test Signal on successful (face) orientation ?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#analysetransform-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#plot-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#task-and-discussion",
    "title": "\n Hearing Loss in Children",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWrite a 2-line story based on the chart, describing your inference/surprise. Is there something counter-intuitive (to a lay person) in the chart?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html",
    "title": "\n Children’s Games",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#setting-up-r-packages",
    "title": "\n Children’s Games",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#introduction",
    "title": "\n Children’s Games",
    "section": "Introduction",
    "text": "Introduction\nChildren in the ages of 6 to 7 years are asked if they want to play two games. This dataset pertains to their responses about the two games. The research is based on this paper:\nLin Bian et al. ,Gender stereotypes about intellectual ability emerge early and influence children’s interests. Science 355,389-391(2017).DOI:10.1126/science.aah6524. This very short and crisp paper is available here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#read-the-data",
    "title": "\n Children’s Games",
    "section": "Read the Data",
    "text": "Read the Data\nThe data is part of the R package openintro. Yes, install it. From the help menu ?children_gender_stereo:\n\nThis data object is more unusual than most. It is a list of 4 data frames. The four data frames correspond to the data used in Studies 1-4 of the referenced paper, and these data frames each have variables (columns) that are explained below:\n\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\nlibrary(openintro)\ndata(\"children_gender_stereo\")\nglimpse(children_gender_stereo)\n\nList of 4\n $ 1:'data.frame':  192 obs. of  5 variables:\n  ..$ subject   : int [1:192] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender    : chr [1:192] \"female\" \"female\" \"female\" \"male\" ...\n  ..$ age       : int [1:192] 7 7 7 6 7 5 5 5 5 5 ...\n  ..$ trait     : chr [1:192] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ stereotype: num [1:192] 0.611 0.278 0.722 0.556 1 ...\n $ 2:'data.frame':  576 obs. of  7 variables:\n  ..$ subject             : int [1:576] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender              : chr [1:576] \"male\" \"male\" \"male\" \"female\" ...\n  ..$ age                 : int [1:576] 6 5 7 5 5 7 6 6 5 5 ...\n  ..$ trait               : chr [1:576] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ target              : chr [1:576] \"adults\" \"adults\" \"adults\" \"adults\" ...\n  ..$ stereotype          : num [1:576] 0.75 1 0.25 1 0.25 0.75 0 0.5 0.75 1 ...\n  ..$ high_achieve_caution: num [1:576] 0.25 1 0.25 1 0.75 0.5 0.5 0.75 0.5 0.5 ...\n $ 3:'data.frame':  128 obs. of  7 variables:\n  ..$ subject   : int [1:128] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender    : chr [1:128] \"female\" \"male\" \"female\" \"male\" ...\n  ..$ age       : int [1:128] 7 7 7 7 7 6 7 7 6 6 ...\n  ..$ game      : chr [1:128] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ interest  : num [1:128] 0.328 0.781 0.781 -0.213 -2.304 ...\n  ..$ difference: num [1:128] 0.244 0.453 0.209 -0.577 -1.523 ...\n  ..$ stereotype: num [1:128] -1.7982 0.0866 -1.7982 0.7784 -0.6051 ...\n $ 4:'data.frame':  96 obs. of  4 variables:\n  ..$ subject : int [1:96] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender  : chr [1:96] \"female\" \"female\" \"female\" \"female\" ...\n  ..$ age     : int [1:96] 6 6 6 6 6 6 6 6 6 6 ...\n  ..$ interest: num [1:96] 0.3924 0.68 -0.7163 -0.4279 -0.0413 ...\n\n\nLet us choose, arbitrarily, the third study:\n\n## Choosing, arbitrarily, the third game/third study\nchildren_gender_stereo[[3]] -&gt; games3\nglimpse(games3)\n\nRows: 128\nColumns: 7\n$ subject    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ gender     &lt;chr&gt; \"female\", \"male\", \"female\", \"male\", \"female\", \"female\", \"ma…\n$ age        &lt;int&gt; 7, 7, 7, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6,…\n$ game       &lt;chr&gt; \"smart\", \"smart\", \"smart\", \"smart\", \"smart\", \"smart\", \"smar…\n$ interest   &lt;dbl&gt; 0.328241235, 0.781351865, 0.781351865, -0.213178560, -2.303…\n$ difference &lt;dbl&gt; 0.24441071, 0.45311063, 0.20869992, -0.57713058, -1.5226918…\n$ stereotype &lt;dbl&gt; -1.79820307, 0.08662039, -1.79820307, 0.77835148, -0.605110…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#data-dictionary",
    "title": "\n Children’s Games",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#analysetransform-the-data",
    "title": "\n Children’s Games",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#research-question",
    "title": "\n Children’s Games",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nIs there a difference the average interest level between Boys and Girls for the two kinds of games, “Smart Game” and “Try Hard Game”? Does that lead to the inference of how children acquire gender stereotypes about play?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#plot-the-data",
    "title": "\n Children’s Games",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#task-and-discussion",
    "title": "\n Children’s Games",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nDoes the Chart answer the Hypothesis? Justify?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html",
    "title": "\n Ikea Furniture",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#setting-up-r-packages",
    "title": "\n Ikea Furniture",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)\n\n\n\nShow the Code```{r}\n#| code-fold: true\n#| message: false\n#| warning: false\n\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nlibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      plot.background = element_rect(fill = \"navajowhite\"),\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#introduction",
    "title": "\n Ikea Furniture",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to furniture prices at IKEA, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data",
    "title": "\n Ikea Furniture",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#download-the-modified-data",
    "title": "\n Ikea Furniture",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Ikea furniture Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data-dictionary",
    "title": "\n Ikea Furniture",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#plot-the-data",
    "title": "\n Ikea Furniture",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#task-and-discussion",
    "title": "\n Ikea Furniture",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nWhat is the kind of plot used in the chart?\nWhat variables have been used in the chart?\nWhat can you say about the scale on X-axis?\nWhat can you say about prices of items that are available in single colour versus those that are available in more than one colour?\nWhat is a good hypothesis to interpret the double-humped nature of some of the curves?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html",
    "title": "\n Valentine’s Day Spending",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#setting-up-r-packages",
    "title": "\n Valentine’s Day Spending",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#introduction",
    "title": "\n Valentine’s Day Spending",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to spending on gifts by various people for Valentine’s Day. This was part of the TidyTuesday Project for February 2024!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#read-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Read the Data",
    "text": "Read the Data\n\ngifts_age &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv\")\nglimpse(gifts_age)\n\nRows: 6\nColumns: 9\n$ Age                 &lt;chr&gt; \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"\n$ SpendingCelebrating &lt;dbl&gt; 51, 40, 31, 19, 18, 13\n$ Candy               &lt;dbl&gt; 70, 62, 58, 60, 50, 42\n$ Flowers             &lt;dbl&gt; 50, 44, 41, 37, 32, 25\n$ Jewelry             &lt;dbl&gt; 33, 34, 29, 20, 13, 8\n$ GreetingCards       &lt;dbl&gt; 33, 33, 42, 42, 43, 44\n$ EveningOut          &lt;dbl&gt; 41, 37, 30, 31, 29, 24\n$ Clothing            &lt;dbl&gt; 33, 27, 26, 20, 19, 12\n$ GiftCards           &lt;dbl&gt; 23, 19, 22, 23, 20, 20",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#data-dictionary",
    "title": "\n Valentine’s Day Spending",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#analysetransform-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#research-question",
    "title": "\n Valentine’s Day Spending",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!! Look at the Chart!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#plot-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#task-and-discussion",
    "title": "\n Valentine’s Day Spending",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWhat might be the Hypothesis / Research Question, based on the Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#references",
    "title": "\n Valentine’s Day Spending",
    "section": "References",
    "text": "References\nTo obtain that cool-looking X-axis in the chart, you need to use a new package called ggprism. Look at the vignette there and copy the code to make the X-axis like what you see here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html",
    "title": "\n Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#goals",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#goals",
    "title": "\n Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#introduction-to-r-and-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#introduction-to-r-and-rstudio",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Introduction to R and RStudio",
    "text": "Introduction to R and RStudio\nThis guide will lead you through the steps to install and use R, a free and open-source software environment for statistical computing and graphics.\nWhat is R?\n\n\nR is the name of the programming language itself, based off S from Bell Labs, which users access through a command-line interpreter (&gt;)\n\nWhat is RStudio?\n\n\nRStudio is a powerful and convenient user interface that allows you to access the R programming language along with a lot of other bells and whistles that enhance functionality (and sanity).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-r",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-r",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install R",
    "text": "Install R\nInstall R from CRAN, the Comprehensive R Archive Network. Please choose a precompiled binary distribution for your operating system.\n\n Check in\nLaunch R by clicking this logo  in your Applications. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-rstudio",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install RStudio",
    "text": "Install RStudio\nInstall the free, open-source edition of RStudio: https://posit.co/download/rstudio-desktop/\nRStudio provides a powerful user interface for R, called an integrated development environment. RStudio includes:\n\na console (the standard command line interface: &gt;),\na syntax-highlighting editor that supports direct code execution, and\ntools for plotting, history, debugging and work space management.\n\n\n Check in\nLaunch RStudio.You should get a window similar to the screenshot you see here:\n\n\n\n\n\nRStudio Default Window\n\nbut yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step Section 3.1.\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#installation-slides",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#installation-slides",
    "title": "\n Introduction to R and RStudio",
    "section": "Installation Slides",
    "text": "Installation Slides\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-packages",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-packages",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install packages",
    "text": "Install packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers. For analytical and graphical super-powers, you’ll need to install add-on packages, which are user-written, to extend/expand your R capabilities. Packages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE) in your CONSOLE.\nPersonal repositories of packages created by practitioners, which are usually in Github.\n\nPlace your cursor in the CONSOLE again (where you last typed x and [4] printed on the screen). You can use the first method to install the following packages directly from CRAN, all of which we will use:\nType these commands in your CONSOLE:\n\ninstall.packages(\"knitr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggformula\")\ninstall.packages(\"babynames\")\n\n\n\n\n\n\n\nImportantInstallation and Usage of R Packages!\n\n\n\n\nTo install a package, you put the name of the package in quotes as in install.packages(\"name_of_package\"). Mind your use of quotes carefully with packages.\nTo use an already installed package, you must load it first, as in library(name_of_package), leaving the name of the package bare. You only need to do this once per RStudio session.\n\n\n\n\nIf you want help, no quotes are needed: help(name_of_package) or ?name_of_package.\nIf you want the citation for a package (and you should give credit where credit is due), ask R as in citation(\"name_of_package\").",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#using-quarto",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#using-quarto",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Using Quarto",
    "text": "Using Quarto\nWe will get acquainted with the Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document. Quarto can be used to generate multiple formats such as HTML, Word, PDF from the same text/code file.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible\nbe a call to action or a recommendation\n\nimpress colleagues, bosses, and faculty\n\n\n Setting up Quarto\nQuarto is already installed along with RStudio!! We can check if all is in order by running a check in the Terminal in RStudio.\n\nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document.\n\n Practice\nLet us now create a brand new Quarto document, create some graphs in R and add some narrative text and see how we can generate our first report!\n\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.\n\nGive a title to your document ( “My First Quarto Document”, for example.\nChange the author name to your own! Keep HTML as your output format\nSwitch to Visual mode, if it is not already there. Use the visual mode tool bar.\n\n\n\nClick on the various buttons to see what happens. Try to create Sections, code chunks, embedding images and tables.\n\n\n\n\n\n\n\nTipAdd Anything Shortcut\n\n\n\nTry the “add anything” shortcut! Type “/” anywhere in your Quarto Doc, while in Visual Mode, and choose what you want to add from the drop-down menu!\n\n\n\nCreate a code chunk as shown below. You can either use the visual tool bar to create it, or simply hit the copy button in the code chunk display on this website and paste the results into your Quarto document. Check every step!\n\n\n```{r}\n#| label: setup\n\n# library(knitr) # to use this….document! More later!!\nlibrary(tidyverse) # Data Management and Plotting!!\nlibrary(babynames) # A package containing, yes, Baby Names\nlibrary(ggformula)\n```\n\n\nHit the green “play” button to run this “setup” chunk to include in your R session all the installed packages you need.\nLet us greet our data first !!\n\n\n```{r}\n#| label: babynames-data\nglimpse(babynames)\nhead(babynames)\ntail(babynames)\nnames(babynames)\n```\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\n  \n\n\n  \n\n\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\n\n\nIf you have done the above and produced sane-looking output, you are ready for the next bit. Use the code below to create a new data frame called my_name_data.\n\n\n```{r}\n#| label: manipulate-name-data\nmy_name_data &lt;- babynames %&gt;%\n  filter(name == \"Arvind\" | name == \"Aravind\") %&gt;%\n  filter(sex == \"M\")\n```\n\n\nThe first bit makes a new dataset called my_name_data that is a copy of the babynames dataset\nthe %&gt;% (pipe) tells you we are doing some other stuff to it later.1\n\nThe second bit filters our babynames to only keep rows where the name is either Arvind or Aravind (read | as “or”.)\nThe third bit applies another filter to keep only those where sex is male.\n\nLet’s check out the data.\n\n```{r}\nmy_name_data\nglimpse(my_name_data)\n```\n\n\n  \n\n\n\nRows: 61\nColumns: 5\n$ year &lt;dbl&gt; 1970, 1972, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983,…\n$ sex  &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ name &lt;chr&gt; \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvi…\n$ n    &lt;int&gt; 5, 8, 7, 5, 9, 6, 7, 6, 8, 6, 7, 7, 7, 13, 8, 11, 6, 8, 12, 10, 1…\n$ prop &lt;dbl&gt; 2.620e-06, 4.780e-06, 4.310e-06, 3.060e-06, 5.260e-06, 3.510e-06,…\n\n\n\nAgain, if you have sane-looking output here, move along to plotting the data!\n\n\n```{r}\n#| label:  plot-name-data\n\nplot &lt;- gf_line(prop ~ year,\n  color = ~name,\n  data = my_name_data\n)\n```\n\nNow if you did this right, you will not see your plot!\n\nBecause we saved the ggplot with a name (plot), R just saved the object for you. But check out the top right pane in RStudio again: under the Environment pane you should see plot, so it is there, you just have to ask for it. Here’s how:\n\n\n```{r}\nplot\n```\n\n\n\n\n\n\n\n\nNow hit the Render button on your Visual toolbar and see what happens!! Try to use the drop down menu next to it and see if there are more output file options!!\n\n Make a new name plot!\n\nEdit my code above to create a new dataset. Pick 2 names to compare how popular they each are (these could be different spellings of your own name, like I did, but you can choose any 2 names that are present in the dataset), and create a new data object with a new name.\nWrite narratives comments wherever suitable in your Quarto document. Make sure you don’t type inside your code chunks. See if you can write your comments in sections which you can create with your visual tool bar, or by using the “add anything” shortcut.\nSave your work ( your Quarto document itself) so you can share your favorite plot.\nShare your Plot: You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults.\n\nType help(ggsave) in your Console.\n\n```{r}\n#| label: Saving\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#conclusions",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#conclusions",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have installed R, RStudio and created our Quarto document, complete with graphs and narrative text. We also rendered our Quarto doc into HTML and other formats!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#references",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#references",
    "title": "\n Introduction to R and RStudio",
    "section": "\n References",
    "text": "References\n\nhttps://www.markdowntutorial.com\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !\nhttps://rmarkdown.rstudio.com/index.html\nSamantha Csik. Customizing Quarto websites. https://ucsb-meds.github.io/customizing-quarto-websites/#/title-slide\nReproducible Reporting with Quarto. https://book.rwithoutstatistics.com/quarto-chapter\nThomas Mock.(). Quarto in Two Hours https://jthomasmock.github.io/quarto-in-two-hours/\nhttps://quarto.org/docs/get-started/hello/rstudio.html\nhttps://quarto.org/docs/authoring/markdown-basics.html How to do more with Quarto HTML format\nhttps://apps.machlis.com/shiny/quartotips/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#assignments",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#assignments",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nLook through the Slides in [reference 2]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#readings",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#readings",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#footnotes",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#footnotes",
    "title": "\n Introduction to R and RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\nInsert the pipe character using the keyboard shortcutCTRL + SHIFT + M.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html",
    "title": "\n Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa6-solid:person-rays\"></iconify-icon> Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#introduction-to-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#introduction-to-radiant",
    "title": "\n Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa6-solid:person-rays\"></iconify-icon> Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#installing-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#installing-radiant",
    "title": "\n Introduction to Radiant",
    "section": "Installing Radiant",
    "text": "Installing Radiant\nYou can download and install Radiant from here:\nhttps://radiant-rstats.github.io/docs/install.html\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This automatically installs R, RStudio, and Radiant on your machine. This is going to be convenient when we start working in R too!\nIt also installs Latex, which allows us to create crisp PDF reports of our analyses.\nThe version of R may not be the latest one, though…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa6-solid:person-rays\"></iconify-icon> Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#basic-tutorials-with-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/index.html#basic-tutorials-with-radiant",
    "title": "\n Introduction to Radiant",
    "section": "Basic Tutorials with Radiant",
    "text": "Basic Tutorials with Radiant\nAll the Tutorials are available on Youtube; the links to individual videos are on the page below\nhttps://radiant-rstats.github.io/docs/radiant-tutorial-series.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Tools",
      "<iconify-icon icon=\"fa6-solid:person-rays\"></iconify-icon> Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html",
    "href": "content/courses/Analytics/Modelling/listing.html",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html#references",
    "href": "content/courses/Analytics/Modelling/listing.html#references",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative way here !! A multiplicative time series can be converted to additive by taking a log of the time series.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies, the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval , i.e. zooming in on the time axis. ( i.e. self-similar and fractal)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;%\n  pivot_wider(.,\n    id_cols = c(Date),\n    names_from = Dept,\n    values_from = Weekly_Sales,\n    names_prefix = \"Sales_\"\n  )\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;%\n  select(Sales_95) %&gt;%\n  ts(start = c(2010, 1), end = c(2012, 52), frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast,\n  fitted = TRUE,\n  timetk_idx = TRUE\n)\n\nsales95_forecast_tidy\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;-\n  sales95_forecast_tidy %&gt;%\n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95),\n    fill = \"#3182bd\", color = NA\n  ) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key),\n    fill = \"#deebf7\", color = NA, alpha = 0.8\n  ) +\n  geom_line(size = 1) +\n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  # coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_time_series(.data = sales95_forecast_tidy, .date_var = index, .value = value, .color_var = key, .smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "title": "Correlation and Regression Explorations",
    "section": "Intro",
    "text": "Intro\nI will work through and “unify” at least two things:\n\nHadley Wickham’s chapter on modelling and his analysis of the linear model for the diamonds dataset\nThe diagnostic aspects of Linear Regression as detailed in Crawley’s book"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "title": "Correlation and Regression Explorations",
    "section": "Explorations into Diagnostic Plots",
    "text": "Explorations into Diagnostic Plots\nLet us create dependent y* variables with different sorts of errors:\n\nx &lt;- 0:300\nen &lt;- rnorm(301, mean = 0, sd = 5)\neu &lt;- (runif(n = 301) - 0.5) * 20\neb &lt;- rnbinom(n = 301, prob = 0.3, size = 2)\neg &lt;- rgamma(n = 301, shape = 1, rate = 1 / x)\nyn &lt;- x + 10 + en\nyu &lt;- x + 10 + eu\nyb &lt;- x + 10 + eb\nyg &lt;- x + 10 + eg\ndata &lt;- tibble(x, yn, yu, yb, yg)\ndata\n\n\n  \n\n\n\nNormal Errors\n\nlm_norm_aug &lt;- lm(yn ~ x, data = data) %&gt;%\n  augment()\nlm_norm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_norm_aug %&gt;%\n  gf_qq(~.resid) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nUniform Errors\n\nlm_unif_aug &lt;- lm(yu ~ x, data = data) %&gt;%\n  augment()\nlm_unif_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_unif_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nNegative Binom Errors\n\nlm_nbinom_aug &lt;- lm(yb ~ x, data = data) %&gt;%\n  augment()\nlm_nbinom_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_nbinom_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nGamma Errors\n\nlm_gamm_aug &lt;- lm(yg ~ x, data = data) %&gt;%\n  augment()\nlm_gamm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_gamm_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "title": "Permutation Tests for Linear Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "title": "Permutation Tests for Linear Regression",
    "section": "Linear Regression using Permutation Tests",
    "text": "Linear Regression using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst[^2], the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic[^3].\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nRead the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nWe will use mosaic and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 10000 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 10000 F-statistics and compare that with the real-world observation of F-stat.\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying the strength of the effect of TempFac on hatching Time. And the p-value is:\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/listing.html",
    "href": "content/courses/Analytics/Prescriptive/listing.html",
    "title": "Prescriptive Analytics",
    "section": "",
    "text": "📐 Intro to Linear Programming\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n💭 The Simplex Method - Intuitively\n\n\nWe will look at developing an intuitive understanding of the Simplex Method for Linear Programming.\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📅 The Simplex Method - In Excel\n\n\nWe will look at mechanizing the Simplex Method in Excel\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📅 The Simplex Method - In Excel"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📅 The Simplex Method - In Excel"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "title": "📅 The Simplex Method - In Excel",
    "section": "Procedure",
    "text": "Procedure\n\nSet up an Excel sheet as shown in the picture below. We enter in the objective function and the constraints in tabular form as shown:\n\n\n\n\n\n\n\n\n\n\nNext we invoke the Solver Add-in: (Data -&gt; Solver):\n\n\n\n\n\n\n\n\n\n\nWe set up the Solver for our problem as follows: Hit the SOLVE button.\n\n\n\n\n\n\n\n\n\n\nChoose to have all the three kinds of Reports from Solver (Answers, Sensitivity, and Limits).\n\n\n\n\n\n\n\n\n\nThis will create three new tabs which give additional information on:\n- How “centered” the solution is, or is it sensitive to variations of some parameters\n- How much slack do the individual constraints still have, at the end\nWe will discuss this in class!\nThe complete Excel file is here for your reference.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Prescriptive Modelling",
      "📅 The Simplex Method - In Excel"
    ]
  },
  {
    "objectID": "content/courses/Analytics/UsingAI/listing.html",
    "href": "content/courses/Analytics/UsingAI/listing.html",
    "title": "Using AI in Analytics",
    "section": "",
    "text": "Luis D. Verde Arregoitia. June 25, 2025). Large Language Model Tools for R. https://luisdva.github.io/llmsr-book/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Using AI in Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/UsingAI/listing.html#references",
    "href": "content/courses/Analytics/UsingAI/listing.html#references",
    "title": "Using AI in Analytics",
    "section": "",
    "text": "Luis D. Verde Arregoitia. June 25, 2025). Large Language Model Tools for R. https://luisdva.github.io/llmsr-book/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Using AI in Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html",
    "href": "content/courses/Analytics/Workflow/listing.html",
    "title": "Workflow",
    "section": "",
    "text": "Facing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html#references",
    "href": "content/courses/Analytics/Workflow/listing.html#references",
    "title": "Workflow",
    "section": "",
    "text": "Facing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/20-Dashboards-Quarto/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/20-Dashboards-Quarto/index.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html",
    "title": " I Publish, therefore I Am",
    "section": "",
    "text": "“Most institutions demand unqualified faith; but the institution of science makes skepticism a virtue.”\n— Robert King Merton, sociologist (4 Jul 1910-2003)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#setting-up-git-and-github",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#setting-up-git-and-github",
    "title": " I Publish, therefore I Am",
    "section": "Setting up Git and GitHub",
    "text": "Setting up Git and GitHub\n\nWindows: Head off https://gitforwindows.org and download and install the git app.\nMacOS: In a terminal type:\n\n\nxcode-select --install\n\nThis will also install git.\n\nHead off to https://github.com and register for an account. Make a note of your emailID and the password used.\nSetup 2FA Authentication for GitHub based on instructions here: https://docs.github.com/en/authentication/securing-your-account-with-two-factor-authentication-2fa",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#linking-rstudio-and-github-one-time-only",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#linking-rstudio-and-github-one-time-only",
    "title": " I Publish, therefore I Am",
    "section": "Linking RStudio and GitHub (One Time Only)",
    "text": "Linking RStudio and GitHub (One Time Only)\n\nIn RStudio, in your console:\n\n\ninstall.packages(usethis)\nlibrary(usethis)\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"Your Email ID used on GitHub\")\n\n\nGenerate a Personal Access Token(PAT) (One Time Only, usually)\n\n\nusethis::create_github_token() This will open a browser window and ask you to log in to GitHub.\n\nIn the popup window, give your token a logical name you can remember(e.g. “For my Mac”), and remember why you created it. Check the scopes of the PAT; the defaults are OK. Make sure selecting repo, user, and workflow are checked. Save the PAT in your password manager. Default Expiry is 30days; we can set the PAT to not expire too. (I have.)\n\nLet RStudio have your PAT (One Time Only, usually)\n\n\ngitcreds::gitcreds_set() and paste the PAT when asked.\n\n\nCheck if all is OK using one or more of:\n\n\nusethis::gh_token_help()\nusethis::git_sitrep()\ngh::gh_whoami()\n\n\nCheck in RStudio if has automatically detected your Git installation.\n\n\nTools -&gt; Global Options -&gt; Git/SVN\n\n\n\n\n\n\n\nFigure 1: RStudio Project Options Window\n\n\n\nThe Git Executable field should be like:\n\nWindows: C:/Program Files/Git/bin/git.exe.\nMacOS:/usr/bin/git\n\n\nLet RStudio know which project branch to commit/push to GitHub. ( Default is “main”)\n\n\nusethis::git_default_branch_configure()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#creating-your-blog-project-in-rstudio-one-time-only",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#creating-your-blog-project-in-rstudio-one-time-only",
    "title": " I Publish, therefore I Am",
    "section": "Creating your Blog Project in RStudio (One Time Only)",
    "text": "Creating your Blog Project in RStudio (One Time Only)\nIn the upper right corner of RStudio, click on:\nProject -&gt; New Project -&gt; New Directory -&gt; Quarto Blog\n\n\n\n\n\n\nFigure 2: New Quarto Blog Project\n\n\n\nName your Blog, browse to where you want the Blog Project Folder (default is fine). Check the Create a git repository. Click Create Project.\nRStudio will restart with a new Blog Project. The Files pane should like this:\n The posts folder contains dummy blog posts, which you can retain for now and delete once your own content has matured.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#making-your-blog-ready-to-publish-now-and-every-time-you-have-new-content",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#making-your-blog-ready-to-publish-now-and-every-time-you-have-new-content",
    "title": " I Publish, therefore I Am",
    "section": "Making your Blog ready to publish (Now, and every time you have new content)",
    "text": "Making your Blog ready to publish (Now, and every time you have new content)\nIn your Terminal:\n\nquarto render This will render all posts and prepare _site and _freeze folders in your Files tab. Check these.\nquarto preview This will pop up your browser and show you a preview of your Blog website. Check the look and feel, the typos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#pushing-to-github-for-the-first-time",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#pushing-to-github-for-the-first-time",
    "title": " I Publish, therefore I Am",
    "section": "Pushing to GitHub for the First Time",
    "text": "Pushing to GitHub for the First Time\nIn your Console:\n\nusethis::use_git(): This will throw up a funky set of messages asking if you are ready to commit all files to push to Github. Choose the appropriate reply and enter.\nusethis::use_github(): This will create your new repository on Github and push all the committed files there. Your browser will open in Github and show you the contents of your new Blog repository.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#connecting-to-netlify",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#connecting-to-netlify",
    "title": " I Publish, therefore I Am",
    "section": "Connecting to Netlify",
    "text": "Connecting to Netlify\n\nHead off to https://www.netlify.com. Log in there with Github.\nClick Add new site -&gt; Import from existing project\n\n\n\n\n\n\n\nFigure 3: Netlify Window\n\n\n\n\nChoose Github, then point to your Blog Github repo.\nGive your Blog site a name: something-idiotic.netlify.app. Check Availability. (Note: netlify.app at the end is not removable, if you want free web-hosting at netlify)\nEnsure that the branch says main, and that the Publish directory says _site.\nHit Deploy. Netlify will ta ke a few minutes and then say the site is deployed. Click on the URL.\nThere!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#adding-content-and-updating",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#adding-content-and-updating",
    "title": " I Publish, therefore I Am",
    "section": "Adding Content and Updating",
    "text": "Adding Content and Updating\n\nCreate a new folder inside your posts folder. Name it something like new-post-todays-date. Hyphens only, no underscores.\nCreate a new Quarto Document: File -&gt; New-File -&gt; New Quarto Document. Save it as index.qmd inside your just-created new-post-todays-date folder.\nEdit / Write up your index.qmd Post. Add pictures, videos, code, visualizations, explanations. Use https://quarto.org to find other Markdown constructs ( tabs , asides, marginal content…) and complete your post.\nIn your Terminal, run quarto renderto update your _site and _freeze folders.\nIn your Terminal run quarto preview to check if your new post previews properly.\nIn your Git pane, you will see a list of changed files. Click on the Commit button, type in a commit message like Added new post on &lt;date&gt;, and then click Commit.\nIn the Git pane, click on the Push button to push your changes to Github.\nIn your browser, head off to your Github repo and check if the new post is there.\nIn your browser, head off to your Netlify site URL. You should see your new post there, in the list of posts.\nUsing GUI for git commit and push: Instead of using the Terminal, or the Git pane in Rstudio, there is a better way. This is a GUI, and one can see the files, the history of commits/pushes, the commit messages, revert back to a point in history, make brances…all that visually, without some tough-looking git commands:\n\n\nSo, install a git client, such as Gitkraken.\nIt offers a ready prompt to log in with Github, accept that, filling in ID and password.\nOpen File -&gt;New tab -&gt; Browse for Repo and browse to your Blog project folder (which is what you open in RStudio)\nYou will a list of previous commits, and a bunch of new edited / changed files which are default labelled WIP. (Work in Progress, peasants)\n\n\n\n\n\n\n\nFigure 4: Gitkraken Window\n\n\n\n\nClick on Stage All and type in a memorable commit message in the box below at right.\nThe WIP will be replaced by your commit message.\nOnce staging is done, hit the Push button on the main tool bar.\nAfter Gitkraken is done, a brief popup will show stating push was successful.\nYour website should be updated in a few minutes.\n\nYeah, peasants, you’re welcome as always.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#branding-your-website",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#branding-your-website",
    "title": " I Publish, therefore I Am",
    "section": "Branding your Website",
    "text": "Branding your Website\nQuarto now has a separate feature to enable a consistent look and feel for your website. All documents, HTML, revealjs presentations, and PDFs can be styled using a _brand_yml file. The _brand_yml file allows setting of fonts for the logo, headings, code, colour schemes, and other visual elements of your website. The details are available from the Quarto Website. Here is a quick sample of the file:\n\n\n\n\n\n\nFigure 5: Sample _brand.yml file\n\n\n\nAlso, Albert Rapp has a nice blog post on how to use the _brand.yml file to style your website.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#workflow-diagram",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#workflow-diagram",
    "title": " I Publish, therefore I Am",
    "section": "Workflow Diagram",
    "text": "Workflow Diagram",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#references",
    "title": " I Publish, therefore I Am",
    "section": "References",
    "text": "References\n\nhttps://jcoliver.github.io/learn-r/010-github.html\nhttps://info5940.infosci.cornell.edu/setup/git/git-configure/\nhttps://posit-conf-2024.github.io/quarto-websites/\nAlice Bartlett. Git for Humans https://speakerdeck.com/alicebartlett/git-for-humans",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nShow the Codemy.sample &lt;- rgamma(16, 1, 1 / 2)\n\nN &lt;- 10^5\nmy.boot &lt;- numeric(N)\nfor (i in 1:N)\n{\n  x &lt;- sample(my.sample, 16, replace = TRUE) # draw resample\n  my.boot[i] &lt;- mean(x) # compute mean, store in my.boot\n}\n\nggplot() +\n  geom_histogram(aes(my.boot), bins = 15)\n\n\n\n\n\n\nShow the Codemean(my.boot) # mean\n\n[1] 1.794764\n\nShow the Codesd(my.boot) # bootstrap SE\n\n[1] 0.3543059\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nShow the CodeBangladesh &lt;- read.csv(\"../../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\nShow the Codeggplot(Bangladesh, aes(sample = Arsenic)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the CodeArsenic &lt;- pull(Bangladesh, Arsenic)\n# Alternatively\n# Arsenic &lt;- Bangladesh$Arsenic\n\nn &lt;- length(Arsenic)\nN &lt;- 10^4\n\narsenic.mean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  x &lt;- sample(Arsenic, n, replace = TRUE)\n  arsenic.mean[i] &lt;- mean(x)\n}\n\nggplot() +\n  geom_histogram(aes(arsenic.mean), bins = 15) +\n  labs(title = \"Bootstrap distribution of means\") +\n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(arsenic.mean) # bootstrap mean\n\n[1] 125.3758\n\nShow the Codemean(arsenic.mean) - mean(Arsenic) # bias\n\n[1] 0.05584321\n\nShow the Codesd(arsenic.mean) # bootstrap SE\n\n[1] 18.13251\n\nShow the Codesum(arsenic.mean &gt; 161.3224) / N\n\n[1] 0.0322\n\nShow the Codesum(arsenic.mean &lt; 89.75262) / N\n\n[1] 0.0164\n\n\nExample 5.4 Skateboard\n\nShow the CodeSkateboard &lt;- read.csv(\"../../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF &lt;- Skateboard %&gt;%\n  filter(Experimenter == \"Female\") %&gt;%\n  pull(Testosterone)\ntestM &lt;- Skateboard %&gt;%\n  filter(Experimenter == \"Male\") %&gt;%\n  pull(Testosterone)\n\nobserved &lt;- mean(testF) - mean(testM)\n\nnf &lt;- length(testF)\nnm &lt;- length(testM)\n\nN &lt;- 10^4\n\nTestMean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF &lt;- sample(testF, nf, replace = TRUE)\n  sampleM &lt;- sample(testM, nm, replace = TRUE)\n  TestMean[i] &lt;- mean(sampleF) - mean(sampleM)\n}\n\ndf &lt;- data.frame(TestMean)\nggplot(df) +\n  geom_histogram(aes(TestMean), bins = 15) +\n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codeggplot(df, aes(sample = TestMean)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(testF) - mean(testM)\n\n[1] 83.0692\n\nShow the Codemean(TestMean)\n\n[1] 82.99758\n\nShow the Codesd(TestMean)\n\n[1] 29.22937\n\nShow the Codequantile(TestMean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 25.64334 140.83774 \n\nShow the Codemean(TestMean) - observed # bias\n\n[1] -0.07162265\n\n\nPermutation test for Skateboard means\n\nShow the CodetestAll &lt;- pull(Skateboard, Testosterone)\n# testAll &lt;- Skateboard$Testosterone\n\nN &lt;- 10^4 - 1 # set number of times to repeat this process\n\n# set.seed(99)\nresult &lt;- numeric(N) # space to save the random differences\nfor (i in 1:N)\n{\n  index &lt;- sample(71, size = nf, replace = FALSE) # sample of numbers from 1:71\n  result[i] &lt;- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result &gt;= observed) + 1) / (N + 1) # P-value\n\n[1] 0.0058\n\nShow the Codeggplot() +\n  geom_histogram(aes(result), bins = 15) +\n  labs(x = \"xbar1-xbar2\", title = \"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(result)\nggplot(df, aes(sample = result)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nDiff &lt;- Diving2017 %&gt;%\n  mutate(Diff = Final - Semifinal) %&gt;%\n  pull(Diff)\n# alternatively\n# Diff &lt;- Diving2017$Final - Diving2017$Semifinal\nn &lt;- length(Diff)\n\nN &lt;- 10^5\nresult &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample &lt;- sample(Diff, n, replace = TRUE)\n  result[i] &lt;- mean(dive.sample)\n}\n\nggplot() +\n  geom_histogram(aes(result), bins = 15)\n\n\n\n\n\n\nShow the Codequantile(result, c(0.025, 0.975))\n\n     2.5%     97.5% \n-6.641771 30.983542 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nShow the CodeVerizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC &lt;- Verizon %&gt;%\n  filter(Group == \"ILEC\") %&gt;%\n  pull(Time)\nTime.CLEC &lt;- Verizon %&gt;%\n  filter(Group == \"CLEC\") %&gt;%\n  pull(Time)\n\nobserved &lt;- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\n\ntime.ILEC.boot &lt;- numeric(N)\ntime.CLEC.boot &lt;- numeric(N)\ntime.diff.mean &lt;- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] &lt;- mean(ILEC.sample)\n  time.CLEC.boot[i] &lt;- mean(CLEC.sample)\n  time.diff.mean[i] &lt;- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n# bootstrap for ILEC\nggplot() +\n  geom_histogram(aes(time.ILEC.boot), bins = 15) +\n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") +\n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") +\n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nShow the Codedf &lt;- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Code# bootstrap for CLEC\nggplot() +\n  geom_histogram(aes(time.CLEC.boot), bins = 15) +\n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") +\n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") +\n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Code# Different in means\nggplot() +\n  geom_histogram(aes(time.diff.mean), bins = 15) +\n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") +\n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.mean)\n\n[1] -8.096489\n\nShow the Codequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nShow the CodeTime.ILEC &lt;- Verizon %&gt;%\n  filter(Group == \"ILEC\") %&gt;%\n  pull(Time)\nTime.CLEC &lt;- Verizon %&gt;%\n  filter(Group == \"CLEC\") %&gt;%\n  pull(Time)\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\ntime.diff.trim &lt;- numeric(N)\n\n# set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] &lt;- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() +\n  geom_histogram(aes(time.diff.trim), bins = 15) +\n  labs(x = \"difference in trimmed means\") +\n  geom_vline(xintercept = mean(time.diff.trim), colour = \"blue\") +\n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.trim)\n\n[1] -10.32079\n\nShow the Codequantile(time.diff.trim, c(0.025, 0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nShow the CodeN &lt;- 10^4\ntime.ratio.mean &lt;- numeric(N)\n\n# set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] &lt;- mean(ILEC.sample) / mean(CLEC.sample)\n}\n\nggplot() +\n  geom_histogram(aes(time.ratio.mean), bins = 12) +\n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) +\n  geom_vline(xintercept = mean(Time.ILEC) / mean(Time.CLEC), col = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.ratio.mean)\n\n[1] 0.5429164\n\nShow the Codesd(time.ratio.mean)\n\n[1] 0.1354238\n\nShow the Codequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nShow the Codehighbp &lt;- rep(c(1, 0), c(55, 3283)) # high blood pressure\nlowbp &lt;- rep(c(1, 0), c(21, 2655)) # low blood pressure\n\nN &lt;- 10^4\nboot.rr &lt;- numeric(N)\nhigh.prop &lt;- numeric(N)\nlow.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  x.high &lt;- sample(highbp, 3338, replace = TRUE)\n  x.low &lt;- sample(lowbp, 2676, replace = TRUE)\n  high.prop[i] &lt;- sum(x.high) / 3338\n  low.prop[i] &lt;- sum(x.low) / 2676\n  boot.rr[i] &lt;- high.prop[i] / low.prop[i]\n}\n\nci &lt;- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() +\n  geom_histogram(aes(boot.rr), bins = 15) +\n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour = \"observed rr\"), lty = 2) +\n  scale_colour_manual(name = \"\", values = c(\"mean of bootstrap\" = \"blue\", \"observed rr\" = \"red\"))\n\n\n\n\n\n\nShow the Codetemp &lt;- ifelse(high.prop &lt; 1.31775 * low.prop, 1, 0)\ntemp2 &lt;- ifelse(high.prop &gt; 3.687 * low.prop, 1, 0)\ntemp3 &lt;- temp + temp2\n\ndf &lt;- data.frame(y = high.prop, x = low.prop, temp, temp2, temp3)\ndf1 &lt;- df %&gt;% filter(temp == 1)\ndf2 &lt;- df %&gt;% filter(temp2 == 1)\ndf3 &lt;- df %&gt;% filter(temp3 == 0)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(data = df1, aes(x = x, y = y), colour = \"green\") +\n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") +\n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") +\n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") +\n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) +\n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) +\n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name = \"\", values = c(\"observed rr\" = \"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(mosaicCore)\nlibrary(mosaicData)\nlibrary(crosstable) # tabulated summary stats\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\n\nlibrary(tidyverse) # Tidy Data Processing\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#setting-up-r-packages",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(mosaicCore)\nlibrary(mosaicData)\nlibrary(crosstable) # tabulated summary stats\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\n\nlibrary(tidyverse) # Tidy Data Processing\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#introduction",
    "title": "Inference for Correlation",
    "section": "\n Introduction",
    "text": "Introduction\nCorrelations define how one variables varies with another. One of the basic Questions we would have of our data is: Does some variable have a significant correlation score with another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question. The block diagram depicts the statistical procedures available to test for the significance of correlation scores between two variables.\nBefore we begin, let us recap a few basic definitions:\nWe have already encountered the variance of a variable:\n\\[\n\\begin{align*}\nvar_x &= \\frac{\\sum_{i=1}^{n}(x_i - \\mu_x)^2}{(n-1)}\\\\\nwhere ~ \\mu_x &= mean(x)\\\\\nn &= sample\\ size\n\\end{align*}\n\\] The standard deviation is:\n\\[\n\\sigma_x = \\sqrt{var_x}\\\\\n\\] The covariance of two variables is defined as:\n\\[\n\\begin{align}\ncov(x,y) &= \\frac{\\sum_{i = 1}^{n}(x_i - \\mu_x)*(y_i - \\mu_y)}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{x_i *\\mu_y}}{n-1} - \\frac{\\sum{y_i *\\mu_x}}{n-1} + \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n\\end{align}\n\\]\nHence covariance is the expectation of the product minus the product of the expectations of the two variables.\n\n\n\n\n\n\nTipCovariance uses z-scores!\n\n\n\nNote that in both cases we are dealing with z-scores: variable minus its mean, \\(x_i - \\mu_x\\), which we have seen when dealing with the CLT and the Gaussian Distribution.\n\n\nSo, finally, the coefficient of correlation between two variables is defined as:\n\\[\n\\begin{align}\ncorrelation ~ r &= \\frac{cov(x,y)}{\\sigma_x * \\sigma_y}\n\\\\\n&= \\frac{cov(x,y)}{\\sqrt{var_x} * \\sqrt{var_y}}\n\\end{align}\n\\tag{1}\\]\nThus correlation coefficient is the covariance scaled by the geometric mean of the variances.\n\n\n\n\n\nflowchart TD\n    A[Inference for Correlation] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-1-galtons-famous-dataset",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-1-galtons-famous-dataset",
    "title": "Inference for Correlation",
    "section": "\n Case Study #1: Galton’s famous dataset",
    "text": "Case Study #1: Galton’s famous dataset\nHow can we start, except by using the famous Galton dataset, now part of the mosaicData package?\n\n Workflow: Read and Inspect the Data\n\ndata(\"Galton\", package = \"mosaicData\")\nGalton\n\n\n  \n\n\n\nThe variables in this dataset are:\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\n\nsex(char): sex of the child\n\nfamily(int): an ID for each family\n\n\n\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\n\nfather(dbl): father’s height in inches\n\nmother(dbl): mother’s height in inches\n\nheight(dbl): Child’s height in inches\n\nnkids(int): Number of children in each family\n\n\n\n\ninspect(Galton)\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max      mean       sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nSo there are several correlations we can explore here: Children’s height vs that of father or mother, based on sex. In essence we are replicating Francis Galton’s famous study.\n\n Workflow: Research Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\n\nBased on this sample, what can we say about the correlation between a son’s height and a father’s height in the population?\n\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\n\nBased on this sample, what can we say about the correlation between a daughter’s height and a father’s height in the population?\n\n\n\nOf course we can formulate more questions, but these are good for now! And since we are going to infer correlations by sex, let us split the dataset into two parts, one for the sons and one for the daughters, and quickly summarise them too:\n\nGalton_sons &lt;- Galton %&gt;%\n  dplyr::filter(sex == \"M\") %&gt;%\n  rename(\"son\" = height)\nGalton_daughters &lt;- Galton %&gt;%\n  dplyr::filter(sex == \"F\") %&gt;%\n  rename(\"daughter\" = height)\ndim(Galton_sons)\n\n[1] 465   6\n\ndim(Galton_daughters)\n\n[1] 433   6\n\n\n\nGalton_sons %&gt;%\n  summarize(across(\n    .cols = c(son, father),\n    .fns = list(mean = mean, sd = sd)\n  ))\n\n\n  \n\n\nGalton_daughters %&gt;%\n  summarize(across(\n    .cols = c(daughter, father),\n    .fns = list(mean = mean, sd = sd)\n  ))\n\n\n  \n\n\n\n\n Workflow: Visualization\nLet us first quickly plot a graph that is relevant to each of the two research questions.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton_sons %&gt;%\n  gf_point(son ~ father) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Heights of Sons vs Fathers\",\n    subtitle = \"Galton dataset\"\n  )\n##\nGalton_daughters %&gt;%\n  gf_point(daughter ~ father) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Heights of Daughters vs Fathers\",\n    subtitle = \"Galton dataset\"\n  )\n\n\n\n\n\n\n\n\n\n\nWe might even plot the overall heights together and colour by sex of the child:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton %&gt;%\n  gf_point(height ~ father,\n    group = ~sex, colour = ~sex\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_labs(\n    title = \"Heights of Children vs Fathers\",\n    subtitle = \"Galton dataset\"\n  )\n\n\n\n\n\n\n\nSo daughters are shorter than sons, generally speaking, and both heights seem related to that of the father.\n\n\n\n\n\n\nNoteWhat did filtering do?\n\n\n\nWhen we filtered the dataset into two, the filtering by sex of the child also effectively filtered the heights of the father (and mother). This is proper and desired; but think!\n\n\n\n Workflow: Assumptions\nFor the classical correlation tests, we need that the variables are normally distributed. As before we check this with the shapiro.test:\nshapiro.test(Galton_sons$father)\nshapiro.test(Galton_sons$son)\n##\nshapiro.test(Galton_daughters$father)\nshapiro.test(Galton_daughters$daughter)\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_sons$father\nW = 0.98529, p-value = 0.0001191\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_sons$son\nW = 0.99135, p-value = 0.008133\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_daughters$father\nW = 0.98438, p-value = 0.0001297\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_daughters$daughter\nW = 0.99113, p-value = 0.01071\n\n\n\nLet us also check the densities and quartile plots of the heights the dataset:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_density(~height,\n    group = ~sex,\n    fill = ~sex\n  ) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(title = \"Facetted Density Plots\")\n##\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_qq(~height,\n    group = ~sex,\n    colour = ~sex, size = 0.5\n  ) %&gt;%\n  gf_qqline(colour = \"black\") %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(\n    title = \"Facetted QQ Plots\",\n    x = \"Theoretical quartiles\",\n    y = \"Actual Data\"\n  )\n\n\n\n\n\n\n\n\n\n\nand the father’s heights:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n##\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_density(~father,\n    group = ~sex, # no this is not weird\n    fill = ~sex\n  ) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(name = \"Sex of Child\", palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(\n    title = \"Fathers: Facetted Density Plots\",\n    subtitle = \"By Sex of Child\"\n  )\n\n\n\n\n\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_qq(~father,\n    group = ~sex, # no this is not weird\n    colour = ~sex, size = 0.5\n  ) %&gt;%\n  gf_qqline(colour = \"black\") %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_refine(scale_colour_brewer(name = \"Sex of Child\", palette = \"Set1\")) %&gt;%\n  gf_labs(\n    title = \"Fathers Heights: Facetted QQ Plots\",\n    subtitle = \"By Sex of Child\",\n    x = \"Theoretical quartiles\",\n    y = \"Actual Data\"\n  )\n\n\n\n\n\n\n\nThe shapiro.test informs us that the child-related height variables are not normally distributed; though visually there seems nothing much to complain about. Hmmm…\nDads are weird anyway, so we must not expect father heights to be normally distributed.\n\n Workflow: Inference\nLet us now see how Correlation Tests can be performed based on this dataset, to infer patterns in the population from which this dataset/sample was drawn.\nWe will go with classical tests first, and then set up a permutation test that does not need any assumptions.\n\n\nClassical Tests\nIntuitive\nPermutation Test\nThe Linear Model\n\n\n\nWe perform the Pearson correlation test first: the data is not normal so we cannot really use this. We should use a non-parametric correlation test as well, using a Spearman correlation.\n\n# Pearson (built-in test)\ncor_son_pearson &lt;- cor.test(son ~ father,\n  method = \"pearson\",\n  data = Galton_sons\n) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Pearson Correlation r\")\ncor_son_pearson\n\n\n  \n\n\ncor_son_spearman &lt;- cor.test(son ~ father, method = \"spearman\", data = Galton_sons) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Spearman Correlation r\")\ncor_son_spearman\n\n\n  \n\n\n\nBoth tests state that the correlation between son and father is significant.\n\n# Pearson (built-in test)\ncor_daughter_pearson &lt;- cor.test(daughter ~ father,\n  method = \"pearson\",\n  data = Galton_daughters\n) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Pearson Correlation r\")\ncor_daughter_pearson\n\n\n  \n\n\n##\ncor_daughter_spearman &lt;- cor.test(daughter ~ father, method = \"spearman\", data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Spearman Correlation r\")\ncor_daughter_spearman\n\n\n  \n\n\n\nAgain both tests state that the correlation between daughter and father is significant.\n\n\nWhat is happening under the hood in cor.test?\nTo be Written Up! But when?\n\n\nWe can of course use a randomization based test for correlation. How would we mechanize this, what aspect would be randomize?\nCorrelation is calculated on a vector-basis: each individual observation of variable#1 is multiplied by the corresponding observation of variable#2. Look at Equation 1! So we might be able to randomize the order of this multiplication to see how uncommon this particular set of multiplications are. That would give us a p-value to decide if the observed correlation is close to the truth. So, onwards with our friend mosaic:\n\nobs_daughter_corr &lt;- cor(Galton_daughters$father, Galton_daughters$daughter)\nobs_daughter_corr\n\n[1] 0.4587605\n\n\n\ncorr_daughter_null &lt;- do(4999) * cor.test(daughter ~ shuffle(father),\n  data = Galton_daughters\n) %&gt;%\n  broom::tidy()\ncorr_daughter_null\n\n\n  \n\n\ncorr_daughter_null %&gt;%\n  gf_histogram(~estimate, bins = 50) %&gt;%\n  gf_vline(\n    xintercept = obs_daughter_corr,\n    color = \"red\", linewidth = 1\n  ) %&gt;%\n  gf_labs(\n    title = \"Permutation Null Distribution\",\n    subtitle = \"Daughter Heights vs Father Heights\"\n  )\n\n\n\n\n\n\n##\np_value_null &lt;- 2.0 * mean(corr_daughter_null$estimate &gt;= obs_daughter_corr)\np_value_null\n\n[1] 0\n\n\nWe see that will all permutations of father, we are never able to hit the actual obs_daughter_corr! Hence there is a definite correlation between father height and daughter height.\n\n\nThe premise here is that many common statistical tests are special cases of the linear model. A linear model estimates the relationship between dependent variable or\n“response” variable height and an explanatory variable or “predictor”, father. It is assumed that the relationship is linear. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of height based the value of father.\n\\[\nheight = \\beta_0 + \\beta_1 \\times father\n\\] The model for Pearson Correlation tests is exactly the Linear Model:\n\\[\n\\begin{aligned}\nheight = \\beta_0 + \\beta_1 \\times father\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nUsing the linear model method we get:\n\n# Linear Model\nlin_son &lt;- lm(son ~ father, data = Galton_sons) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  select(term, estimate, p.value)\nlin_son\n\n\n  \n\n\n##\nlin_daughter &lt;- lm(daughter ~ father, data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  select(term, estimate, p.value)\nlin_daughter\n\n\n  \n\n\n\nWhy are the respective \\(r\\)-s and \\(\\beta_1\\)-s different, though the p-value-s is suspiciously the same!? Did we miss a factor of \\(\\frac{sd(son/daughter)}{sd(father)} = ??\\) somewhere…??\nLet us scale the variables to within {-1, +1} : (subtract the mean and divide by sd) and re-do the Linear Model with scaled versions of height and father:\n\n# Scaled linear model\nlin_scaled_galton_daughters &lt;- lm(scale(daughter) ~ 1 + scale(father), data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\"))\nlin_scaled_galton_daughters\n\n\n  \n\n\n\nNow you’re talking!! The estimate is the same in both the classical test and the linear model! So we conclude:\n\nWhen both target and predictor have the same standard deviation, the slope from the linear model and the Pearson correlation are the same.\nThere is this relationship between the slope in the linear model and Pearson correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nThe slope is usually much more interpretable and informative than the correlation coefficient.\n\nHence a linear model using scale() for both variables will show slope = r.\n\nSlope_Scaled: 0.4587605 = Correlation: 0.4587605\n\nFinally, the p-value for Pearson Correlation and that for the slope in the linear model is the same (\\(0.04280043\\)). Which means we cannot reject the NULL hypothesis of “no relationship” between daughter-s and father-s heights.\n\nCan you complete this for the sons?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-2-study-and-grades",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-2-study-and-grades",
    "title": "Inference for Correlation",
    "section": "\n Case Study #2: Study and Grades",
    "text": "Case Study #2: Study and Grades\nIn some cases the LINE assumptions may not hold.\nNonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman’s ranked correlation scores. (Ranked, not sign-ranked.).\nSee the example below: We choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         &lt;dbl&gt; 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours &lt;dbl&gt; 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n\n\nWe can plot this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nggplot(gpa_study_hours, aes(x = study_hours, y = gpa)) +\n  geom_point() +\n  geom_smooth() +\n  labs(\n    title = \"GPA vs Study Hours\",\n    subtitle = \"Pearson Correlation Test\"\n  )\n\n\n\n\n\n\n\nHmm…not normally distributed, and there is a sort of increasing relationship, however is it linear? And there is some evidence of heteroscedasticity, so the LINE assumptions are clearly in violation. Pearson correlation would not be the best idea here.\nLet us quickly try it anyway, using a Linear Model for the scaled gpa and study_hours variables, from where we get:\n\n# Pearson Correlation as Linear Model\nmodel_gpa &lt;-\n  lm(scale(gpa) ~ 1 + scale(study_hours), data = gpa_study_hours)\n##\nmodel_gpa %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  cbind(confint(model_gpa) %&gt;% as_tibble()) %&gt;%\n  select(term, estimate, p.value, `2.5 %`, `97.5 %`)\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is \\(0.065\\) (and the confidence interval includes \\(0\\)).\nHence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship. But can this be right?\nShould we use another test, that does not need the LINE assumptions?\n“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. (In some cases the signed-rank of the data values is used instead of the data itself.)\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on. This gives is ranked data.\nGive each of the ranks the sign of the original observation ( + or -). This gives us signed ranked data.\n\n\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nPlotting Original and Signed Rank Data\nLet us see how this might work by comparing data and its signed-rank version…A quick set of plots:\nSo the means of the ranks three separate variables seem to be in the same order as the means of the data variables themselves.\nHow about associations between data? Do ranks reflect well what the data might?\nThe slopes are almost identical, \\(0.25\\) for both original data and ranked data for \\(y1\\sim x\\). So maybe ranked and even sign_ranked data could work, and if it can work despite LINE assumptions not being satisfied, that would be nice!\nHow does Sign-Rank data work?\nTBD: need to add some explanation here.\nSpearman correlation = Pearson correlation using the rank of the data observations. Let’s check how this holds for a our x and y1 data:\nSo the Linear Model for the Ranked Data would be:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nCode\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the Spearman correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are “independent” of sd )\nExample\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncars93 %&gt;%\n  ggplot(aes(weight, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, lty = 2) +\n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\") +\n  theme_classic()\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncor.test(cars93$price, cars93$weight, method = \"spearman\") %&gt;% broom::tidy()\n\n\n  \n\n\n# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %&gt;% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: &lt; 2.2e-16\n\n# Stats Plot\nggstatsplot::ggscatterstats(\n  data = cars93, x = weight,\n  y = price,\n  type = \"nonparametric\",\n  title = \"Cars93: Weight vs Price\",\n  subtitle = \"Spearman Correlation\"\n)\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman’s \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882.\n\n# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours) %&gt;%\n  broom::tidy() %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n\n  \n\n\n\n\nstatsExpressions::corr_test(\n  data = gpa_study_hours,\n  x = study_hours,\n  y = gpa\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#wait-but-why",
    "title": "Inference for Correlation",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nCorrelation tests are useful to understand the relationship between two variables, but they do not imply causation. A high correlation does not mean that one variable causes the other to change. It is essential to consider the context and other factors that may influence the relationship.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#conclusion",
    "title": "Inference for Correlation",
    "section": "\n Conclusion",
    "text": "Conclusion\nCorrelation tests are a powerful way to understand the relationship between two variables. They can be performed using classical methods like Pearson and Spearman correlation, or using more robust methods like permutation tests. The linear model approach provides a deeper understanding of the relationship, especially when the assumptions of normality and homoscedasticity are met.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#your-turn",
    "title": "Inference for Correlation",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the datasets in the infer package. Use data(package = \"infer\") in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. babynames) to read it.\nSame with the resampledata and resampledata3 packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#references",
    "title": "Inference for Correlation",
    "section": "\n References",
    "text": "References\n\n\nCommon statistical tests are linear models (or: how to teach stats) by Jonas Kristoffer LindeløvCheatSheet\n\n\nCommon statistical tests are linear models: a work through by Steve Doogue\n\n\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics\n\nModern Statistics with R: From wrangling and exploring data to inference and predictive modelling by Måns Thulin\n\nJeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\neasystats\n0.7.4\nLüdecke et al. (2022)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nggstatsplot\n0.13.1\nPatil (2021b)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata3\n1.0\nChihara and Hesterberg (2022)\n\n\nstatsExpressions\n1.7.0\nPatil (2021a)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura, and Tim Hesterberg. 2022. Resampledata3: Data Sets for “Mathematical Statistics with Resampling and R” (3rd Ed). https://doi.org/10.32614/CRAN.package.resampledata3.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M. Wiernik, Etienne Bacher, Rémi Thériault, and Dominique Makowski. 2022. “easystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.” CRAN. https://doi.org/10.32614/CRAN.package.easystats.\n\n\nPatil, Indrajeet. 2021a. “statsExpressions: R Package for Tidy Dataframes and Expressions with Statistical Details.” Journal of Open Source Software 6 (61): 3236. https://doi.org/10.21105/joss.03236.\n\n\n———. 2021b. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  #\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n\ngss_summary"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Table Plots",
    "text": "Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\nUsing ggplot\n\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes(x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty),\n    stat = \"identity\",\n    position = \"fill\",\n    colour = \"black\"\n  ) +\n  geom_text(aes(label = scales::percent(edu_prop)),\n    position = position_stack(vjust = 0.5)\n  ) +\n\n\n  # if labels are desired\n  facet_grid(~Education, scales = \"free_x\", space = \"free_x\") +\n  theme(scale_fill_brewer(palette = \"RdYlGn\")) +\n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void()\n\n\n\n\n\n\n\nUsing ggmosaic\n\n\n# library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "Observed Statistic: the X^2 metric\nWhen there are multiple proportions involved, the X^2 test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n$$ H_0: Education Does Not affect Votes on Death Penalty\\\nH_a: Education affects Votes on Death Penalty\n$$\nWe should now repeat the test with permutations on Education:\n\nnull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n9.999e-05 \n\n\nThe p-value is well below our threshold of \\(0.05%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nSo, what do you think?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "title": "Basics of Randomization Tests",
    "section": "Introduction: The Lady who Drank Tea",
    "text": "Introduction: The Lady who Drank Tea\n\nThere is a famous story about a lady who claimed that tea with milk tasted different depending on whether the milk was added to the tea or the tea added to the milk. The story is famous because of the setting in which she made this claim. She was attending a party in Cambridge, England, in the 1920s. Also in attendance were a number of university dons and their wives. The scientists in attendance scoffed at the woman and her claim. What, after all, could be the difference?\nAll the scientists but one, that is. Rather than simply dismiss the woman’s claim, he proposed that they decide how one should test the claim. The tenor of the conversation changed at this suggestion, and the scientists began to discuss how the claim should be tested. Within a few minutes cups of tea with milk had been prepared and presented to the woman for tasting.\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology[^1]. Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological.\n\nGame\nLet’s try an experiment. I’ll flip 10 coins. You guess which are heads and which are tails, and we’ll see how you do. Please write down a sequence of “H” or “T”. Comparing with your classmates, we will undoubtedly see that some of you did better and others worse.\nWhat would be your impression of one of you got 9 guesses correct? Is that SKILL or is that something else? What would be your immediate reaction and next move?\nAnalysis\nBack to the Lady who drank Tea !!\n\nLet’s suppose we decide to test the lady with ten cups of tea. We’ll flip a coin to decide which way to prepare the cups. If we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct – maybe even all 10. But how likely is that?\nNow let’s suppose the lady gets 9 out of 10 correct. That’s not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher’s great idea: Let’s figure out how hard it is to get 9 out of 10 by guessing. If it’s not so hard to do, then perhaps that’s just what happened ( that she was guessing ), so we won’t be too impressed with the lady’s tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something ( and has an unusual Skill).\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? Let’s just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here’s the plan. We’ll flip 10 coins, and repeat that experiment 10000 times. We’ll call the heads correct guesses and the tails incorrect guesses.\n\n\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   9  110  413 1145 2096 2446 2066 1141  469   97    8 \n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about \\(\\frac{97+8}{10000} = 1.05\\%\\) of our simulations). So one of two things must be true:\n• The lady got unusually “lucky”, by chance; OR\n• The lady is not just guessing and really has some ability in this regard.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#commentary",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#commentary",
    "title": "Basics of Randomization Tests",
    "section": "Commentary",
    "text": "Commentary\nFirst we realize something is surprising, and that we have a question or doubt. This is based on something we see, or measure, a test statistic. In our story, it is the score of \\(10/10\\) that the Lady was able to achieve about how the Tea was made.\nWe then assume the Lady is guessing and somehow by chance able to guess correctly. This would be our….NULL Hypothesis. This is our (conservative) belief about the Real World.\nWe then randomly generate many Parallel Counterfactual Worlds, where we repeat the experiment many many times, each time calculating the test statistic, under the assumption of the NULL Hypothesis is TRUE.\nWe see how often our Parallel Worlds can mimic or exceed Real World measurement of the the test statistic by comparison. If this is common (i.e. probability is high) we say we cannot reject the NULL Hypothesis (and the Lady is lucky). If the occurrence is rare, as in our case, we say we have reason to reject the NULL Hypothesis and reason to believe an underlying pattern (and Lady’s ability is beyond Question !)\nThis is the essence of the Simulation Method in statistical modelling. Take one more look at the picture from Allen Downey’s blog, below:\n\nFrom Reference #1:\n\nHypothesis testing can be thought of as a 4-step process:\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the p-value.\n\nDraw a conclusion.\nIn a traditional introductory statistics course, once this general framework has been mastered, the main work is in applying the correct formula to compute the standard test statistics in step 2 and using a table or computer to determine the p-value based on the known (usually approximate) theoretical distribution of the test statistic under the null hypothesis.\nIn a simulation-based approach, steps 2 and 3 change. In Step 2, it is no longer required that the test statistic be normalized to conform with a known, named distribution. Instead, natural test statistics, like the difference between two sample means \\(y1 − y2\\) can be used.\nIn Step 3, we use randomization to approximate the sampling distribution of the test statistic. Our lady tasting tea example demonstrates how this can be done from first principles. More typically, we will use randomization to create new simulated data sets ( “Parallel Worlds”) that are like our original data in some ways, but make the null hypothesis true. For each simulated data set, we calculate our test statistic, just as we did for the original sample. Together, this collection of test statistics computed from the simulated samples constitute our randomization distribution.\nWhen creating a randomization distribution, we will attempt to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we don’t do this, we won’t be testing our null hypothesis.\nUse the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For example, a null hypothesis about a mean doesn’t tell us about the shape of the population distribution, but the data give us some indication.\nReflect the way the original data were collected.\n\n\nFrom Chihara and Hesterberg:\n\nThis is the core idea of statistical significance or classical hypothesis testing – to calculate how often pure random chance would give an effect as large as that observed in the data, in the absence of any real effect. If that probability is small enough, we conclude that the data provide convincing evidence of a real effect.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#references",
    "title": "Basics of Randomization Tests",
    "section": "References",
    "text": "References\n\nChapter 11, Hypothesis Testing with Randomization in Introduction to Modern Statistics (1st Ed) by Mine Çetinkaya-Rundel and Johanna Hardin.\nAllen Downey, There is still only one test\nR.A. Fisher. Statistical Methods for Research Workers. Oliver & Boyd, 1925\nhttps://timesofindia.indiatimes.com/sports/cricket/icc-mens-t20-world-cup/in-numbers-virat-kohli-and-his-strange-luck-with-the-coin-toss/articleshow/87538443.cms\nLaura Chihara, Tim Hesterberg, Mathematical Statistics with Resampling and R, Wiley, 2019.\nD. Salsburg. The Lady Tasting Tea: How statistics revolutionized science in the twentieth century. W.H. Freeman, New York, 2001\nDaniel Kaplan, Nicholas J. Horton, and Randall Pruim, Simulation-based inference with mosaic https://www.mosaic-web.org/mosaic/articles/Resampling.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(gt) # for tables\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#setting-up-r-packages",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(gt) # for tables\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#introduction-to-inference-for-paired-data",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#introduction-to-inference-for-paired-data",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Introduction to Inference for Paired Data",
    "text": "Introduction to Inference for Paired Data\n\n What is Paired Data?\nSometimes the data is collected on the same set of individual categories, e.g. scores by sport persons in two separate tournaments, or sales of identical items in two separate locations of a chain store. Or say the number of customers in the morning and in the evening, at a set of store locations. In this case we treat the two sets of observations as paired, since they correspond to the same set of observable entities. This is how some experiments give us paired data.\nWe would naturally be interested in the differences in means across these two sets, which exploits this paired nature. In this module, we will examine tests for this purpose.\n\n Workflow for Inference for Paired Means\n\n\n\n\n\nflowchart TD\n    A[Inference for Paired Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test with paired=TRUE]\n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction, paired =TRUE]\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test with paired = TRUE]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n \n\n\n\n\n\n\nWe will now use a couple to case studies to traverse all the possible pathways in the Workflow above.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-1-results-from-a-diving-championship",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-1-results-from-a-diving-championship",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Case Study #1: Results from a Diving Championship",
    "text": "Case Study #1: Results from a Diving Championship\nHere we have swimming records across a Semi-Final and a Final:\n\n Inspecting and Charting Data\n\ndata(\"Diving2017\", package = \"resampledata3\")\nDiving2017\nDiving2017_inspect &lt;- inspect(Diving2017)\nDiving2017_inspect$categorical\nDiving2017_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThe data is made up of paired observations per swimmer, one for the semi-final and one for the final race. There are 12 swimmers and therefore 12 paired records. How can we quickly visualize this data?\nLet us first make this data into long form:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nDiving2017_long &lt;- Diving2017 %&gt;%\n  pivot_longer(\n    cols = c(Final, Semifinal),\n    names_to = \"race\",\n    values_to = \"scores\"\n  )\nDiving2017_long\n\n\n\n\n\n  \n\n\n\n\n\nNext, histograms and densities of the two variables at hand:\n\n\n\nDiving2017_long %&gt;%\n  gf_density(~scores,\n    fill = ~race,\n    alpha = 0.5,\n    title = \"Diving Scores\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(~race) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiving2017_long %&gt;%\n  gf_col(\n    fct_reorder(Name, scores) ~ scores,\n    fill = ~ race,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Scores\",\n    ylab = \"Name\",\n    title = \"Diving Scores\"\n  )  %&gt;% \n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n\n\n\n\nError in parse(text = input): &lt;text&gt;:12:0: unexpected end of input\n10:   )  %&gt;% \n11:   gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n   ^\n\n\n\n\n\n\n\nDiving2017_long %&gt;%\n  gf_boxplot(\n    scores ~ race,\n    fill = ~ race,\n    alpha = 0.5,\n    xlab = \"Race\",\n    ylab = \"Scores\",\n    title = \"Diving Scores\"\n  ) %&gt;% \n    gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n\n\n\n\nError in parse(text = input): &lt;text&gt;:12:0: unexpected end of input\n10:     gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n11: \n   ^\n\n\n\n\nWe see that:\n\nThe data are not normally distributed. With just such few readings (n &lt; 30) it was just possible…more readings would have helped. We will verify this aspect formally very shortly.\n\nThere is no immediately identifiable trend in score changes from one race to the other.\n\nAlthough the two medians appear to be different, the box plots overlap considerably. So one cannot visually conclude that the two sets of race timings have different means.\nA.  Check for Normality\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nshapiro.test(Diving2017$Final)\nshapiro.test(Diving2017$Semifinal)\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Final\nW = 0.9184, p-value = 0.273\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Semifinal\nW = 0.86554, p-value = 0.05738\n\n\n\nHmmm….the Shapiro-Wilk test suggests that both scores are normally distributed (!!!), though Semifinal is probably marginally so.\nCan we check this comparison also with plots? We can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nset.rseed(1234)\nDiving2017 %&gt;%\n  mutate(\n    Final_norm = rnorm(\n      n = 12,\n      mean = mean(Final),\n      sd = sd(Final)\n    ),\n    Semifinal_norm = rnorm(\n      n = 12,\n      mean = mean(Semifinal),\n      sd = sd(Semifinal)\n    )\n  ) %&gt;%\n  pivot_longer(\n    cols =\n      c(Semifinal, Final, Semifinal_norm, Final_norm),\n    names_to = \"score_type\", values_to = \"value\"\n  ) %&gt;%\n  gf_boxplot(value ~ score_type,\n    fill = ~score_type,\n    show.legend = FALSE\n  ) %&gt;%\n  gf_labs(title = \"Comparing Data and Normal Boxplots\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n###\nDiving2017_long %&gt;%\n  gf_qq(~ scores | race, size = 2) %&gt;%\n  gf_qqline(ylab = \"scores\", xlab = \"theoretical normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the boxplots are not very evocative, we see in the QQ-plots that the Final scores are closer to the straight line than the Semifinal scores. But it is perhaps still hard to accept the data as normally distributed…hmm.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test(scores ~ race,\n  data = Diving2017_long,\n  ratio = 1, # What we believe\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe variances are not significantly different, as seen by the \\(p.value = 0.08\\).\nSo to summarise our data checks:\n\n\n\n\n\n\nNoteConditions\n\n\n\n\ndata are normally distributed\n\nvariances are not significantly different\n\n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in performance between the population of swimmers who might have participated in these two races. So accordingly:\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\n\\]\n\\[\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, with the argument only.2 = FALSE to allow for paired data:\n\nobs_diff_swim &lt;- diffmean(scores ~ race,\n  data = Diving2017_long,\n  only.2 = FALSE\n) # paired data\n\n# Can use this also\n# formula method is better for permutation test!\n# obs_diff_swim &lt;- mean(~ (Final - Semifinal), data = Diving2017)\n\nobs_diff_swim\n\ndiffmean \n -11.975 \n\n\n\n Inference\n\n\nUsing the paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nSince the data variables satisfy the assumption of being normally distributed, and the variances are not significantly different, we may attempt the classical t.test with paired data. (we will use the mosaic variant). Type help(t.test) in your Console. Our model would be:\n\\[\nmean(Final(i) - Semi\\_final(i)) = \\beta_0 \\\\\n\\]\nAnd that: \\[\nH_0: \\mu_{final} - \\mu_{semifinal} = 0;\n\\] \\[\nH_a: \\mu_{final} - \\mu_{semifinal} \\ne 0;\\\\\n\\]\n\nmosaic::t.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE, var.equal = FALSE\n) %&gt;% broom::tidy()\n\n\n  \n\n\n\nThe confidence interval spans the zero value, and the p.value is a high \\(0.259\\), so there is no reason to accept alternative hypothesis that the means are different. Hence we say that there is no evidence of a difference between SemiFinal and Final scores.\n\n\nWell, we might consider ( based on knowledge of the sport ) that at least one of the variables does not meet the normality criteria, and though their variances are not significantly different. So we would attempt a non-parametric Wilcoxon test, that uses the signed-rank of the paired data differences, instead of the data variables. Our model would be:\n\\[\nmean(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ]\\ ) = \\beta_0 \\\\\n\\] \\[\nH_0: \\mu_{final} - \\mu_{semifinal} = 0;\n\\] \\[\nH_a: \\mu_{final} - \\mu_{semifinal} \\ne 0;\\\\\n\\]\n\nwilcox.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  mu = 0, # belief\n  alternative = \"two.sided\", # difference either way\n  paired = TRUE,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nHere also with the p.value being \\(0.3804\\), we have no reason to accept the Alternative Hypothesis. The parametric t.test and the non-parametric wilcox.test agree in their inferences.\n\n\nWe can apply the linear-model-as-inference interpretation both to the original data and to the sign.rank data: \n\\[\nlm(y_i - x_i \\sim 1) = \\beta_0\\\\\n\\\\ and\\\\\nlm(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ] \\sim 1) = \\beta_0\\\\\n\\]\nAnd the Hypothesis for both interpretations would be:\\[\nH_0: \\beta_0 = 0\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\\\\\n\\]\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nlm(signed_rank(Semifinal - Final) ~ 1,\n  data = Diving2017\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nWe observe that using the linear model method for the original scores and the sign-rank scores both sdo not permit us to reject the \\(H_0\\) Null Hypothesis, since p.values are high, and the confidence.intervals straddle \\(0\\).\n\n\nFor the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis (paired data!!) and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\npolarity &lt;- c(rep(1, 6), rep(-1, 6))\n# 12 +/- 1s,\n# 6 each to make sure there is equal probability\npolarity\n##\nnull_dist_swim &lt;- do(4999) *\n  mean(\n    data = Diving2017,\n    ~ (Final - Semifinal) * # take (pairwise) differences\n      mosaic::resample(polarity, # Swap polarity randomly\n        replace = TRUE\n      )\n  )\n##\nnull_dist_swim\n\n\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\n\n\n  \n\n\n\n\nLet us plot the NULL distribution and compare it with the actual observed differences in the race times:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(data = null_dist_swim, ~mean) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_swim,\n    colour = \"red\",\n    linewidth = 1\n  )\n###\ngf_ecdf(data = null_dist_swim, ~mean, linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_swim,\n    colour = \"red\",\n    linewidth = 1\n  )\n###\nprop1(~ mean &lt;= obs_diff_swim, data = null_dist_swim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n   0.1272 \n\n\n\nHmm…so by generating 4999 shuffles of score-difference polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between Semi-Final and Final swimming scores.\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\")\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model\")\n\nwilcox.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Wilcoxon test\")\n\nlm(signed_rank(Semifinal - Final) ~ 1,\n  data = Diving2017\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with sign.rank\")\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.975\n-1.190339\n0.2589684\n11\n-34.11726\n10.16726\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-11.975\n10.06016\n-1.190339\n0.2589684\n-34.11726\n10.16726\n\n\n\n\n\n\n\n\nWilcoxon test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n27\n0.3803711\nWilcoxon signed rank exact test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with sign.rank\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2\n2.135558\n-0.9365236\n0.3691097\n-6.70033\n2.70033\n\n\n\n\n\nThe linear model and the t.test are nearly identical in performance; the p.values are the same. The same is also true of the wilcox.test and the linear model with sign-rank data differences. This is of course not surprising!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-2-walmart-vs-target",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-2-walmart-vs-target",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Case Study #2: Walmart vs Target",
    "text": "Case Study #2: Walmart vs Target\nIs there a difference in the price of Groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day. We will:\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Would we reach the same conclusion?\n\n\n Inspecting and Charting Data\n\ndata(\"Groceries\")\nGroceries &lt;- Groceries %&gt;%\n  mutate(Product = stringr::str_squish(Product)) # Knock off extra spaces\nGroceries\nGroceries_inspect &lt;- inspect(Groceries)\nGroceries_inspect$categorical\nGroceries_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThere are just 30 prices for each vendor….just barely enough to get an idea of what the distribution might be. Let us plot the prices for the products, as box plots after pivoting the data to long form, 1 and as bar charts:\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long &lt;- Groceries %&gt;%\n  pivot_longer(\n    cols = c(Walmart, Target),\n    names_to = \"store\",\n    values_to = \"prices\"\n  ) %&gt;%\n  mutate(store = as_factor(store))\n\n\n\nLet us plot histograms/densities of the two variables that we wish to compare. We will also overlay a Gaussian distribution for comparison:\n\n\n\nGroceries_long %&gt;%\n  gf_dhistogram(~prices,\n    fill = ~store,\n    alpha = 0.5,\n    title = \"Grocery Costs\"\n  ) %&gt;%\n  gf_facet_grid(~store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroceries_long %&gt;%\n  gf_density(~prices,\n    fill = ~store,\n    alpha = 0.5,\n    title = \"Grocery Costs\"\n  ) %&gt;%\n  gf_facet_grid(~store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNot close to the Gaussian…there is clearly some skew to the right, with some items being very costly compared to the rest. More when we check the assumptions on data for the tests.\nHow about price differences, what we are interested in?\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long %&gt;%\n  gf_boxplot(prices ~ store,\n    fill = ~store\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long %&gt;%\n  gf_col(fct_reorder(Product, prices) ~ prices,\n    fill = ~store,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Prices\",\n    ylab = \"\",\n    title = \"Groceries Costs\"\n  ) %&gt;%\n  gf_col(\n    data =\n      Groceries_long %&gt;%\n        filter(\n          Product == \"Quaker Oats Life Cereal Original\"\n        ),\n    fct_reorder(Product, prices) ~ prices,\n    fill = ~store,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Apart from this Product, the rest have no discernible trend either way. Let us check observed statistic (the mean difference in prices)\n\n\nobs_diff_price &lt;- diffmean(prices ~ store,\n  data = Groceries_long,\n  only.2 = FALSE\n)\n# Can also use\n# obs_diff_price &lt;-  mean( ~ Walmart - Target, data = Groceries)\nobs_diff_price\n\n\n\n  diffmean \n0.05666667 \n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in prices, per product between the two Store chains. So accordingly:\n\\[\nH_0: \\mu_{Walmart} = \\mu_{Target}\\\\\n\\]\n\\[\nH_a: \\mu_{Walmart} \\ne \\mu_{Target}\\\n\\]\nTesting for Assumptions on the Data\nThere are a few checks we need to make of our data, to decide what test procedure to use.\nA.  Check for Normality\n\n\n\nshapiro.test(Groceries$Walmart)\nshapiro.test(Groceries$Target)\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Walmart\nW = 0.78662, p-value = 3.774e-05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Target\nW = 0.79722, p-value = 5.836e-05\n\n\n\n\nFor both tests, we see that the p.value is very small, indicating that the data are unlikely to be normally distributed. This means we cannot apply a standard paired t.test and need to use the non-parametric wilcox.test, that does not rely on the assumption of normality.\nB.  Check for Variances\nLet us check if the two variables have similar variances:\n\nvar.test(Groceries$Walmart, Groceries$Target)\n\n\n    F test to compare two variances\n\ndata:  Groceries$Walmart and Groceries$Target\nF = 0.97249, num df = 29, denom df = 29, p-value = 0.9406\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4628695 2.0431908\nsample estimates:\nratio of variances \n         0.9724868 \n\n\nIt appears from the \\(p.value = 0.9\\) and the \\(Confidence Interval = [0.4629, 2.0432]\\), which includes \\(1\\), that we cannot reject the NULL Hypothesis that the variances are not significantly different.\n\n Inference\n\n\nUsing paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nWell, the variables are not normally distributed, so a standard t.test is not advised, even if the variances are similar. We can still try:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target, paired = TRUE) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.64\\) ! And the Confidence Interval straddles \\(0\\). So the t.test gives us no reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(Groceries$Walmart, Groceries$Target,\n  data = Groceries_long,\n  digits.rank = 7, paired = TRUE,\n  conf.int = TRUE, conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe Wilcoxon test result is very interesting: the p.value says there is a significant difference between the two store prices, and the confidence.interval also is unipolar…\n\n\nAs before we can do the linear model for both the original data and the sign.rank data. The test statistic is again the difference between thetwo variables:\n\nlm(Target - Walmart ~ 1, data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nlm(signed_rank(Target - Walmart) ~ 1,\n  data = Groceries\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nVery interesting results, but confirming what we saw earlier: The Linear Model with the original data reports no significant difference, but the linear model with sign-ranks, suggests there is a significant difference in means prices between stores!\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\n# | layout: [[15, 85, 15]]\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\npolarity &lt;- c(rep(1, 15), rep(-1, 15))\n##\nnull_dist_price &lt;- do(9999) *\n  mean(\n    data = Groceries,\n    ~ (Target - Walmart) *\n      resample(polarity, replace = TRUE)\n  )\nnull_dist_price\n\n\n  \n\n\n##\ngf_histogram(data = null_dist_price, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nprop1(~mean, data = null_dist_price)\n\nprop_-0.292 \n      2e-04 \n\n\nDoes not seem to be any significant difference in prices…\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target, paired = TRUE) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"cyan\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\")\n###\nlm(Target - Walmart ~ 1, data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"cyan\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model\")\n###\nwilcox.test(Groceries$Walmart, Groceries$Target,\n  digits.rank = 7,\n  paired = TRUE,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"palegreen\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Wilcoxon Test\")\n###\nlm(signed_rank(Target - Walmart) ~ 1,\n  data = Groceries\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"palegreen\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Sign.Ranks\")\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.05666667\n-0.4704556\n0.6415488\n29\n-0.3030159\n0.1896825\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.05666667\n0.1204506\n0.4704556\n0.6415488\n-0.1896825\n0.3030159\n\n\n\n\n\n\n\n\nWilcoxon Test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.104966\n95\n0.01431746\n-0.1750051\n-0.03005987\nWilcoxon signed rank test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Sign.Ranks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n8.533333\n2.888834\n2.953902\n0.006167464\n2.625004\n14.44166\n\n\n\n\n\nClearly, the parametric tests do not detect a significant difference in prices, whereas the non-parametric tests do.\nSuppose we knock off the Quaker Cereal data item…(note the spaces in the product name)\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nset.seed(12345)\nGroceries_less &lt;- Groceries %&gt;%\n  filter(Product != \"Quaker Oats Life Cereal Original\")\n##\nGroceries_less_long &lt;- Groceries_less %&gt;%\n  pivot_longer(\n    cols = c(Target, Walmart),\n    names_to = \"store\",\n    values_to = \"prices\"\n  )\n##\nwilcox.test(Groceries_less$Walmart,\n  Groceries_less$Target,\n  paired = TRUE, digits.rank = 7,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n##\nobs_diff_price_less &lt;-\n  mean(~ (Target - Walmart), data = Groceries_less)\nobs_diff_price_less\npolarity_less &lt;- c(rep(1, 15), rep(-1, 14))\n# Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;-\n  do(9999) * mean(\n    data = Groceries_less,\n    ~ (Target - Walmart) * resample(polarity_less, replace = TRUE)\n  )\n##\ngf_histogram(data = null_dist_price_less, ~mean) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_price_less,\n    colour = \"red\"\n  )\n##\nmean(null_dist_price_less &gt;= obs_diff_price_less)\n\n\n\n\n  \n\n\n\n[1] 0.1558621\n\n\n\n\n\n\n\n\n\n\n[1] 0.01370137\n\n\n\nWe see that removing the Quaker Oats product item from the data does give a significant difference in mean prices !!! That one price difference was in the opposite direction compared to the general trend in differences, so when it was removed, we obtained a truer picture of price differences.\nTry to do a regular parametric t.test with this reduced data!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#wait-but-why",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nPaired data is a special case of dependent data, where the observations are paired in some way. This can be due to repeated measures on the same subjects, or matched subjects in a case-control study. The paired t-test and the Wilcoxon signed-rank test are both designed for paired data, and take into account the fact that the observations are not independent.\nCan you think of an underlying experiment where the data may be paired?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#conclusion",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for paired-means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#your-turn",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the datasets in the openintro package. Use data(package = \"openintro\") in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. babynames) to read it.\nSame with the resampledata and resampledata3 packages.\nTry the datasets in the PairedData package. Yes, install this too, peasants.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#references",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n References",
    "text": "References\n\nPaired Independence test with the package infer: https://infer.netlify.app/articles/paired\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, StartTeaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngt\n1.0.0\n@gt\n\n\ninfer\n1.0.9\n@infer\n\n\nMKinfer\n1.2\n@MKinfer\n\n\nopenintro\n2.5.0\n@openintro",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#footnotes",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/tidyr-pivoting.gif↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "title": "Inference for numerical data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file: type this in your console\n\n\n\n\n\n\nNote\n\n\n\nhelp(yrbss)\n\n\n\n\n\n\n\n\nNote\n\n\n\n1 . What are the cases in this data set? How many cases are there in our sample?\n\n\nYou will first start with analyzing the weight of the participants in kilograms: weight.\nUsing visualization and summary statistics, describe the distribution of weights. The inspect() function from the mosaic package produces nice summaries of the variables in the dataset, separating categorical (character) variables from quantitative variables.\n\nmosaic::inspect(yrbss)\n\n\ncategorical variables:  \n                      name     class levels     n missing\n1                   gender character      2 13571      12\n2                    grade character      5 13504      79\n3                 hispanic character      2 13352     231\n4                     race character      5 10778    2805\n5               helmet_12m character      6 13272     311\n6   text_while_driving_30d character      8 12665     918\n7  hours_tv_per_school_day character      7 13245     338\n8 school_night_hours_sleep character      7 12335    1248\n                                   distribution\n1 male (51.2%), female (48.8%)                 \n2 9 (26.6%), 12 (26.3%), 11 (23.6%) ...        \n3 not (74.4%), hispanic (25.6%)                \n4 White (59.5%) ...                            \n5 never (52.6%), did not ride (34.3%) ...      \n6 0 (37.8%), did not drive (36.7%) ...         \n7 2 (20.4%), &lt;1 (16.4%), 3 (16.1%) ...         \n8 7 (28.1%), 8 (21.8%), 6 (21.5%) ...          \n\nquantitative variables:  \n                  name   class   min    Q1 median    Q3    max      mean\n1                  age integer 12.00 15.00  16.00 17.00  18.00 16.157041\n2               height numeric  1.27  1.60   1.68  1.78   2.11  1.691241\n3               weight numeric 29.94 56.25  64.41 76.20 180.99 67.906503\n4 physically_active_7d integer  0.00  2.00   4.00  7.00   7.00  3.903005\n5 strength_training_7d integer  0.00  0.00   3.00  5.00   7.00  2.949948\n          sd     n missing\n1  1.2637373 13506      77\n2  0.1046973 12579    1004\n3 16.8982128 12579    1004\n4  2.5641046 13310     273\n5  2.5768522 12407    1176\n\n\nNext, consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss &lt;- yrbss %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 2, \"yes\", \"no\"),\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  )\n# Let us check\nyrbss %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMake a side-by-side violin box plots of physical_3plus and weight.\nIs there a relationship between these two variables? What did you expect and why?\n\n\n\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss,\n  draw_quantiles = TRUE\n)\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for numerical data",
    "section": "Inference",
    "text": "Inference\n\n\n\n\n\n\nImportant\n\n\n\nAre all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes with the summarize command above by defining a new variable with the definition n().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWrite the hypotheses for testing if the average weights are different for those who exercise at least times a week and those who don’t.\nWrite here !\n\n\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\n\n\n  \n\n\nobs_diff_mosaic &lt;- diffmean(~ weight | physical_3plus, data = yrbss)\nobs_diff_mosaic\n\n diffmean \n-1.694383 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nInference Using `infer`\nInference Using `mosaic`\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nRecall that the specify() function is used to specify the variables you are considering (notated y ~x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\n\nnull_dist &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\ngf_histogram(data = null_dist, ~stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAdd a vertical red line to the plot above, demonstrating where the observed difference in means (obs_diff_mosaic) falls on the distribution.\nHow many of these null_dist permutations have a difference at least as large (or larger) as obs_diff_mosaic?\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = obs_diff_infer, direction = \"two_sided\")\n\n\n  \n\n\n\n\n\n\n\nWhat warning message do you get? Why do you think you get this warning message?\nConstruct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\n\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(1000) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean &gt;= obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "title": "Inference for numerical data",
    "section": "More Practice",
    "text": "More Practice\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets\n\n\n\n\n“When Alexander the Great visited Diogenes and asked whether he could do anything for the famed teacher, Diogenes replied:”Only stand out of my light.” Perhaps some day we shall know how to heighten creativity. Until then, one of the best things we can do for creative men and women is to stand out of their light.”\n— John W. Gardner, author and educator (8 Oct 1912-2002)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#slides-and-tutorials",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets\n\n\n\n\n“When Alexander the Great visited Diogenes and asked whether he could do anything for the famed teacher, Diogenes replied:”Only stand out of my light.” Perhaps some day we shall know how to heighten creativity. Until then, one of the best things we can do for creative men and women is to stand out of their light.”\n— John W. Gardner, author and educator (8 Oct 1912-2002)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#setting-up-r-packages",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(skimr) # Good to Examine data\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\nlibrary(infer)\n\nlibrary(tidyverse) # Data Processing in R\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-population",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-population",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Population?",
    "text": "What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population in science research, or a target audience in design. We mathematically denote the population’s size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the mean height of all Bangaloreans, the population parameter of interest is the population mean \\(\\mu\\).\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\n\n\n\n\n\nImportant Parameters\n\n\n\nPopulations Parameters are usually indicated by Greek Letters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-sample",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-sample",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Sample?",
    "text": "What is a Sample?\nSampling is the act of collecting a small subset from the population, which we generally do when we can’t perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population’s size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample.\n\n\n\n\n\n\nNoteWhy do we sample?\n\n\n\nBecause we cannot conduct a census ( not always ) — and sometimes we won’t even know how big the population is — we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\n\n\n\n\n\n\n\n\nImportantAn Alliterative Mnemonic\n\n\n\nNOTE: there is an alliterative mnemonic here: Samples have Statistics; Populations have Parameters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#population-parameters-and-sample-statistics",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#population-parameters-and-sample-statistics",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Population Parameters and Sample Statistics",
    "text": "Population Parameters and Sample Statistics\n\nParameters and Statistics\n\n\nPopulation Parameter\nSample Statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard Deviation\n\\(\\sigma\\)\ns\n\n\nProportion\np\n\\(\\hat{p}\\)\n\n\nCorrelation\n\\(\\rho\\)\nr\n\n\nSlope (Regression)\n\\(\\beta_1\\)\n\\(b_1\\)\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1. What is the mean commute time for workers in a particular city?\nA.1. The parameter is the mean commute time \\(\\mu\\) for a population containing all workers who work in the city. We estimate it using \\(\\bar{x}\\), the mean of the random sample of people who work in the city.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the correlation between the size of dinner bills and the size of tips at a restaurant?\nA.2. The parameter is \\(\\rho\\) , the correlation between bill amount and tip size for a population of all dinner bills at that restaurant. We estimate it using r, the correlation from a random sample of dinner bills.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. How much difference is there in the proportion of 30 to 39-year-old residents who have only a cell phone (no land line phone) compared to 50 to 59-year-olds in the country?\nA.3. The population is all citizens of the country, and the parameter is \\(p_1 - p_2\\), the difference in proportion of 30 to 39-year-old residents who have only a cell phone (\\(p_1\\)) and the proportion with the same property among all 50 to 59-year olds (\\(p_2\\)). We estimate it using (\\(\\hat{p_1} - \\hat{p_2}\\)), the difference in sample proportions computed from random samples taken from each group.\n\n\nSample statistics vary and in the following we will estimate this uncertainty and decide how reliable they might be as estimates of population parameters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#case-study-1-sampling-the-nhanes-dataset",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#case-study-1-sampling-the-nhanes-dataset",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Case Study #1: Sampling the NHANES dataset",
    "text": "Case Study #1: Sampling the NHANES dataset\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and glimpse it.\n\ndata(\"NHANES\")\nglimpse(NHANES)\n\nRows: 10,000\nColumns: 76\n$ ID               &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 5164…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, male, female, f…\n$ Age              &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, …\n$ AgeDecade        &lt;fct&gt;  30-39,  30-39,  30-39,  0-9,  40-49,  0-9,  0-9,  40…\n$ AgeMonths        &lt;int&gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795,…\n$ Race1            &lt;fct&gt; White, White, White, Other, White, White, White, Whit…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, High School, High School, NA, Some Colle…\n$ MaritalStatus    &lt;fct&gt; Married, Married, Married, NA, LivePartner, NA, NA, M…\n$ HHIncome         &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 3…\n$ HHIncomeMid      &lt;int&gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 8750…\n$ Poverty          &lt;dbl&gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00,…\n$ HomeRooms        &lt;int&gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3,…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, O…\n$ Work             &lt;fct&gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, N…\n$ Weight           &lt;dbl&gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.…\n$ BMI              &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.2…\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plus…\n$ Pulse            &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 8…\n$ BPSysAve         &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, …\n$ BPDiaAve         &lt;int&gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, 6…\n$ BPSys1           &lt;int&gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124, …\n$ BPDia1           &lt;int&gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, 6…\n$ BPSys2           &lt;int&gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108, …\n$ BPDia2           &lt;int&gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, 6…\n$ BPSys3           &lt;int&gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114, …\n$ BPDia3           &lt;int&gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, 7…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, 2…\n$ TotChol          &lt;dbl&gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, 5…\n$ UrineVol1        &lt;int&gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113, …\n$ UrineFlow1       &lt;dbl&gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1.…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Diabetes         &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HealthGen        &lt;fct&gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vgo…\n$ DaysPhysHlthBad  &lt;int&gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0,…\n$ DaysMentHlthBad  &lt;int&gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA,…\n$ LittleInterest   &lt;fct&gt; Most, Most, Most, NA, Several, NA, NA, None, None, No…\n$ Depressed        &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None,…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, N…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, N…\n$ SleepTrouble     &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, Y…\n$ PhysActive       &lt;fct&gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes, …\n$ PhysActiveDays   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, N…\n$ CompHrsDayChild  &lt;int&gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, N…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA, …\n$ AlcoholYear      &lt;int&gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, N…\n$ SmokeNow         &lt;fct&gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA, …\n$ Smoke100         &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No, …\n$ Smoke100n        &lt;fct&gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smoke…\n$ SmokeAge         &lt;int&gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Ye…\n$ AgeFirstMarij    &lt;int&gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, N…\n$ RegularMarij     &lt;fct&gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes,…\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, N…\n$ HardDrugs        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes, …\n$ SexEver          &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ SexAge           &lt;int&gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, N…\n$ SexNumPartnLife  &lt;int&gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA, …\n$ SexNumPartYear   &lt;int&gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1,…\n$ SameSex          &lt;fct&gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, N…\n$ SexOrientation   &lt;fct&gt; Heterosexual, Heterosexual, Heterosexual, NA, Heteros…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nLet us create a NHANES (sub)-dataset without duplicated IDs and only adults and select the Height variable:\n\nNHANES_adult &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(Height) %&gt;%\n  drop_na(Height)\nNHANES_adult\n\n\n  \n\n\n\n\n An “Assumed” Population\n\n\n\n\n\n\nImportantAn “Assumed” Population\n\n\n\nNormally, we very rarely have access to a population. All we can do is sample it. However, for now, and in order to build up our intuition, we will treat this single-variable dataset as our Population. So this is our population, with appropriate population parameters such as pop_mean, and pop_sd.\n\n\nLet us calculate the population parameters for the Height data from our “assumed” population:\n\n# NHANES_adult is assumed population\npop_mean &lt;- mosaic::mean(~Height, data = NHANES_adult)\n\npop_sd &lt;- mosaic::sd(~Height, data = NHANES_adult)\n\npop_mean\n\n[1] 168.3497\n\npop_sd\n\n[1] 10.15705\n\n\n\n Sampling\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\nsample_50 &lt;- mosaic::sample(NHANES_adult, size = 50) %&gt;%\n  select(Height)\nsample_50\nsample_mean_50 &lt;- mean(~Height, data = sample_50)\nsample_mean_50\n# Plotting the histogram of this sample\nsample_50 %&gt;%\n  gf_histogram(~Height, bins = 10) %&gt;%\n  gf_vline(\n    xintercept = ~sample_mean_50,\n    color = \"purple\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    colour = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (pop_mean + 8),\n    label = \"Population Mean\",\n    color = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (sample_mean_50 - 8),\n    label = \"Sample Mean\", color = \"purple\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Distribution and Mean of a Single Sample\",\n    subtitle = \"Sample Size = 50\"\n  )\n\n\n\n\n  \n\n\n\n[1] 168.464\n\n\n\n\n\n\n\n Repeated Samples and Sample Means\nOK, so the sample_mean_50 is not too far from the pop_mean. Is this always true?\nLet us check: we will create 500 repeated samples, each of size 50. And calculate their mean as the sample statistic, giving us a data frame containing 500 sample means. We will then see if these 500 means lie close to the pop_mean:\nsample_50_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;% # drop sampling related column \"orig.id\"\n    summarise(\n      sample_mean = mean(Height),\n      sample_sd = sd(Height),\n      sample_min = min(Height),\n      sample_max = max(Height)\n    )\n}\nsample_50_500\ndim(sample_50_500)\n\n\n\n\n  \n\n\n\n[1] 500   6\n\n\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_mean, color = \"purple\") %&gt;%\n  gf_labs(\n    title = \"Sample Means are close to the Population Mean\",\n    subtitle = \"Sample Means are Random!\",\n    caption = \"Grey lines represent our 500 samples\"\n  ) %&gt;%\n  gf_segment(\n    .index + .index ~ sample_min + sample_max,\n    color = \"grey\",\n    linewidth = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(annotate(\n    geom = \"text\",\n    x = 190, y = 250,\n    size = 5,\n    label = \"Grey Lines represent our 500 samples\"\n  )) %&gt;%\n  gf_label(-25 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"black\"\n  )\n##\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_sd,\n    color = \"purple\",\n    title = \"Sample SDs are close to the Population Sd\",\n    subtitle = \"Sample SDs are Random!\",\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample SDs\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_sd,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_sd,\n    label = \"Population SD\",\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(lims(x = c(4, 16)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Means and Sample SDs are a Random Variables!!1\n\n\n\nThe sample_means (purple dots in the two graphs), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (vertical black line). And hence they will have a mean and sd too 😱. Do not get confused ;-D\nAnd the sample_sds are also random and have their own distribution, around the pop_sd.2\n\n\n\n Distribution of Sample-Means\nSince the sample-means are themselves random variables, let’s plot the distribution of these 500 sample-means themselves, called a distribution of sample-means. We will also plot the position of the population mean pop_mean parameter, the mean of the Height variable.\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Sampling Mean Distribution\",\n    subtitle = \"500 means\"\n  )\n\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n#\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  ## Add the population histogram\n  gf_histogram(~Height,\n    data = NHANES_adult,\n    alpha = 0.2, fill = \"blue\",\n    bins = 30\n  ) %&gt;%\n  gf_label(0.025 ~ (pop_mean + 20),\n    label = \"Population Distribution\", color = \"blue\"\n  ) %&gt;%\n  gf_labs(title = \"Sampling Mean Distribution\", subtitle = \"Original Population overlay\")\n\n\n\n\n\nSample Means\n\n\n\n\n\nSample Means and Population\n\n\n\n\n\nDistributions\n\n\n\n\n Deriving the Central Limit Theorem (CLT)\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around the pop_mean. ( Mean of the sample means = pop_mean!! 😱)\nThat the “spread” of the distribution of sample means is less than pop_sd. Curiouser and curiouser! But exactly how much is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let’s repeatedly sample Height and compute the sample-means, and look at the resulting histograms. (We will deal with sample-standard-deviations shortly.) We will also use sample sizes of c(8, 16, ,32, 64) and generate 1000 samples each time, take the means and plot these 4 * 1000 means:\n\n# set.seed(12345)\n\nsamples_08_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 08))\n\nsamples_16_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_32_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_64_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\n# samples_128_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_08_1000)\n\n\n  \n\n\n\nLet us plot their individual histograms to compare them:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we overlay these histograms on top of one another:\n\n\n\n\n\n\n\n\nFrom the histograms we learn that the sample-means are normally distributed around the population mean. This feels intuitively right because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\nLet us calculate the means of the sample-distributions:\n\n\n\nmean(~mean, data = samples_08_1000) # Mean of means!!!;-0\nmean(~mean, data = samples_16_1000)\nmean(~mean, data = samples_32_1000)\nmean(~mean, data = samples_64_1000)\npop_mean\n\n\n\n\n[1] 168.1684\n\n\n[1] 168.3989\n\n\n[1] 168.3931\n\n\n[1] 168.3581\n\n\n[1] 168.3497\n\n\n\n\nAll are pretty close to the population mean !!!\nConsider the standard deviations of the sampling distributions:\n\\[\n\\Large{sd_i = \\sqrt{\\frac{\\Sigma[{x_i - \\bar{x_i}}]^2}{n_i}}}\n\\]\n\n\nWe take the sum of all squared differences from the mean in a sample. If we divide this sum-of-squared-differences by the sample length \\(n\\), we get a sort of average squared difference, or per-capita squared error from the mean, which is called the variance.\nTaking the square root gives us an average error, which we would call, of course, the standard deviation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npop_sd\nsd(~mean, data = samples_08_1000)\nsd(~mean, data = samples_16_1000)\nsd(~mean, data = samples_32_1000)\nsd(~mean, data = samples_64_1000)\n\n\n\n\n[1] 10.15705\n\n\n[1] 3.557632\n\n\n[1] 2.513188\n\n\n[1] 1.804117\n\n\n[1] 1.242051\n\n\n\n\nThese are also decreasing steadily with sample size. How are they related to the pop_sd?\n\n\n\npop_sd\npop_sd / sqrt(8)\npop_sd / sqrt(16)\npop_sd / sqrt(32)\npop_sd / sqrt(64)\n\n\n\n\n[1] 10.15705\n\n\n[1] 3.591058\n\n\n[1] 2.539262\n\n\n[1] 1.795529\n\n\n[1] 1.269631\n\n\n\n\nAs we can see, the standard deviations of the sampling-mean-distributions is inversely proportional to the squre root of lengths of the sample.\n\n\n\n\n\n\nNoteCentral Limit Theorem\n\n\n\nNow we have enough to state the Central Limit Theorem (CLT)\n\nthe sample-means are normally distributed around the population mean. So any mean of a single sample is a good, unbiased estimate for the pop_mean\n\nthe sample-mean distributions narrow with sample length, i.e their sd decreases with increasing sample size.\n\n\\(sd \\sim \\frac{1}{sqrt(n)}\\)\nThis is regardless of the distribution of the population itself.3\n\n\n\n\nThis theorem underlies all our procedures and techniques for statistical inference, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#the-standard-error",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#the-standard-error",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n The Standard Error",
    "text": "The Standard Error\nConsider once again the standard deviations in each of the sample-distributions that we have generated. As we saw these decrease with sample size.\nsd = pop_sd/sqrt(sample_size)where sample-size4 here is one of c(8, 16, 32, 64).\nWe reserve the term Standard Deviation for the population, and name this computed standard deviation of the sample-mean-distributions as the Standard Error. This statistic derived from the sample-mean-distribution will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\n\\Large{Standard\\ Error\\ \\pmb {se} = \\frac{pop.sd}{\\sqrt[]{n}}}\\\\\n\\] However, we don’t normally know the pop_sd!! So now what?? So is this chicken and egg??\nLet us take SINGLE SAMPLES of each size, as we normally would:\n\nsample_08 &lt;- mosaic::sample(NHANES_adult, size = 8) %&gt;%\n  select(Height)\nsample_16 &lt;- mosaic::sample(NHANES_adult, size = 16) %&gt;%\n  select(Height)\nsample_32 &lt;- mosaic::sample(NHANES_adult, size = 32) %&gt;%\n  select(Height)\nsample_64 &lt;- mosaic::sample(NHANES_adult, size = 64) %&gt;%\n  select(Height)\n##\nsd(~Height, data = sample_08)\n\n[1] 10.25572\n\nsd(~Height, data = sample_16)\n\n[1] 10.86533\n\nsd(~Height, data = sample_32)\n\n[1] 10.19689\n\nsd(~Height, data = sample_64)\n\n[1] 10.50403\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs we saw at the start of this module, the sample-means are distributed around the pop_mean, AND it appears that the sample-sds are also distributed around the pop_sd!\nHowever, the sample_sd distribution is not Gaussian (it can be never be negative!), and the sample_sd is also not an unbiased estimate for the pop_sd. However, when the sample size \\(n\\) is large (\\(n &gt;= 30\\)), the sample-sd distribution approximates the Gaussian, and the bias is small.\n\n\nAnd so, in the same way, we approximate the pop_sd with the sd of a single sample of the same length. Hence the Standard Error can be computed from a single sample as:\n\\[\n\\Large{Standard\\ Error\\ \\pmb {se} = \\frac{sample ~ sd}{\\sqrt[]{n}}}\n\\]\nWith these, the Standard Errors(SE) evaluate to:\n\n\n\npop_sd &lt;- sd(~Height, data = NHANES_adult)\npop_sd\nsd(~Height, data = sample_08) / sqrt(8)\nsd(~Height, data = sample_16) / sqrt(16)\nsd(~Height, data = sample_32) / sqrt(32)\nsd(~Height, data = sample_64) / sqrt(64)\n\n\n\n\n[1] 10.15705\n\n\n[1] 3.625945\n\n\n[1] 2.716332\n\n\n[1] 1.802572\n\n\n[1] 1.313003",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#confidence-intervals",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#confidence-intervals",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Confidence intervals",
    "text": "Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean, not a large number of them. Given that sample-means are normally distributed around the pop_mean, we can say that \\(68\\%\\) of all possible sample-mean lie within \\(\\pm SE\\) of the population mean; and further that \\(95 \\%\\) of all possible sample-mean lie within \\(\\pm 2*SE\\) of the population mean. These two constants \\(\\pm 1\\) and \\(\\pm 2\\) are the z-scores from the z-distribution we saw earlier:\n\n\n\n\n\n\n\n\nThese two intervals \\(sample.mean \\pm SE\\) and \\(sample.mean \\pm 2*SE\\) are called the confidence intervals for the population mean, at levels \\(68\\%\\) and \\(95 \\%\\) probability respectively.\nHow do these vary with sample size \\(n\\)?f\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ntbl_1 &lt;- get_ci(samples_08_1000, level = 0.95)\ntbl_2 &lt;- get_ci(samples_16_1000, level = 0.95)\ntbl_3 &lt;- get_ci(samples_32_1000, level = 0.95)\ntbl_4 &lt;- get_ci(samples_64_1000, level = 0.95)\nrbind(tbl_1, tbl_2, tbl_3, tbl_4) %&gt;%\n  rownames_to_column(\"index\") %&gt;%\n  cbind(\"sample_size\" = c(8, 16, 32, 64)) %&gt;%\n  gf_segment(index + index ~ lower_ci + upper_ci) %&gt;%\n  gf_vline(xintercept = pop_mean) %&gt;%\n  gf_labs(\n    title = \"95% Confidence Intervals for the Mean\",\n    subtitle = \"Varying samples sizes 8-16-32-64\",\n    y = \"Sample Size\", x = \"Mean Ranges\"\n  ) %&gt;%\n  gf_refine(scale_y_discrete(labels = c(8, 16, 32, 64))) %&gt;%\n  gf_refine(annotate(geom = \"label\", x = pop_mean + 1.75, y = 1.5, label = \"Population Mean\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantConfidence Intervals Shrink!\n\n\n\nYes, with sample size! Why is that a good thing?\n\n\nSo finally, we can pretend that our sample is also distributed normally ( i.e Gaussian) and use its sample_sd as a substitute for pop_sd, plot the standard errors on the sample histogram, and see where our believed mean is.\n\nsample_mean &lt;- mean(~Height, data = sample_16)\nse &lt;- sd(~Height, data = sample_16) / sqrt(16)\n#\nxqnorm(\n  p = c(0.025, 0.975),\n  mean = sample_mean,\n  sd = sd(~Height, data = sample_16),\n  return = c(\"plot\"), verbose = F\n) %&gt;%\n  gf_vline(xintercept = ~pop_mean, colour = \"black\") %&gt;%\n  gf_vline(xintercept = mean(~Height, data = sample_16), colour = \"purple\") %&gt;%\n  gf_labs(\n    title = \"Confidence Intervals and the Bell Curve. N=16\",\n    subtitle = \"Sample is plotted as theoretical Gaussian Bell Curve\"\n  ) %&gt;%\n  gf_refine(\n    annotate(geom = \"label\", x = pop_mean + 15, y = 0.05, label = \"Population Mean\"),\n    annotate(geom = \"label\", x = sample_mean - 15, y = 0.05, label = \"Sample Mean\", colour = \"purple\")\n  )\n\n\n\n\n\n\n\n\npop_mean\nse &lt;- sd(~Height, data = sample_16) / sqrt(16)\nmean(~Height, data = sample_16) - 2.0 * se\nmean(~Height, data = sample_16) + 2.0 * se\n\n[1] 168.3497\n[1] 164.3423\n[1] 175.2077\n\n\nSo if our believed mean is within the Confidence Intervals, then OK, we can say our belief may be true. If it is way outside the confidence intervals, we have to think that our belief may be flawed and accept and alternative interpretation.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-workflow",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-workflow",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n CLT Workflow",
    "text": "CLT Workflow\nThus if we want to estimate a population mean:\n\nwe take one random sample from the population of length \\(n\\)\n\nwe calculate the mean from the sample sample-mean\n\nwe calculate the sample-sd\n\nwe calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\)\n\nwe calculate 95% confidence intervals for the population parameter based on the formula \\(CI_{95\\%}= sample.mean \\pm 2*SE\\).\nSince Standard Error decreases with sample size, we need to make our sample of adequate size.( \\(n=30\\) seems appropriate in most cases. Why?)\nAnd we do not have to worry about the distribution of the population. It need not be normal/Gaussian/Bell-shaped !!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-assumptions",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n CLT Assumptions",
    "text": "CLT Assumptions\n\nSample is of “decent length” \\(n &gt;= 30\\);\nTherefore sample histogram is Gaussian",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#an-interactive-sampling-app",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#an-interactive-sampling-app",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n An interactive Sampling app",
    "text": "An interactive Sampling app\nHere below is an interactive sampling app. Play with the different settings, especially the distribution in the population to get a firm grasp on Sampling and the CLT.\nhttps://gallery.shinyapps.io/CLT_mean/",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#references",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n References",
    "text": "References\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\n\nRafael Irizzary. Introduction to Data Science. Chapter 15. Statistical Inference.https://rafalab.dfci.harvard.edu/dsbook/inference.html#populations-samples-parameters-and-estimates\n\nStats Test Wizard.https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics. Available online. https://www.openintro.org/book/os/\n\nMåns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling. http://www.modernstatisticswithr.com/\n\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\n\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\n\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\n\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/\n\nAdam Loy, Lendie Follett & Heike Hofmann (2016) Variations of Q–Q Plots: The Power of Our Eyes!, The American Statistician, 70:2, 202-214, DOI: 10.1080/00031305.2015.1077728\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nregressinator\n0.2.0\nReinhart (2024)\n\n\nsmovie\n1.1.6\nNorthrop (2023)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nNorthrop, Paul J. 2023. smovie: Some Movies to Illustrate Concepts in Statistics. https://doi.org/10.32614/CRAN.package.smovie.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nReinhart, Alex. 2024. regressinator: Simulate and Diagnose (Generalized) Linear Models. https://doi.org/10.32614/CRAN.package.regressinator.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#footnotes",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/06%3A_Sampling_Distributions/6.01%3A_The_Mean_and_Standard_Deviation_of_the_Sample_Mean↩︎\nhttps://mathworld.wolfram.com/StandardDeviationDistribution.html↩︎\nThe `Height` variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise in the tutorials.↩︎\nOnce sample size = population, we have complete access to the population and there is no question of estimation error! So sample_sd = pop_sd!↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html",
    "title": "🃏 Inference for Two Independent Means",
    "section": "",
    "text": "To be nobody but myself – in a world which is doing its best, night and day, to make you everybody else – means to fight the hardest battle which any human being can fight, and never stop fighting.\n— E.E. Cummings, poet (14 Oct 1894-1962)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#setting-up-r-packages",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nShow the Codelibrary(mosaic) # Our go-to package\nlibrary(ggformula)\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n##\nlibrary(visStatistics) # All-in-one Stats test package\n\nlibrary(tidyverse)\n\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#introduction",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Introduction",
    "text": "Introduction\n\n\n\n\n\nflowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both Parametric| D[t.test]\n    C --&gt;|Yes, but not variance Parametric| W[t.test with Welch Correction]\n    C --&gt;|No Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model with Signed-Ranks]\n    C --&gt;|No Non-Parametric| P[Bootstrap or Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #1: A Simple Data set with Two Quant Variables",
    "text": "Case Study #1: A Simple Data set with Two Quant Variables\nLet us look at the MathAnxiety dataset from the package resampledata. Here we have “anxiety” scores for boys and girls, for different components of mathematics.\n\n Inspecting and Charting Data\n\nShow the Codedata(\"MathAnxiety\", package = \"resampledata\")\nMathAnxiety\nMathAnxiety_inspect &lt;- inspect(MathAnxiety)\nMathAnxiety_inspect$categorical\nMathAnxiety_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have ~600 data entries, and with 4 Quant variables; Age,AMAS, RCMAS, and AMAS; and two Qual variables, Gender and Grade. A simple dataset, with enough entries to make it worthwhile to explore as our first example.\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nIs there a difference between boy and girl “anxiety” levels for AMAS (test) in the population from which the MathAnxiety dataset is a sample?\n\n\nFirst, histograms, densities and counts of the variable we are interested in, after converting data into long format:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nMathAnxiety %&gt;%\n  gf_density(\n    ~AMAS,\n    fill = ~Gender,\n    alpha = 0.5,\n    title = \"Math Anxiety Score Densities\",\n    subtitle = \"Boys vs Girls\"\n  )\n##\nMathAnxiety %&gt;%\n  pivot_longer(\n    cols = -c(Gender, Age, Grade),\n    names_to = \"type\",\n    values_to = \"value\"\n  ) %&gt;%\n  dplyr::filter(type == \"AMAS\") %&gt;%\n  gf_jitter(\n    value ~ Gender,\n    group = ~type, color = ~Gender,\n    width = 0.08, alpha = 0.3,\n    ylab = \"AMAS Anxiety Scores\",\n    title = \"Math Anxiety Score Jitter Plots\",\n    subtitle = \"Illustrating Difference in Means\"\n  ) %&gt;%\n  gf_summary(geom = \"point\", size = 3, colour = \"black\") %&gt;%\n  gf_line(\n    stat = \"summary\", linewidth = 1,\n    geom = \"line\", colour = ~\"MeanDifferenceLine\"\n  )\n##\nMathAnxiety %&gt;% count(Gender)\nMathAnxiety %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(mean = mean(AMAS))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nThe distributions for anxiety scores for boys and girls overlap considerably and are very similar, though the boxplot for boys shows a significant outlier. Are they close to being normal distributions too? We should check.\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution. We will also look at Q-Q plots for both variables:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nMathAnxiety %&gt;%\n  gf_density(~AMAS,\n    fill = ~Gender,\n    alpha = 0.5,\n    title = \"Math Anxiety scores for boys and girls\"\n  ) %&gt;%\n  gf_facet_grid(~Gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nMathAnxiety %&gt;%\n  gf_qqline(~AMAS,\n    color = ~Gender,\n    title = \"Math Anxiety Score..are they Normally Distributed?\"\n  ) %&gt;%\n  gf_qq() %&gt;%\n  gf_facet_wrap(~Gender) # independent y-axis\n\n\n\n\n\n\n\n\n\n\nLet us split the dataset into subsets, to execute the normality check test (Shapiro-Wilk test):\n\nShow the Codeboys_AMAS &lt;- MathAnxiety %&gt;%\n  filter(Gender == \"Boy\") %&gt;%\n  select(AMAS)\n##\ngirls_AMAS &lt;- MathAnxiety %&gt;%\n  filter(Gender == \"Girl\") %&gt;%\n  select(AMAS)\n\n\n\n\n\nShow the Codeshapiro.test(boys_AMAS$AMAS)\nshapiro.test(girls_AMAS$AMAS)\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  boys_AMAS$AMAS\nW = 0.99043, p-value = 0.03343\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  girls_AMAS$AMAS\nW = 0.99074, p-value = 0.07835\n\n\n\n\nThe distributions for anxiety scores for boys and girls are almost normal, visually speaking. With the Shapiro-Wilk test we find that the scores for girls are normally distributed, but the boys scores are not so. Sigh.\n\n\n\n\n\n\nNote\n\n\n\nThe p.value obtained in the shapiro.wilk test suggests the chances of the data being so, given the Assumption that they are normally distributed.\n\n\nWe see that MathAnxiety contains discrete-level scores for anxiety for the two variables (for Boys and Girls) anxiety scores. The boys score has a significant outlier, which we saw earlier and perhaps that makes that variable lose out, perhaps narrowly.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nShow the Codevar.test(AMAS ~ Gender,\n  data = MathAnxiety,\n  conf.int = TRUE, conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\nShow the Code##\nqf(0.975, 275, 322)\n\n[1] 1.254823\n\n\nThe variances are quite similar as seen by the \\(p.value = 0.82\\). We also saw it visually when we plotted the overlapped distributions earlier.\n\n\n\n\n\n\nImportantConditions:\n\n\n\n\nThe two variables are not both normally distributed.\nThe two variances are significantly similar.\n\n\n\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is any difference in the mean anxiety score between Girls and Boys, in the population from which the dataset MathAnxiety has been drawn. So accordingly:\n\\[\nH_0: \\mu_{Boys} = \\mu_{Girls}\\\\\n\\]\n\\[\nH_a: \\mu_{Girls} \\ne \\mu_{Boys}\\\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function:\n\nShow the Codeobs_diff_amas &lt;- diffmean(AMAS ~ Gender, data = MathAnxiety)\nobs_diff_amas\n\ndiffmean \n  1.7676 \n\n\nGirls’ AMAS anxiety scores are, on average, \\(1.76\\) points higher than those for Boys in the dataset/sample.\n\n\n\n\n\n\nNoteOn Observed Difference Estimates\n\n\n\nDifferent tests here will show the difference as positive or negative, but with the same value! This depends upon the way the factor variable Gender is used, i.e. Boy-Girl or Girl-Boy…\n\n\n\n Inference\n\n\nUsing the Parametric t.test\nUsing the Mann-Whitney Test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\n\n\n\nSince the data are not both normally distributed, though the variances similar, we typically cannot use a parametric t.test. However, we can still examine the results:\n\nShow the Codemosaic::t_test(AMAS ~ Gender, data = MathAnxiety) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.001\\) ! And the Confidence Interval does not straddle \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar and that there is a significant difference between Boys and Girls when it comes to AMAS anxiety. But can we really believe this, given the non-normality of data?\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and though the variances are similar, we use the classical wilcox.test (Type help(wilcox.test) in your Console.) which implements what we need here: the Mann-Whitney U test:3\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\n\\[\nmean(rank(AMAS_{Girls})) - mean(rank(AMAS_{Boys})) = diff\n\\]\n\\[\nH_0: \\mu_{Boys} - \\mu_{Girls} = 0\n\\]\n\\[\nH_a: \\mu_{Boys} - \\mu_{Girls} \\ne 0\n\\]\n\n\n\n\n\n\n\n\n\nShow the Codewilcox.test(AMAS ~ Gender,\n  data = MathAnxiety,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is very similar, \\(0.00077\\), and again the Confidence Interval does not straddle \\(0\\), and we are hence able to reject the NULL hypothesis that the means are equal and accept the alternative hypothesis that there is a significant difference in mean anxiety scores between Boys and Girls.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(AMAS) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\]\n\\[\nH_0: \\beta_1 = 0\\\\\n\\]\n\\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n\n\n\n\n\n\n\n\nShow the Codelm(rank(AMAS) ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The Gender variable was treated as a binary “dummy” variable4.\n\n\nHere too we see that the p.value for the slope term (“GenderGirl”) is significant at \\(7.4*10^{-4}\\).\n\n\nWe pretend that Gender has no effect on the AMAS anxiety scores. If this is our position, then the Gender labels are essentially meaningless, and we can pretend that any AMAS score belongs to a Boy or a Girl. This means we can mosaic::shuffle (permute) the Gender labels and see how uncommon our real data is. And we do not have to really worry about whether the data are normally distributed, or if their variances are nearly equal.\n\n\n\n\n\n\nImportant\n\n\n\nThe “pretend” position is exactly the NULL Hypothesis!! The “uncommon” part is the p.value under NULL!!\n\n\n\nShow the Codenull_dist_amas &lt;-\n  do(4999) * diffmean(data = MathAnxiety, AMAS ~ shuffle(Gender))\nnull_dist_amas\n\n\n  \n\n\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(data = null_dist_amas, ~diffmean, bins = 25) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_amas,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Histogram\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n###\ngf_ecdf(\n  data = null_dist_amas, ~diffmean,\n  linewidth = 1\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_amas,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Cumulative Density\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\n\n\n\nShow the Code1 - prop1(~ diffmean &lt;= obs_diff_amas, data = null_dist_amas)\n\nprop_TRUE \n    6e-04 \n\n\nClearly the observed_diff_amas is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nShow the Codemosaic::t_test(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nwilcox.test(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Original Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(rank(AMAS) ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\n\n\n\n\n\nt.test\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-1.7676\n21.16718\n22.93478\n-3.291843\n0.001055808\n580.2004\n-2.822229\n-0.7129706\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nwilcox.test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n37483\n0.0007736219\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Original Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n21.16718\n0.3641315\n58.130602\n5.459145e-248\n20.4520482\n21.882317\n\n\nGenderGirl\n1.76760\n0.5364350\n3.295087\n1.042201e-03\n0.7140708\n2.821129\n\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n278.04644\n9.535561\n29.158898\n6.848992e-117\n259.31912\n296.7738\n\n\nGenderGirl\n47.64559\n14.047696\n3.391701\n7.405210e-04\n20.05668\n75.2345\n\n\n\n\n\n\nAs we can see, all tests are in agreement that there is a significant effect of Gender on the AMAS anxiety scores!\n\n One Test to Rule Them All: visStatistics\nWe can use the visStatistics package to run all the tests in one go, using the in-built decision tree. This is a very useful package for teaching statistics, and it can be used to run all the tests we have seen so far, and more. Here goes: we use the visstat function to run all the tests, and then we can summarize the results. The visstat function takes a dataset, a quantitative variable, a qualitative variable, and some options for the tests to run.\nFrom the visStatistics package documentation:\n\nvisStatistics automatically selects and visualises appropriate statistical hypothesis tests between two column vectors of type of class “numeric”, “integer”, or “factor”. The choice of test depends on the class, distribution, and sample size of the vectors, as well as the user-defined ‘conf.level’. The main function visstat() visualises the selected test with appropriate graphs (box plots, bar charts, regression lines with confidence bands, mosaic plots, residual plots, Q-Q plots), annotated with the main test results, including any assumption checks and post-hoc analyses.\n\n\nShow the Code# Generate the annotated plots and statistics\nvisstat(\n  x = MathAnxiety$Gender,\n  y = MathAnxiety$AMAS,\n  conf.level = 0.95, numbers = TRUE\n) %&gt;%\n  summary()\n\n\n\n\n\n\n\nSummary of visstat object\n\n--- Named components ---\n[1] \"dependent variable (response)\"    \"independent variables (features)\"\n[3] \"t-test-statistics\"                \"Shapiro-Wilk-test_sample1\"       \n[5] \"Shapiro-Wilk-test_sample2\"       \n\n--- Contents ---\n\n$dependent variable (response):\n[1] \"AMAS\"\n\n$independent variables (features):\n[1] Boy  Girl\nLevels: Boy Girl\n\n$t-test-statistics:\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -3.2918, df = 580.2, p-value = 0.001056\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.8222293 -0.7129706\nsample estimates:\nmean of x mean of y \n 21.16718  22.93478 \n\n\n$Shapiro-Wilk-test_sample1:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99043, p-value = 0.03343\n\n\n$Shapiro-Wilk-test_sample2:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99074, p-value = 0.07835\n\n\n--- Attributes ---\n$plot_paths\ncharacter(0)\n\n\nThe tool runs the Welch t-test and declares the p-value to be significant. The Shapiro-Wilk test results here also confirm what we had performed earlier. Hence we can say that we may reject the NULL Hypothesis and state that there is a statistically significant difference in AMAS anxiety scores between Boys and Girls.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-2-youth-risk-behavior-surveillance-system-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-2-youth-risk-behavior-surveillance-system-yrbss-survey",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey",
    "text": "Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey\nEvery two years, the Centers for Disease Control and Prevention in the USA conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from highschoolers (9th through 12th grade), to analyze health patterns. We will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.The yrbss dataset is part of the openintro package. Type this in your console: help(yrbss).\n\n Inspecting and Charting Data\n\nShow the Codedata(yrbss, package = \"openintro\")\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file.Type this in your console: help(yrbss)\nFirst, histograms, densities and counts of the variable we are interested in:\n\nShow the Codeyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender) %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  )\n###\nyrbss_select_gender %&gt;%\n  gf_jitter(weight ~ gender,\n    color = ~gender,\n    show.legend = FALSE,\n    width = 0.05, alpha = 0.25,\n    ylab = \"Weight\",\n    title = \"Weights of Boys and Girls\"\n  ) %&gt;%\n  gf_summary(\n    group = ~1, # See the reference link above. Damn!!!\n    fun = \"mean\", geom = \"line\", colour = \"lightblue\",\n    lty = 1, linewidth = 2\n  ) %&gt;%\n  gf_summary(\n    fun = \"mean\", colour = \"firebrick\",\n    size = 4, geom = \"point\"\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\n    breaks = c(\"male\", \"female\"),\n    labels = c(\"male\", \"female\"),\n    guide = \"prism_bracket\"\n  )) %&gt;%\n  gf_refine(\n    annotate(x = 0.75, y = 60, geom = \"text\", label = \"Mean\\n Girls Weights\"),\n    annotate(x = 2.25, y = 60, geom = \"text\", label = \"Mean\\n Boys Weights\"),\n    annotate(x = 1.5, y = 100, geom = \"label\", label = \"Slope indicates\\n differences in mean\", fill = \"moccasin\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow the Codeyrbss_select_gender %&gt;% count(gender)\n\n\n  \n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians. In this Case Study, our research question is:\n\n Hypothesis\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?\n\n\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{weight-male} = \\mu_{weight-female}\n\\]\n\\[\nH_a: \\mu_{weight-male} \\ne \\mu_{weight-female}\n\\]\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nWe will complete a visual check for normality with plots, and since we cannot do a shapiro.test (length(data) &gt;= 5000) we can use the Anderson-Darling test.\nLet us plot frequency distribution and Q-Q plots5 for both variables.\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nmale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"male\") %&gt;%\n  select(weight)\n##\nfemale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"female\") %&gt;%\n  select(weight)\n\n# shapiro.test(male_student_weights$weight)\n# shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_facet_grid(~gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nyrbss_select_gender %&gt;%\n  gf_qqline(~ weight | gender, ylab = \"scores\") %&gt;%\n  gf_qq()\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nNo real evidence (visually) of the variables being normally distributed.\n\nShow the Codelibrary(nortest)\nnortest::ad.test(male_student_weights$weight)\n\n\n    Anderson-Darling normality test\n\ndata:  male_student_weights$weight\nA = 113.23, p-value &lt; 2.2e-16\n\nShow the Codenortest::ad.test(female_student_weights$weight)\n\n\n    Anderson-Darling normality test\n\ndata:  female_student_weights$weight\nA = 157.17, p-value &lt; 2.2e-16\n\n\np-values are very low and there is no reason to think that the data is normal.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.testdoes this for us, with a NULL hypothesis that the variances are not significantly different:\n\nShow the Codevar.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test.\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nShow the Codeobs_diff_gender &lt;- diffmean(weight ~ gender,\n  data = yrbss_select_gender\n)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089 \n\n\n\n\n Inference\n\n\nUsing the Mann-Whitney test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test,\nOur model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) = \\beta_1;\n\\]\n\\[\nH_0: \\mu_{weight-male} = \\mu_{weight-female}\n\\]\n\\[\nH_a: \\mu_{weight-male} \\ne \\mu_{weight-female}\n\\]\nRecall the earlier graph showing ranks of anxiety-scores against Gender.\n\nShow the Codewilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\]\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nShow the Code# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression lm()! The gender variable was treated as a binary “dummy” variable6.\n\n\n\n\nFor the specific data at hand, we need to shuffle the gender and take the test statistic (difference in means) each time.\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist_weight &lt;-\n  do(4999) * diffmean(\n    data = yrbss_select_gender,\n    weight ~ shuffle(gender)\n  )\nnull_dist_weight\n###\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n###\ngf_histogram(\n  data = null_dist_weight, ~diffmean,\n  bins = 25\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_gender,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Histogram\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n###\ngf_ecdf(\n  data = null_dist_weight, ~diffmean,\n  linewidth = 1\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_gender,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Cumulative Density\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n  \n\n\n\nprop_TRUE \n        1 \n\n\n\n\n\n\n\n\n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nShow the Codewilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!\n\n One Test to Rule Them All: visStatistics again\nWe need to use a smaller sample of the dataset yrbss_select_gender, for the (same) reason: visstat() defaults to using the shapiro.wilk test internally:\n\nShow the Codeyrbss_select_gender_sample &lt;- yrbss_select_gender %&gt;%\n  slice_sample(n = 4999)\n\nvisstat(\n  x = yrbss_select_gender_sample$gender,\n  y = yrbss_select_gender_sample$weight,\n  conf.level = 0.95, numbers = TRUE\n) %&gt;%\n  summary()\n\n\n\n\n\n\n\nSummary of visstat object\n\n--- Named components ---\n[1] \"dependent variable (response)\"    \"independent variables (features)\"\n[3] \"t-test-statistics\"                \"Shapiro-Wilk-test_sample1\"       \n[5] \"Shapiro-Wilk-test_sample2\"       \n\n--- Contents ---\n\n$dependent variable (response):\n[1] \"weight\"\n\n$independent variables (features):\n[1] male   female\nLevels: female male\n\n$t-test-statistics:\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -26.238, df = 4920, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.87028 -11.08069\nsample estimates:\nmean of x mean of y \n 62.05522  74.03070 \n\n\n$Shapiro-Wilk-test_sample1:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.88784, p-value &lt; 2.2e-16\n\n\n$Shapiro-Wilk-test_sample2:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.93506, p-value &lt; 2.2e-16\n\n\n--- Attributes ---\n$plot_paths\ncharacter(0)\n\n\nCompare these results with those calculated earlier!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-3-weight-vs-exercise-in-the-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-3-weight-vs-exercise-in-the-yrbss-survey",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #3: Weight vs Exercise in the YRBSS Survey",
    "text": "Case Study #3: Weight vs Exercise in the YRBSS Survey\nFinally, consider the possible relationship between a highschooler’s weight and their physical activity.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nShow the Codeyrbss_select_phy &lt;- yrbss %&gt;%\n  drop_na(physically_active_7d, weight) %&gt;%\n  ## add new variable physical_3plus\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 3,\n      \"yes\", \"no\"\n    ),\n    # Convert it to a factor Y/N\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  ) %&gt;%\n  select(weight, physical_3plus)\n\n# Let us check\nyrbss_select_phy %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight vary based on whether students exercise on more or less than 3 days a week? (physically_active_7d &gt;= 3 days)\n\n\n\n Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n###\nyrbss_select_phy %&gt;%\n  gf_jitter(weight ~ physical_3plus,\n    group = ~physical_3plus,\n    width = 0.08, alpha = 0.08,\n    xlab = \"Days of Exercise &gt;=3\"\n  ) %&gt;%\n  gf_summary(\n    geom = \"point\", size = 3, group = ~physical_3plus,\n    colour = ~physical_3plus\n  ) %&gt;%\n  gf_line(\n    group = 1, # weird remedy to fix groups error message!\n    stat = \"summary\", linewidth = 1,\n    geom = \"line\", colour = ~\"MeanDifferenceLine\"\n  )\n###\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n)\n\n\n\n\n\n\n\n\n\n\nThe jitter and density plots show the comparison between the two means. We can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nShow the Codeyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nyrbss_select_phy %&gt;%\n  gf_density(~weight,\n    fill = ~physical_3plus,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Exercise Frequency\"\n  ) %&gt;%\n  gf_facet_grid(~physical_3plus) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nyrbss_select_phy %&gt;%\n  gf_qq(~ weight | physical_3plus,\n    color = ~physical_3plus\n  ) %&gt;%\n  gf_qqline(ylab = \"Weight\")\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nShow the Codevar.test(weight ~ physical_3plus,\n  data = yrbss_select_phy,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# Critical F value\nqf(0.975, 4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\n\\]\n\\[\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nShow the Codeobs_diff_phy &lt;- diffmean(weight ~ physical_3plus,\n  data = yrbss_select_phy\n)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n Inference\n\n\nUsing parametric t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Interpretation\n\n\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nShow the Codemosaic::t_test(weight ~ physical_3plus,\n  var.equal = FALSE, # Welch Correction\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\nShow the Code# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n  conf.int = TRUE,\n  conf.level = 0.95,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nShow the Codelm(rank(weight) ~ physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\n\n\n\nUsing Permutation Tests\nFor this last Case Study, we will do this in two ways, just for fun: one using our familiar mosaic package, and the other using the package infer.\nBut first, we need to initialize the test, which we will save as obs_diff_**.\nShow the Codeobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(\n    stat = \"diff in means\",\n    order = c(\"yes\", \"no\")\n  )\nobs_diff_infer\n##\nobs_diff_mosaic &lt;-\n  mosaic::diffmean(~ weight | physical_3plus,\n    data = yrbss_select_phy\n  )\nobs_diff_mosaic\n##\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nUsing infer\nUsing mosaic\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nIn infer, the specify() function is used to specify the variables you are considering (notated y ~ x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\nShow the Codenull_dist &lt;- yrbss_select_phy %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 999, type = \"permute\") %&gt;%\n  calculate(\n    stat = \"diff in means\",\n    order = c(\"yes\", \"no\")\n  )\nnull_dist\n\n\n\n\n  \n\n\n\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist %&gt;%\n  visualise() + # Note this plus sign!\n  shade_p_value(obs_diff_infer,\n    direction = \"two-sided\"\n  )\n\n\n\n\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nShow the Codenull_dist %&gt;%\n  get_p_value(\n    obs_stat = obs_diff_infer,\n    direction = \"two_sided\"\n  )\n\n\n  \n\n\n\nWhat warning message do you get? Why do you think you get this warning message? Let us construct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\nShow the Codenull_dist %&gt;%\n  infer::get_confidence_interval(\n    point_estimate = obs_diff_infer,\n    level = 0.95\n  )\n\n\n  \n\n\n\nIt does look like the observed_diff_infer is too far away from this confidence interval. Hence if there was no difference in weight caused by physical_3plus, we would never have observed it! Hence the physical_3plus does have an effect on weight!\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nShow the Codenull_dist_mosaic &lt;- do(999) *\n  diffmean(~ weight | shuffle(physical_3plus),\n    data = yrbss_select_phy\n  )\n\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_mosaic,\n    colour = \"red\", linewidth = 2\n  )\n\n\n\n\n\n\nShow the Code# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\nShow the Code# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)\n\n\n  \n\n\n\nAgain, it does look like the observed_diff_infer is too far away from this NULL distribution. Hence if there was no difference in weight caused by physical_3plus, we would never have observed it! Hence the physical_3plus does have an effect on weight!\n\n\n\nClearly there is a serious effect of Physical Exercise on the body weights of students in the population from which this dataset is drawn.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#wait-but-why",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe need often to infer differences in means between Quantitative variables in a Population\nWe treat our dataset as a sample from the population, which we cannot access\nWe can apply all the CLT ideas +t.test if the two variables in the dataset satisfy the conditions of normality, equal variance\nElse use non-parametric wilcox.test\n\nAnd by treating the two variables as one Quant in two groups, we can simply perform a Permutation test",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#conclusion",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nWe have learnt how to perform inference for independent means.\n\nWe have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met.\n\nWe have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#your-turn",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the SwimRecords dataset from the mosaicData package.\nTry some of the datasets in the moderndive package. Install it , peasants. And type in your Console data(package = \"moderndive\") to see what you have. Teacher evals might interest you!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#sec-references",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.5\n@explore\n\n\ninfer\n1.0.9\n@infer\n\n\nopenintro\n2.5.0\n@openintro\n\n\nresampledata\n0.3.2\n@resampledata\n\n\nTeachHist\n0.2.1\n@TeachHist\n\n\nTeachingDemos\n2.13\n@TeachingDemos\n\n\nvisStatistics\n0.1.7\n@visStatistics",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#footnotes",
    "title": "🃏 Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "title": "Permutation Tests for Two Means",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0: \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\n\\]\n\\[\nH_a:\\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\n\nRecidivism\n\n\n  \n\n\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the incidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\nRecidivism\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://github.com/rudeboybert/resampledata↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html",
    "title": "\n Quantities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time.”\n— Mark Twain",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#slides-and-tutorials",
    "title": "\n Quantities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time.”\n— Mark Twain",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#setting-up-r-packages",
    "title": "\n Quantities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(crosstable) # Fast stats for multiple variables in table form\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\nlibrary(tidyverse) # Most important the last\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-graphs-will-we-see-today",
    "title": "\n Quantities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Quantities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#inspiration",
    "title": "\n Quantities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Golf Drive Distance over the years\n\n\nWhat do we see here? In about two-and-a-half decades, golf drive distances have increased, on the average, by 35 yards. The maximum distance has also gone up by 30 yards, and the minimum is now at 250 yards, which was close to average in 1983! What was a decent average in 1983 is just the bare minimum in 2017!!\nIs it the dimples that the golf balls have? But these have been around a long time…or is it the clubs, and the swing technique invented by more recent players?\nNow, let us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#how-do-histograms-work",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#how-do-histograms-work",
    "title": "\n Quantities",
    "section": "\n How do Histograms Work?",
    "text": "How do Histograms Work?\nHistograms are best to show the distribution of raw Quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins. We use a Quant variable on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. The x-axis is typically broken up into “buckets” or ranges for the x-variable. And usually you can adjust the bucket ranges to explore frequency patterns. For example, you can widen histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc.\nAlthough Bar Charts may look similar to Histograms, the two are different. Bar Charts show counts of observations with respect to a Qualitative variable. For instance, bar charts show categorical data with multiple levels, such as fruits, clothing, household products in an inventory. Each bar has a height proportional to the count per shirt-size, in this example.\nHistograms do not usually show spaces between buckets because the buckets represent contiguous ranges, while bar charts show spaces to separate each (unconnected) category/level within a Qual variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-1-diamonds-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-1-diamonds-dataset",
    "title": "\n Quantities",
    "section": "\n Case Study-1: diamonds dataset",
    "text": "Case Study-1: diamonds dataset\nWe will first look at at a dataset that is directly available in R, the diamonds dataset.\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr::glimpse\n skimr::skim\n mosaic::inspect\n web-r\n\n\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\n\n\nskim(diamonds)\n\n\nData summary\n\n\nName\ndiamonds\n\n\nNumber of rows\n53940\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncut\n0\n1\nTRUE\n5\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n0\n1\nTRUE\n7\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n0\n1\nTRUE\n8\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ncarat\n0\n1\n0.80\n0.47\n0.2\n0.40\n0.70\n1.04\n5.01\n▇▂▁▁▁\n\n\ndepth\n0\n1\n61.75\n1.43\n43.0\n61.00\n61.80\n62.50\n79.00\n▁▁▇▁▁\n\n\ntable\n0\n1\n57.46\n2.23\n43.0\n56.00\n57.00\n59.00\n95.00\n▁▇▁▁▁\n\n\nprice\n0\n1\n3932.80\n3989.44\n326.0\n950.00\n2401.00\n5324.25\n18823.00\n▇▂▁▁▁\n\n\nx\n0\n1\n5.73\n1.12\n0.0\n4.71\n5.70\n6.54\n10.74\n▁▁▇▃▁\n\n\ny\n0\n1\n5.73\n1.14\n0.0\n4.72\n5.71\n6.54\n58.90\n▇▁▁▁▁\n\n\nz\n0\n1\n3.54\n0.71\n0.0\n2.91\n3.53\n4.04\n31.80\n▇▁▁▁▁\n\n\n\n\n\n\n\n\ninspect(diamonds)\n\n\ncategorical variables:  \n     name   class levels     n missing\n1     cut ordered      5 53940       0\n2   color ordered      7 53940       0\n3 clarity ordered      8 53940       0\n                                   distribution\n1 Ideal (40%), Premium (25.6%) ...             \n2 G (20.9%), E (18.2%), F (17.7%) ...          \n3 SI1 (24.2%), VS2 (22.7%), SI2 (17%) ...      \n\nquantitative variables:  \n   name   class   min     Q1  median      Q3      max         mean           sd\n1 carat numeric   0.2   0.40    0.70    1.04     5.01    0.7979397    0.4740112\n2 depth numeric  43.0  61.00   61.80   62.50    79.00   61.7494049    1.4326213\n3 table numeric  43.0  56.00   57.00   59.00    95.00   57.4571839    2.2344906\n4 price integer 326.0 950.00 2401.00 5324.25 18823.00 3932.7997219 3989.4397381\n5     x numeric   0.0   4.71    5.70    6.54    10.74    5.7311572    1.1217607\n6     y numeric   0.0   4.72    5.71    6.54    58.90    5.7345260    1.1421347\n7     z numeric   0.0   2.91    3.53    4.04    31.80    3.5387338    0.7056988\n      n missing\n1 53940       0\n2 53940       0\n3 53940       0\n4 53940       0\n5 53940       0\n6 53940       0\n7 53940       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\n\nFigure 2: Diamond Dimensions\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\ncarat(dbl): weight of the diamond 0.2-5.01\n\ndepth(dbl): depth total depth percentage 43-79\n\ntable(dbl): width of top of diamond relative to widest point 43-95\n\nprice(dbl): price in US dollars $326-$18,823\n\nx(dbl): length in mm 0-10.74\n\ny(dbl): width in mm 0-58.9\n\nz(dbl): depth in mm 0-31.8\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ncut: diamond cut Fair, Good, Very Good, Premium, Ideal\n\n\ncolor: diamond color J (worst) to D (best). (7 levels)\n\n\nclarity. measurement of how clear the diamond is I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best).\n\nThese have 5, 7, and 8 levels respectively. The fact that the class for these is ordered suggests that these are factors and that the levels have a sequence/order.\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the diamonds dataset\n\n\n\n\nThis is a large dataset (54K rows).\nThere are several Qualitative variables:\n\ncarat, price, x, y, z, depth and table are Quantitative variables.\nThere are no missing values for any variable, all are complete with 54K entries.\n\n\n\n\n Data Munging\nWe will not do any data munging for this dataset, as it is already clean and ready to use.\n\n Hypothesis and Research Questions\nLet us formulate a few Questions about this dataset. At some point, we might develop a hunch or two, and these would become our hypotheses to investigate. This is an iterative process!\n\n\n\n\n\n\nNoteHypothesis and Research Questions\n\n\n\n\nThe target variable for an experiment that resulted in this data might be the price variable. Which is a numerical Quant variable.\n\nThere are also predictor variables such as carat (Quant), color(Qual), cut(Qual), and clarity(Qual).\n\nOther predictor variables might be x, y, depth, table(all Quant)\n\nResearch Questions:\n\nWhat is the distribution of the target variable price?\nWhat is the distribution of the predictor variable carat?\nDoes a price distribution vary based upon type of cut, clarity, and color?\n\n\n\nThese should do for now. Try and think of more Questions!\n\n\n\n Plotting Histograms\nLet’s plot some histograms to answer each of the Hypothesis questions above.\n\n\n Question-1: What is the distribution of the target variable price?\n\n\n\n\n\n\nNoteQuestion-1: What is the distribution of the target variable price?\n\n\n\n\n\nggformula-1\nggplot-1\n web-r-1\n\n\n\n\n\n\ngf_histogram(~price, data = diamonds) %&gt;%\n  gf_labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## More bins\ngf_histogram(~price,\n  data = diamonds,\n  bins = 100\n) %&gt;%\n  gf_labs(\n    title = \"Plot 1B: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = price)) +\n  labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n## More bins\nggplot(data = diamonds) +\n  geom_histogram(aes(x = price), bins = 100) +\n  labs(\n    title = \"Plot 1B: Diamond Prices\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nThe price distribution is heavily skewed to the right.\n\nThere are a great many diamonds at relatively low prices, but there are a good few diamonds at very high prices too.\n\nUsing a high number of bins does not materially change the view of the histogram.\n\n\n\n\n\n Question-2: What is the distribution of the predictor variable carat?\n\n\n\n\n\n\nNoteQuestion-1: Question-2: What is the distribution of the predictor variable carat?\n\n\n\n\n\nggformula-2\nggplot-2\n web-r-2\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~carat) %&gt;%\n  gf_labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## More bins\ndiamonds %&gt;%\n  gf_histogram(~carat,\n    bins = 100\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = carat)) +\n  labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggplot\"\n  )\n## More bins\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = carat), bins = 100) +\n  labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\n\ncarat also has a heavily right-skewed distribution.\n\nHowever, there is a marked “discreteness” to the distribution. Some values of carat are far more common than others. For example, 1, 1.5, and 2 carat diamonds are large in number.\n\nWhy does the X-axis extend up to 5 carats? There must be some, very few, diamonds of very high carat value!\n\n\n\n\n Question-3: Does a price distribution vary based upon type of cut, clarity, and color?\n\n\n\n\n\n\nNoteDoes a price distribution vary based upon type of cut, clarity, and color?\n\n\n\n\n\nggformula-3\nggplot-3\n web-r-3\n\n\n\n\n\n\ngf_histogram(~price, fill = ~cut, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Plot 3B: Prices by Cut\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1\n    )\n  ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Prices Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(x = price, fill = cut), alpha = 0.3) +\n  labs(title = \"Plot 3A: Prices by Cut\", caption = \"ggplot\")\n##\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  labs(title = \"Plot 3B: Prices filled by Cut\", caption = \"ggplot\")\n##\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  facet_wrap(facets = vars(cut)) +\n  labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggplot\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n##\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  facet_wrap(facets = vars(cut), scales = \"free_y\") +\n  labs(\n    title = \"Plot D: Prices by Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggplot\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\nThe price distribution is heavily skewed to the right AND This long-tailed nature of the histogram holds true regardless of the cut of the diamond.\nSee the x-axis range for each plot in Plot D! Price ranges are the same regardless of cut !! Very surprising! So cut is perhaps not the only thing that determines price…\nFacetting the plot into small multiples helps look at patterns better: overlapping histograms are hard to decipher. Adding color defines the bars in the histogram very well.\n\n\n\n\n\n\n\n\n\nImportantA Hypothesis\n\n\n\nThe surprise insight above should lead you to make a Hypothesis! You should decide whether you want to investigate this question further, making more graphs, as we will see. Here, we are making a Hypothesis that more than just cut determines the price of a diamond.\n\n\n\n\nAn Interactive App for Histograms\nType in your Console:\n\n```{r}\n#| eval: false\ninstall.packages(\"shiny\")\nlibrary(shiny)\nrunExample(\"01_hello\") # an interactive histogram\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-2-race-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-2-race-dataset",
    "title": "\n Quantities",
    "section": "\n Case Study-2: race dataset",
    "text": "Case Study-2: race dataset\n\n Import data\nThese data come from the TidyTuesday, project, a weekly social learning project dedicated to gaining practical experience with R and data science. In this case the TidyTuesday data are based on International Trail Running Association (ITRA) data but inspired by Benjamin Nowak. We will use the TidyTuesday data that are on GitHub. Nowak’s data are also available on GitHub.\n\n R\n\n\n\nrace_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv\")\nrank_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the race Data\nLet us look at the dataset using all our three methods:\n\n\n dplyr\n skimr\n mosaic\n crosstable\n web-r\n\n\n\n\nglimpse(race_df)\n\nRows: 1,207\nColumns: 13\n$ race_year_id   &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68241,…\n$ event          &lt;chr&gt; \"Peak District Ultras\", \"UTMB®\", \"Grand Raid des Pyréné…\n$ race           &lt;chr&gt; \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"PERSENK UL…\n$ city           &lt;chr&gt; \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asenovgrad\", \"…\n$ country        &lt;chr&gt; \"United Kingdom\", \"France\", \"France\", \"Bulgaria\", \"Turk…\n$ date           &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 2021-0…\n$ start_time     &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, 17:0…\n$ participation  &lt;chr&gt; \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\",…\n$ distance       &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 163.9,…\n$ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410, 31…\n$ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -4660, …\n$ aid_stations   &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, 12, 15, 0,…\n$ participants   &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100, 300, 50,…\n\nglimpse(rank_df)\n\nRows: 137,803\nColumns: 8\n$ race_year_id    &lt;dbl&gt; 68140, 68140, 68140, 68140, 68140, 68140, 68140, 68140…\n$ rank            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, NA, NA,…\n$ runner          &lt;chr&gt; \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSON Phill\", …\n$ time            &lt;chr&gt; \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"30H 53M 37…\n$ age             &lt;dbl&gt; 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, 47, 52, 49…\n$ gender          &lt;chr&gt; \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\", \"M\", \"M\",…\n$ nationality     &lt;chr&gt; \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\"…\n$ time_in_seconds &lt;dbl&gt; 95725, 97229, 103747, 111217, 117981, 118000, 120601, …\n\n\n\n\n\nskim(race_df)\n\n\nData summary\n\n\nName\nrace_df\n\n\nNumber of rows\n1207\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n1\n\n\ndifftime\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nevent\n0\n1.00\n4\n57\n0\n435\n0\n\n\nrace\n0\n1.00\n3\n63\n0\n371\n0\n\n\ncity\n172\n0.86\n2\n30\n0\n308\n0\n\n\ncountry\n4\n1.00\n4\n17\n0\n60\n0\n\n\nparticipation\n0\n1.00\n4\n5\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ndate\n0\n1\n2012-01-14\n2021-09-03\n2017-09-30\n711\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nstart_time\n0\n1\n0 secs\n82800 secs\n05:00:00\n39\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1\n27889.65\n20689.90\n2320\n9813.5\n23565.0\n42686.00\n72496.0\n▇▃▃▂▂\n\n\ndistance\n0\n1\n152.62\n39.88\n0\n160.1\n161.5\n165.15\n179.1\n▁▁▁▁▇\n\n\nelevation_gain\n0\n1\n5294.79\n2872.29\n0\n3210.0\n5420.0\n7145.00\n14430.0\n▅▇▇▂▁\n\n\nelevation_loss\n0\n1\n-5317.01\n2899.12\n-14440\n-7206.5\n-5420.0\n-3220.00\n0.0\n▁▂▇▇▅\n\n\naid_stations\n0\n1\n8.63\n7.63\n0\n0.0\n9.0\n14.00\n56.0\n▇▆▁▁▁\n\n\nparticipants\n0\n1\n120.49\n281.83\n0\n0.0\n21.0\n150.00\n2900.0\n▇▁▁▁▁\n\n\n\n\n\n\nskim(rank_df)\n\n\nData summary\n\n\nName\nrank_df\n\n\nNumber of rows\n137803\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nrunner\n0\n1.00\n3\n52\n0\n73629\n0\n\n\ntime\n17791\n0.87\n8\n11\n0\n72840\n0\n\n\ngender\n30\n1.00\n1\n1\n0\n2\n0\n\n\nnationality\n0\n1.00\n3\n3\n0\n133\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1.00\n26678.70\n20156.18\n2320\n8670\n21795\n40621\n72496\n▇▃▃▂▂\n\n\nrank\n17791\n0.87\n253.56\n390.80\n1\n31\n87\n235\n1962\n▇▁▁▁▁\n\n\nage\n0\n1.00\n46.25\n10.11\n0\n40\n46\n53\n133\n▁▇▂▁▁\n\n\ntime_in_seconds\n17791\n0.87\n122358.26\n37234.38\n3600\n96566\n114167\n148020\n296806\n▁▇▆▁▁\n\n\n\n\n\n\n\nWe can also try our new friend mosaic::favstats:\n\nrace_df %&gt;%\n  favstats(~distance, data = .)\n\n\n  \n\n\n##\nrace_df %&gt;%\n  favstats(~participants, data = .)\n\n\n  \n\n\n##\nrank_df %&gt;%\n  drop_na() %&gt;%\n  favstats(time_in_seconds ~ gender, data = .)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNoteIntroducing crosstable\n\n\n\nmosaic::favstats allows to summarise just one variable at a time. On occasion we may need to see summaries of several Quant variables, over levels of Qual variables. This is where the package crosstable is so effective: note how crosstable also conveniently uses the formula interface that we are getting accustomed to.\nWe will find occasion to meet crosstable again when we do Inference.\n\n\n\n## library(crosstable)\ncrosstable(time_in_seconds + age ~ gender, data = rank_df) %&gt;%\n  crosstable::as_flextable()\n\n\n\n\n\n\nlabel\nvariable\ngender\n\n\nM\nW\nNA\n\n\n\n\ntime_in_seconds\nMin / Max\n3600.0 / 2.9e+05\n9191.0 / 3.0e+05\n8131.0 / 2.2e+05\n\n\nMed [IQR]\n1.2e+05 [9.7e+04;1.5e+05]\n1.1e+05 [9.7e+04;1.3e+05]\n1.2e+05 [9.9e+04;1.5e+05]\n\n\nMean (std)\n1.2e+05 (3.8e+04)\n1.2e+05 (3.5e+04)\n1.2e+05 (4.4e+04)\n\n\nN (NA)\n101643 (15073)\n18341 (2716)\n28 (2)\n\n\nage\nMin / Max\n0 / 133.0\n0 / 81.0\n29.0 / 59.0\n\n\nMed [IQR]\n47.0 [40.0;53.0]\n45.0 [39.0;52.0]\n40.5 [36.0;50.5]\n\n\nMean (std)\n46.4 (10.2)\n45.3 (9.7)\n41.7 (9.0)\n\n\nN (NA)\n116716 (0)\n21057 (0)\n30 (0)\n\n\n\n\n\n\n(The as_flextable command from the crosstable package helped to render this elegant HTML table we see. It should be possible to do Word/PDF also, which we might see later.)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nFrom race_df, we have the following Quantitative variables:\n\n\nrace_year_id: A number uniquely identifying the race event\n\ndistance: Race Distance (miles?)\n\nelevation_gain: Gain in Elevation along the route (feet?)\n\nelevation_loss: Loss in Elevation along the route (feet?)\n\nparticants: No. of participants\n\naid_stations: No. of aid stations along the race route.\n\nAnd from rank_df we have the following Quantitative variables:\n\n\nrank: Placement Rank of the Athlete\n\ntime: Race completion time ( h:m:s)\n\ntime_in_seconds: Race Completion Time in seconds\n\nage: Age of the athlete in years;\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ncountry: Country of the Race\n\ncity: Location\n\ngender: Of the Athlete\n\nparticipation: Solo / solo, relay, team. Badly coded?.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from race data\n\n\n\n\nWe have two datasets, one for races (race_df) and one for the ranking of athletes (rank_df).\nThere is atleast one common column between the two, the race_year_id variable.\nOverall, there are Qualitative variables such as country, city,gender, and participation. This last variables seems badly coded, with entries showing solo and Solo.\n\nQuantitative variables are rank, time,time_in_seconds, age from rank_df; and distance, elevation_gain, elevation_loss,particants, and aid_stations from race_df.\nWe have 1207 races and over 130K participants! But some races do show zero participants!! Is that an error in data entry?\n\n\n\n\n EDA with race datasets\nSince this dataset is somewhat complex, we may for now just have a detailed set of Questions, that helps us get better acquainted with it. On the way, we may get surprised by some finding then want to go deeper, with a hunch or hypothesis.\n\n Question-1: Max. Races and participants\n\n\n\n\n\n\nNoteQuestion #1\n\n\n\nWhich countries host the maximum number of races? Which countries send the maximum number of participants??\n\n\n R\n web-r\n\n\n\n\nShow the Coderace_df %&gt;%\n  count(country) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(3, n)\n\n\n  \n\n\nShow the Coderank_df %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(6, n)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe top three locations for races were the USA, UK, and France. These are also the countries that send the maximum number of participants, naturally!\n\n\n\n Question-2: Max. Winners\n\n\n\n\n\n\nNoteQuestion #2\n\n\n\nWhich countries have the maximum number of winners (top 3 ranks)?\n\n\n R\n web-r\n\n\n\n\nShow the Coderank_df %&gt;%\n  filter(rank %in% c(1, 2, 3)) %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(6, n)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n1240 Participants from the USA have been top 3 finishers. Across all races…\n\n\n\n Question-3: Which countries have had the most top-3 finishes?\n\n\n\n\n\n\nNoteQuestion #3\n\n\n\nWhich countries have had the most top-3 finishes in the longest distance race?\nHere we see we have ranks in one dataset, and race details in another! How do we do this now? We have to join the two data frames into one data frame, using a common variable that uniquely identifies observations in both datasets.\n\n\n R\n web-r\n\n\n\nShow the Codelongest_races &lt;- race_df %&gt;%\n  slice_max(n = 5, order_by = distance) %&gt;% # Longest distance races\n  select(race_year_id, country, distance) # Select only relevant columns)\nlongest_races\n### Now join this with the `rank_df` dataset\nlongest_races %&gt;%\n  left_join(., rank_df, by = \"race_year_id\") %&gt;% # total participants in longest 4 races\n  filter(rank %in% c(1:10)) %&gt;% # Top 10 ranks\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWow….France has one the top 10 positions 26 times in the longest races… which take place in France, Thailand, Chad, Australia, and Portugal. So although the USA has the greatest number of top 10 finishes, when it comes to the longest races, it is 🇫🇷 vive la France!\n\n\n\n Question-4: What is the distribution of the finishing times?\n\n\n\n\n\n\nNoteQuestion #4\n\n\n\nWhat is the distribution of the finishing times, across all races and all ranks?\n\n\n R\n web-r\n\n\n\n\n\n\nrank_df %&gt;%\n  gf_histogram(~time_in_seconds, bins = 75) %&gt;%\n  gf_labs(title = \"Histogram of Race Times\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSo the distribution is (very) roughly bell-shaped, spread over a 2X range. And some people may have dropped out of the race very early and hence we have a small bump close to zero time! The histogram shows three bumps…at least one reason is that the distances to be covered are not the same…but could there be other reasons? Like altitude_gained for example?\n\n\n\n Question-5: What is the distribution of race distances?\n\n\n\n\n\n\nNoteQuestion #5\n\n\n\nWhat is the distribution of race distances?\n\n\n R\n web-r\n\n\n\n\n\n\nrace_df %&gt;%\n  gf_histogram(~distance, bins = 50) %&gt;%\n  gf_labs(title = \"Histogram of Race Distances\")\n\n\n\n\n\n\n\n\n\n\n\n\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nrace_df %&gt;%\n  filter(distance == 0)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCurious…some of these zero-distance races have had participants too! Perhaps these were cancelled events…all of them are stated to be 100 mile events…\n\n\n\n Question-6: What is the distribution of finishing times for race distance around 150?\n\n\n\n\n\n\nNoteQuestion #6\n\n\n\nFor all races that have a distance around 150, what is the distribution of finishing times? Can these be split/facetted using start_time of the race (i.e. morning / evening) ?\n\n\n R\n web-r\n\n\n\nLet’s make a count of start times:\n\nrace_times &lt;- race_df %&gt;%\n  count(start_time) %&gt;%\n  arrange(desc(n))\nrace_times\n\n\n  \n\n\n\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\n# Demo purposes only!\n\nrace_start_factor &lt;- race_df %&gt;%\n  filter(distance == 0) %&gt;% # Races that actually took place\n  mutate(\n    start_day_time =\n      case_when(\n        start_time &gt; hms(\"02:00:00\") &\n          start_time &lt;= hms(\"06:00:00\") ~ \"early_morning\",\n        start_time &gt; hms(\"06:00:01\") &\n          start_time &lt;= hms(\"10:00:00\") ~ \"late_morning\",\n        start_time &gt; hms(\"10:00:01\") &\n          start_time &lt;= hms(\"14:00:00\") ~ \"mid_day\",\n        start_time &gt; hms(\"14:00:01\") &\n          start_time &lt;= hms(\"18:00:00\") ~ \"afternoon\",\n        start_time &gt; hms(\"18:00:01\") &\n          start_time &lt;= hms(\"22:00:00\") ~ \"evening\",\n        start_time &gt; hms(\"22:00:01\") &\n          start_time &lt;= hms(\"23:59:59\") ~ \"night\",\n        start_time &gt;= hms(\"00:00:00\") &\n          start_time &lt;= hms(\"02:00:00\") ~ \"postmidnight\",\n        .default = \"other\"\n      )\n  ) %&gt;%\n  mutate(\n    start_day_time =\n      as_factor(start_day_time) %&gt;%\n        fct_collapse(\n          .f = .,\n          night = c(\"night\", \"postmidnight\")\n        )\n  )\n##\n# Join with rank_df\nrace_start_factor %&gt;%\n  left_join(rank_df, by = \"race_year_id\") %&gt;%\n  drop_na(time_in_seconds) %&gt;%\n  gf_histogram(\n    ~time_in_seconds,\n    bins = 75,\n    fill = ~start_day_time,\n    color = ~start_day_time,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(start_day_time), ncol = 2, scales = \"free_y\") %&gt;%\n  gf_labs(title = \"Race Times by Start-Time\")\n\n\n\n\n\n\n\n\n\nLet’s make a count of start times:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that finish times tend to be longer for afternoon and evening start races; these are lower for early morning and night time starts. Mid-day starts show a curious double hump in finish times that should be studied.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#distributions-and-densities-in-the-wild",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#distributions-and-densities-in-the-wild",
    "title": "\n Quantities",
    "section": "\n Distributions and Densities in the Wild",
    "text": "Distributions and Densities in the Wild\nBefore we conclude, let us look at a real world dataset: populations of countries. This dataset was taken from Kaggle https://www.kaggle.com/datasets/ulrikthygepedersen/populations. Click on the icon below to save the file into a subfolder called data in your project folder:\n Download the Populations data \npop &lt;- read_csv(\"data/populations.csv\")\npop\ninspect(pop)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n          name     class levels     n missing\n1 country_code character    265 16400       0\n2 country_name character    265 16400       0\n                                   distribution\n1 ABW (0.4%), AFE (0.4%), AFG (0.4%) ...       \n2 Afghanistan (0.4%) ...                       \n\nquantitative variables:  \n   name   class  min       Q1  median       Q3        max         mean\n1  year numeric 1960   1975.0    1991     2006       2021 1.990529e+03\n2 value numeric 2646 986302.5 6731400 46024452 7888408686 2.140804e+08\n            sd     n missing\n1 1.789551e+01 16400       0\n2 7.040554e+08 16400       0\n\n\n\nLet us plot densities/histograms for value:\ngf_histogram(~value, data = pop, title = \"Long Tailed Histogram\")\ngf_density(~value, data = pop, title = \"Long Tailed Density\")\n\n\n\n\n\n\n\n\n\n\nThese graphs convey very little to us: the data is very heavily skewed to the right and much of the chart is empty. There are many countries with small populations and a few countries with very large populations. Such distributions are also called “long tailed” distributions. To develop better insights with this data, we should transform the variable concerned, using say a “log” transformation:\ngf_histogram(~ log10(value), data = pop, title = \"Histogram with Log transformed x-variable\")\ngf_density(~ log10(value), data = pop, title = \"Density with Log transformed x-variable\")\n\n\n\n\n\n\n\n\n\n\nBe prepared to transform your data with log or sqrt transformations when you see skewed distributions!\n\n Pareto, Power Laws, and Fat Tailed Distributions\nCity Populations, Sales across product categories, Salaries, Instagram connections, number of customers vs Companies, net worth / valuation of Companies, extreme events on stock markets….all of these could have highly skewed distributions. In such a case, the standard statistics of mean/median/sd may not convey too much information. With such distributions, one additional observation on say net worth, like say Mr Gates’, will change these measures completely. (More when we discuss Sampling)\nSince very large observations are indeed possible, if not highly probable, one needs to look at the result of such an observation and its impact on a situation rather than its (mere) probability. Classical statistical measures and analysis cannot apply with long-tailed distributions. More on this later in the Module on Statistical Inference, but for now, here is a video that talks in detail about fat-tailed distributions, and how one should use them and get used to them:\n\nSeveral distribution shapes exist, here is an illustration of the 6 most common ones:\n\n\n\n\n\n\n\nFigure 3: Types of Distributions\n\n\n\n\nWhat insights could you develop based on these distribution shapes?\n\n\nBimodal: Maybe two different systems or phenomena or regimes under which the data unfolds. Like our geyser above. Or a machine that works differently when cold and when hot. Intermittent faulty behaviour…\n\n\nComb: Some specific Observations occur predominantly, in an otherwise even spread or observations. In a survey many respondents round off numbers to nearest 100 or 1000. Check the distribution of the diamonds dataset for carat values which are suspiciously integer numbers in too many cases.\n\n\nEdge Peak: Could even be a data entry artifact!! All unknown / unrecorded observations are recorded as \\(999\\) !!🙀\n\n\nNormal: Just what it says! Course Marks in a Univ cohort…\n\n\nSkewed: Income, or friends count in a set of people. Do UI/UX peasants have more followers on Insta than say CAP people?\n\n\nUniform: The World is not flat. Anything can happen within a range. But not much happens outside! Sharp limits…",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#z-scores",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#z-scores",
    "title": "\n Quantities",
    "section": "\n Z-scores",
    "text": "Z-scores\nOften when we compute wish to compare distributions with different values for means and standard deviations, we resort to a scaling of the variables that are plotted in the respective distributions.\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.9772499\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.7475075\n\n\n\nAlthough the densities all look the same, they are are quite different! The x-axis in each case has two scales: one is the actual value of the x-variable, and the other is the z-score which is calculated as:\n\\[\nz_x = \\frac{x - \\mu_{x}}{\\sigma_x}\n\\]\nWith similar distributions (i.e. normal distributions), we see that the variation in density is the same at the same values of z-score for each variable. However since the \\(\\mu_i\\) and \\(\\sigma_i\\) are different, the absolute value of the z-score is different for each variable. In the first plot (from the top left), \\(z = 1\\) corresponds to an absolute change of \\(5\\) units; it is \\(15\\) units in the plot directly below it.\nOur comparisons are done easily when we compare differences in probabilities at identical z-scores, or differences in z-scores at identical probabilities.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#wait-but-why",
    "title": "\n Quantities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nHistograms are used to study the distribution of one or a few Quant variables.\n\nChecking the distribution of your variables one by one is probably the first task you should do when you get a new dataset.\n\nIt delivers a good quantity of information about spread, how frequent the observations are, and if there are some outlandish ones.\n\nComparing histograms side-by-side helps to provide insight about whether a Quant measurement varies with situation (a Qual variable). We will see this properly in a statistical way soon.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#conclusion",
    "title": "\n Quantities",
    "section": "\n Conclusion",
    "text": "Conclusion\nTo complicate matters: Having said all that, the histogram is really a bar chart in disguise! You probably suspect that the “bucketing” of the Quant variable is tantamount to creating a Qual variable! Each bucket is a level in this fictitious bucketed Quant variable.\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\n\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nWe can split histograms on the basis of another Qualitative variable.\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#your-turn",
    "title": "\n Quantities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nOld Faithful Data in R (Find it!)\n\n\n\n\n\n\n\nNote2. Wage and Education Data from Canada\n\n\n\n Download the Wages/Education Dataset \n\n\n\n\n\n\n\n\nNote3. Time taken to Open or Close Packages\n\n\n\nSome students/HCD peasants tested Elderly people, some with and some without hand pain, and observed how long they took to open or close typical packages for milk, cheese, bottles etc.\n Download the Package Opening Times xlsx \n Download the Package Closing Times xlsx \n\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution.\n\n\n\n\n\n\nTip\n\n\n\nNote: reading xlsx files into R may need the the readxl package. Install it!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#ai-generated-summary-and-podcast",
    "title": "\n Quantities",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis module is an excerpt from a guide on learning statistical analysis using metaphors. The text focuses on the concept of quantitative variables and how histograms can be used to visualize their distribution. The author illustrates these concepts through real-world examples using datasets such as diamond prices, ultramarathon race times, and global population figures. By analyzing these datasets with histograms, the author explores various aspects of data distributions, including skewness, bimodality, and the presence of outliers. The guide also introduces additional tools like the crosstable package and z-scores to enhance data analysis. Finally, the author encourages readers to apply these concepts to real-world datasets, developing questions and insights through the use of histograms and statistical measures.\n\nWhat patterns emerge from the distributions of quantitative variables in each dataset, and what insights can we gain about the relationships between these variables?\nHow do different qualitative variables impact the distribution of quantitative variables in the datasets, and what are the implications of these findings for understanding the underlying phenomena?\nBased on the distributions and relationships between variables, what are the most relevant questions to ask about the datasets, and what further analyses could be conducted\n\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#references",
    "title": "\n Quantities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncrosstable\n0.8.1\nChaltiel (2024)\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nChaltiel, Dan. 2024. crosstable: Crosstables for Descriptive Analyses. https://doi.org/10.32614/CRAN.package.crosstable.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html",
    "title": "\n Space",
    "section": "",
    "text": "Spatial Data\nStatic Maps\nInteractive Maps with Leaflet\nInteractive Maps with Mapview\n\n\nSpatialData \n\nStaticMaps \n Interactive Mapswith leaflet\n Interactive Maps with mapview\n\n\n\n\n\n“If we were to wake up some morning and find that everyone was the same race, creed, and color, we would find some other cause for prejudice by noon.”\n— George D. Aiken, US senator (20 Aug 1892-1984)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#slides-and-tutorials",
    "title": "\n Space",
    "section": "",
    "text": "Spatial Data\nStatic Maps\nInteractive Maps with Leaflet\nInteractive Maps with Mapview\n\n\nSpatialData \n\nStaticMaps \n Interactive Mapswith leaflet\n Interactive Maps with mapview\n\n\n\n\n\n“If we were to wake up some morning and find that everyone was the same race, creed, and color, we would find some other cause for prejudice by noon.”\n— George D. Aiken, US senator (20 Aug 1892-1984)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#setting-up-r-packages",
    "title": "\n Space",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\n# install.packages(\"remotes\")\n# remotes::install_github(\"r-tmap/tmap.mapgl\")\nlibrary(tmap.mapgl) # Free mapgl maps\nlibrary(osmdata)\nlibrary(rnaturalearth)\n## Interactive Maps\nlibrary(leaflet)\nlibrary(leaflet.providers)\nlibrary(leaflet.extras)\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-graphs-will-we-see-today",
    "title": "\n Space",
    "section": "What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQual\nChoropleth and Symbols Maps, Cartograms\n\n{{&lt; iconify gis statistic-map size=4x &gt;}}",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#inspiration",
    "title": "\n Space",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Infosys in the EU\n\n\n\n\n\n\n\n\n\n(b) Population Cartogram\n\n\n\n\n\n\nFigure 1: Choropleth and Cartogram\n\n\n\n\n\n\n\n\n\n\n\n(a) Where’s the next Houthi attack?\n\n\n\n\n\n\n\n\n\n(b) US Cities\n\n\n\n\n\n\nFigure 2: Symbol Maps",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#how-do-these-charts-work",
    "title": "\n Space",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn Figure 1 (a), we have a choropleth map. What does choropleth1 mean? And what kind of information could this map represent? The idea is to colour a specific area of the map, a district or state, based on a Quant or a Qual variable.\nThe Figure 1 (b) deliberately distorts and scales portions of the map in proportion to a Quant variable, in this case, population in 2018.\nIn Figure 2 (a) and Figure 2 (b), symbols are used to indicate either the location/presence of an item of interest, or a quantity by scaling their size in proportion to a Quant variable",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#introduction",
    "title": "\n Space",
    "section": "\n Introduction",
    "text": "Introduction\nFirst; let us watch a short, noisy video on maps:",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-kind-of-visualizations-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-kind-of-visualizations-will-we-make",
    "title": "\n Space",
    "section": "\n What kind of visualizations will we make?",
    "text": "What kind of visualizations will we make?\nLet us first understand the idea of a Geographical Information System, GIS: \n\nWe will first understand the structure of spatial data and where to find it. For now, we will deal with vector spatial data; the discussion on raster data will be dealt with in another future module.\nWe will get hands-on with making maps, both static and interactive.\n\n Choropleth Map\n\n\n\n Bubble Map\nWhat information could this map below represent?\n\n\nLet us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata and osmplotr. We will also make interactive maps with leaflet and mapview; tmap is also capable of creating interactive maps.\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#your-turn",
    "title": "\n Space",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n Animal and Bird Migration\n\nHead off to movebank.org. Look at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile). Note: You will get a .zip file with a good many files in it. Save all of them, but read only the .shp file into R.\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species….based on the data you download.\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\n\n UFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv\n\n Sales Data from kaggle\nHead off to Kaggle and search for Geographical Sales related data. Make both static and interactive maps with this data. Justify your decisions for type of map.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#references",
    "title": "\n Space",
    "section": "\n References",
    "text": "References\n\nHadley Wickham, Danielle Navarro and Thomas Lin Pedersen. ggplot2: Elegant Graphics for Data Analysis, https://ggplot2-book.org/maps.html\n\nMartijn Tennekes and Jakub Nowosad (2025). Elegant and informative maps with tmap. https://tmap.geocompx.org\n\nRobin Lovelace, Jakub Nowosad, Jannes Muenchow. Geocomputation with R. https://r.geocompx.org/\n\nEmine Fidan. Guide to Creating Interactive Maps in R, https://bookdown.org/eneminef/DRR_Bookdown/\n\nNikita Voevodin. R, Not the Best Practices, https://bookdown.org/voevodin_nv/R_Not_the_Best_Practices/maps.html\n\nWant to make a cute logo-like map? Try https://prettymapp.streamlit.app\n\nFree Map Tile services. https://alexurquhart.github.io/free-tiles/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nleaflet\n2.2.2\nCheng et al. (2024)\n\n\nosmdata\n0.2.5\nMark Padgham et al. (2017)\n\n\nrnaturalearth\n1.0.1\nMassicotte and South (2023)\n\n\nsf\n1.0.21\n\nPebesma (2018); Pebesma and Bivand (2023)\n\n\n\ntmap\n4.1\nTennekes (2018)\n\n\n\n\n\n\nCheng, Joe, Barret Schloerke, Bhaskar Karambelkar, and Yihui Xie. 2024. leaflet: Create Interactive Web Maps with the JavaScript “Leaflet” Library. https://doi.org/10.32614/CRAN.package.leaflet.\n\n\nMark Padgham, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. “Osmdata.” Journal of Open Source Software 2 (14): 305. https://doi.org/10.21105/joss.00305.\n\n\nMassicotte, Philippe, and Andy South. 2023. rnaturalearth: World Map Data from Natural Earth. https://doi.org/10.32614/CRAN.package.rnaturalearth.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#footnotes",
    "title": "\n Space",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "title": "Tutorial on Evolutions and Flow",
    "section": "",
    "text": "Tutorial Content to be written up when Arvind has time !!!\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{venkatadri,\n  author = {Venkatadri, Arvind},\n  title = {Tutorial on {Evolutions} and {Flow}},\n  url = {https://av-quarto.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVenkatadri, Arvind. n.d. “Tutorial on Evolutions and Flow.”\nhttps://av-quarto.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html",
    "title": "\n Groups",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“In keeping silent about evil, in burying it so deep within us that no sign of it appears on the surface, we are implanting it, and it will rise up a thousand fold in the future.”\n— Aleksandr Solzhenitsyn",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#slides-and-tutorials",
    "title": "\n Groups",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“In keeping silent about evil, in burying it so deep within us that no sign of it appears on the surface, we are implanting it, and it will rise up a thousand fold in the future.”\n— Aleksandr Solzhenitsyn",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#setting-up-r-packages",
    "title": "\n Groups",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\nlibrary(visStatistics) # All in one plot + stats test package\nlibrary(palmerpenguins) # Our new favourite dataset\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-graphs-will-we-see-today",
    "title": "\n Groups",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nQual\nBox Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Groups",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation\n                \n\n4\n                  What, Who, Where, Whom, Which\n                  Name, Place, Animal, Thing\n                  Qualitative/Nominal\n                  Name\n                  Count no. of cases,Mode\n                \n\n\n\n\n\n\n\n Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on uh, shades1 of meaning for Jane Austen Gen-Z phrases! Bah.\n\n How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable.\n\n\n\n\n\n\n\nFigure 2: Box Plot Illustration\n\n\n\n\nThe internal process of this plot is as follows:\n(Hat tip to student Tanya Michelle Justin for a good question on outlier calculation)\n\nMake groups of the Quant variable for each level of the Qual\nIn each group, rank the Quant variable values in increasing order\nCalculate:\n\n\nQuartiles: The values for median = Q2, Q1, and Q3 based on rank!!\n\nValues for min, max, and then IQR = Q1 - Q3\n\nCalculate outlier limits:\n\n\\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nWhiskers: All values within \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\nOutliers: All values outside of \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nPlot these as a vertical or horizontal box structure, as shown.\n\nAs a result of this, while the box-part of the boxplot always shows 2 full quartiles, the whiskers may not stretch through their quartiles, since some values may be outliers on either side.\n\n\n\n\n\n\nNoteRanks and Values\n\n\n\nThe Quant variable is ordered based on the values from min to max. So you could imagine that each value has a rank or sequence number. The min value has \\(rank = 1\\) and the max value has \\(rank = length(var)\\).\n\n\n\n\n\n\n\n\nNoteHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\nBox plots can show skew in distributions, with a large number of outliers on one side, as in Figure 3.\n\n\n\n\n\n\n\nFigure 3: Skewed Distributions and Boxplots\n\n\n\n\nIn other cases, there may be no ouliers, but the “bottom” and the “lid” of the box may not be the same size!\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 4: Box Plot Discussions\n\n\nIn the Figure 4, we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 4 (b).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#inspiration",
    "title": "\n Groups",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on uh, shades1 of meaning for Jane Austen Gen-Z phrases! Bah.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#how-do-these-charts-work",
    "title": "\n Groups",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable.\n\n\n\n\n\n\n\nFigure 2: Box Plot Illustration\n\n\n\n\nThe internal process of this plot is as follows:\n(Hat tip to student Tanya Michelle Justin for a good question on outlier calculation)\n\nMake groups of the Quant variable for each level of the Qual\nIn each group, rank the Quant variable values in increasing order\nCalculate:\n\n\nQuartiles: The values for median = Q2, Q1, and Q3 based on rank!!\n\nValues for min, max, and then IQR = Q1 - Q3\n\nCalculate outlier limits:\n\n\\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nWhiskers: All values within \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\nOutliers: All values outside of \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nPlot these as a vertical or horizontal box structure, as shown.\n\nAs a result of this, while the box-part of the boxplot always shows 2 full quartiles, the whiskers may not stretch through their quartiles, since some values may be outliers on either side.\n\n\n\n\n\n\nNoteRanks and Values\n\n\n\nThe Quant variable is ordered based on the values from min to max. So you could imagine that each value has a rank or sequence number. The min value has \\(rank = 1\\) and the max value has \\(rank = length(var)\\).\n\n\n\n\n\n\n\n\nNoteHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\nBox plots can show skew in distributions, with a large number of outliers on one side, as in Figure 3.\n\n\n\n\n\n\n\nFigure 3: Skewed Distributions and Boxplots\n\n\n\n\nIn other cases, there may be no ouliers, but the “bottom” and the “lid” of the box may not be the same size!\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 4: Box Plot Discussions\n\n\nIn the Figure 4, we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 4 (b).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#case-study-1-gss_wages-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#case-study-1-gss_wages-dataset",
    "title": "\n Groups",
    "section": "\n Case Study-1: gss_wages dataset",
    "text": "Case Study-1: gss_wages dataset\nWe will first look at Wage data from the General Social Survey (1974-2018) conducted in the USA, which is used to illustrate wage discrepancies by gender (while also considering respondent occupation, age, and education). This is available on Vincent Arel-Bundock’s superb repository of datasets. Let us read into R directly from the website.\n\n R\n\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\n\n\nskim(wages)\n\n\nData summary\n\n\nName\nwages\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\nFrom the dataset documentation page, we note that this is a large dataset (61K rows), with 11 variables:\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nyear(dbl): the survey year\n\nrealrinc(dbl): the respondent’s base income (in constant 1986 USD\n\nage(dbl): the respondent’s age in years\n\nocc10(dbl): respondent’s occupation code (2010)\n\nprestg10(dbl): respondent’s occupational prestige score (2010)\n\nchilds(dbl): number of children (0-8)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\noccrecode(chr): recode of the occupation code into one of 11 main categories\n\n\nwrkstat(chr): the work status of the respondent (full-time, part-time, temporarily not working, unemployed (laid off), retired, school, housekeeper, other). 8 levels. \n\ngender(chr): respondent’s gender (male or female). 2 levels.\n\n\neduccat(chr): respondent’s degree level (Less Than High School, High School, Junior College, Bachelor, or Graduate). 5 levels.\n\n\nmaritalcat(chr): respondent’s marital status (Married, Widowed, Divorced, Separated, Never Married). 5 levels.\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights based on wages dataset\n\n\n\n\nFair amount of missing data; however with 61K rows, we can for the present simply neglect the missing data.\nGood mix of Qual and Quant variables\n\n\n\n\n Hypothesis and Research Questions\n\nThe target variable for an experiment that resulted in this data might be the realinc variable, the resultant income of the individual. Which is numerical variable.\n\n\n\n\n\n\n\nNoteResearch Questions:\n\n\n\n\nWhat is the basic distribution of realrinc?\nIs realrinc affected by gender?\nBy educcat? By maritalcat?\nIs realrinc affected by child?\nDo combinations of these factors have an effect on the target variable?\n\nThese should do for now! But we should make more questions when have seen some plots!\n\n\n\n Data Munging\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This is not advised at all as a general procedure!! Data is valuable and there are better ways to manage this problem!\n\n\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#plotting-box-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#plotting-box-plots",
    "title": "\n Groups",
    "section": "\n Plotting Box Plots",
    "text": "Plotting Box Plots\n\n\n Question-1: What is the basic distribution of realrinc?\n\n\n\n\n\n\nNoteQuestion-1: What is the basic distribution of realrinc?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% # Dummy X-axis \"variable\"\n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = realrinc, x = \"Income\")) + # Dummy X-axis \"variable\"\n  labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nIncome is a very skewed distribution, as might be expected.\nPresence of many higher-side outliers is noted.\n\n\n\n\n Question-2: Is realrinc affected by gender?\n\n\n\n\n\n\nNoteQuestion-2: Is realrinc affected by gender?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\nSplit by Gender\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 2B: Log(Income) by Gender\")\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10(), scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\nWith log scale\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = realrinc)) +\n  labs(title = \"Plot 2A: Income by Gender\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = log10(realrinc))) +\n  labs(title = \"Plot 2B: Log(Income) by Gender\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = realrinc, fill = gender)) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\n\n(a) Split by Gender\n\n\n\n\n\n\n\n\n\n(b) With log income\n\n\n\n\n\n\n\n\n\n(c) With log scale\n\n\n\n\n\n\nFigure 5: Income by Gender\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\nEven when split by gender, realincome presents a skewed set of distributions.\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men.\nlog10 transformation helps to view and understand the regions of low realrinc.\nThere are outliers on both sides, indicating that there may be many people who make very small amounts of money and large amounts of money in both genders.\n\n\n\n\n Question-3: Is realrinc affected by educcat?\n\n\n\n\n\n\nNoteQuestion-3: Is realrinc affected by educcat?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\nSplit by Education Category\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Log(Income) by Education Category, sorted\",\n    x = \"Log Income\",\n    y = \"Education Category\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\",\n    x = \"Income\",\n    y = \"Education Category\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\nLog Income Scale\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(realrinc, educcat)) + # (x,y) format\n  labs(title = \"Plot 3A: Income by Education Category\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(log10(realrinc), educcat)) +\n  labs(title = \"Plot 3B: Log(Income) by Education Category\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log(realrinc),\n      reorder(educcat, realrinc, FUN = median),\n      fill = educcat\n    ),\n    alpha = 0.5\n  ) +\n  labs(\n    title = \"Plot 3C: Log(Income) by Education Category, sorted\",\n    x = \"Log Income\", y = \"Education Category\"\n  )\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(realrinc,\n      reorder(educcat, realrinc, FUN = median),\n      fill = educcat\n    ),\n    alpha = 0.5\n  ) +\n  scale_x_log10() +\n  labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income Scale\",\n    x = \"Income\", y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\n\n(a) Split by Education Category\n\n\n\n\n\n\n\n\n\n(b) With log income\n\n\n\n\n\n\n\n\n\n\n\n(c) With log scale\n\n\n\n\n\n\n\n\n\n(d) Split by Education Category\n\n\n\n\n\n\nFigure 6: Income by Education Category\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\n\nrealrinc rises with educcat, which is to be expected.\nHowever, there are people with very low and very high income in all categories of educcat\n\nHence educcat alone may not be a good predictor for realrinc.\n\n\n\nWe can do similar work with the other Qual variables. Let us now see how we can use more than one Qual variable and answer the last hypothesis, Question 4.\n\n Question-4: Is the target variable realrinc affected by combinations of Qual factors gender, educcat, maritalcat and childs?\n\n\n\n\n\n\nImportant\n\n\n\nThis is a rather complex question and could take us deep into Modelling. Ideally we ought to:\n\ntake each Qual variable, explain its effect on the target variable\n\n\nremove that effect and model the remainder ( i.e. residual) with the next Qual variable\n\nProceed in this way until we have a good model.\nif we are going to do this manually.\n\nThere are more modern Modelling Workflows, that can do things much faster and without such manual tweaking.\n\n\nSo will simply plot box plots showing effects on the target variable of combinations of Qual variables taken two at a time. (We will of course use facetted box plots!)\nWe will also drop NA values all around this time, to avoid seeing boxplots for undocumented categories.\n\n\n\n\n\n\nNoteQuestion-4: Is realrinc affected by combinations of factors?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nFigure 7: Split by Education Category and Family Size\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nFigure 8: Split by Gender and Family Size\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log10(realrinc), reorder(educcat, realrinc),\n      fill = educcat\n    ), # aes() closes here\n    alpha = 0.5\n  ) +\n  facet_wrap(vars(childs)) +\n  scale_fill_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(title = \"Plot 4A: Log Income by Education Category and Family Size\", x = \"Log income\", y = \"No. of Children\")\n##\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log10(realrinc), childs,\n      group = childs,\n      fill = childs\n    ), # aes() closes here\n    alpha = 0.5\n  ) +\n  facet_wrap(vars(gender)) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\n(a) Split by Education Category and Family Size\n\n\n\n\n\n\n\n\n\n(b) Split by Gender and Family Size\n\n\n\n\n\n\nFigure 9: Income and Other Qual Variables\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-4\n\nFrom Figure 7, we see that realrinc increases with educcat, across (almost) all family sizes childs.\nHowever, this trend breaks a little when family sizes childs is large, say &gt;= 7. Be aware that the data observations for such large families may be sparse and this inference may not be necessarily valid.\nFrom Figure 8, we see that the effect of childs on realrinc is different for each gender! For females, the income steadily drops with the number of children, whereas for males it actually increases up to a certain family size before decreasing again.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#are-the-differences-significant",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#are-the-differences-significant",
    "title": "\n Groups",
    "section": "\n Are the Differences Significant?",
    "text": "Are the Differences Significant?\n\n\n\n\n\n\nImportantHunches and Hypotheses\n\n\n\nIn data analysis, we always want to know2, as in life, how important things are, whether they matter. To do this, we make up hunches, or more precisely, Hypotheses. We make two in fact:\n\\(H_0\\): Nothing is happening;\\(H_a\\): (“a” for Alternate): Something is happening and it is important enough to pay attention to.\nWe then pretend that \\(H_0\\) is true and ask that our data prove us wrong; if it does, we reject \\(H_0\\) in favour of \\(H_a\\).\nThis is a very important idea of Hypothesis Testing which helps you justify your hunch. We will study this when we do Stats Tests for differences between two means(t-tests), and those between more than two means(ANOVA).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#wait-but-why",
    "title": "\n Groups",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots are a powerful statistical graphic that give us a combined view of data ranges, quartiles, medians, and outliers.\nBox plots can compare groups within our Quant variable, based on levels of a Qual variable. This is a very common and important task in research!\nIn your design research, you would have numerical Quant data that is accompanied by categorical Qual data pertaining to groups within your target audience.\nAnalyzing for differences in the Quant across levels of the Qual (e.g household expenditure across groups of people) is a vital step in justifying time, effort, and money for further actions in your project. Don’t faff this.\n\nBox plots are ideal for visualizing statistical tests for difference in mean values across groups (t-test and ANOVA). (Even though they plot medians)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#conclusion",
    "title": "\n Groups",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nBox Plots “dwell upon” medians and Quartiles\n\nBox Plots can show distributions of a Quant variable over levels of a Qual variable\nThis allows a comparison of box plots side by side to visibly detect differences in medians and IQRs across such levels.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#your-turn",
    "title": "\n Groups",
    "section": "\n Your Turn",
    "text": "Your Turn\nHere are a couple of datasets that you might want to analyze with box plots:\n\n\n\n\n\n\nNoteInsurance Data\n\n\n\n Download the Insurance data \n\n\n\n\n\n\n\n\nNotePolitical Donations\n\n\n\n Download the Donations data \n\n\n\n\n\n\n\n\nNoteUFO Encounters\n\n\n\n\n\n UFO Sighting data\n\n\nThe data dictionary for this dataset is here at the TidyTuesday Website.. The TidyTuesday Website is a treasure trove of interesting datasets!\n\n\n\n\n\n\n\n\nNoteGPT-based Language detectors are biased against non-native English writers.\n\n\n\n\n\n AI Dectector data\n\n\nWhat story can you tell, and deduction can you make from Figure 10 below? How would you replicate it? What would you add?\n\n\n\n\n\nFigure 10: AI Detectors",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#ai-generated-summary-and-podcast",
    "title": "\n Groups",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis excerpt from “Groups – Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors” provides a comprehensive guide to understanding and utilizing box plots for data visualization and analysis. The text explores the purpose, functionality, and application of box plots within the context of exploring relationships between quantitative and qualitative variables. The author illustrates these concepts using a case study of the “gss_wages” dataset, examining wage discrepancies by gender, occupation, age, and education. Through this analysis, the author highlights the effectiveness of box plots in visualizing distributions, identifying outliers, and comparing groups, providing valuable insights into the complexities of data. The text concludes with a call to action, encouraging readers to explore real-world datasets and apply these techniques to uncover hidden trends and patterns within data.\n\nWhat are the relationships between qualitative and quantitative variables in the gss_wages dataset?\nHow do box plots help visualize and understand the distribution of income across different groups?\nWhat insights can be gained by analyzing the impact of multiple qualitative factors on income distribution?\n\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#references",
    "title": "\n Groups",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nBevans, R. (2023, June 22). An Introduction to t Tests | Definitions, Formula and Examples. Scribbr. https://www.scribbr.com/statistics/t-test/\n\nBrown, Angus. (2008). The Strange Origins of the t-test. Physiology News | No. 71 | Summer 2008| https://static.physoc.org/app/uploads/2019/03/22194755/71-a.pdf\n\nStephen T. Ziliak.(2008). Guinnessometrics: The Economic Foundation of “Student’s” t. Journal of Economic Perspectives—Volume 22, Number 4—Fall 2008—Pages 199–216. https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.22.4.199\n\nhttps://quillette.com/2024/08/03/xy-athletes-in-womens-olympic-boxing-paris-2024-controversy-explained-khelif-yu-ting/\nSenefeld JW, Lambelet Coleman D, Johnson PW, Carter RE, Clayburn AJ, Joyner MJ. Divergence in Timing and Magnitude of Testosterone Levels Between Male and Female Youths. JAMA. 2020;324(1):99–101. doi:10.1001/jama.2020.5655. https://jamanetwork.com/journals/jama/fullarticle/2767852\n\nDoriane Lambelet Coleman.(2017) Sex in Sport, 80 Law and Contemporary Problems. Available at: https://scholarship.law.duke.edu/lcp/vol80/iss4/5\n\nDistributome - An Interactive Web-based Resource for Probability Distributions https://distributome.org\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nsn\n2.1.1\nAzzalini (2023)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.1\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisStatistics\n0.1.7\nSchilling (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nAzzalini, Azzalini A. 2023. The R Package sn: The Skew-Normal and Related Distributions Such as the Skew-\\(t\\) and the SUN (Version 2.1.1). Università degli Studi di Padova, Italia. https://cran.r-project.org/package=sn.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSchilling, Sabine. 2025. visStatistics: Automated Selection and Visualisation of Statistical Hypothesis Tests. https://doi.org/10.32614/CRAN.package.visStatistics.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#footnotes",
    "title": "\n Groups",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe term throwing a shade can be found in Jane Austen’s novel Mansfield Park (1814). Young Edmund Bertram is displeased with a dinner guest’s disparagement of the uncle who took her in: “With such warm feelings and lively spirits it must be difficult to do justice to her affection for Mrs. Crawford, without throwing a shade on the Admiral.”↩︎\n“Ah, Misha, he has a stormy spirit. His mind is in bondage. He is haunted by a great, unsolved doubt. He is one of those who don’t want millions, but an answer to their questions.” ― Fyodor Dostoevsky, The Brothers Karamazov: A Novel in Four Parts With Epilogue↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "title": "\n Surveys",
    "section": "",
    "text": "Figure 1: Rural Survey",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "title": "\n Surveys",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(resampledata) # More datasets\n\nlibrary(ggstats) # Likert Scale Plots\nlibrary(labelled) # Creating Labelled Data for Likert Plots\nlibrary(sjPlot) # Another package for Likert Plots\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(tinytable) # Elegant Tables for our data\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\n#\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(tidyverse)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#inspiration",
    "title": "\n Surveys",
    "section": "\n Inspiration",
    "text": "Inspiration\nTo be Found !!\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-graphs-will-we-see-today",
    "title": "\n Surveys",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\n“Chart Shape”\n\n\nQual\nQual\nLikert Plots",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Surveys",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#how-do-these-charts-work",
    "title": "\n Surveys",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn many business and design situations, we perform say customer surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. These are then plotted in a chart to get a distribution of opinions for each question in the survey. Some examples of Likert Scales are shown below.\n\n\n\n\n\nFigure 2: Likert Scale Questionnaire Samples\n\n\nAs seen, we can use Likert Scale based questionnaire for a variety of aspects in our survey instruments.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "title": "\n Surveys",
    "section": "\n Plots for Survey Data",
    "text": "Plots for Survey Data\nHow does this data look like, and how does one plot it? Let us consider a fictitious example, followed by a real world dataset.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "title": "\n Surveys",
    "section": "\n Case Study-1: A fictitious app Survey dataset",
    "text": "Case Study-1: A fictitious app Survey dataset\n\n\n\n\n\n\nNoteA fictitious QuickEZ app\n\n\n\nWe are a start-up that has an app called QuickEZ for delivery of groceries. We conduct a survey of 200 people at a local store, with the following questions,\n\n“Have your heard of the QuickEZ app?”\n“Do you use the QuickEZ app?”\n“Do you find it easy to use the QuickEZ app?”\n“Will you continue to use the QuickEZ app?”\n\nwhere each questions is to be answered on a scale of : “always”, “often”, “sometimes”,“rarely”, “never”.\n\n\nSuch data may look for example as follows:\n\n\n\n\nFirst 10 Responses\n\nq1\nq2\nq3\nq4\n\n\n\nnever\nrarely\nalways\nalways\n\n\nnever\nalways\nalways\nnever\n\n\noften\nrarely\nnever\nrarely\n\n\nnever\nalways\nalways\nnever\n\n\nalways\nnever\noften\nalways\n\n\nrarely\nalways\nalways\nalways\n\n\nalways\nrarely\nrarely\nrarely\n\n\noften\nalways\nalways\nnever\n\n\nalways\nnever\nalways\nalways\n\n\nalways\nnever\nalways\nalways\n\n\n\n\n\n \n\n\ntibble [200 × 4] (S3: tbl_df/tbl/data.frame)\n $ q1: Factor w/ 4 levels \"never\",\"rarely\",..: 1 1 3 1 4 2 4 3 4 4 ...\n  ..- attr(*, \"label\")= chr \"Have your heard of the QuickEZ app?\"\n $ q2: Factor w/ 4 levels \"never\",\"rarely\",..: 2 4 2 4 1 4 2 4 1 1 ...\n  ..- attr(*, \"label\")= chr \"Do you use the QuickEZ app?\"\n $ q3: Factor w/ 4 levels \"never\",\"rarely\",..: 4 4 1 4 3 4 2 4 4 4 ...\n  ..- attr(*, \"label\")= chr \"Do you find it easy to use the QuickEZ app?\"\n $ q4: Factor w/ 4 levels \"never\",\"rarely\",..: 4 1 2 1 4 4 2 1 4 4 ...\n  ..- attr(*, \"label\")= chr \"Will you continue to use the QuickEZ app?\"\n\n\n\nThe columns here correspond to the 4 questions (q1-q4) and the rows contain the 200 responses, which have been coded as (1:4). Such data is also a form of Categorical data and we need to count and plot counts for each of the survey questions. Such a plot is called a Likert plot and it looks like this:\n\n\n\n\n\n\n\n\nBased on this chart, since it looks like about 40% the survey respondents have not heard of our app, we need more publicity, and many do not find it easy to use 😿, so we have serious re-design and user testing to do !! But at least those who have managed to get past the hurdles are stating they will continue to use the app, so it does the job, but we can make it easier to use.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "title": "\n Surveys",
    "section": "\n Case Study-2: EUROFAM Survey dataset",
    "text": "Case Study-2: EUROFAM Survey dataset\nHere is another example of Likert data from the healthcare industry.\nefc is a German data set from a European study titled EUROFAM study, on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e. caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. The research questions in this EUROFAM study were:\n\n\nTo what extent do family carers of older people use support services or receive financial allowances across Europe? What kind of supports and allowances do they mainly use?\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\nIn order to improve support provision, what can be understood about the service characteristics considered crucial by carers, and how far are these needs met? and,\nWhich channels or actors can provide the greatest help in underpinning future policy efforts to improve access to services/supports?\n\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n\ndata(efc, package = \"sjPlot\")\nglimpse(efc)\n\nRows: 908\nColumns: 26\n$ c12hour  &lt;dbl&gt; 16, 148, 70, 168, 168, 16, 161, 110, 28, 40, 100, 25, 25, 24,…\n$ e15relat &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 1, 4, 2, 2, 1, 8, 2, 1, 2, 2, 1, 1, 2, 1, 2…\n$ e16sex   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2…\n$ e17age   &lt;dbl&gt; 83, 88, 82, 67, 84, 85, 74, 87, 79, 83, 68, 97, 80, 75, 82, 8…\n$ e42dep   &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 3, 1, 3, 3, 4, 4…\n$ c82cop1  &lt;dbl&gt; 3, 3, 2, 4, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 4, 3, 3, 3…\n$ c83cop2  &lt;dbl&gt; 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2…\n$ c84cop3  &lt;dbl&gt; 2, 3, 1, 3, 1, 3, 4, 2, 3, 1, 4, 3, 2, 4, 3, 1, 1, 1, 1, 4, 3…\n$ c85cop4  &lt;dbl&gt; 2, 3, 4, 1, 2, 3, 1, 1, 2, 2, 4, 1, 2, 4, 3, 3, 2, 2, 2, 2, 2…\n$ c86cop5  &lt;dbl&gt; 1, 4, 1, 1, 2, 3, 1, 1, 2, 1, 4, 3, 2, 1, 2, 3, 1, 1, 2, 1, 1…\n$ c87cop6  &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1…\n$ c88cop7  &lt;dbl&gt; 2, 3, 1, 1, 1, 2, 4, 2, 3, 1, 4, 4, 2, 2, 1, 2, 2, 2, 3, 3, 1…\n$ c89cop8  &lt;dbl&gt; 3, 2, 4, 2, 4, 1, 1, 3, 1, 1, 1, 3, 4, 4, 1, 1, 4, 3, 1, 2, 1…\n$ c90cop9  &lt;dbl&gt; 3, 2, 3, 4, 4, 1, 4, 3, 3, 3, 1, 1, 4, 4, 1, 3, 4, 3, 4, 2, 3…\n$ c160age  &lt;dbl&gt; 56, 54, 80, 69, 47, 56, 61, 67, 59, 49, 66, 47, 58, 75, 49, 5…\n$ c161sex  &lt;dbl&gt; 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2…\n$ c172code &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, NA, 2, 2, 2, 3, 1, 3, 2, 2, 2, 3, 3, …\n$ c175empl &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ barthtot &lt;dbl&gt; 75, 75, 35, 0, 25, 60, 5, 35, 15, 0, 25, 85, 15, 70, NA, 0, 9…\n$ neg_c_7  &lt;dbl&gt; 12, 20, 11, 10, 12, 19, 15, 11, 15, 10, 28, 18, 13, 18, 16, 1…\n$ pos_v_4  &lt;dbl&gt; 12, 11, 13, 15, 15, 9, 13, 14, 13, 13, 9, 8, 14, 14, 9, 14, 1…\n$ quol_5   &lt;dbl&gt; 14, 10, 7, 12, 19, 8, 20, 20, 8, 15, 1, 19, 12, 8, 8, 6, 16, …\n$ resttotn &lt;dbl&gt; 0, 4, 0, 2, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 3, 0, 0, 0, 3, 0, 1…\n$ tot_sc_e &lt;dbl&gt; 4, 0, 1, 0, 1, 3, 0, 1, 2, 1, 1, 1, 3, 0, 3, 2, 2, 0, 1, 7, 1…\n$ n4pstu   &lt;dbl&gt; 0, 0, 2, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 1, 0, 0, 0, 2, 1, 2, 4…\n$ nur_pst  &lt;dbl&gt; NA, NA, 2, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 1, NA, NA, NA, 2, 1,…\n\n\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  head(20)\n##\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  str()\n\n\n\n\n  \n\n\n\n\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. The labels are actually ( and perhaps usually ) the questions in the survey.\nWe can plot this data using the gglikert function from package ggstats:\n\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  gglikert(labels_size = 3, width = 0.75) +\n  labs(title = \"Caregiver Survey from EUROFAM\") +\n  scale_fill_brewer(\n    name = \"Responses\",\n    labels = c(\"never\", \"sometimes\", \"often\", \"always\"),\n    palette = \"Set3\", direction = -1\n  ) +\n  theme(legend.position = \"bottom\") + theme_custom()\n\n\n\n\n\n\n\nMany questions here have strong negative responses. This may indicate that policy and publicity related efforts may be required.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "title": "\n Surveys",
    "section": "\n Labelled Data",
    "text": "Labelled Data\nNote how the y-axis has been populated with Survey Questions: this is an example of a labelled dataset, where not only do the variables have names i.e. column names, but also have longish text labels that add information to the data variables. The data values ( i.e scores) in the columns is also labelled as per the the Likert scale (Like/Dislike/Strongly Dislike OR never/sometimes/often/always) etc. These Likert scores are usually a set of contiguous integers.\n\n\n\n\n\n\nNoteVariable Labels and Value Labels\n\n\n\nVariable label is human readable description of the variable. R supports rather long variable names and these names can contain even spaces and punctuation but short variables names make coding easier. Variable label can give a nice, long description of variable. With this description it is easier to remember what those variable names refer to.\nValue labels are similar to variable labels, but value labels are descriptions of the values a variable can take. Labeling values means we don’t have to remember if 1=Extremely poor and 7=Excellent or vice-versa. We can easily get dataset description and variables summary with info function.\n\n\nLet us manually create one such dataset, since this is a common-enough situation1 that we have survey data and then have to label the variables and the values before plotting. We will use the R package labelled to label our data.2.\nIt is also possible to label the tibble, the columns, and the values in similar fashion using the sjlabelled package3 and the labelr package4.\n\nset.seed(42)\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nvariable_labels &lt;- c(\n  \"Do you practice Analytics?\",\n  \"Do you code in R?\",\n  \"Have you published your R Code?\",\n  \"Do you use Quarto as your Workflow in R?\",\n  \"Will you use R at Work?\"\n)\n##\nvalue_labels &lt;- c(\"never\", \"sometimes\", \"often\", \"always\")\n##\nmy_survey_data &lt;-\n  # Create toy survey data\n  # 200 responses to 5 questions\n  # responses on Likert Scale\n  # 1:4 = \"never\", \"sometimes\",\"often\",\"always\")\n\n  tibble(\n    q1 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.2, 0.2, 0.5, 0.1)\n    ),\n    q2 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.3, 0.3, 0.3, 0.1)\n    ),\n    q3 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.2, 0.1, 0.1, 0.6)\n    ),\n    q4 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.4, 0.2, 0.1, 0.3)\n    ),\n    q5 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.1, 0.2, 0.5, 0.2)\n    )\n  ) %&gt;%\n  # Set VARIABLE labels\n  labelled::set_variable_labels(\n    .data = .,\n    q1 = variable_labels[1],\n    q2 = variable_labels[2],\n    q3 = variable_labels[3],\n    q4 = variable_labels[4],\n    q5 = variable_labels[5]\n  )\n# Values within the variables are already labelled\n###\nhead(my_survey_data, 6)\n\n\n  \n\n\n###\nstr(my_survey_data)\n\ntibble [200 × 5] (S3: tbl_df/tbl/data.frame)\n $ q1: chr [1:200] \"always\" \"always\" \"often\" \"sometimes\" ...\n  ..- attr(*, \"label\")= chr \"Do you practice Analytics?\"\n $ q2: chr [1:200] \"sometimes\" \"often\" \"sometimes\" \"often\" ...\n  ..- attr(*, \"label\")= chr \"Do you code in R?\"\n $ q3: chr [1:200] \"always\" \"always\" \"never\" \"always\" ...\n  ..- attr(*, \"label\")= chr \"Have you published your R Code?\"\n $ q4: chr [1:200] \"always\" \"never\" \"sometimes\" \"never\" ...\n  ..- attr(*, \"label\")= chr \"Do you use Quarto as your Workflow in R?\"\n $ q5: chr [1:200] \"never\" \"always\" \"often\" \"sometimes\" ...\n  ..- attr(*, \"label\")= chr \"Will you use R at Work?\"\n\n##\nmy_survey_data %&gt;%\n  gglikert(labels_size = 3, width = 0.5) +\n  labs(\n    title = \"Do you use R Survey\",\n    subtitle = \"Creating and Using Labelled Data\",\n    caption = \"Using gglikert from ggstats package\"\n  ) +\n  scale_fill_brewer(\n    palette = \"Spectral\",\n    name = \"Responses\",\n    labels = c(\"never\", \"sometimes\", \"often\", \"always\"),\n  ) +\n  geom_vline(xintercept = 0) +\n  theme_custom() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIt seems many people in the survey plan to use R at work!! And have published R code as well. But Quarto seems to have mixed results! But of course this is a toy dataset!!\nSo there we are with Survey data analysis and plots!\nThere are a few other plots with this type of data, which are useful in very specialized circumstances. One example of this is the agreement plot5 which captures the agreement between two (sets) of evaluators, on ratings given on a shared ordinal scale to a set of items. An example from the field of medical diagnosis is the opinions of two specialists on a common set of patients. However, that is for a more advanced course!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#wait-but-why",
    "title": "\n Surveys",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nLikert plots are like stacked bar-charts aligned horizontally, back to back . They are useful to indicate aspects like opinion, belief, and habits.\nThe scale for Likert data is ordinal: it should not be assumed that the points on the Likert scale (“never”, “sometimes”, “often”, “always”) are separated by the same distance.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "title": "\n Surveys",
    "section": "\n Conclusion",
    "text": "Conclusion\nLikert Plots for Survey data are not too different from Bar Plots; we can view the Likert Charts as a set of stacked bar charts, based on Likert-scale response counts. At a pinch we can make a Likert Plot with vanilla bar graphs, but the elegance and power of the ggstat package is undeniable. The packages sjPlot and sjlabelled also feature a plot_likert graphing function which is very elegant too.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "title": "\n Surveys",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").\nIncluding Edible Insects in our Diet!\n\n Download the Edible Insects Dataset \nThere are several questions here for each “area” of preference for edible insects: experience, fear, concern for the environment, etc. Take all the columns marked as average as your data for your Likert Plot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "title": "\n Surveys",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nShelomi. (2022). Dataset for: Factors Affecting Willingness and Future Intention to Eat Insects in Students of an Edible Insect Course. [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7379294\n\nWhat is a Likert Scale? https://www.surveymonkey.com/mp/likert-scale/\n\nRickards, G., Magee, C., & Artino, A. R., Jr (2012). You Can’t Fix by Analysis What You’ve Spoiled by Design: Developing Survey Instruments and Collecting Validity Evidence. Journal of graduate medical education, 4(4), 407–410. https://doi.org/10.4300/JGME-D-12-00239.1\n\nJamieson, S. (2004). Likert scales: how to (ab)use them. Medical Education, 38(12), 1217–1218. https://doi:10.1111/j.1365-2929.2004.02012.x \nMark Bounthavong. (May 16, 2019). Communicating data effectively with data visualization – Part 15 (Diverging Stacked Bar Chart for Likert scales). https://mbounthavong.com/blog/2019/5/16/communicating-data-effectively-with-data-visualization-part-15-divergent-stacked-bar-chart-for-likert-scales\n\nAnthony R. Artino Jr., Jeffrey S. La Rochelle, Kent J. Dezee & Hunter Gehlbach (2014). Developing questionnaires for educational research: AMEE Guide. No. 87, Medical Teacher, 36:6, 463-474, DOI:10.3109/0142159X.2014.889814 To link to this article: https://doi.org/10.3109/0142159X.2014.889814\n\nNaomi B. Robbins, Richard M. Heiberger. Plotting Likert and Other Rating Scales. Section on Survey Research Methods – JSM 2011. PDF Available here",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#additional-readings",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#additional-readings",
    "title": "\n Surveys",
    "section": "\n Additional Readings",
    "text": "Additional Readings\n\nDaniel Lüdecke. (2024-05-13). Plotting Likert Scales with sjPlot. https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_likert_scales.html\n\nJoseph Larmarange. Plot Likert-type items with gglikert(). https://cran.r-project.org/web/packages/ggstats/vignettes/gglikert.html\n\nPiping Hot Data. Leveraging Labelled Data in R. https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/\\\n\nBangdiwala, S.I., Shankar, V. The agreement chart. BMC Med Res Methodol 13, 97 (2013). https://doi.org/10.1186/1471-2288-13-97. Open Access.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggstats\n0.10.0\nLarmarange (2025a)\n\n\nlabelled\n2.14.1\nLarmarange (2025b)\n\n\nsjPlot\n2.8.17\nLüdecke (2024)\n\n\n\n\n\n\nLarmarange, Joseph. 2025a. ggstats: Extension to “ggplot2” for Plotting Stats. https://doi.org/10.32614/CRAN.package.ggstats.\n\n\n———. 2025b. labelled: Manipulating Labelled Data. https://doi.org/10.32614/CRAN.package.labelled.\n\n\nLüdecke, Daniel. 2024. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "title": "\n Surveys",
    "section": "Footnotes",
    "text": "Footnotes\n\nPiping Hot Data: Leveraging Labelled Data in R, https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/↩︎\nIntroduction to labelled:https://larmarange.github.io/labelled/articles/intro_labelled.html#using-labelled-with-dplyrmagrittr↩︎\nLabel Support in R:https://cran.r-project.org/web/packages/sjlabelled/index.html↩︎\nUsing the labelr package: https://cran.r-project.org/web/packages/labelr/vignettes/labelr-introduction.html↩︎\nBangdiwala, S.I., Shankar, V. The agreement chart. BMC Med Res Methodol 13, 97 (2013). https://doi.org/10.1186/1471-2288-13-97↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\nTo be coded up.\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once.\n# install.packages(\"extrafontdb\")\n# library(extrafont)\n# extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once.\n# install.packages(\"extrafontdb\")\n# library(extrafont)\n# extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "title": "The Grammar of Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nTipExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(\n  nodes = grey_nodes,\n  edges = grey_edges,\n  directed = FALSE\n)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line\n# About what that line does for the overall graph\n\nggraph(graph = ga, layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(\n    shape = 21, size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n\n  labs(\n    title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\"\n  ) +\n\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc.\n\nggraph(graph = ga, layout = \"kk\") +\n  geom_edge_link0(width = 2) +\n  geom_node_point(\n    shape = 21, size = 4,\n    fill = \"moccasin\",\n    color = \"firebrick\",\n    stroke = 2\n  ) +\n  labs(\n    title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\"\n  ) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n\n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n  labs(\n    title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n    subtitle = \"Colouring Nodes by Attribute\",\n    caption = \"Grey's Anatomy\"\n  ) +\n\n  scale_edge_width(range = c(0.2, 2)) +\n  set_graph_style(family = \"roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\", title = \"Grey's Anatomy\", subtitle = \"Arc Layout\") +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n\n  geom_node_point(size = 3, colour = \"red\") +\n  geom_node_text(aes(label = name),\n    repel = TRUE, size = 2, check_overlap = TRUE,\n    max.overlaps = 25\n  ) +\n  labs(edge_width = \"Weight\") +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class\n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;- tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto\")\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") +\n  geom_edge_diagonal() +\n  labs(title = \"Dendrogram\")\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) +\n  geom_edge_diagonal0() +\n  geom_node_point(aes(filter = leaf)) +\n  coord_fixed() +\n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth), size = 0.25) +\n  scale_fill_distiller(palette = \"Pastel1\") +\n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) +\n  scale_fill_distiller(palette = \"Accent\") +\n  coord_fixed() +\n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") +\n  geom_node_tile(aes(y = -y, fill = depth)) +\n  scale_fill_distiller(palette = \"Set3\") +\n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") +\n  coord_fixed() +\n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n##\nset_graph_style(family = \"Roboto\", size = 8)\n##\n# facet edges by type\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_edges(~type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0() +\n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_graph(type ~ race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\n\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;%\n  filter(degree &gt; 0) %&gt;%\n  activate(edges) %&gt;%\n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto\")\n##\nggraph(ga, layout = \"nicely\") +\n  geom_edge_link0(aes(alpha = centrality_edge_betweenness())) +\n\n  geom_node_point(aes(\n    colour = centrality_degree(),\n    size = centrality_degree()\n  )) +\n\n  geom_node_text(aes(label = name), repel = TRUE, size = 1.5) +\n\n  scale_size(name = \"Degree\", range = c(0.5, 5)) +\n\n  scale_color_gradient(\n    name = \"Degree\", # SAME NAME!!\n    low = \"blue\", high = \"red\",\n    aesthetics = c(\"colour\", \"fill\"),\n    guide = guide_legend(reverse = FALSE)\n  ) +\n\n  scale_edge_alpha(name = \"Betweenness\", range = c(0.05, 1)) +\n  labs(\n    title = \"Grey's Anatomy\",\n    subtitle = \"Nodes Scaled by Degree, Edges shaded by Betweenness\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto\")\n##\n# visualize communities of nodes\nga %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(aes(color = community), size = 3) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;%\n  rowid_to_column(var = \"id\") %&gt;%\n  rename(\"label\" = name) %&gt;%\n  mutate(sex = case_when(\n    sex == \"F\" ~ \"Female\",\n    sex == \"M\" ~ \"Male\"\n  )) %&gt;%\n  replace_na(., list(sex = \"Transgender?\")) %&gt;%\n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;%\n  select(from, to) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"from\" = \"label\")\n  ) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"to\" = \"label\")\n  ) %&gt;%\n  select(\"from\" = id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;%\n  visNodes(font = list(size = 40)) %&gt;%\n  # Colour and icons for each of the gender-groups\n  visGroups(\n    groupname = \"Female\", shape = \"icon\",\n    icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\", shape = \"icon\",\n    icon = list(code = \"f183\", size = 75, color = \"slateblue\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\", shape = \"icon\",\n    icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  # visLegend() %&gt;%\n  # Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;%\n  # Add Interaction Controls\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis, ) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;-\n  starwars_nodes %&gt;%\n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;-\n  starwars_edges %&gt;%\n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;%\n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;%\n  # Select \"id.x\" and \"id.y\" ONLY\n  # Rename them as \"from\" and \"to\"\n  # keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(font = list(size = 30)) %&gt;%\n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(\n    font = list(size = 30), shape = \"icon\",\n    icon = list(code = \"f1e3\", size = 75)\n  ) %&gt;%\n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;%\n  visInteraction(hover = TRUE) %&gt;%\n  addFontAwesome(version = \"4.7.0\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a readymade dataset\nStep 0. Sine qua non! Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit your entire project in a .zip file.\nMake1 - Datasets:\n\n\n\n\n\n\nNoteAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;%\n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nNoteThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\", package = \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nNoteGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nNoteOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "title": "The Grammar of Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.2.2\nDavid Schoch (2023)\n\n\nigraph\n2.1.4\n\nCsardi and Nepusz (2006); Csárdi et al. (2025)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://doi.org/10.32614/CRAN.package.visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://doi.org/10.32614/CRAN.package.igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://doi.org/10.32614/CRAN.package.sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://doi.org/10.32614/CRAN.package.showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html",
    "title": "\n Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.\n\n\n\n\n\n\n\nImportantClick on any Picture to Zoom\n\n\n\nAll embedded figures are displayed full-screen when clicked.\n\n\n\n\n“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#using-web-r",
    "title": "\n Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.\n\n\n\n\n\n\n\nImportantClick on any Picture to Zoom\n\n\n\nAll embedded figures are displayed full-screen when clicked.\n\n\n\n\n“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#setting-up-r-packages",
    "title": "\n Data",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\n\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR)\nlibrary(Lock5Data) # Some neat little datasets from a lovely textbook\nlibrary(kableExtra)\n\nPlot Fonts and Theme\n\nShow the Code```{r}\n#| label: plot-theme\n#| code-fold: true\n#| messages: false\n#| warning: false\n\nlibrary(showtext)\nfont_add(family = \"Alegreya\", regular = \"../../../../fonts/Alegreya/Alegreya-Regular.ttf\")\nfont_add(family = \"Roboto Condensed\", regular = \"../../../../fonts/RobotoCondensed-Regular.ttf\")\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-where-data",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-where-data",
    "title": "\n Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides. (Also embedded below!)\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#what-are-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#what-are-data-types",
    "title": "\n Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-data-types",
    "title": "\n Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions! Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\n\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#some-examples-of-data-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#some-examples-of-data-variables",
    "title": "\n Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\nExample 1: AllCountries\n\n\n\nBase R\n web-r\n\n\n\n\nhead(AllCountries, 5) %&gt;% arrange(desc(Internet))\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.1 The answer is \\(70.5\\%\\).\n\n\nExample 2:StudentSurveys\n\n\n\nBase R\n web-r\n\n\n\n\nhead(StudentSurvey, 5)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable2! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#conclusion",
    "title": "\n Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nLet us take a look at Wickham and Grolemund’s Data Science workflow picture:\n\n\n\n\n\nFigure 1: Data Science Workflow\n\n\nSo there we have it:\n\nWe import and clean the data\n\nQuestions lead us to identify Types of Variables (Quant and Qual)\n\nSometimes we may need to transform the data (long to wide, summarize, create new variables…)\nFurther Questions lead to relationships between variables, which we describe using Data Visualizations\n\nWhich is finally Communicated\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter when we make Graphs next. Other parts of the Workflow (Transformation, Analysis and Modelling) are also following similar grammars, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#ai-generated-summary-and-podcast",
    "title": "\n Data",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on data visualization using the R programming language. It introduces concepts such as data types, variables, and visualization techniques. The tutorial utilizes metaphors to explain these concepts, emphasizing the use of geometric aesthetics to represent data. It also highlights the importance of both visual and analytic approaches in understanding data. The tutorial then demonstrates basic chart types, including histograms, scatterplots, and bar charts, and discusses the “Grammar of Graphics” philosophy that guides data visualization in R. The text concludes with a workflow diagram for data science, emphasizing the iterative process of data import, cleaning, transformation, visualization, hypothesis generation, analysis, and communication.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#references",
    "title": "\n Data",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\n\nAdam E.M. Eltorai, Jeffrey A. Bakal, Paige C. Newell, Adena J. Osband (editors). (March 22, 2023) Translational Surgery: Handbook for Designing and Conducting Clinical and Translational Research. A very lucid and easily explained set of chapters. ( I have a copy. Yes.)\n\nPart III. Clinical: fundamentals\nPart IV: Statistical principles\n\n\nhttps://safetyculture.com/topics/design-of-experiments/\nEmi Tanaka. https://emitanaka.org/teaching/monash-wcd/2020/week09-DoE.html\n\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\n\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\n\nTim C. Hesterberg (2015). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician, 69:4, 371-386, DOI:10.1080/00031305.2015.1089789. PDF here\n\nAlbert Rapp. Adding images to ggplot. https://albert-rapp.de/posts/ggplot2-tips/27_images/27_images\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nLock5Data\n3.0.0\nLock (2021)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLock, Robin. 2021. Lock5Data: Datasets for “Statistics: UnLocking the Power of Data”. https://doi.org/10.32614/CRAN.package.Lock5Data.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#footnotes",
    "title": "\n Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html",
    "title": "\n Time",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP \n\n\n\n\n“Remember that sometimes not getting what you want is a wonderful stroke of luck.”\n— Dalai Lama XIV",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#sec-slides-and-tutorials",
    "title": "\n Time",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP \n\n\n\n\n“Remember that sometimes not getting what you want is a wonderful stroke of luck.”\n— Dalai Lama XIV",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#setting-up-r-packages",
    "title": "\n Time",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(fpp3)\n\n# Wrangling\n# library(tsibble) # loads with ffp3\n# library(tsibbledata) # loads with fpp3\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl)\nlibrary(TSstudio)\nlibrary(timetk)\nlibrary(tsbox)\nlibrary(gghighlight) # Highlight specific parts of charts\nlibrary(ggtime) # Mitchell Ohara-Wild June 2025\n##\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(skimr)\n# library(lubridate)  # Deal with dates. Loads with tidyverse\nlibrary(tidyverse)\n\nThe fpp3 packages loads a good few other packages:\n\n\n [1] \"cli\"         \"crayon\"      \"dplyr\"       \"fable\"       \"fabletools\" \n [6] \"feasts\"      \"ggplot2\"     \"lubridate\"   \"purrr\"       \"rstudioapi\" \n[11] \"tibble\"      \"tidyr\"       \"tsibble\"     \"tsibbledata\"\n\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-graphs-will-we-see-today",
    "title": "\n Time",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nQual\nLine Chart, CandleStick Plot, Heatmap",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Time",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#inspiration",
    "title": "\n Time",
    "section": "\n Inspiration",
    "text": "Inspiration\nShown below are the temperatures over time in two US cities:\n\n\nWhere would need ACs in all rooms? And heaters?",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#introduction",
    "title": "\n Time",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…).\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHow might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-formats-conversion-and-plotting",
    "title": "\n Time",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe tsibble format: this is a new format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using the package tsbox) to a tibble or a tsibble and work with these.\n\n\n\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with a “vanilla” tibble format that we can plot as is. We will then look at a tibbledata that does have a time-oriented variable. We will then perform conversion to tsibble format to plot it, and then a final example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\n\n R\n web-r\n\n\n\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis can be easily plotted using base R:\n\n\n R\n web-r\n\n\n\n\n\n\n# Base R\nplot(AirPassengers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\n\n R\n web-r\n\n\n\n\ndata(ChickWeight)\nstr(ChickWeight)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:\n $ weight: num  42 51 59 64 76 93 106 125 149 171 ...\n $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 15 15 15 15 15 15 15 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"formula\")=Class 'formula'  language weight ~ Time | Chick\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Diet\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time\"\n  ..$ y: chr \"Body weight\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(gm)\"\n\nhead(ChickWeight)\n\n\n  \n\n\n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;%\n  dplyr::filter(Chick == 1, Diet == 1) %&gt;%\n  dplyr::select(weight, Time)\n\n## stats::ts does not accept pipe format\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight,\n  frequency = 2\n)\nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\n\n\n\nplot(ChickWeight_ts) # Using base-R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that the weights of a young chick specimen #1 increases over time.\n\ntibble data\nThe ts data format can handle only one time series; in the above example, we could not have plotted the weight of two chicks, if we had wanted to. If we want to plot/analyze multiple time series, based on say Qualitative variables, (e.g. sales figures over time across multiple products and locations) we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\n\ngapminder data\n\n\n\n\ncountry\nyear\ngdpPercap\npop\nlifeExp\ncontinent\n\n\n\nAfghanistan\n1952\n779.4453\n8425333\n28.801\nAsia\n\n\nAfghanistan\n1957\n820.8530\n9240934\n30.332\nAsia\n\n\nAfghanistan\n1962\n853.1007\n10267083\n31.997\nAsia\n\n\nAfghanistan\n1967\n836.1971\n11537966\n34.020\nAsia\n\n\nAfghanistan\n1972\n739.9811\n13079460\n36.088\nAsia\n\n\n\n\n\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in and inspect it.\n\n\n R\n web-r\n\n\n\n\nbirths_2000_2014 &lt;- read_csv(\"data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nskim(births_2000_2014)\n\n\nData summary\n\n\nName\nbirths_2000_2014\n\n\nNumber of rows\n5479\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n2007.00\n4.32\n2000\n2003\n2007\n2011\n2014\n▇▇▇▇▇\n\n\nmonth\n0\n1\n6.52\n3.45\n1\n4\n7\n10\n12\n▇▅▅▅▇\n\n\ndate_of_month\n0\n1\n15.73\n8.80\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\nday_of_week\n0\n1\n4.00\n2.00\n1\n2\n4\n6\n7\n▇▃▃▃▇\n\n\nbirths\n0\n1\n11350.07\n2325.82\n5728\n8740\n12343\n13082\n16081\n▂▂▁▇▁\n\n\n\n\nbirths_2000_2014\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns and not explicitly time columns. There are no Qualitative variables (yet!).\nPlotting tibble-oriented time data\n\n\nUsing ggformula\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~day_of_week,\n    color = ~day_of_week\n  ) %&gt;%\n  gf_point(\n    title = \"Births, By Day of Week\",\n    subtitle = \"Over the Years\"\n  ) %&gt;%\n  gf_refine(scale_colour_distiller(palette = \"Paired\"))\n\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~date_of_month,\n    color = ~date_of_month\n  ) %&gt;%\n  gf_point(\n    title = \"Births, By Date of Month\",\n    subtitle = \"Over the Years\"\n  ) %&gt;%\n  gf_refine(scale_colour_distiller(palette = \"Paired\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize.\nSummaries will help, so we could calculate the the mean births per month in each year and plot that:\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n####\nbirths_2000_2014_monthly %&gt;%\n  ##\n  gf_line(mean_monthly_births ~ year,\n    group = ~month,\n    colour = ~month, linewidth = 1\n  ) %&gt;%\n  ##\n  gf_point(\n    size = 1.5,\n    title = \"Summaries of Monthly Births over the years\"\n  ) %&gt;%\n  ## palette for 12 colours\n  gf_refine(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\n##\nbirths_2000_2014_weekly\n##\nbirths_2000_2014_weekly %&gt;%\n  gf_line(mean_daily_births ~ year,\n    group = ~day_of_week,\n    colour = ~day_of_week,\n    linewidth = 1,\n    data = .\n  ) %&gt;%\n  gf_point(size = 2, title = \"Births over the Years by Day of Week\") %&gt;%\n  # palette for 12 colours\n  gf_refine(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n## Set the theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    group = day_of_week,\n    color = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Births, By Day of Week\",\n    subtitle = \"Over the Years\"\n  ) +\n  scale_colour_distiller(palette = \"Paired\")\n##\n\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    color = date_of_month,\n    group = date_of_month\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Births, By Date of Month\",\n    subtitle = \"Over the Years\"\n  ) +\n  scale_colour_distiller(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;%\n  ggplot(aes(year, mean_monthly_births,\n    group = month,\n    colour = month\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  labs(title = \"Summaries of Monthly Births over the years\") +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births,\n    group = day_of_week,\n    colour = day_of_week\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\") +\n  labs(title = \"Births over the Years by Day of Week\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n Small Multiples using gghighlight\nInstead of looking at multiple overlapping time series graphs, we could split these up into small multiples or facets and still retain the overall picture that is offered by the overlapping graphs. The trick here is the highlight one of the graphs at a time, while keeping all other graphs in the background. We can do this with the gghighlight package.\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n# library(gghighlight)\nbirths_2000_2014_monthly\n###\nbirths_2000_2014_monthly %&gt;% ggplot() +\n  geom_line(aes(\n    y = mean_monthly_births,\n    x = year,\n    group = month\n  )) +\n  labs(\n    x = \"Year\", y = \"Mean Monthly Births over the Years\",\n    title = \"Mean Births by Month\",\n    caption = \"Using gghighlight package\"\n  ) +\n\n  ### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))\n  ) +\n\n  ### Add faceting\n  facet_wrap(vars(month))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly\n###\nbirths_2000_2014_weekly %&gt;% ggplot() +\n  geom_line(aes(y = mean_daily_births, x = year, group = day_of_week)) +\n  labs(\n    x = \"Year\", y = \"Mean Daily Births over the Years\",\n    title = \"Mean Births by Day of Week\",\n    caption = \"Using gghighlight package\"\n  ) +\n\n  ### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))\n  ) +\n\n  ### Add faceting\n  facet_wrap(vars(day_of_week))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\nAnd more births in September? That should be a no-brainer!! 😀\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it into a time series format. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\nPlotting tibble time-series\nNow, we can convert the time-oriented columns in this dataset into a single date variable, giving us a proper tibble time-series:\n\nbirths_tibble_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(year, month, date_of_month)) %&gt;%\n  ## Drop off the individual columns ( year, month, day_of_month)\n  select(date, births)\n\nbirths_tibble_timeseries\n\n\n  \n\n\n\nNote that we have a proper date formatted column, as desired. This is a single time series, but if we had other Qualitative variables such as say city, we could easily have had multiple series here. We can plot this with ggformula/ggplot as we have done before, and with now with timetk:\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_tibble_timeseries %&gt;%\n  timetk::plot_time_series(\n    .date_var = date,\n    .value = births,\n    .interactive = FALSE,\n    .title = \"Births over Time\",\n    .x_lab = \"Time\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary with time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data. Run data(package = \"tsibbledata\") in your Console to find out about these.\nLet us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.\n\ndata(PBS, package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n\nData Dictionary\n\n\n\n\n\n\nNote\n\n\n\nData Description: This is a large-ish dataset.Run PBS in your console)\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package.\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. (Type help(\"PBS\") in your Console.)\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\n\n\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\nUsing timetk\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(title = \"PBS Costs vs time\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  ggplot(aes(Month, Cost)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PBS Costs vs time\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  timetk::plot_time_series(\n    .date_var = Month, .value = Cost,\n    .interactive = FALSE,\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs.\nWe will do that in the Section 1.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-heatmaps",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-heatmaps",
    "title": "\n Time",
    "section": "\n Time Series Heatmaps",
    "text": "Time Series Heatmaps\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(\n    births &gt;= 10000 ~ \"high\",\n    births &lt;= 8000 ~ \"low\",\n    TRUE ~ \"fine\"\n  )) %&gt;%\n  mutate(birthrate = base::factor(birthrate,\n    labels = c(\"high\", \"fine\", \"low\"),\n    ordered = TRUE\n  )) %&gt;%\n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~birthrate,\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(scale_x_time(\n    breaks = 1:12,\n    labels = c(\n      \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n      \"May\", \"Jun\", \"Jul\", \"Aug\",\n      \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    )\n  )) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"OrRd\", direction = -1))\n\n\n\n\n\n\n\n\n\n\n\n\nNote how both X and Y axis seem to be a time-oriented variable in a heatmap!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#your-turn",
    "title": "\n Time",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. (Install and load them first! ) Plot basic, filtered and model-based graphs for these and interpret.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#wait-but-why",
    "title": "\n Time",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nMany datasets show quantities varying over time. These are called time-series data.\nThe X-axis in these cases becomes a time axis.\nTime-series data come in many different formats!\nThe time-aspect in a dataset creates for two dimensions of data-aggregation and averaging: One based on factors as before, and a new one based on intervals of time\nWe are interested in decomposing a time-series into averages, trends, seasonal components, and random variations\nWe are also interested in modelling a time-series as additive or multiplicative time-series, using techniques such as Holt-Winters, and ARIMA\nAnd of course we are interested in forecasting!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#conclusion",
    "title": "\n Time",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them.\nIn the Tutorial Section 1, we will explore:\n\nwrangling with Time series to produce grouped and filtered aggregates/summaries and plots with these\nhow to decompose time series into periodic and aperiodic components, which can be used to make business decisions.\nProducing Interactive Plots for Time Series\n\nmodelling and forecasting of time series.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#references",
    "title": "\n Time",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition).available online\n\n\nTime Series Analysis at Our Coding Club\n\n\nThe Nuclear Threat—The Shadow Peace, part 1\n\n\n11 Ways to Visualize Changes Over Time – A Guide\n\n\nWhat is seasonal adjustment and why is it used?\n\n\nThe start-at-zero rule\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nfpp3\n1.0.1\nHyndman (2024)\n\n\ngghighlight\n0.5.0\nYutani (2025)\n\n\ntimetk\n2.9.0\nDancho and Vaughan (2023)\n\n\ntsbox\n0.4.2\nSax (2021)\n\n\ntsdl\n0.1.0\nHyndman and Yang (2025)\n\n\ntsibble\n1.1.6\nWang, Cook, and Hyndman (2020)\n\n\ntsibbledata\n0.4.1\nO’Hara-Wild et al. (2022)\n\n\nTSstudio\n0.1.7\nKrispin (2023)\n\n\n\n\n\n\nDancho, Matt, and Davis Vaughan. 2023. timetk: A Tool Kit for Working with Time Series. https://doi.org/10.32614/CRAN.package.timetk.\n\n\nHyndman, Rob. 2024. Fpp3: Data for “Forecasting: Principles and Practice” (3rd Edition). https://doi.org/10.32614/CRAN.package.fpp3.\n\n\nHyndman, Rob, and Yangzhuoran Yang. 2025. tsdl: Time Series Data Library. https://github.com/FinYang/tsdl.\n\n\nKrispin, Rami. 2023. TSstudio: Functions for Time Series Analysis and Forecasting. https://doi.org/10.32614/CRAN.package.TSstudio.\n\n\nO’Hara-Wild, Mitchell, Rob Hyndman, Earo Wang, and Rakshitha Godahewa. 2022. tsibbledata: Diverse Datasets for “tsibble”. https://doi.org/10.32614/CRAN.package.tsibbledata.\n\n\nSax, Christoph. 2021. tsbox: Class-Agnostic Time Series in in R. https://docs.ropensci.org/tsbox/.\n\n\nWang, Earo, Dianne Cook, and Rob J Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics 29 (3): 466–78. https://doi.org/10.1080/10618600.2019.1695624.\n\n\nYutani, Hiroaki. 2025. gghighlight: Highlight Lines and Points in “ggplot2”. https://doi.org/10.32614/CRAN.package.gghighlight.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#footnotes",
    "title": "\n Time",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(tidy = \"styler\")\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(scales) # Some nice time-oriented scales in graphs!\nlibrary(tsibble)\nlibrary(timetk)\n\n# Datasets\nlibrary(tsibbledata)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Introduction",
    "text": "Introduction\nWe have now arrived at the need to start from raw, multiple time series data and filter, group, and summarize these time series grasp their meaning, a process known as “wrangling”.\n\n\n\n\n\n\nNoteWrangling with dplyr\n\n\n\nThe tutorial for wrangling using dplyr is here.\n\n\nHere, we will first use the births data we encountered earlier which had a single time series, and then proceed to a more complex example which has multiple time-series."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe can do this in two ways, and with two packages:\n\n\n\n\n\n\nNoteTwo Wrangling “Dimensions”\n\n\n\nFor all the above operations, we can either use time variable as the basis, by filtering for specific periods, or computing summaries over larger intervals of time e.g. month, quarter, year;\nAND/OR\nWe can do the same over space variables, i.e. the Qualitative variables that define individual time series, and based on which we can filter and and analyze these specific time series. Each unique setting of these Qualitative variables could potentially define a time series! There are 336 groups/combinations of them in PBS, but not all are unique time series, since some of the Qual variables are nested inside others, e.g ATC1_desc provides more info on each value of ATC1 and is not truly a separate Qual variable.\n\n\nAnd the packages are:\n\n\n\n\n\n\nTiptsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\n(Filtering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.)\n\n\n\n\n\n\n\n\nTiptimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other Qual variables (i.e non-time)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #1: Births Dataset",
    "text": "Case Study #1: Births Dataset\nAs a second example let us read and inspect in the now familiar US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n R\n web-r\n\n\n\n\n# Step1: Read the data\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n\nLet us make a date column out of the individual year/month/day columns:\n\n\n\n# Step2: Convert year + month + date_of_month to \"date\"\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births)\n\nbirths_timeseries\nclass(births_timeseries)\n\nNote that this is still just a tibble, with a time-formatted column. Next let us create a full-blown tsibble with the same data:\n\n\n\n\n  \n\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n# Step3: Convert to tsibble\n# combine the year/month/date_of_month columns into a date\n# drop them thereafter\nbirths_tsibble &lt;-\n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n\nbirths_tsibble\nclass(births_tsibble)\n\nBoth data frames look identical, except for data class difference. This is DAILY data of course.\n\n\n\n\n  \n\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nWe will (sadly) need both formats; the tsibble packages needs, well, tsibble-formats, and timetk cannot, it seems, handle tsibble-formats and needs regular tibbles. Sigh.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#basic-time-series-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#basic-time-series-plot",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Basic Time Series Plot",
    "text": "Basic Time Series Plot\nLet us plot the timeseries using the tsibble data, with both ggformula and timetk:\n\n\n R\n web-r\n\n\n\nLet us try a basic plot with both tsibble vs timetk packages.\n# column: body-outset-right\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nbirths_tsibble %&gt;%\n  gf_line(births ~ index,\n    data = .,\n    title = \"Basic tsibble plotted with ggformula\"\n  )\n# timetk **can** plot tsibbles.\nbirths_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = index,\n    .value = births, .interactive = FALSE,\n    .title = \"Tsibble Plotted with timetk\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#aggregation-and-averaging",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#aggregation-and-averaging",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Aggregation and Averaging",
    "text": "Aggregation and Averaging\nLet us plot the time series using the tsibble data, with both ggformula and timetk, this time grouping by month and get monthly aggregates to get a summary:\n\n\n R\n web-r\n\n\n\nHere we plot Monthly Aggregates with both ggformula and timetk:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nbirths_tsibble %&gt;%\n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ month_index,\n    data = .,\n    title = \"Monthly Aggregate with tsibble + ggformula\"\n  ) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\") %&gt;%\n  gf_labs(x = \"Year\", y = \"Mean Monthly Births\")\n\n##\n##\n##\n##\nbirths_timeseries %&gt;%\n  # cannot use tsibble here\n  # tsibble format cannot be summarized/wrangled by timetk\n\n  timetk::summarize_by_time(\n    .date_var = date,\n    .by = \"month\",\n    month_mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, month_mean,\n    .title = \"Monthly aggregate births with timetk\",\n    .interactive = FALSE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier. We will analyse seasonal trends in another module.\nLet us try getting annual aggregates.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;%\n  ## tsibble does not have a \"year\" function? So using lubridate..\n  ## Summarize\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  ## Plot\n  gf_point(mean_births ~ year_index, data = .) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\n##\n##\n##\n##\n##\nbirths_timeseries %&gt;%\n  ## Summarize\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"year\",\n    mean = mean(births)\n  ) %&gt;%\n  ## Plot\n  timetk::plot_time_series(date, mean,\n    .title = \"Yearly aggregate births with timetk\",\n    .interactive = FALSE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Yearly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n A small detour\nAh yes….errors. There is a curious interplay between dplyr and tsibble…they play together but not all the time, it would seem.\nThe original births tibble dataset allows dplyr:group_by + summarize:\n\n# The original dataset allows dplyr:group_by + summarize\nbirths_2000_2014 %&gt;%\n  dplyr::group_by(year) %&gt;%\n  summarise(mean_births = mean(births, na.rm = TRUE))\n\n\n  \n\n\n\nHowever, tsibble-converted data does not quite work with dplyr::group_by+summarize:\n\n```{r}\n#| label: Errors-2\n#| eval: false\n\n# This code will not work\nbirths_tsibble %&gt;%\n  # Grouping does not work. Here is the problem\n  dplyr::group_by(index) %&gt;%\n  # Trying to get Annual Birth Average as before\n  # Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE))\n```\n\nEven if we pull out the year information in index, it gives confusing results…\n\nbirths_tsibble %&gt;%\n  # All right, try to pull the year info from `index` then\n  mutate(dplyr_year = lubridate::year(index)) %&gt;%\n  # Grouping does not work\n  dplyr::group_by(dplyr_year) %&gt;%\n  # Trying to get Annual Birth Average as before\n  # Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE))\n\n\n  \n\n\n\nThis grouping does not give a proper result (though it does show 15 groups.)\nUsing tsibble::index_by() and then dplyr::summarize() does the trick…so all right. The index_by() operation is different from that of dplyr::group_by()!\n\n# tsibble works with index_by + summarize\n# 15 rows, one for each year\nbirths_tsibble %&gt;%\n  # tsibble can get year info from index\n  tsibble::index_by(year_date = year(index)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month, quarter or year?\n\n Monthly Box Plots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;%\n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  # Plot the groups\n  # 180 plots!!\n  gf_boxplot(births ~ index,\n    group = ~month_index,\n    fill = ~month_index,\n    data = .,\n    title = \"Boxplots of Births by Month\",\n    caption = \"tsibble + ggformula\"\n  )\n\n\n####\n####\n####\n####\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"month\",\n    .plotly_slider = TRUE,\n    .title = \"Boxplots of Births by Month\",\n    .interactive = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\ntimetk can take tsibble-format data to plot with, but cannot perform aggregation: summarize_by_time() will throw an error!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see 180 boxplots…yes this is still too busy a plot for us to learn much from."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Quarterly boxplots",
    "text": "Quarterly boxplots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ index,\n    group = ~qrtr_index,\n    fill = ~qrtr_index,\n    data = .\n  ) # 60 plots!!\n###\n###\n###\n###\n###\n\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"quarter\",\n    .title = \"Quarterly births with timetk\",\n    .interactive = TRUE,\n    .plotly_slider = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Quarterly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe have 60 boxplots…over a period of 15 years, one box plot per quarter…\n\n Yearwise boxplots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n  # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ index,\n    group = ~year_index,\n    fill = ~year_index,\n    data = .\n  ) %&gt;% # plot the groups 15 plots\n  gf_theme(scale_fill_distiller(palette = \"Spectral\"))\n####\n####\n####\n####\n####\n####\n\nbirths_tsibble %&gt;%\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"year\",\n    .title = \"Yearly aggregate births with timetk\",\n    .interactive = TRUE,\n    .plotly_slider = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis looks much better…We can more easily see that 2006-2009 the births were somewhat higher, because the medians in these years are the highest."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #2: PBS Dataset",
    "text": "Case Study #2: PBS Dataset\nWe previously encountered the PBS dataset from the tsibbledata package earlier, which is a dataset containing Monthly Medicare prescription data in Australia. We will resume from there:\n\n\n R\n web-r\n\n\n\n\ndata(\"PBS\", package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\n# skimr::skim(PBS) # does not work, need to investigate\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Counts by Qual variables\nLet us first see how many observations there are for each combo of keys:\n\n\n R\n web-r\n\n\n\n\n\n\n## Types\nPBS %&gt;%\n  dplyr::count(Type) # 2 Types\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## Concessions\nPBS %&gt;% count(Concession) # 2 Types\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## ATC1\nPBS %&gt;% count(ATC1) # 15 ATC1 groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## ATC2\nPBS %&gt;% count(ATC2) # 84 ATC2 groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# dplyr grouping with ATC1 and ATC2\nPBS %&gt;%\n  dplyr::group_by(ATC1, ATC2) %&gt;%\n  count() # Still 84; ATC2 is nested in ATC1\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## All possible groups\nPBS %&gt;%\n  group_by(ATC1, ATC2, Concession, Type) %&gt;%\n  count() # 336 overall groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteData Dictionary for PBS\n\n\n\nThis is a large-ish dataset: (Run PBS in your console)\n\n67K observations\nQuant Variables: Two Quant variables (Scripts and Cost)\n\nTime Variable:\n\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month\n\nformatted as yearmonth, a new type of variable introduced in the tsibble package. yearmonth does not show in glimpse output!\n\n\n\nQual variables:\n\n\nConcession: Concessional and General (Concessional scripts are given to pensioners, unemployed, dependents, and other card holders)\n\nType: Co-payments and Safety Net\n\n\nATC1: Anatomical Therapeutic Chemical index (level 1).\n\n15 types\n\n\n\n\n\nATC2: Anatomical Therapeutic Chemical index (level 2).\n\n84 types, nested inside ATC1.\n\n\n\n\n\nWe will start with the familiar basic messy plot, and work our way towards filtering, summaries, and averages.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(\n    title = \"PBS Costs vs time\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAs noted earlier, this basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs. We will perform summaries with tsibble and plots with ggformula first. Then we will use timetk to perform both operations.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\"\n  ) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs per Month for General A category patients\") %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen:\n\nstrongly seasonal for both Types of graphs;\nseasonal variation increasing over the years, a clear sign of a multiplicative time series, especially for Safety net.\nUpward trend with both types of subsidies, Safety net and Co-payments.\n\nCo-payments type have some kind of dip around the year 2000…\nBut this is still messy and overwhelming and we could certainly use some summaries/aggregates/averages.\n\n\n\nWe can now use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time, and the size the data frame decrease with each aggregation:\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Month, which is the original data\n# New Variable Name to make grouping visible\nPBS_month &lt;- PBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\"\n  ) %&gt;%\n  tsibble::index_by(Month_Date = Month) %&gt;%\n  dplyr::summarise(\n    across(\n      .cols = c(Cost, Scripts),\n      .fn = mean,\n      .names = \"mean_{.col}\"\n    )\n  )\n\nPBS_month\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\nPBS_month %&gt;%\n  mutate(Month_Date = as_date(Month_Date)) %&gt;%\n  gf_line(mean_Cost ~ Month_Date) %&gt;%\n  gf_line(mean_Scripts ~ Month_Date,\n    title = \"Mean Costs and Scripts for General + A category\",\n    subtitle = \"Means over General + A category \"\n  ) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: To Be Written Up !!!\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Quarter\nPBS_quarter &lt;-\n  PBS %&gt;%\n  tsibble::index_by(Quarter_Date = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\nPBS_quarter\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nPBS_quarter %&gt;%\n  gf_line(mean_Cost ~ Quarter_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: TBD\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Year\nPBS_year &lt;- PBS %&gt;%\n  index_by(Year_Date = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\nPBS_year\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nPBS_year %&gt;%\n  gf_line(mean_Cost ~ Year_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: TBD. I must write this up soon!\n\n\nUsing timetk\n\n\n\n\n\n\n\nNoteThe time variable for timetk\n\n\n\nThe PBS-derived tsibbles have their “time-oriented” variables formatted asyearmonth,yearquarter and dbl, as seen. We need to mutate these into a proper date format for the timetk package to summarise them successfully. (Plotting a tsibble with timetk is possible, as seen earlier.)\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"month\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Month\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"quarter\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Quarter\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"year\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Year\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to filter, summarize and compute various aggregate metrics from them and to plot these. Both tsibble and timetk offer similar capability here."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the data sets in the tsdl and in the tsibbledata packages. Plot basic, filtered and summarized graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "title": "🕔 Time Series Wrangling",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\n\n\nTime Series Analysis at Our Coding Club\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngapminder\n1.0.1\n(gapminder?)\n\n\ntimetk\n2.9.0\n(timetk?)\n\n\ntsibble\n1.1.6\n(tsibble?)\n\n\ntsibbledata\n0.4.1\n(tsibbledata?)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html",
    "title": "\n Ratings and Rankings",
    "section": "",
    "text": "“I have no respect for people who deliberately try to be weird to attract attention, but if that’s who you honestly are, you shouldn’t try to”normalize” yourself.”\n— Alicia Witt, actress, singer-songwriter, and pianist (b. 21 Aug 1975)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#inspiration",
    "title": "\n Ratings and Rankings",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n(a) Energy Sources in the USA in 2024\n\n\n\n\n\n\n\n(b) 5 tools Players in Baseball\n\n\n\n\nFigure 1: Dumbbell and Radar Charts for Ranking\n\n\nWhat do we see here? From https://www.visualcapitalist.com/sp/americas-cheapest-sources-of-electricity-in-2024/ :\nFrom Figure 1 (a):\n\n\nOnshore wind power effectively costs USD0 per megawatt-hour (MWh) when subsidies are included!\n\nDemand for storage solutions is rising quickly. If storage is included, the minimum cost for onshore wind increases to $8 per MWh.\n\nSolar photovoltaics (PV) have similarly attractive economics. With subsidies, the minimum cost is USD6 per MWh. When including storage, USD38 per MWh. Notably, the maximum cost of solar PV with storage has significantly increased from USD102 in 2023 to USD 210 in 2024.\n\nFor gas-combined cycle plants, which combine natural gas and steam turbines for efficient electricity generation, the maximum price has climbed $7 year-over-year to $108 per MWh.\n\n\nAnd from From Figure 1 (b)?\n\nThere is a clear difference in the capabilities of the three players compared, though all of them are classified as “5 tools” players.\n\nEach player is better than the others at one unique skill: Betts at Throwing, Judge at Hit_power, and Trout at Hit_avg.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#setting-up-r-packages",
    "title": "\n Ratings and Rankings",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(RColorBrewer) # colour palettes\n\nlibrary(ggbump) # Bump Charts\nlibrary(ggiraphExtra) # Radar, Spine, Donut and Donut-Pie combo charts !!\nlibrary(ggalt) # New geometries, coordinate systems, statistical transformations, scales and fonts\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"ricardo-bion/ggradar\")\nlibrary(ggradar) # Radar Plots\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nlibrary(tidyverse) # includes ggplot for plotting\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#what-graphs-are-we-going-to-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#what-graphs-are-we-going-to-see-today",
    "title": "\n Ratings and Rankings",
    "section": "\n What graphs are we going to see today?",
    "text": "What graphs are we going to see today?\nWhen we wish to compare the size of things and rank them, there are quite a few ways to do it.\nBar Charts and Lollipop Charts are immediately obvious when we wish to rank things on one aspect or parameter, e.g. mean income vs education. We can also put two lollipop charts back-to-back to make a Dumbbell Chart to show comparisons/ranks across two datasets based on one aspect, e.g change in mean income over two years, across gender.\nWhen we wish to rank the multiple objects against multiple aspects or parameters, then we can use Bump Charts and Radar Charts, e.g performance of one or more products against multiple criteria (cost, size, performance…)s.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#lollipop-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#lollipop-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Lollipop Charts",
    "text": "Lollipop Charts\nLet’s make a toy dataset of Products and Ratings:\n\n# Sample data set\nset.seed(1)\ndf1 &lt;- tibble(\n  product = LETTERS[1:10],\n  rank = sample(20:35, 10, replace = TRUE)\n)\ndf1\n\n\n  \n\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nUsing ggalt\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n###\ngf_segment(0 + rank ~ product + product, data = df1) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n\n  gf_point(rank ~ product,\n    colour = ~product,\n    size = 5,\n    ylab = \"Rank\",\n    xlab = \"Product\"\n  ) %&gt;%\n  gf_labs(title = \"Product Ratings\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\ngf_segment(\n  0 + rank ~ fct_reorder(product, -rank) +\n    fct_reorder(product, -rank),\n  data = df1\n) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n\n  gf_point(rank ~ product, colour = ~product, size = 5) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_labs(x = \"Product\", y = \"Rank\", title = \"Product Ratings\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe have flipped the chart horizontally and reordered the \\(x\\) categories in order of decreasing ( or increasing ) \\(y\\), using forcats::fct_reorder.\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df1) +\n  geom_segment(aes(\n    y = 0, yend = rank,\n    x = product,\n    xend = product\n  )) +\n  geom_point(aes(y = rank, x = product, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\")\n###\nggplot(df1) +\n  geom_segment(aes(\n    y = 0, yend = rank,\n    x = fct_reorder(product, -rank),\n    xend = fct_reorder(product, -rank)\n  )) +\n  geom_point(aes(x = product, y = rank, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nYes, R has ( nearly) everything, including a geom_lollipop command: Here!\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df1) +\n  geom_lollipop(aes(x = rank, y = product),\n    point.size = 3, horizontal = F\n  ) +\n  labs(title = \"What is this BS chart?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df1) +\n  geom_lollipop(aes(y = rank, x = product),\n    point.size = 3, horizontal = T\n  ) +\n  labs(title = \"This also looks like BS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df1) +\n  geom_lollipop(aes(y = rank, x = product),\n    point.size = 3, , horizontal = F\n  ) +\n  labs(\n    title = \"Yeah, but I want this horizontal...\",\n    subtitle = \"And with colour and sorted and...\",\n    caption = \"Peasants...they want everything...\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df1) +\n  geom_lollipop(\n    aes(\n      x = rank,\n      y = reorder(product, rank),\n      colour = product\n    ),\n    stroke = 2,\n    point.size = 3, horizontal = T\n  ) +\n  labs(\n    title = \"Now you're talking\",\n    x = \"Rank\", y = \"Product\",\n    caption = \"Take that, peasants!\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Lollipop Plots\n\n\n\n\nVery simple chart, almost like a bar chart\nDifferences between the same set of data across one aspect (i.e. rank) is very quickly apparent\n\nOrdering the dataset by the attribute (i.e ordering product by rank) makes the message very clear.\nEven a large number of data can safely be visualized and understood",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#dumbbell-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#dumbbell-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Dumbbell Charts",
    "text": "Dumbbell Charts\nA lollipop chart compares a set of data against one aspect. What if we have more than one? Say sales in many product lines across two years?\nLet us once again construct a very similar looking toy dataset, but with two columns for ratings, one for each of two years:\n\n# Sample data set\n# Wide Format data!\nset.seed(2)\ndf2 &lt;- tibble(\n  product = LETTERS[1:10],\n  rank_year1 = sample(20:35, 10, replace = TRUE),\n  rank_year2 = sample(15:45, 10, replace = TRUE)\n)\ndf2\n\n\n  \n\n\n\n\nA short diversion: we can also make this data into long form: this will become useful very shortly!\n\n\n\n\n\n\nNote Wide Form and Long Form Data\n\n\n\nLook at the data: this is wide form data. The columns pertaining to each of the Product-Features would normally be stacked into two columns, one with the Feature and the other with the score. Note the trio: Qual(product) + Qual(year) + Quant(scores):\n\n# With Long Format Data\ndf2_long &lt;- df2 %&gt;%\n  pivot_longer(\n    cols = c(dplyr::starts_with(\"rank\")),\n    names_to = \"year\", values_to = \"scores\"\n  )\ndf2_long\n\n\n  \n\n\n\nA cool visualization of this operation was created by Garrick Aden-Buie:\n\n\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nUsing ggalt\nComparison barchart\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n## With Wide Form Data\n##\ndf2 %&gt;%\n  gf_segment(product + product ~ rank_year1 + rank_year2,\n    size = 3, color = \"grey60\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_point(product ~ rank_year1,\n    size = 3,\n    colour = \"#123456\"\n  ) %&gt;%\n  gf_point(product ~ rank_year2,\n    size = 3,\n    colour = \"#bad744\"\n  ) %&gt;%\n  gf_labs(x = \"Rank\", y = \"Product\", title = \"Product Ranks in Year1 and Year2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n## Rearranging `product` in order of rank_year2\ndf2 %&gt;%\n  gf_segment(\n    reorder(product, rank_year2) +\n      reorder(product, rank_year2) ~\n      rank_year1 + rank_year2,\n    size = 3, color = \"grey60\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) %&gt;%\n  gf_point(product ~ rank_year1,\n    size = 3,\n    colour = \"#123456\"\n  ) %&gt;%\n  gf_point(product ~ rank_year2,\n    size = 3,\n    colour = \"#bad744\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Rank\", y = \"Product\",\n    title = \"In Decreasing order of Year2 Rank\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n## With Wide Format Data\nggplot(df2, aes(y = product, yend = product, x = rank_year1, xend = rank_year2)) +\n  geom_segment(\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) +\n  geom_point(aes(rank_year1, product),\n    colour = \"#5b8124\", size = 3\n  ) +\n  geom_point(aes(rank_year2, product),\n    colour = \"#bad744\", size = 3\n  ) +\n  labs(x = \"Rank\", y = \"Product\")\n\n\n\n\n\n\n## Rearranging `product` in order of rank_year2\nggplot(df2, aes(y = reorder(product, rank_year2), yend = reorder(product, rank_year2), x = rank_year1, xend = rank_year2)) +\n  geom_segment(\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) +\n  geom_point(aes(rank_year1, product),\n    colour = \"#5b8124\", size = 3\n  ) +\n  geom_point(aes(rank_year2, product),\n    colour = \"#bad744\", size = 3\n  ) +\n  labs(\n    x = \"Rank\", y = \"Product\",\n    title = \"In Decreasing order of Year2 Rank\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\ndf2 %&gt;% ggplot() +\n  geom_dumbbell(\n    aes(\n      y = reorder(product, rank_year2),\n      x = rank_year1,\n      xend = rank_year2\n    ),\n    size = 3, color = \"grey60\",\n    colour_x = \"#5b8124\",\n    colour_xend = \"#bad744\",\n    dot_guide = TRUE, # Try FALSE\n    dot_guide_size = 0.25\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"ggplot2 geom_dumbbell with dot guide\",\n    subtitle = \"Products in Decreasing order of Year2 Rank\",\n    caption = \"Made with ggalt\"\n  ) +\n  theme(panel.grid.major.x = element_line(size = 0.05)) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\ndf2_long %&gt;%\n  gf_col(product ~ scores,\n    group = ~year,\n    fill = ~year, position = \"dodge\"\n  ) %&gt;%\n  gf_labs(title = \"Bar Plot for Comparison\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Dumbbell Plots\n\n\n\n\nDumbbell Plots are clearly they are more intuitive and clear than the bar chart\nDifferences between the same set of data at two different aspects is very quickly apparent\n\nDifferences in differences(DID) are also quite easily apparent. Experiments do use these metrics and these plots would be very useful there.\n\nggalt works nicely with additional visible guides rendered in the chart\n\n\n\n\n\n\n\n Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\n  \"A\", \"A\", \"A\",\n  \"B\", \"B\", \"B\",\n  \"C\", \"C\", \"C\",\n  \"D\", \"D\", \"D\"\n)\n\ndf3 &lt;- tibble(year, position, product) %&gt;%\n  mutate(product = as_factor(product))\n\ndf3\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent. ( Though it may be possible with a combination of gf_point and gf_polygon, and pre-computing the coordinates. Seems long-winded.)\nNote the + syntax with ggplot code!!\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\ndf3 %&gt;%\n  ggplot() +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  labs(title = \"Bump Chart: Product Ranks over Time\") +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_color_brewer(palette = \"Set1\") + # Change Colour Scale\n  scale_x_continuous(breaks = c(2019:2021), labels = c(2019:2021))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df3) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  # Same as before up to here\n  # Add the labels at start and finish\n\n  geom_text(\n    data = df3 %&gt;% filter(year == min(year)),\n    aes(\n      x = year - 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 1\n  ) +\n  geom_text(\n    data = df3 %&gt;% filter(year == max(year)),\n    aes(\n      x = year + 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 0\n  ) +\n  labs(\n    title = \"Bump Chart: Product Ranks over Time\",\n    subtitle = \"Note the Labels!\"\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_x_continuous(breaks = c(2019:2021), labels = c(2019:2021)) +\n  theme(legend.position = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Bump Charts\n\n\n\n\nBump charts are good for depicting Ranks/Scores pertaining to a set of data, as they vary over another aspect, for a set of products\nCannot have too many levels in the aspect parameter, else the graph gets too hard to make sense with.\nFor instance if we had 10 years in the data above, we would have lost the plot, literally! Perhaps better to use a Sankey in that case!!\n\n\n\n\n\n Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes. Such a plot is called a radar plot.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN (and has some other capabilities too, which are worth exploring!)\nLet us generate some toy data first:\n\nset.seed(4)\ndf4 &lt;- tibble(\n  Product = c(\"G1\", \"G2\", \"G3\"),\n  Power = runif(3),\n  Cost = runif(3),\n  Harmony = runif(3),\n  Style = runif(3),\n  Size = runif(3),\n  Manufacturability = runif(3),\n  Durability = runif(3),\n  Universality = runif(3)\n)\ndf4\n\n\n  \n\n\n\nAnd now plot it with both packages.\n\n\n\n Using ggradar\n Using ggiraphExtra\n\n\n\n\n\n\nggradar::ggradar(\n  plot.data = df4,\n  plot.title = \"Made with ggradar\",\n  axis.label.size = 3, # Titles of Params\n  grid.label.size = 4, # Score Values/Circles\n  group.point.size = 3, # Product Points Sizes\n  group.line.width = 1, # Product Line Widths\n  group.colours = c(\"#123456\", \"#fad744\", \"#03e2e1\"), # Product Colours\n  fill = TRUE, # fill the radar polygons\n  fill.alpha = 0.3, # Not too dark, Arvind\n  legend.title = \"Product\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n\n\nggiraphExtra::ggRadar(\n  data = df4,\n  aes(colour = Product),\n  interactive = FALSE, # try TRUE\n  rescale = F, # rescale = TRUE makes it look different...try!!\n  title = \"Using ggiraphExtra\"\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Radar Plots\n\n\n\n\nDifferences in scores for a given item across several aspect or parameters are readily apparent.\nThese can also be compared, parameter for parameter, with more than one item\nthe same set of data at two different aspects is very quickly apparent\nData is clearly in wide form\nBoth ggradar and ggiraphExtra render very similar-looking radar charts and the syntax is not too intimidating!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#bump-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#bump-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Bump Charts",
    "text": "Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\n  \"A\", \"A\", \"A\",\n  \"B\", \"B\", \"B\",\n  \"C\", \"C\", \"C\",\n  \"D\", \"D\", \"D\"\n)\n\ndf3 &lt;- tibble(year, position, product) %&gt;%\n  mutate(product = as_factor(product))\n\ndf3\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent. ( Though it may be possible with a combination of gf_point and gf_polygon, and pre-computing the coordinates. Seems long-winded.)\nNote the + syntax with ggplot code!!\n\n\n\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\ndf3 %&gt;%\n  ggplot() +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  labs(title = \"Bump Chart: Product Ranks over Time\") +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_color_brewer(palette = \"Set1\") + # Change Colour Scale\n  scale_x_continuous(breaks = c(2019:2021), labels = c(2019:2021))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\n\n\n## Set the theme\ntheme_set(new = theme_custom())\n\nggplot(df3) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  # Same as before up to here\n  # Add the labels at start and finish\n\n  geom_text(\n    data = df3 %&gt;% filter(year == min(year)),\n    aes(\n      x = year - 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 1\n  ) +\n  geom_text(\n    data = df3 %&gt;% filter(year == max(year)),\n    aes(\n      x = year + 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 0\n  ) +\n  labs(\n    title = \"Bump Chart: Product Ranks over Time\",\n    subtitle = \"Note the Labels!\"\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_x_continuous(breaks = c(2019:2021), labels = c(2019:2021)) +\n  theme(legend.position = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Bump Charts\n\n\n\n\nBump charts are good for depicting Ranks/Scores pertaining to a set of data, as they vary over another aspect, for a set of products\nCannot have too many levels in the aspect parameter, else the graph gets too hard to make sense with.\nFor instance if we had 10 years in the data above, we would have lost the plot, literally! Perhaps better to use a Sankey in that case!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#radar-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#radar-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Radar Charts",
    "text": "Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes. Such a plot is called a radar plot.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN (and has some other capabilities too, which are worth exploring!)\nLet us generate some toy data first:\n\nset.seed(4)\ndf4 &lt;- tibble(\n  Product = c(\"G1\", \"G2\", \"G3\"),\n  Power = runif(3),\n  Cost = runif(3),\n  Harmony = runif(3),\n  Style = runif(3),\n  Size = runif(3),\n  Manufacturability = runif(3),\n  Durability = runif(3),\n  Universality = runif(3)\n)\ndf4\n\n\n  \n\n\n\nAnd now plot it with both packages.\n\n\n\n Using ggradar\n Using ggiraphExtra\n\n\n\n\n\n\nggradar::ggradar(\n  plot.data = df4,\n  plot.title = \"Made with ggradar\",\n  axis.label.size = 3, # Titles of Params\n  grid.label.size = 4, # Score Values/Circles\n  group.point.size = 3, # Product Points Sizes\n  group.line.width = 1, # Product Line Widths\n  group.colours = c(\"#123456\", \"#fad744\", \"#03e2e1\"), # Product Colours\n  fill = TRUE, # fill the radar polygons\n  fill.alpha = 0.3, # Not too dark, Arvind\n  legend.title = \"Product\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n\n\nggiraphExtra::ggRadar(\n  data = df4,\n  aes(colour = Product),\n  interactive = FALSE, # try TRUE\n  rescale = F, # rescale = TRUE makes it look different...try!!\n  title = \"Using ggiraphExtra\"\n) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Radar Plots\n\n\n\n\nDifferences in scores for a given item across several aspect or parameters are readily apparent.\nThese can also be compared, parameter for parameter, with more than one item\nthe same set of data at two different aspects is very quickly apparent\nData is clearly in wide form\nBoth ggradar and ggiraphExtra render very similar-looking radar charts and the syntax is not too intimidating!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#wait-but-why",
    "title": "\n Ratings and Rankings",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBump Charts can show changes in Rating and Ranking over time, or some other Qual variable too!\nLollipop Charts are useful in comparing multiple say products or services, with only one aspect for comparison, or which defines the rank\nRadar Charts are also useful in comparing multiple say products or services, but against several aspects or parameters for simultaneous comparisons.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#conclusion",
    "title": "\n Ratings and Rankings",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nThese are easy and simple charts to use and are easily understood too\nBear in mind the data structure requirements for different charts/packages: Wide vs Long.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#your-turn",
    "title": "\n Ratings and Rankings",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake the HELPrct dataset from our well used mosaicData package. Plot ranking charts using each of the public health issues that you can see in that dataset. What choice will you make for the the axes?\nTry the SaratogaHouses dataset also from mosaicData.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#references",
    "title": "\n Ratings and Rankings",
    "section": "\n References",
    "text": "References\n\nHighcharts Blog. Why you need to start using dumbbell chartshttps://github.com/hrbrmstr/ggalt#lollipop-charts\n\nSee this use of Radar Charts in Education. Choose the country/countries of choice and plot their ranks on various educational parameters in a radar chart. https://gpseducation.oecd.org/Home\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggalt\n0.4.0\nRudis, Bolker, and Schulz (2017)\n\n\nggbump\n0.1.0\nSjoberg (2020)\n\n\nggiraphExtra\n0.3.0\nMoon (2020)\n\n\nggradar\n0.2\nBion (2025)\n\n\n\n\n\n\nBion, Ricardo. 2025. ggradar: Create Radar Charts Using Ggplot2. https://github.com/ricardo-bion/ggradar.\n\n\nMoon, Keon-Woong. 2020. ggiraphExtra: Make Interactive “ggplot2.” Extension to “ggplot2” and “ggiraph”. https://doi.org/10.32614/CRAN.package.ggiraphExtra.\n\n\nRudis, Bob, Ben Bolker, and Jan Schulz. 2017. ggalt: Extra Coordinate Systems, “Geoms,” Statistical Transformations, Scales and Fonts for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggalt.\n\n\nSjoberg, David. 2020. ggbump: Bump Chart and Sigmoid Curves. https://doi.org/10.32614/CRAN.package.ggbump.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNotePair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nNoteHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  mosaic::cor() %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;%\n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n)\ncor_test(height ~ mother, data = Galton %&gt;%\n  filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height.\n\n\n\n\n\n\n\n\n\nNoteCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;%\n  e_charts(predictor) %&gt;%\n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;%\n  e_y_axis(\n    name = \"Correlation with `height`\",\n    nameLocation = \"middle\", nameGap = 35\n  ) %&gt;%\n  e_x_axis(\n    name = \"Parameter\", nameLocation = \"center\",\n    nameGap = 35, type = \"category\"\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\nall_corrs %&gt;%\n  mutate(sd = (conf.high - conf.low) / 2) %&gt;%\n  plot_ly() %&gt;%\n  add_bars(\n    y = ~estimate, x = ~predictor,\n    error_y = ~ list(array = sd, color = \"black\")\n  )\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(father, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ father, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"father\", nameLocation = \"middle\", nameGap = 35,\n    min = 60, max = 80\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n# for the mother\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(mother, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ mother, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n    min = 55, max = 75\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nNoteGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r.\n\n\n\nWe will “live code” this in class!\n\nWe have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Let us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNotePair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nNoteHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  mosaic::cor() %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;%\n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n)\ncor_test(height ~ mother, data = Galton %&gt;%\n  filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "NoteCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;%\n  e_charts(predictor) %&gt;%\n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;%\n  e_y_axis(\n    name = \"Correlation with `height`\",\n    nameLocation = \"middle\", nameGap = 35\n  ) %&gt;%\n  e_x_axis(\n    name = \"Parameter\", nameLocation = \"center\",\n    nameGap = 35, type = \"category\"\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\nall_corrs %&gt;%\n  mutate(sd = (conf.high - conf.low) / 2) %&gt;%\n  plot_ly() %&gt;%\n  add_bars(\n    y = ~estimate, x = ~predictor,\n    error_y = ~ list(array = sd, color = \"black\")\n  )\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(father, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ father, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"father\", nameLocation = \"middle\", nameGap = 35,\n    min = 60, max = 80\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n# for the mother\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(mother, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ mother, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n    min = 55, max = 75\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nNoteGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will “live code” this in class!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html",
    "title": "\n Parts of a Whole",
    "section": "",
    "text": "“There is no such thing as a”self-made” man. We are made up of thousands of others. Everyone who has ever done a kind deed for us, or spoken one word of encouragement to us, has entered into the make-up of our character and of our thoughts.”\n— George Matthew Adams, newspaper columnist (23 Aug 1878-1962)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#setting-up-the-packages",
    "title": "\n Parts of a Whole",
    "section": "\n Setting up the Packages",
    "text": "Setting up the Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\n# devtools::install_github(\"zmeers/ggparliament\")\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\nlibrary(data.tree) # Many plots related to heirarchical data\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#what-graphs-will-we-see-today",
    "title": "\n Parts of a Whole",
    "section": "\n What Graphs will we see today?",
    "text": "What Graphs will we see today?\nThere are a good few charts available to depict things that constitute other bigger things. We will discuss a few of these: Pie, Fan, and Donuts; Waffle and Parliament charts; Trees, Dendrograms, and Circle Packings. (The last three visuals we will explore along with network diagrams in a later module.)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#pies-and-fans",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#pies-and-fans",
    "title": "\n Parts of a Whole",
    "section": "\n Pies and Fans",
    "text": "Pies and Fans\nSo let us start with “eating humble pie”: discussing a Pie chart first.\nA pie chart is a circle divided into sectors that each represent a proportion of the whole. It is often used to show percentage, where the sum of the sectors equals 100%.\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nAnd we will also see that there is an attractive, and similar-looking alternative, called a fan chart which we will explore here.\n\n\nUsing Base R\nUsing ggformula\nUsing echarts4r\n\n\n\nBase R has a simple pie command that does the job. Let’s create some toy data first:\n\n\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n\n  # Labels MUST be character entries for `pie` to work\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\",\n    \"Other\", \"Vanilla Cream\"\n  )\n)\npie_data\n\n\n  \n\n\npie(\n  x = pie_data$sales,\n  labels = pie_data$labels, # Character Vector is a MUST\n\n  # Pie is within a square of 1 X 1 units\n  # Reduce radius if needed to see labels properly\n  radius = 0.95,\n  init.angle = 90, # First slice starts at 12 o'clock position\n\n  # Change the default colours. Comment this and see what happens.\n  col = grDevices::hcl.colors(palette = \"Plasma\", n = 6)\n)\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe create a bar chart or a column chart as appropriate, with bars filled by category. The width parameter is set to 1 so that the bars touch. The bars have a fixed width along the x-axis; the height of the bar varies based on the number we wish to show. Then the coord_polar(theta = \"y\") converts the bar plot into a pie.\n# Using gf_col since we have a count/value column already\npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~labels, width = 1, color = \"black\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~labels, width = 1, color = \"black\") %&gt;%\n  gf_refine(coord_polar(theta = \"y\")) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n# Using gf_bar since we don't have ready made counts\ngf_bar(\n  data = mpg,\n  ~1,\n  fill = ~drv,\n  color = \"black\", # border for the bars/slices\n  width = 1\n) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\ngf_bar(\n  data = mpg,\n  ~0.5,\n  fill = ~drv,\n  color = \"black\", # border for the bars/slices\n  width = 1\n) %&gt;%\n  gf_refine(coord_polar(theta = \"y\")) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a basic interactive pie chart withecharts4r:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\",\n    \"Vanilla Cream\"\n  )\n)\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90\n  ) %&gt;%\n  e_legend(list(\n    orient = \"vertical\",\n    left = \"right\"\n  )) %&gt;%\n  e_tooltip()\n\n\n\n\n\nWe can add more bells and whistles to the humble-pie chart, and make a Nightingale rosechart out of it:\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\",\n    \"Vanilla Cream\"\n  )\n)\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90,\n    roseType = \"area\"\n  ) %&gt;% # try \"radius\"\n\n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90,\n    roseType = \"radius\"\n  ) %&gt;%\n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nFor more information and customization look at https://echarts.apache.org/en/option.html#series-pie\n\n\n\nThe fan Plot\nThe fan plot (from the plotrix package) displays numerical values as arcs of overlapping sectors. This allows for more effective comparison:\n\n\n\nplotrix::fan.plot(\n  x = pie_data$sales,\n  labels = pie_data$labels,\n  col = grDevices::hcl.colors(palette = \"Lajolla\", n = 6), # Try hcl.pals()\n  shrink = 0.03,\n  # How much to shrink each successive sector\n\n  label.radius = 1.15,\n  main = \"Fan Plot of Ice Cream Flavours\",\n  # ticks = 360,\n  # if we want tick marks on the circumference\n\n  max.span = pi\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no fan plot possible with echarts4r, as far as I know.\nThe Donut Chart\nThe donut chart suffers from the same defects as the pie, so should be used with discretion. The donut chart is essentially a gf_rect from ggformula, plotted on a polar coordinate set of of axes:\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\nLet us make some toy data:\n# Data\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\n\ndf &lt;-\n  df %&gt;%\n  dplyr::mutate(\n    fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\n\ndf\ndf %&gt;%\n  # gf_rect() formula: ymin + ymax ~ xmin + xmax\n  # Bars with varying thickness (y) proportional to data\n  # Fixed length x (2 to 4)\n  gf_rect(ymin + ymax ~ 2 + 4,\n    fill = ~group, colour = \"black\"\n  ) %&gt;%\n  gf_label(labelPosition ~ 3.5,\n    label = ~label, colour = \"black\",\n    size = 4\n  ) %&gt;%\n  # When switching to polar coords:\n  # x maps to radius\n  # y maps to angle theta\n  # so we create a \"hole\" in the radius, in x\n  gf_refine(coord_polar(\n    theta = \"y\",\n    direction = 1\n  )) %&gt;%\n  # Up to here will give us a pie chart\n\n  # Now to create the hole\n  # try to play with the \"0\"\n  # Recall x = [2,4]\n  gf_refine(xlim(c(-2, 5)), scale_fill_brewer(palette = \"Spectral\")) %&gt;%\n  gf_theme(theme = theme_void()) %&gt;%\n  gf_theme(legend.position = \"none\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nThe donut chart is simply a variant of the pie chart in echarts4r:\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\n\ndf &lt;-\n  df %&gt;%\n  dplyr::mutate(\n    fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\ndf\ndf %&gt;%\n  e_charts(x = group, width = 400) %&gt;%\n  e_pie(\n    serie = value,\n    clockwise = TRUE,\n    startAngle = 90,\n    radius = c(\"50%\", \"70%\")\n  ) %&gt;%\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#waffle-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#waffle-charts",
    "title": "\n Parts of a Whole",
    "section": "\n Waffle Charts",
    "text": "Waffle Charts\nWaffle charts are often called “square pie charts” !\nHere we will need to step outside of ggformula and get into ggplot itself momentarily. (Always remember that ggformula is a simplified and intuitive method that runs on top of ggplot.) We will use the waffle package.\n\n\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\n# Data\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\ndf\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Waffle plot\n# Using ggplot, sadly not yet ggformula\n\n\nggplot(df, aes(fill = group, values = value)) +\n  geom_waffle(\n    n_rows = 8,\n    size = 0.33,\n    colour = \"white\",\n    na.rm = TRUE\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  labs(\n    title = \"Waffle Chart\",\n    subtitle = \"A square pie chart\",\n    caption = \"Source: Toy Data\"\n  ) +\n  coord_equal()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#parliament-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#parliament-charts",
    "title": "\n Parts of a Whole",
    "section": "\n Parliament Charts",
    "text": "Parliament Charts\nThe package ggpol offers an interesting visualization in the shape of a array of “seats” in a parliament. (There is also a package called ggparliament which in my opinion is a bit cumbersome, having a two-step procedure to convert data into “parliament form” etc. )\n\n\n\n# Same toy dataset\n# df &lt;- tibble(group = LETTERS[1:3],\n#                  value = c(25, 20, 35))\n#\n# Parliament Plot\nggplot(df) +\n  ggpol::geom_parliament(\n    aes(\n      seats = value,\n      fill = group\n    ),\n    r0 = 2, # inner radius\n    r1 = 4 # Outer radius\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  labs(\n    title = \"Parliament Chart\",\n    subtitle = \"A circular array of seats\",\n    caption = \"Source: Toy Data\"\n  ) +\n  coord_equal()",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#trees-dendrograms-and-circle-packings",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#trees-dendrograms-and-circle-packings",
    "title": "\n Parts of a Whole",
    "section": "Trees, Dendrograms, and Circle Packings",
    "text": "Trees, Dendrograms, and Circle Packings\nThere are still more esoteric plots to explore, if you are hell-bent on startling people ! There is an R package called ggraph, that can do these charts, and many more:\n\nggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar.\n\nWe will explore these charts when we examine network diagrams. For now, we can quickly see what these diagrams look like. Although the R-code is visible to you, it may not make sense at the moment!\n\n Dendrograms\nFrom the R Graph Gallery Website :\n\nDendrograms can be built from:\n\nHierarchical dataset: think about a CEO managing team leads managing employees and so on.\nClustering result: clustering divides a set of individuals in group according to their similarity. Its result can be visualized as a tree.\n\n\n\n\n\n# create an edge list data frame giving the hierarchical structure of your individuals\nd1 &lt;- tibble(from = \"origin\", to = paste(\"group\", seq(1, 5), sep = \"\"))\nd2 &lt;- tibble(from = rep(d1$to, each = 5), to = paste(\"subgroup\", seq(1, 25), sep = \"_\"))\nedges &lt;- rbind(d1, d2)\nedges\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Create a graph object\nmygraph1 &lt;- tidygraph::as_tbl_graph(edges)\nmygraph1\n\n\n\n\n# A tbl_graph: 31 nodes and 30 edges\n#\n# A rooted tree\n#\n# Node Data: 31 × 1 (active)\n   name      \n   &lt;chr&gt;     \n 1 origin    \n 2 group1    \n 3 group2    \n 4 group3    \n 5 group4    \n 6 group5    \n 7 subgroup_1\n 8 subgroup_2\n 9 subgroup_3\n10 subgroup_4\n# ℹ 21 more rows\n#\n# Edge Data: 30 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     2\n2     1     3\n3     1     4\n# ℹ 27 more rows\n\n\n\n\n\n\n\n# Basic tree\nggraph(mygraph1,\n  layout = \"dendrogram\",\n  circular = TRUE\n) +\n  geom_edge_diagonal() +\n  geom_node_point(size = 3) +\n  geom_node_label(aes(label = name),\n    size = 3, repel = TRUE\n  ) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# create a data frame\ndata &lt;- tibble(\n  level1 = \"CEO\",\n  level2 = c(rep(\"boss1\", 4), rep(\"boss2\", 4)),\n  level3 = paste0(\"mister_\", letters[1:8])\n)\n\n# transform it to a edge list!\nedges_level1_2 &lt;- data %&gt;%\n  select(level1, level2) %&gt;%\n  unique() %&gt;%\n  rename(from = level1, to = level2)\n\nedges_level2_3 &lt;- data %&gt;%\n  select(level2, level3) %&gt;%\n  unique() %&gt;%\n  rename(from = level2, to = level3)\n\nedge_list &lt;- rbind(edges_level1_2, edges_level2_3)\nedge_list\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nmygraph2 &lt;- as_tbl_graph(edge_list)\nmygraph2\n\n\n\n\n# A tbl_graph: 11 nodes and 10 edges\n#\n# A rooted tree\n#\n# Node Data: 11 × 1 (active)\n   name    \n   &lt;chr&gt;   \n 1 CEO     \n 2 boss1   \n 3 boss2   \n 4 mister_a\n 5 mister_b\n 6 mister_c\n 7 mister_d\n 8 mister_e\n 9 mister_f\n10 mister_g\n11 mister_h\n#\n# Edge Data: 10 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     2\n2     1     3\n3     2     4\n# ℹ 7 more rows\n\n\n\n\n\n\n\n# Now we can plot that\nggraph(mygraph2, layout = \"dendrogram\", circular = FALSE) +\n  geom_edge_diagonal() +\n  geom_node_point(size = 3) +\n  geom_node_label(aes(label = name), repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nCircle Packing\n\n\n\ngraph_flare &lt;- tbl_graph(flare$vertices, flare$edges)\ngraph_flare\n\n\n\n\n# A tbl_graph: 252 nodes and 251 edges\n#\n# A rooted tree\n#\n# Node Data: 252 × 3 (active)\n   name                                            size shortName            \n   &lt;chr&gt;                                          &lt;dbl&gt; &lt;chr&gt;                \n 1 flare.analytics.cluster.AgglomerativeCluster    3938 AgglomerativeCluster \n 2 flare.analytics.cluster.CommunityStructure      3812 CommunityStructure   \n 3 flare.analytics.cluster.HierarchicalCluster     6714 HierarchicalCluster  \n 4 flare.analytics.cluster.MergeEdge                743 MergeEdge            \n 5 flare.analytics.graph.BetweennessCentrality     3534 BetweennessCentrality\n 6 flare.analytics.graph.LinkDistance              5731 LinkDistance         \n 7 flare.analytics.graph.MaxFlowMinCut             7840 MaxFlowMinCut        \n 8 flare.analytics.graph.ShortestPaths             5914 ShortestPaths        \n 9 flare.analytics.graph.SpanningTree              3416 SpanningTree         \n10 flare.analytics.optimization.AspectRatioBanker  7074 AspectRatioBanker    \n# ℹ 242 more rows\n#\n# Edge Data: 251 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1   221     1\n2   221     2\n3   221     3\n# ℹ 248 more rows\n\n\n\n\n\n\n\nset.seed(1)\nggraph(graph_flare, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = as_factor(depth)), size = 0.25, n = 50) +\n  coord_fixed() +\n  scale_fill_brewer(name = \"Depth\", palette = \"Set1\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#your-turn",
    "title": "\n Parts of a Whole",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nUse the penguins dataset from the palmerpenguins package and plot pies, fans, and donuts as appropriate.\nLook at the whigs and highschool datasets in the package ggraph. Plot Pies, Fans and if you are feeling confident, Trees, Dendrograms, and Circle Packings as appropriate for these.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#references",
    "title": "\n Parts of a Whole",
    "section": "\n References",
    "text": "References\n\nIaroslava.2020. A Parliament Diagram in R, https://datavizstory.com/a-parliament-diagram-in-r/\n\nVenn Diagrams in R, Venn diagram in ggplot2 | R CHARTS (r-charts.com)\n\nGenerate icon-array charts without code! https://iconarray.com\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ndata.tree\n1.1.0\nGlur (2023)\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggparliament\n3.1.6\nHickman, Meers, and Leeper (2024)\n\n\nggpol\n0.0.7\nTiedemann (2020)\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nplotrix\n3.8.4\nJ (2006)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nwaffle\n1.0.2\nRudis and Gandy (2023)\n\n\n\n\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://doi.org/10.32614/CRAN.package.echarts4r.\n\n\nGlur, Christoph. 2023. data.tree: General Purpose Hierarchical Data Structure. https://doi.org/10.32614/CRAN.package.data.tree.\n\n\nHickman, Robert, Zoe Meers, and Thomas J. Leeper. 2024. ggparliament: Parliament Plots. https://github.com/zmeers/ggparliament.\n\n\nJ, Lemon. 2006. “Plotrix: A Package in the Red Light District of r.” R-News 6 (4): 8–12.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nRudis, Bob, and Dave Gandy. 2023. waffle: Create Waffle Chart Visualizations. https://doi.org/10.32614/CRAN.package.waffle.\n\n\nTiedemann, Frederik. 2020. ggpol: Visualizing Social Science Data with “ggplot2”. https://doi.org/10.32614/CRAN.package.ggpol.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "title": "\n Proportions",
    "section": "",
    "text": "…“Thinking is difficult, that’s why most people judge.”\n— C.G. Jung",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "title": "\n Proportions",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\n\nlibrary(resampledata) # More datasets\n\nlibrary(GGally) # Correlation Plots\nlibrary(visStatistics) # Comprehensive all-in-one stats viz/test package\nlibrary(ca) # Correspondence Analysis, for use some day\n\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n#\n\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(tidyverse)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-graphs-will-we-see-today",
    "title": "\n Proportions",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nPies, and Mosaic Charts",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Proportions",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "title": "\n Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nTo recall, a categorical variable is one for which the possible measured or assigned values consist of a discrete set of categories, which may be ordered or unordered. Some typical examples are:\n\nGender, with categories “Male,” “Female.”\nMarital status, with categories “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.”\nFielding position (in baseball cricket), with categories “Slips,”Cover “,”Mid-off “Deep Fine Leg”, “Close-in”, “Deep”…\nSide effects (in a pharmacological study), with categories “None,” “Skin rash,” “Sleep disorder,” “Anxiety,” . . ..\nPolitical attitude, with categories “Left,” “Center,” “Right.”\nParty preference (in India), with categories “BJP” “Congress,” “AAP,” “TMC”…\nTreatment outcome, with categories “no improvement,” “some improvement,” or “marked improvement.”\nAge, with categories “0–9,” “10–19,” “20–29,” “30–39,” . . . .\nNumber of children, with categories 0, 1, 2, . . . .\n\nAs these examples suggest, categorical variables differ in the number of categories: we often distinguish binary variables (or dichotomous variables) such as Gender from those with more than two categories (called polytomous variables).",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#inspiration",
    "title": "\n Proportions",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Obesity across the World\n\n\n\n\n\n\n\n\n\n(b) Covid Deaths https://datatopics.worldbank.org/sdgatlas/goal-3-good-health-and-well-being?lang=en\n\n\n\n\n\n\nFigure 1: Depicting Proportions\n\n\nFrom Figure 1 (a), it is seen that Egypt, Qatar, and the United States are the only countries with a population greater than 1 million on this list. Poor food habits are once again a factor, with some cultural differences. In Egypt, high food inflation has pushed residents to low-cost high-calorie meals. To combat food insecurity, the government subsidizes bread, wheat flour, sugar and cooking oil, many of which are the ingredients linked to weight gain. In Qatar, a country with one of the highest per capita GDPs in the world, a genetic predisposition towards obesity and sedentary lifestyles worsen the impact of rich diets. And in the U.S., bigger portions are one of the many reasons cited for rampant adult and child obesity. For example, Americans ate 20% more calories in the year 2000 than they did in 1983. They consume 195 lbs of meat annually compared to 138 lbs in 1953. And their grain intake has increased 45% since 1970.\nIt’s worth noting however that this dataset is based on BMI values, which do not fully account for body types with larger bone and muscle mass.\nFrom Figure 1 (b), according to World Bank, six countries (India, Russia, Indonesia, United States, Brazil, and Mexico) accounted for over 60 percent of the total additional deaths in the first two years of the pandemic.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#how-do-these-charts-work",
    "title": "\n Proportions",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nWe saw with Bar Charts that when we deal with single Qual variables, we perform counts for each level of the variable. For a single Qual variable, even with multiple levels ( e.g. Education Status: High school, College, Post-Graduate, PhD), we can count the observations as with Bar Charts and plot Pies.\nWe can also plot Pie Charts when the number of levels in a single Qual variable are not too many. Almost always, a Bar chart is preferred. The problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in design and business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nWhat if there are two Quals? Or even more? The answer is to take them pair-wise, make all combinations of levels for both and calculate counts for these. This is called a Contingency Table. Then we plot that table. We’ll see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "title": "\n Proportions",
    "section": "\n Categorical Data",
    "text": "Categorical Data\nFrom the {vcd package} vignette:\n\nThe first thing you need to know is that categorical data can be represented in three different forms in R, and it is sometimes necessary to convert from one form to another, for carrying out statistical tests, fitting models or visualizing the results.\n\n\nCase Data\nFrequency Data\nCross-Tabular Count Data\n\nLet us first see examples of each.\n\n\nCase Form\nFrequency Data Form\nTable form\n\n\n\nFrom Michael Friendly Discrete Data Analysis and Visualization :\n\nIn many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Containing individual observations with one or more categorical factors, used as classifying variables. The total number of observations is nrow(X), and the number of variables is ncol(X).\n\n\n\n R\n web-r\n\n\n\n\nclass(Arthritis)\n\n[1] \"data.frame\"\n\n# Tibble as HTML for presentation\nArthritis %&gt;%\n  head(10) %&gt;%\n  tt(theme = \"striped\", caption = \"Arthritis Treatments and Effects&lt;br&gt; First 10 Observations\", centering = TRUE)\n\n\n\n    \n\n      \n\nArthritis Treatments and Effects First 10 Observations\n              \nID\n                Treatment\n                Sex\n                Age\n                Improved\n              \n\n\n\n57\n                  Treated\n                  Male\n                  27\n                  Some\n                \n\n46\n                  Treated\n                  Male\n                  29\n                  None\n                \n\n77\n                  Treated\n                  Male\n                  30\n                  None\n                \n\n17\n                  Treated\n                  Male\n                  32\n                  Marked\n                \n\n36\n                  Treated\n                  Male\n                  46\n                  Marked\n                \n\n23\n                  Treated\n                  Male\n                  58\n                  Marked\n                \n\n75\n                  Treated\n                  Male\n                  59\n                  None\n                \n\n39\n                  Treated\n                  Male\n                  59\n                  Marked\n                \n\n33\n                  Treated\n                  Male\n                  63\n                  None\n                \n\n55\n                  Treated\n                  Male\n                  63\n                  None\n                \n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe Arthritis data set has three factors and two integer variables. One of the three factors Improved is an ordered factor.\n\nID\nTreatment: a factor; Placebo or Treated\nSex: a factor, M / F\nAge: integer\nImproved: Ordinal factor; None &lt; Some &lt; Marked\n\nEach row in the Arthritis dataset is a separate case or observation.\n\n\nData in frequency form has already been tabulated and aggregated by counting over the (combinations of) categories of the table variables. When the data are in case form, we can always trace any observation back to its individual identifier or data record, since each row is a unique observation or case; the reverse, with the Frequency Form is rarely possible.\nFrequency Data is usually a data frame, with columns of categorical variables and at least one column containing frequency or count information.\n\n\n R\n web-r\n\n\n\n\n\n\nstr(GSS)\n\n# Tibble as HTML for presentation\nGSS %&gt;%\n  tt(\n    theme = \"striped\", caption = \"General Social Survey\",\n    centering = TRUE\n  )\n\n\n\n\n'data.frame':   6 obs. of  3 variables:\n $ sex  : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2\n $ party: Factor w/ 3 levels \"dem\",\"indep\",..: 1 1 2 2 3 3\n $ count: num  279 165 73 47 225 191\n\n\n\n\n    \n\n      \n\nGeneral Social Survey\n              \nsex\n                party\n                count\n              \n\n\n\nfemale\n                  dem\n                  279\n                \n\nmale\n                  dem\n                  165\n                \n\nfemale\n                  indep\n                  73\n                \n\nmale\n                  indep\n                  47\n                \n\nfemale\n                  rep\n                  225\n                \n\nmale\n                  rep\n                  191\n                \n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nRespondents in the GSS survey were classified by sex and party identification. As can be seen, there is a count for every combination of the two categorical variables, sex and party.\n\n\nTable Form Data can be a matrix, array or table object, whose elements are the frequencies in an n-way table. The variable names (factors) and their levels are given by dimnames(X).\n\n\n\nHairEyeColor\nclass(HairEyeColor)\n\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n[1] \"table\"\n\n\n\nHairEyeColor is a “two-way” table, consisting of two tables, one for Sex = Female and the other for Sex = Male. The total number of observations is sum(X). The number of dimensions of the table is length(dimnames(X)), and the table sizes are given by sapply(dimnames(X), length). The data looks like a n-dimensional cube and needs n-way tables to represent.\n\nsum(HairEyeColor)\n\n[1] 592\n\ndimnames(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Brown\" \"Red\"   \"Blond\"\n\n$Eye\n[1] \"Brown\" \"Blue\"  \"Hazel\" \"Green\"\n\n$Sex\n[1] \"Male\"   \"Female\"\n\nsapply(dimnames(HairEyeColor), length)\n\nHair  Eye  Sex \n   4    4    2 \n\n\nA good way to think of tabular data is to think of a Rubik’s Cube.\n\n\n\n\n\nRubik’s Cube model for Multi-Table Data\n\n\n\n\n\n\n\nTipRubik’s Cube and Categorical Data Tables\n\n\n\nEach of the edges is an Ordinal Variable, each segment represents a level in the variable. So each face of the Cube represents two ordinal variables. Any segment is at the intersection of two (independent) levels of two variables, and the colour may be visualized as a count. This array of counts on a face is a 2D or 2-Way Table. ( More on this later )\n\n\nSince we can only print 2D tables, we hold one face in front and the image we see is a 2-Way Table. Turning the Cube by 90 degrees gives us another face with 2 variables, with one variable in common with the previous face. If we consider two faces together, we get two 2-way tables, effectively allowing us to contemplate 3 categorical variables.\nMultiple 2-Way tables can be flattened into a single long table that contains all counts for all combinations of categorical variables. This can be visualized as “opening up” and laying flat the Rubik’s cube, as with a cardboard model of it.\n\nftable(HairEyeColor)\n\n            Sex Male Female\nHair  Eye                  \nBlack Brown       32     36\n      Blue        11      9\n      Hazel       10      5\n      Green        3      2\nBrown Brown       53     66\n      Blue        50     34\n      Hazel       25     29\n      Green       15     14\nRed   Brown       10     16\n      Blue        10      7\n      Hazel        7      7\n      Green        7      7\nBlond Brown        3      4\n      Blue        30     64\n      Hazel        5      5\n      Green        8      8\n\n\nFinally, we may need to convert the (multiple) tables into a data frame or tibble:\n## Convert the two tables into a data frame\nHairEyeColor %&gt;%\n  as_tibble()\n# Tibble as HTML for presentation\nHairEyeColor %&gt;%\n  as_tibble() %&gt;% # Convert\n  tt(\n    theme = \"striped\", caption = \"Hair Eye and Color&lt;br&gt; as a Data Frame\",\n    centering = TRUE\n  )\n\n\n\n\n  \n\n\n\n\n\n    \n\n      \n\nHair Eye and Color as a Data Frame\n              \nHair\n                Eye\n                Sex\n                n\n              \n\n\n\nBlack\n                  Brown\n                  Male\n                  32\n                \n\nBrown\n                  Brown\n                  Male\n                  53\n                \n\nRed\n                  Brown\n                  Male\n                  10\n                \n\nBlond\n                  Brown\n                  Male\n                  3\n                \n\nBlack\n                  Blue\n                  Male\n                  11\n                \n\nBrown\n                  Blue\n                  Male\n                  50\n                \n\nRed\n                  Blue\n                  Male\n                  10\n                \n\nBlond\n                  Blue\n                  Male\n                  30\n                \n\nBlack\n                  Hazel\n                  Male\n                  10\n                \n\nBrown\n                  Hazel\n                  Male\n                  25\n                \n\nRed\n                  Hazel\n                  Male\n                  7\n                \n\nBlond\n                  Hazel\n                  Male\n                  5\n                \n\nBlack\n                  Green\n                  Male\n                  3\n                \n\nBrown\n                  Green\n                  Male\n                  15\n                \n\nRed\n                  Green\n                  Male\n                  7\n                \n\nBlond\n                  Green\n                  Male\n                  8\n                \n\nBlack\n                  Brown\n                  Female\n                  36\n                \n\nBrown\n                  Brown\n                  Female\n                  66\n                \n\nRed\n                  Brown\n                  Female\n                  16\n                \n\nBlond\n                  Brown\n                  Female\n                  4\n                \n\nBlack\n                  Blue\n                  Female\n                  9\n                \n\nBrown\n                  Blue\n                  Female\n                  34\n                \n\nRed\n                  Blue\n                  Female\n                  7\n                \n\nBlond\n                  Blue\n                  Female\n                  64\n                \n\nBlack\n                  Hazel\n                  Female\n                  5\n                \n\nBrown\n                  Hazel\n                  Female\n                  29\n                \n\nRed\n                  Hazel\n                  Female\n                  7\n                \n\nBlond\n                  Hazel\n                  Female\n                  5\n                \n\nBlack\n                  Green\n                  Female\n                  2\n                \n\nBrown\n                  Green\n                  Female\n                  14\n                \n\nRed\n                  Green\n                  Female\n                  7\n                \n\nBlond\n                  Green\n                  Female\n                  8",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#simple-plots-for-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#simple-plots-for-categorical-data",
    "title": "\n Proportions",
    "section": "\n Simple Plots for Categorical Data",
    "text": "Simple Plots for Categorical Data\n\nWe have already examined Bar Charts.\nPie Charts are discussed here.\nThese are both good for single Qual variables. Bars are more suited when there are many levels and/or when there is more than one Qual variable, as discussed earlier.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plotting-nested-proportions",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plotting-nested-proportions",
    "title": "\n Proportions",
    "section": "\n Plotting Nested Proportions",
    "text": "Plotting Nested Proportions\nWhen we want to visualize proportions based on Multiple Qual variables, we are looking at what Claus Wilke calls nested proportions: groups within groups. Making counts with combinations of levels for two Qual variables gives us a data structure called a Contingency Table, which we will use to build our plot for nested proportions. The Statistical tests for Proportions ( the \\(\\chi^2\\) test ) also needs Contingency Tables. The Frequency Table we encountered earlier is very close to being a full-fledged Contingency Table; one only needs to add the margin counts! So what is a Contingency Table?\n\n Creating Contingency Tables\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts. More precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\nIn this section we understand how to make Contingency Tables from each of the three forms. We will use vcd, mosaic and the tidyverse packages for our purposes. Then we will see how they can be visualized.\n\n\nUsing mosaic\nUsing base R\nUsing vcd\nUsing tidyverse\n\n\n\nI think this is the simplest and most elegant way of obtaining Contingency Tables:\n\ndata(\"GSS2002\", package = \"resampledata\")\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\n##\nmosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nPlotting this as an HTML table, we get:\n\n\n\n\n\n    \n\n      \n\nGSS Social Survey\n              \nDeathPenalty\n                Left HS\n                HS\n                Jr Col\n                Bachelors\n                Graduate\n                Sum\n              \n\n\n\nFavor\n                  117\n                  511\n                  71\n                  135\n                  64\n                  898\n                \n\nOppose\n                  72\n                  200\n                  16\n                  71\n                  50\n                  409\n                \n\nSum\n                  189\n                  711\n                  87\n                  206\n                  114\n                  1307\n                \n\n\n\n\n\n\nFigure 2: Contingency Table Picture\n\n\n\nHow was this computed?\nSo \\(117\\) is the number of people who Left HS and Favor the death penalty, and \\(71\\) is the count for Bachelors who Oppose the death penalty. And so on.\n\n\n\n# One Way Table ( one variable )\ntable(Arthritis$Treatment) # Contingency Table\n\n\nPlacebo Treated \n     43      41 \n\n# 1-way Contingency Table\ntable(Arthritis$Treatment) %&gt;% addmargins() # Contingency Table with margins\n\n\nPlacebo Treated     Sum \n     43      41      84 \n\n# 2-Way Contingency Tables\n# Choosing Treatment and Improved\ntable(Arthritis$Treatment, Arthritis$Improved) %&gt;% addmargins()\n\n         \n          None Some Marked Sum\n  Placebo   29    7      7  43\n  Treated   13    7     21  41\n  Sum       42   14     28  84\n\n# Choosing Treatment and Sex\ntable(Arthritis$Sex, Arthritis$Improved) %&gt;% addmargins()\n\n        \n         None Some Marked Sum\n  Female   25   12     22  59\n  Male     17    2      6  25\n  Sum      42   14     28  84\n\n\nWe can use table() ( and also xtabs() ) to generate multi-dimensional tables too (More than 2-way) These will be printed out as a series of 2D tables, one for each value/level of the “third” parameter. We can then flatten this set of tables using ftable() and add margins to convert into a Contingency Table:\n\nmy_arth_table &lt;- table(Arthritis$Treatment, Arthritis$Sex, Arthritis$Improved)\nmy_arth_table\n\n, ,  = None\n\n         \n          Female Male\n  Placebo     19   10\n  Treated      6    7\n\n, ,  = Some\n\n         \n          Female Male\n  Placebo      7    0\n  Treated      5    2\n\n, ,  = Marked\n\n         \n          Female Male\n  Placebo      6    1\n  Treated     16    5\n\n# Now flatten\nftable(my_arth_table)\n\n                None Some Marked\n                                \nPlacebo Female    19    7      6\n        Male      10    0      1\nTreated Female     6    5     16\n        Male       7    2      5\n\nftable(my_arth_table) %&gt;% addmargins()\n\n             Sum\n    19  7  6  32\n    10  0  1  11\n     6  5 16  27\n     7  2  5  14\nSum 42 14 28  84\n\n\nA bit strange that the column labels disappear in the ftable when margins are added…maybe need to investigate the FUN argument to add_margins().\n\n\nThe vcd ( Visualize Categorical Data ) package by Michael Friendly has a convenient function to create Contingency Tables: structable(); this function produces a ‘flat’ representation of a high-dimensional contingency table constructed by recursive splits (similar to the construction of mosaic charts/graphs). structable tends to render flat tables, of the kind that can be thought of as a “text representation” of the vcd::mosaic plot:\nThe arguments of structable are:\n\na formula \\(y + p \\sim x + z\\) which shows which variables are to be included as columns and rows respectively on a table;\na data argument, which can indicate a data frame from where the variables are drawn.\n\narth_vcd &lt;- vcd::structable(data = Arthritis, Treatment ~ Improved)\narth_vcd\nclass(arth_vcd)\narth_vcd %&gt;% addmargins()\n\n\n\n         Treatment Placebo Treated\nImproved                          \nNone                    29      13\nSome                     7       7\nMarked                   7      21\n[1] \"structable\" \"ftable\"    \n        Treatment\nImproved Placebo Treated Sum\n  None        29      13  42\n  Some         7       7  14\n  Marked       7      21  28\n  Sum         43      41  84\n\n\n\n# With Margins\narth_vcd %&gt;%\n  as.matrix() %&gt;%\n  addmargins()\n\n\n\n        Treatment\nImproved Placebo Treated Sum\n  None        29      13  42\n  Some         7       7  14\n  Marked       7      21  28\n  Sum         43      41  84\n\n\n\n# HairEyeColor is in multiple table form\nHairEyeColor\n# structable flattens these into one, as for a mosaic chart\nvcd::structable(HairEyeColor)\n# As tibble\nvcd::structable(HairEyeColor) %&gt;% as_tibble()\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n             Eye Brown Blue Hazel Green\nHair  Sex                              \nBlack Male          32   11    10     3\n      Female        36    9     5     2\nBrown Male          53   50    25    15\n      Female        66   34    29    14\nRed   Male          10   10     7     7\n      Female        16    7     7     7\nBlond Male           3   30     5     8\n      Female         4   64     5     8\n\n\n\n\n\n  \n\n\n\n\nUCBAdmissions is already in Frequency Form i.e. a Contingency Table. But it is a set of (two-way) Contingency Tables:\nUCBAdmissions\n###\nvcd::structable(UCBAdmissions)\n###\nstructable(UCBAdmissions) %&gt;%\n  as.matrix() %&gt;%\n  addmargins()\n\n\n\n, , Dept = A\n\n          Gender\nAdmit      Male Female\n  Admitted  512     89\n  Rejected  313     19\n\n, , Dept = B\n\n          Gender\nAdmit      Male Female\n  Admitted  353     17\n  Rejected  207      8\n\n, , Dept = C\n\n          Gender\nAdmit      Male Female\n  Admitted  120    202\n  Rejected  205    391\n\n, , Dept = D\n\n          Gender\nAdmit      Male Female\n  Admitted  138    131\n  Rejected  279    244\n\n, , Dept = E\n\n          Gender\nAdmit      Male Female\n  Admitted   53     94\n  Rejected  138    299\n\n, , Dept = F\n\n          Gender\nAdmit      Male Female\n  Admitted   22     24\n  Rejected  351    317\n\n\n              Gender Male Female\nAdmit    Dept                   \nAdmitted A            512     89\n         B            353     17\n         C            120    202\n         D            138    131\n         E             53     94\n         F             22     24\nRejected A            313     19\n         B            207      8\n         C            205    391\n         D            279    244\n         E            138    299\n         F            351    317\n\n\n\n\n            Gender\nAdmit_Dept   Male Female  Sum\n  Admitted_A  512     89  601\n  Admitted_B  353     17  370\n  Admitted_C  120    202  322\n  Admitted_D  138    131  269\n  Admitted_E   53     94  147\n  Admitted_F   22     24   46\n  Rejected_A  313     19  332\n  Rejected_B  207      8  215\n  Rejected_C  205    391  596\n  Rejected_D  279    244  523\n  Rejected_E  138    299  437\n  Rejected_F  351    317  668\n  Sum        2691   1835 4526\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that structable does not permit the adding of margins directly; it needs to be converted to a matrix for addmargins() to do its work.\n\n\n\n\nSo far these packages give Contingency Tables that are easy to see for humans; some of these structures are also capable being passed directly to commands such as stats::chisq.test() or janitor::chisq.test().\nOften we need Contingency Tables that are in tibble form, and we need to perform some data processing using dplyr to get there. Doing this with the tidyverse set of packages may seem counter-intuitive and long-winded, but the workflow is easily understandable.\nFirst we develop the counts:\ndiamonds %&gt;% count(cut)\ndiamonds %&gt;% count(clarity)\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n())\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\nWe need to have the individual levels of cut as rows and the individual levels of clarity as columns. This means that we need to pivot this from “long to wide”1 to obtain a Contingency Table:\n\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n()) %&gt;%\n  pivot_wider(\n    id_cols = cut,\n    names_from = clarity,\n    values_from = count\n  ) %&gt;%\n  # Now add the row and column totals using the `janitor` package\n  janitor::adorn_totals(where = c(\"row\", \"col\")) %&gt;%\n  # Recover to tibble since janitor gives a \"tabyl\" format\n  # ( which can be useful too !)\n  as_tibble() -&gt; diamonds_ct\ndiamonds_ct\n\n\n  \n\n\n### Another Way\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n()) %&gt;%\n  pivot_wider(\n    id_cols = cut,\n    names_from = clarity,\n    values_from = count\n  ) %&gt;%\n  # Now add the row and column totals using the `dplyr` package\n  # From: https://stackoverflow.com/a/67885521\n  mutate(\"row_totals\" = sum(across(where(is.integer)))) %&gt;%\n  ungroup() %&gt;%\n  add_row(cut = \"col_total\", summarize(., across(where(is.integer), sum)))\n\n\n  \n\n\n\n\n\n\nNow then, how does one plot a set of data that looks like this, a matrix? No column is a single variable, nor is each row a single observation, which is what we understand with the idea of tidy data.\n\n Mosaic Plots\nThe answer is provided in the very shape of the data: we plot this as a set of tiles, where \\[ \\pmb{area~of~tile \\sim count} \\] We recursively partition off a (usually) square area into vertical and horizontal pieces whose area is proportional to the count at a specific combination of levels of the two Qual variables. So we might follow the process as shown below:\n\nTake the bottom row of per-column totals and create vertical rectangles with these widths\n\nTake the individual counts in the rows and partition each rectangle based in the counts in these rows.\n\n\n\n\n\n\n\n\n\n\n(a) GSS Mosaic Chart Step #1\n\n\n\n\n\n\n\n\n\n(b) GSS Mosaic Chart Step #2\n\n\n\n\n\n\nFigure 3: Mosaic Chart for GSS Data\n\n\nThe first split shows the various levels of Education and their counts as widths. Order is alphabetical! This splitting corresponds to the bottom ROW of the Figure 2. HS is clearly the largest subgroup in Education.\nIn the second step, the columns from Figure 3 (a) are sliced horizontally into tiles, in proportion to the number of people in each Education category/level who support/do not support DeathPenalty. This is done in proportion to all the entries in each COLUMN, giving us Figure 3 (b).\nLet us now make this plot with a variety of approaches.\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\nUsing visStatistics\n\n\n\nThe vcd::mosaic() function needs the data in contingency table form. We already built one using mosaic::tally() and that is easily plotted:\n\n# Code used earlier\ndata(\"GSS2002\", package = \"resampledata\")\ngss2002 &lt;- GSS2002 %&gt;%\n  # select two categorical variables from the dataset\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  drop_na(Education, DeathPenalty)\ngss2002\n\n\n  \n\n\n# make a tally table\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n# gss_table is *not* a tibble, but a *table* object.\n\nvcd::mosaic(gss_table,\n  gp = shading_hsv,\n  main = \"mosaic::tally() + vcd::mosaic()\"\n)\n\n\n\n\n\n\n\nThere is also a command within vcd itself to create a Contingency Table, vcd::structable():\narthritis_table &lt;- vcd::structable(~ Treatment + Improved,\n  data = Arthritis\n)\narthritis_table\nvcd::mosaic(arthritis_table,\n  gp = shading_max,\n  main = \"Arthritis Treatment Dataset\"\n)\n\n\n\n          Improved None Some Marked\nTreatment                          \nPlacebo              29    7      7\nTreated              13    7     21\n\n\n\n\n\n\n\n\nggmosaic takes a tibble with Qualitative variables, internally computes the counts/table, and plots the mosaic plot:\n\n\n\ngss2002\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# library(ggmosaic)\n#\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n##\nggplot(data = gss2002) +\n  ggmosaic::geom_mosaic(\n    aes(\n      x = product(DeathPenalty, Education),\n      fill = DeathPenalty\n    )\n  ) +\n  theme(legend.position = \"top\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nℹ Please use `unite()` instead.\nℹ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\nThis needs quite some work, to convert the Contingency Table into a mosaic plot; perhaps not the most intuitive of methods either. This code has been developed using this Stackoverflow post.\n\n# Reference\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary &lt;- gss2002 %&gt;%\n  dplyr::group_by(Education, DeathPenalty) %&gt;%\n  dplyr::summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Data is still grouped by `Education`\n  # Add two more columns to facilitate mosaic Plot\n  # These two columns are quite unusual...\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\ngss_summary\n\n\n  \n\n\n\n\n\n\n# This works but is not very intuitive...\n\ngf_col(edu_prop ~ Education,\n  data = gss_summary,\n  width = ~edu_count, # Not typically used in a column chart\n  fill = ~DeathPenalty,\n  stat = \"identity\",\n  position = \"fill\",\n  color = \"black\"\n) %&gt;%\n  gf_text(edu_prop ~ Education,\n    label = ~ scales::percent(edu_prop),\n    position = position_stack(vjust = 0.5)\n  ) %&gt;%\n  gf_facet_grid(~Education,\n    scales = \"free_x\",\n    space = \"free_x\"\n  ) %&gt;%\n  gf_theme(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\nWarning: Ignoring unknown aesthetics: width\n\n\n\n\n\n\n\n\n\n\n\n\nvisStatistics is a recent package that allows a very wide variety of statistical charts to be created automagically based on the variables chosen. Let us plot a mosaic chart directly with this package\n\nvisstat(gss2002$DeathPenalty, gss2002$Education)",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#coloured-tiles-actual-and-expected-contingency-tables",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#coloured-tiles-actual-and-expected-contingency-tables",
    "title": "\n Proportions",
    "section": "Coloured Tiles: Actual and Expected Contingency Tables",
    "text": "Coloured Tiles: Actual and Expected Contingency Tables\nWe notice that the mosaic plots has coloured some tiles blue and some red. Why was this done? Consider the set of mosaic plots below:\n\nvcd::mosaic(arthritis_table, gp = shading_max, legend = FALSE)\nvcd::mosaic(arthritis_table, type = \"expected\")\nvcd::assoc(arthritis_table)\n\n\n\n\n\n\n\n\nFigure 4: Actual Contingency Table\n\n\n\n\n\n\n\n\n\nFigure 5: Expected Contingency Table\n\n\n\n\n\n\n\n\n\nFigure 6: Tile-Wise Differences\n\n\n\n\n\n\nFrom an inspection of these plots, we see the (tile-wise) difference between situations when Qualitative variables are related to that when they not related.\nThe graph on the left show the mosaic plot of the actual Contingency Table.\nThe graph in the middle shows a similar but fictitious plot but with the cuts neatly horizontal or vertical. This mosaic would be what we would expect, if Education and the opinion on Death Penalty were independent!!\nClearly, there are differences in area of the corresponding tiles in the two mosaics, actual and expected, as shown in the graph on the right. Some differences are positive, and some negative. In the actual mosaic, Figure 4, tiles with large positive differences are coloured blue, and those with large negative differences are coloured red.\nThe higher the absolute values of these differences, the greater the effect of one Qual on the other. More when we get into Inference for Two Proportions.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#dataset-titanic",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#dataset-titanic",
    "title": "\n Proportions",
    "section": "\n Dataset: Titanic",
    "text": "Dataset: Titanic\nBanzai!!! That was quite some journey! Let us end it by quickly looking at a sadly famous dataset:\n\ndata(\"titanic\", package = \"ggmosaic\")\ntitanic\n\n\n  \n\n\n\nThere were 2201 passengers, as per this dataset.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nNone.\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nSurvived: (chr) yes or no\n\nClass: (chr) Class of Travel, else “crew”\n\nAge: (chr) Adult, Child\n\nSex: (chr) Male / Female.\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1. What is the dependence of survived upon sex?\n\n\n\n\n\n\nvcd::structable(Survived ~ Sex, data = titanic) %&gt;%\n  vcd::mosaic(gp = shading_max)\n\n\n\n\n\n\n\n\n\n\n\n\nNote the huge imbalance in survived with sex: men have clearly perished in larger numbers than women. Which is why the colouring by the Pearson Residuals show large positive residuals for men who died, and large negative residuals for women who died.\nSo sadly Jack is far more likely to have died than Rose.\n\n\n\n\n\n\n\n\nNoteQ.2. How does Survived depend upon Class?\n\n\n\n\n\n\nvcd::structable(Survived ~ Class, data = titanic) %&gt;%\n  vcd::mosaic(gp = shading_max)\n\n\n\n\n\n\n\n\n\n\n\n\nCrew has seen deaths in large numbers, as seen by the large negative residual for crew-survivals. First Class passengers have had speedy access to the boats and have survived in larger proportions than say second or third class. There is a large positive residual for first-class survivals.\nRose travelled first class and Jack was third class. So again the odds are stacked against him.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "title": "\n Proportions",
    "section": "\n Balloon Plots",
    "text": "Balloon Plots\nThere is another visualization of Categorical Data, called a Balloon Plot. We will use the housetasks dataset from the package ggpubr.\n\n Dataset: Who Does the Housework?\n\nhousetasks &lt;- read.delim(\n  system.file(\"demo-data/housetasks.txt\",\n    package = \"ggpubr\"\n  ),\n  row.names = 1\n)\nhousetasks\n\n\n  \n\n\ninspect(housetasks)\n\n\nquantitative variables:  \n         name   class min Q1 median Q3 max     mean       sd  n missing\n1        Wife integer   0 10     32 77 156 46.15385 50.05971 13       0\n2 Alternating integer   1 11     14 24  51 19.53846 16.26149 13       0\n3     Husband integer   1  5      9 23 160 29.30769 44.97663 13       0\n4     Jointly integer   2  4     15 57 153 39.15385 44.09808 13       0\n\n\nWe see that we have 13 observations.\n\n\n\n\n\n\nImportant\n\n\n\nThis data is already in Contingency Table form (without the margin totals)!\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nFreq: (int) No of times a task was carried out by specific people\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nWho: (chr) Who carried out the task?\n\nTask: (chr) Task? Which task? Can’t you see I’m tired?\n\n\n\n\n\n\nggpubr::ggballoonplot(housetasks,\n  fill = \"value\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"A Balloon Plot for Categorical Data\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnd repeat with the familiar HairEyeColor dataset:\n\n\n\ndf &lt;- as_tibble(HairEyeColor)\ndf\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nggballoonplot(df,\n  x = \"Hair\",\n  y = \"Eye\", size = \"n\",\n  fill = \"n\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"Balloon Plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Balloon Plot with facetting\nggballoonplot(df,\n  x = \"Hair\",\n  y = \"Eye\", size = \"n\",\n  fill = \"n\",\n  facet.by = \"Sex\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(\n    title = \"Balloon Plot with Facetting by Sex\",\n    subtitle = \"Hair and Eye Color\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nNote the somewhat different syntax with ggballoonplot: the variable names are enclosed in quotes.\nBalloon Plots work because they use color and size aesthetics to represent categories and counts respectively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#wait-but-why",
    "title": "\n Proportions",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can detect correlation between Quant variables using the scatter plots and regression lines\n\nAnd we can detect association between Qual variables using mosaics, sieves (which we did not see here, but is possible in R), and with balloon plots.\nYour project primary research data may be pure Qualitative too, as with a Questionnaire / Survey instrument.\nOne such Qual variable therein will be your target variable\n\nYou will need to justify whether the target variable is dependent upon the other Quals, and then to decide what to do about that.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast",
    "title": "\n Proportions",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis excerpt from a course on Data Analysis using Metaphors focuses on the importance of understanding and visualizing categorical data. It discusses different ways to represent categorical data in R, including case data, frequency data, and cross-tabular count data. The text also explores various visualization techniques like bar plots, pie charts, mosaic plots, and balloon plots. It emphasizes the use of contingency tables for analyzing relationships between categorical variables, illustrating how to create them and visualize them using R packages. Additionally, the text delves into the concept of Pearson residuals, which help to identify associations between categorical variables and highlight deviations from independence.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "title": "\n Proportions",
    "section": "\n Conclusion",
    "text": "Conclusion\nHow are the bar plots for categorical data different from histograms? Why don’t “regular” scatter plots simply work for Categorical data? Discuss!\nThere are quite a few things we can do with Qualitative/Categorical data:\n\nMake simple bar charts with colours and facetting\nMake Contingency Tables for a \\(X^2\\)-test\nMake Mosaic Plots to show how the categories stack up\nMake Balloon Charts as an alternative\nThen, draw your inferences and tell the story!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "title": "\n Proportions",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").\nTry the housetasks dataset that we used for Balloon Plots, to create a mosaic plot with Pearson Residuals.\n\n\n\n\n\n\n\nNoteClothing and Intelligence Rating of Children!!\n\n\n\nAre well-dressed students actually smarter?\n Download the Gilby Study dataset \n\n\n\n\n\n\n\n\nNotePre-marital Sex and Divorce.\n\n\n\n Download the pre- and extra-marital sex and divorce dataset \n\n\n\n\n\n\n\n\nNoteAre Emily and Greg More Employable Than Lakisha and Jamal?\n\n\n\nAre first names a basis for racial discrimination, in the US?\nThis dataset was generated as part of a landmark research study done by Marianne Bertrand and Senthil Mullainathan. Read the description therein to really understand how you can prove causality with a well-crafted research experiment.\n Download the Resume Name dataset",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast-1",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast-1",
    "title": "\n Proportions",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis module focuses on the importance of understanding and visualizing categorical data. It discusses different ways to represent categorical data in R, including case data, frequency data, and cross-tabular count data. The text also explores various visualization techniques like bar plots, pie charts, mosaic plots, and balloon plots. It emphasizes the use of contingency tables for analyzing relationships between categorical variables, illustrating how to create them and visualize them using R packages. Additionally, the text delves into the concept of Pearson residuals, which help to identify associations between categorical variables and highlight deviations from independence.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "title": "\n Proportions",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nNice Chi-square interactive story at https://statisticalstories.xyz/chi-square\n\nChittaranjan Andrade(July 22, 2015). Understanding Relative Risk, Odds Ratio, and Related Terms: As Simple as It Can Get. https://www.psychiatrist.com/jcp/understanding-relative-risk-odds-ratio-related-terms/\n\nMine Cetinkaya-Rundel and Johanna Hardin. An Introduction to Modern Statistics, Chapter 4. https://openintro-ims.netlify.app/explore-categorical.html\n\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\n\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\n\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\n\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf\n\n\nVisualizing Categorical Data in R\n\nH. Riedwyl & M. Schüpbach (1994), Parquet diagram to plot contingency tables. In F. Faulbaum (ed.), Softstat ’93: Advances in Statistical Software, 293–299. Gustav Fischer, New York.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nggpubr\n0.6.1\nKassambara (2025)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.1\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvcd\n1.4.13\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2024)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\nvisStatistics\n0.1.7\nSchilling (2025)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nFriendly, Michael. 2023. vcdExtra: “vcd” Extensions and Additions. https://doi.org/10.32614/CRAN.package.vcdExtra.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nKassambara, Alboukadel. 2025. ggpubr: “ggplot2” Based Publication Ready Plots. https://doi.org/10.32614/CRAN.package.ggpubr.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2024. vcd: Visualizing Categorical Data. https://doi.org/10.32614/CRAN.package.vcd.\n\n\nSchilling, Sabine. 2025. visStatistics: Automated Selection and Visualisation of Statistical Hypothesis Tests. https://doi.org/10.32614/CRAN.package.visStatistics.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "title": "\n Proportions",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://tidyr.tidyverse.org/articles/pivot.html↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html",
    "title": "\n Densities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.”\n— Marcus Aurelius",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#slides-and-tutorials",
    "title": "\n Densities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.”\n— Marcus Aurelius",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#setting-up-r-packages",
    "title": "\n Densities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n\n# install.packages(\"remotes\")\n# library(remotes)\n# remotes::install_github(\"wilkelab/ggridges\")\nlibrary(ggridges)\nlibrary(skimr)\nlibrary(palmerpenguins) # Our new favourite dataset\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Set the theme\ntheme_set(new = theme_custom())\n```\n\nError in theme_set(new = theme_custom()): could not find function \"theme_set\"\n\nShow the Code```{r}\n#| cache: false\n#| code-fold: true\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n```\n\nError in update_geom_defaults(geom = \"text\", new = list(family = \"Roboto Condensed\", : could not find function \"update_geom_defaults\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-graphs-will-we-see-today",
    "title": "\n Densities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nDensity plot, Ridge Density Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Densities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#inspiration",
    "title": "\n Densities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\nApril is the cruelest month, said T.S Eliot. But December in Nebraska must be tough.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-is-a-density-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-is-a-density-plot",
    "title": "\n Densities",
    "section": "\n What is a “Density Plot”?",
    "text": "What is a “Density Plot”?\nAs we saw earlier, Histograms are best to show the distribution of raw Quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins.\nSometimes it is useful to consider a chart where the bucket width shrinks to zero!\nYou might imagine a density chart as a histogram where the buckets are infinitesimally small, i.e. zero width. Think of the frequency density as a differentiation (as in calculus) of the histogram. By taking the smallest of steps \\(\\sim 0\\), we get a measure of the slope of distribution. This may seem counter-intuitive, but densities have their uses in spotting the ranges in the data where there are more frequent values. In this, they serve a similar purpose as do histograms, but may offer insights not readily apparent with histograms, especially with default bucket widths. The chunkiness that we see in the histograms is removed and gives us a smooth curve showing in which range the data are more frequent.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#case-study-1-penguins-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#case-study-1-penguins-dataset",
    "title": "\n Densities",
    "section": "\n Case Study-1: penguins dataset",
    "text": "Case Study-1: penguins dataset\nWe will first look at at a dataset that is directly available in R, the penguins dataset. Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\n\n\ninspect(penguins)\n\n\ncategorical variables:  \n     name  class levels   n missing\n1 species factor      3 344       0\n2  island factor      3 344       0\n3     sex factor      2 333      11\n                                   distribution\n1 Adelie (44.2%), Gentoo (36%) ...             \n2 Biscoe (48.8%), Dream (36%) ...              \n3 male (50.5%), female (49.5%)                 \n\nquantitative variables:  \n               name   class    min       Q1  median     Q3    max       mean\n1    bill_length_mm numeric   32.1   39.225   44.45   48.5   59.6   43.92193\n2     bill_depth_mm numeric   13.1   15.600   17.30   18.7   21.5   17.15117\n3 flipper_length_mm integer  172.0  190.000  197.00  213.0  231.0  200.91520\n4       body_mass_g integer 2700.0 3550.000 4050.00 4750.0 6300.0 4201.75439\n5              year integer 2007.0 2007.000 2008.00 2009.0 2009.0 2008.02907\n           sd   n missing\n1   5.4595837 342       2\n2   1.9747932 342       2\n3  14.0617137 342       2\n4 801.9545357 342       2\n5   0.8183559 344       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nsex: male and female penguins\n\nisland: they have islands to themselves!!\n\nspecies: Three adorable types!\n\n\n\n\n\n\n\n\n\n\nFigure 1: Penguin Species\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nbill_length_mm: The length of the penguins’ bills\n\nbill_depth_mm: See the picture!!\n\nflipper_length_mm: Flippers! Penguins have “hands”!!\n\nbody_mass_gm: Grams? Grams??? Why, these penguins are like human babies!!❤️\n\n\n\n\n\n\n\n\n\n\nFigure 2: Penguin Features\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the penguins dataset\n\n\n\n\nThis is a smallish dataset (344 rows, 8 columns).\nThere are a few missing values in sex(11 missing entries) and all the Quant variables (2 missing entries each).\n\n\n\n\n Plotting Densities\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\npenguins &lt;- penguins %&gt;% drop_na()\n\ngf_density(~body_mass_g, data = penguins) %&gt;%\n  gf_labs(title = \"Plot A: Penguin Masses\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\npenguins %&gt;%\n  gf_density(~body_mass_g,\n    fill = ~species,\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(scale_color_viridis_d(\n    option = \"magma\",\n    aesthetics = c(\"colour\", \"fill\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Plot B: Penguin Body Mass by Species\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  gf_density(\n    ~body_mass_g,\n    fill = ~species,\n    color = \"black\",\n    alpha = 0.3\n  ) %&gt;%\n  gf_facet_wrap(vars(sex)) %&gt;%\n  gf_labs(title = \"Plot C: Penguin Body Mass by Species and facetted by Sex\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  gf_density(~body_mass_g, fill = ~species, color = \"black\") %&gt;%\n  gf_facet_wrap(vars(sex), scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot D: Penguin Body Mass by Species and facetted by Sex\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1\n  )))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins &lt;- penguins %&gt;% drop_na()\n\nggplot(data = penguins) +\n  geom_density(aes(x = body_mass_g)) +\n  labs(title = \"Plot A: Penguin Masses\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  scale_color_brewer(\n    palette = \"Set1\",\n    aesthetics = c(\"colour\", \"fill\")\n  ) +\n  labs(\n    title = \"Plot B: Penguin Body Mass by Species\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  facet_wrap(vars(sex)) +\n  labs(title = \"Plot C: Penguin Body Mass by Species and facetted by Sex\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  facet_wrap(vars(sex), scales = \"free_y\", nrow = 2) +\n  labs(\n    title = \"Plot D: Penguin Body Mass by Species and facetted by Sex\",\n    subtitle = \"Free y-scale\", caption = \"ggplot\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from penguin Densities\n\n\n\nPretty much similar conclusions as with histograms. Although densities may not be used much in business contexts, they are better than histograms when comparing multiple distributions! So you should use thems!\n\n\n\n\n Ridge Plots\nSometimes we may wish to show the distribution/density of a Quant variable, against several levels of a Qual variable. For instance, the prices of different items of furniture, based on the furniture “style” variable. Or the sales of a particular line of products, across different shops or cities. We did this with both histograms and densities, by colouring based on a Qual variable, and by facetting using a Qual variable. There is a third way, using what is called a ridge plot. ggformula support this plot by importing/depending upon the ggridges package. ggridges provides direct support for ridge plots, and can be used as an extension to # ggplot2 and ggformula.\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\ngf_density_ridges(drv ~ hwy,\n  fill = ~drv,\n  alpha = 0.5, # colour saturation\n  rel_min_height = 0.005, # separation between plots\n  data = mpg\n) %&gt;%\n  gf_refine(\n    scale_y_discrete(expand = c(0.01, 0)),\n    scale_x_continuous(expand = c(0.01, 0)),\n    scale_fill_brewer(\n      name = \"Drive Type\",\n      palette = \"Spectral\"\n    )\n  ) %&gt;%\n  gf_labs(\n    title = \"Ridge Plot\", x = \"Highway Mileage\",\n    y = \"Drive Type\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngf_density_ridges(drv ~ hwy,\n  fill = ~drv,\n  alpha = 0.5, # colour saturation\n  rel_min_height = 0.005, data = mpg\n) %&gt;%\n  gf_refine(\n    scale_y_discrete(expand = c(0.01, 0)),\n    scale_x_continuous(expand = c(0.01, 0)),\n    scale_fill_brewer(\n      name = \"Drive Type\",\n      palette = \"Spectral\"\n    )\n  ) %&gt;%\n  gf_labs(\n    title = \"Ridge Plot\", x = \"Highway Mileage\",\n    y = \"Drive Type\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from mpg Ridge Plots\n\n\n\nThis is another way of visualizing multiple distributions, of a Quant variable at different levels of a Qual variable. We see that the distribution of hwy mileage varies substantially with drv type.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#wait-but-why",
    "title": "\n Densities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nDensities are sometimes easier to compare side by side. That is what Claus Wilke says, at least. Perhaps because they look less “busy” than histograms.\nRidge Density Plots are very cool when it comes to comparing the density of a Quant variable as it varies against the levels of a Qual variable, without having to facet or group.\nIt is possible to plot 2D-densities too, for two Quant variables, which give very evocative contour-like plots. Try to do this with the faithful dataset in R.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#conclusion",
    "title": "\n Densities",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms and Frequency Distributions are both used for Quantitative data variables\nWhereas Histograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#your-turn",
    "title": "\n Densities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteStar Trek Books\n\n\n\n\n\n Start Trek Book data\n\n\nWhich would be the Group By variables here? And what would you summarize? With which function?\n\n\n\n\n\n\n\n\nNoteMath Anxiety! Hah! Peasants.\n\n\n\n\n\n Math Anxiety data",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#references",
    "title": "\n Densities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nresampledata3\n1.0\nChihara and Hesterberg (2022)\n\n\nrtrek\n0.5.2\nLeonawicz (2025)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.1\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nChihara, Laura, and Tim Hesterberg. 2022. Resampledata3: Data Sets for “Mathematical Statistics with Resampling and R” (3rd Ed). https://doi.org/10.32614/CRAN.package.resampledata3.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nLeonawicz, Matthew. 2025. rtrek: Data Analysis Relating to Star Trek. https://doi.org/10.32614/CRAN.package.rtrek.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Analytics",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/work-related/fsp-manifesto/index.html",
    "href": "content/work-related/fsp-manifesto/index.html",
    "title": "My Teaching Manifesto",
    "section": "",
    "text": "This is a short Statement of Values, Beliefs, and Content in my Teaching.\n\nArvind Venkatadri.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#instructions",
    "href": "content/work-related/DSU/A1.html#instructions",
    "title": "A1",
    "section": "Instructions",
    "text": "Instructions\n\nEach Question in this Assignment is a chart.\nEach Chart is accompanied by a set of short questions.\nYour responses to these can be R-code, or text.\nPlease number your answers as 1.a, 1.b, 1.c…..2.a, 2.b…on your Answer Sheet.\nAll aboard? Let’s go!"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1",
    "href": "content/work-related/DSU/A1.html#question-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1-1",
    "href": "content/work-related/DSU/A1.html#question-1-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nWhat is the ggplot geometry used in this graph?\nWhat do the colours mean?"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-2",
    "href": "content/work-related/DSU/A1.html#question-2",
    "title": "A1",
    "section": "Question 2",
    "text": "Question 2\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-3",
    "href": "content/work-related/DSU/A1.html#question-3",
    "title": "A1",
    "section": "Question 3",
    "text": "Question 3\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-4",
    "href": "content/work-related/DSU/A1.html#question-4",
    "title": "A1",
    "section": "Question 4",
    "text": "Question 4\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-5",
    "href": "content/work-related/DSU/A1.html#question-5",
    "title": "A1",
    "section": "Question 5",
    "text": "Question 5\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-6",
    "href": "content/work-related/DSU/A1.html#question-6",
    "title": "A1",
    "section": "Question 6",
    "text": "Question 6\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-7",
    "href": "content/work-related/DSU/A1.html#question-7",
    "title": "A1",
    "section": "Question 7",
    "text": "Question 7\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-8",
    "href": "content/work-related/DSU/A1.html#question-8",
    "title": "A1",
    "section": "Question 8",
    "text": "Question 8\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-9",
    "href": "content/work-related/DSU/A1.html#question-9",
    "title": "A1",
    "section": "Question 9",
    "text": "Question 9\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-10",
    "href": "content/work-related/DSU/A1.html#question-10",
    "title": "A1",
    "section": "Question 10",
    "text": "Question 10\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-11",
    "href": "content/work-related/DSU/A1.html#question-11",
    "title": "A1",
    "section": "Question 11",
    "text": "Question 11\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-12",
    "href": "content/work-related/DSU/A1.html#question-12",
    "title": "A1",
    "section": "Question 12",
    "text": "Question 12\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-13",
    "href": "content/work-related/DSU/A1.html#question-13",
    "title": "A1",
    "section": "Question 13",
    "text": "Question 13\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-14",
    "href": "content/work-related/DSU/A1.html#question-14",
    "title": "A1",
    "section": "Question 14",
    "text": "Question 14\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-15",
    "href": "content/work-related/DSU/A1.html#question-15",
    "title": "A1",
    "section": "Question 15",
    "text": "Question 15\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/fsp-portfolio-2022/index.html",
    "href": "content/work-related/fsp-portfolio-2022/index.html",
    "title": "Teaching in this post(?) - Pandemic Year 2021-2022",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this post(?)-pandemic year, 2021-2022, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "readme.html#get-started-with-quarto",
    "href": "readme.html#get-started-with-quarto",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  }
]