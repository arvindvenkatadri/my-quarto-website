---
title: "Modelling with Linear Regression"
author: "Arvind V."
date: 13/Apr/2023
date-modified: "`r Sys.Date()`"
order: 20
image: featured.jpg
image-alt: ""
categories: 
  - Linear Regression
  - Quantitative Predictor
  - Quantitative Response
  - Sum of Squares
  - Residuals
abstract: "Predicting Quantitative Target Variables"
bibliography: 
  - grateful-refs.bib
citation: true
#suppress-bibliography: true
editor:
  markdown:
    wrap: 72
webr:
  packages: ['readr', 'tidyr', 'dplyr','mosaic', 'skimr', 'ggformula','ggridges', 'palmerpenguins']
---

## {{< fa folder-open >}} Slides and Tutorials {#sec-linreg}

|                                                                                                                                       |                                                                                                                                  |                                                                                                             |
|------------------------|--------------------------|----------------------|
| <a href="./files/forward-selection-1.qmd"><i class="fa-brands        fa-r-project"></i> Multiple Regression - Forward Selection</a>   | <a href="./files/backward-selection-1.qmd"><i class="fa-brands fa-r-project"></i> Multiple Regression - Backward Selection</a>   | <a href="./files/lin-perm.qmd"> <i class="fa-brands fa-r-project"></i> Permutation Test for Regression</a>  |

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
#| message: false
#| warning: false
#| include: true

library(ggformula)
library(mosaic)
library(GGally)
library(corrplot)
library(corrgram)
library(ggstatsplot)
library(tidyverse)
```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(nomnoml)
library(grateful)
library(equatiomatic)
library(geomtextpath)
library(mlbench)
library(reghelper)

```

#### Plot Fonts and Theme

```{r}
#| label: plot-theme
#| echo: true
#| code-fold: true
#| messages: false
#| warning: false

library(systemfonts)
library(showtext)
## Clean the slate
systemfonts::clear_local_fonts()
systemfonts::clear_registry()
##
showtext_opts(dpi = 96) #set DPI for showtext
sysfonts::font_add(family = "Alegreya",
  regular = "../../../../../../fonts/Alegreya-Regular.ttf",
  bold = "../../../../../../fonts/Alegreya-Bold.ttf",
  italic = "../../../../../../fonts/Alegreya-Italic.ttf",
  bolditalic = "../../../../../../fonts/Alegreya-BoldItalic.ttf")

sysfonts::font_add(family = "Roboto Condensed", 
  regular = "../../../../../../fonts/RobotoCondensed-Regular.ttf",
  bold = "../../../../../../fonts/RobotoCondensed-Bold.ttf",
  italic = "../../../../../../fonts/RobotoCondensed-Italic.ttf",
  bolditalic = "../../../../../../fonts/RobotoCondensed-BoldItalic.ttf")
showtext_auto(enable = TRUE) #enable showtext
##
theme_custom <- function(){ 
    font <- "Alegreya"   #assign font family up front
    
    theme_classic(base_size = 14, base_family = font) %+replace%    #replace elements we want to change
    
    theme(
      text = element_text(family = font),  #set base font family
      
      #text elements
      plot.title = element_text(                 #title
                   family = font,          #set font family
                   size = 24,                    #set font size
                   face = 'bold',                #bold typeface
                   hjust = 0,                    #left align
                   margin = margin(t = 5, r = 0, b = 5, l = 0)), #margin
      plot.title.position = "plot", 
      
      plot.subtitle = element_text(              #subtitle
                   family = font,          #font family
                   size = 14,                   #font size
                   hjust = 0,                   #left align
                   margin = margin(t = 5, r = 0, b = 10, l = 0)), #margin
      
      plot.caption = element_text(               #caption
                   family = font,          #font family
                   size = 9,                     #font size
                   hjust = 1),                   #right align
      
      plot.caption.position = "plot",            #right align
      
      axis.title = element_text(                 #axis titles
                   family = "Roboto Condensed",  #font family
                   size = 12),                   #font size
      
      axis.text = element_text(                  #axis text
                   family = "Roboto Condensed",  #font family
                   size = 9),                    #font size
      
      axis.text.x = element_text(                #margin for axis text
                    margin = margin(5, b = 10))
      
      #since the legend often requires manual tweaking 
      #based on plot content, don't define it here
    )
}

```

```{r}
#| cache: false
#| echo: fenced
#| code-fold: true
## Set the theme
theme_set(new = theme_custom())

## Use available fonts in ggplot text geoms too!
update_geom_defaults(geom = "text",new = list(
  family = "Roboto Condensed",
  face = "plain",
  size = 3.5,
  color = "#2b2b2b"
)
)


```


## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

One of the most common problems in Prediction Analytics is that of
predicting a Quantitative response variable, based on one or more
Quantitative predictor variables or *features*. This is called ***Linear
Regression***. We will use the intuitions built up during our study of
ANOVA to develop our ideas about Linear Regression.

Suppose we have data on salaries in a Company, with years of study and
previous experience. Would we be able to predict the prospective salary
of a new candidate, based on their years of study and experience? Or
based on the mileage done, could we predict the resale price of a used car?
These are typical problems in Linear Regression.

In this tutorial, we will use the Boston housing dataset. Our research
question is:

::: callout-note

## Research Question
How do we predict the price of a house in Boston, based on other
parameters Quantitative parameters such as area, location, rooms, and
crime-rate in the neighbourhood?

:::

## {{< iconify radix-icons box-model >}} The Linear Regression Model

The premise here is that many common statistical tests are special cases
of the linear model.

A linear model estimates the relationship between one *continuous* or
*ordinal* variable (dependent variable or "response") and one or more
other variables (explanatory variable or "predictors"). It is assumed
that the relationship is linear:[^1]

[^1]: The model is linear in the **parameters** $\beta_i$, e.g. We can
    have this: 
    

$$
\Large{y_i \sim \beta_1*x_i + \beta_0\\}
$${#eq-linear-model-1}
    
or 
    
$$
\Large{y_1 \sim exp(\beta_1)*x_i + \beta_0}
$${#eq-linear-model-2}

but not: 
    
$$
\color{red}{y_i \sim \beta_1*exp(\beta_2*x_i) + \beta_0\\}
$$



or 

$$
\color{red}{y_i \sim \beta_1 *x^{\beta_2} + \beta_0}
$$



In @eq-linear-model-1, $\beta_0$ is the *intercept* and $\beta_1$ is the slope of the linear fit, that **predicts** the value of y based the value of x. Each prediction leaves a small "residual" error between the actual and
predicted values. $\beta_0$ and $\beta_1$ are calculated based on
minimizing the *sum of square*s of these residuals, and hence this
method is called "ordinary least squares" (**OLS**) regression.


![Least Squares](../../../../../materials/images/OLS.png){#fig-least-squares height="360"}


The net *area* of all the shaded squares is minimized in the calculation
of $\beta_0$ and $\beta_1$. As per Lindoloev, many statistical tests, going from one-sample `t-tests` to `two-way ANOVA`, are special cases of this system. Also see [Jeffrey Walker "A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables).



## {{< iconify simple-icons hypothesis >}} Linear Models as Hypothesis Tests

Using linear models is based on the idea of **Testing of Hypotheses**.
The Hypothesis Testing method typically defines a NULL Hypothesis where
the statements read as "**there is no relationship**" between the
variables at hand, explanatory and responses. The Alternative Hypothesis
typically states that there *is* a relationship between the variables.

Accordingly, in fitting a linear model, we follow the process as
follows: 

::: callout-note

### Modelling Process

With $y = \beta_0 + \beta_1 *x$

1.  Make the following hypotheses: 

$$
    NULL\ Hypothesis\ H_0 => x\ and\ y\ are\ unrelated.\ (\beta_1 = 0)
$$ 
    
$$
    Alternate\ Hypothesis\ H_1 => x\ and\ y\ are\ linearly\ related\ (\beta_1 \ne 0)
$$

2.  We "assume" that $H_0$ is true.
3.  We calculate $\beta_1$.
4.  We then find probability **p**($\beta_1 = Estimated\ Value$
    **when the NULL Hypothesis** is **assumed** TRUE). This is the
    **p-value**. If that probability is **p\>=0.05**, we say we "cannot
    reject" $H_0$ and there is unlikely to be significant linear
    relationship.
5.  However, if **p\<= 0.05** can we reject the NULL hypothesis, and say
    that there could be a significant linear relationship, because the
    probability **p** that $\beta_1 = Estimated\ Value$ by mere chance
    under $H_0$ is very small.

:::


## {{< iconify icon-park-outline thinking-problem >}} Assumptions in Linear Models{#sec-assumptions-in-linear-models}

When does a Linear Model work? We can write the assumptions in Linear Regression Models as an acronym,
**LINE**:\
1. **L**: $\color{blue}{linear}$ relationship\ between variables
2. **I**: Errors are **independent** (across observations)\
3. **N**: $y$ is $\color{red}{normally}$ distributed at each "level" of
$x$.\
4. **E**: $y$ has the same variance at all levels of $x$. No *heteroscedasticity*.\

![OLS Assumptions](../../../../../materials/images/ols_assumptions.png){#fig-ols-assumptions}




Hence a very concise way of expressing the Linear Model is:

$$
\Large{y \sim N(x_i^T * \beta, ~~\sigma^2)}
$$

::: callout-important
## General Linear Models

The target variable $y$ is modelled as a *normally* distribute variable
**whose mean depends upon a linear combination of predictor variables**
$x$, and whose variance is $\sigma^2$.
:::



## {{< iconify flat-color-icons workflow >}} Linear Model Workflow

OK, on with the computation!

### {{< iconify flat-color-icons workflow >}} Workflow: Read the Data

Let us now read in the data and check for these assumptions as part of
our Workflow.
```{r}

data("BostonHousing2", package = "mlbench")
housing <- BostonHousing2
inspect(housing)

```

The original data are 506 observations on 14 variables, `medv` being the
target variable:

|         |                                                                       |
|:-------------------|:---------------------------------------------------|
| crim    | per capita crime rate by town                                         |
| zn      | proportion of residential land zoned for lots over 25,000 sq.ft       |
| indus   | proportion of non-retail business acres per town                      |
| chas    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| nox     | nitric oxides concentration (parts per 10 million)                    |
| rm      | average number of rooms per dwelling                                  |
| age     | proportion of owner-occupied units built prior to 1940                |
| dis     | weighted distances to five Boston employment centres                  |
| rad     | index of accessibility to radial highways                             |
| tax     | full-value property-tax rate per USD 10,000                           |
| ptratio | pupil-teacher ratio by town                                           |
| b       | $1000(B - 0.63)^2$ where B is the proportion of Blacks by town        |
| lstat   | percentage of lower status of the population                          |
| medv    | median value of owner-occupied homes in USD 1000's                    |

The corrected data set has the following additional columns:

|       |                                                              |
|:------|:-------------------------------------------------------------|
| cmedv | corrected median value of owner-occupied homes in USD 1000's |
| town  | name of town                                                 |
| tract | census tract                                                 |
| lon   | longitude of census tract                                    |
| lat   | latitude of census tract                                     |

Our response variable is `cmedv`, the *corrected median value of
owner-occupied homes in USD 1000'*s. Their are many Quantitative feature
variables that we can use to predict `cmedv`. And there are two
Qualitative features, `chas` and `tax`.

### {{< iconify flat-color-icons workflow >}} Workflow: EDA

In order to fit the linear model, we need to choose **predictor**
variables that have strong correlations with the **target** variable. We
will first do this with `GGally`, and then with the `tidyverse` itself.
Both give us a very unique view into the correlations that exist within
this dataset.

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}

### {{< iconify flat-color-icons workflow >}} Workflow: Correlations with GGally

Let us select a few sets of Quantitative and Qualitative features, along
with the target variable `cmedv` and do a pairs-plots with them:

```{r}

# Set graph theme
theme_set(new = theme_custom())
#

housing %>%
  # Target variable cmedv
  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access
  select(cmedv, rm, age, dis) %>%
  GGally::ggpairs(title = "Plot 1",
                  progress = FALSE,
                  lower = list(continuous = wrap("smooth", 
                                                 alpha = 0.2))) 
##
housing %>%
  # Target variable cmedv
  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000
  select(cmedv, rad, zn, indus, tax) %>%
  GGally::ggpairs(title = "Plot 2", 
                  progress = FALSE,
                  lower = list(continuous = wrap("smooth", 
                                                 alpha = 0.2))) 
##
housing %>%
  # Target variable cmedv
  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population
  select(cmedv, crim, nox, rad, b, lstat) %>%
  GGally::ggpairs(title = "Plot 3", 
                  progress = FALSE,
                  lower = list(continuous = wrap("smooth", 
                                                 alpha = 0.2))) 

```

See the top row of the pairs plots. Clearly, `rm` (avg. number of
rooms) is a big determining feature for median price `cmedv`. This we
infer based on the large correlation of `rm` with`cmedv`, $0.696$. The
variable`age` (proportion of owner-occupied units built prior to 1940)
may also be a significant influence on `cmedv`, with a correlation of
$-0.378$.

None of the Quant variables `rad, zn, indus, tax` have a overly strong
correlation with `cmedv`. .

The variable `lstat` (proportion of lower classes in the neighbourhood)
as expected, has a strong (negative) correlation with `cmedv`;
`rad`(index of accessibility to radial highways), `nox`(nitrous oxide)
and `crim`(crime rate) also have fairly large correlations with `cmedv`,
as seen from the pairs plots.

::: callout-important
### Correlation Scores and Uncertainty

Recall that `cor.test` reports a correlation score and the `p-value` for
the same. There is also a `confidence interval` reported for the
correlation score, an interval within which we are 95% sure that the
true correlation value is to be found.

Note that `GGally` too reports the significance of the correlation
scores using stars, `***` or `**`. This indicates the p-value in the
scores obtained by `GGally`; Presumably, there is an internal `cor.test`
that is run for each pair of variables and the p-value and confidence
levels are also computed internally.
:::

Let us plot (again) scatter plots of Quant Variables that have strong
correlation with `cmedv`:

```{r}
#| layout-ncol: 2
# Set graph theme
theme_set(new = theme_custom())
#
gf_point(
  data = housing,
  cmedv ~ age,
  title = "Price vs Proportion of houses older than 1940",
  ylab = "Median Price",
  xlab = "Proportion of older-than-1940 buildings")

##
gf_point(
  data = housing,
  cmedv ~ lstat,
  title = "Price vs Proportion of lower classes..."
  subtitle = "...In the neighbourhood",
  ylab = "Median Price",
  xlab = "proportion of lower classes in the neighbourhood")
##
gf_point(
  data = housing,
  cmedv ~ rm,
  title = "Price vs Average no. of Rooms",
  ylab = "(cmedv) Median Price",
  xlab = "(rm) Avg. No. of Rooms")


```

So, `rm` does have a positive effect on `cmedv`, and `age` may have a
(mild?) negative effect on `cmedv`; `lstat` seems to have a pronounced
negative effet on `cmedv`. We have now managed to get a decent idea
which *Quant predictor variables* might be useful in modelling `cmedv`:
`rm`, `lstat` for starters, then perhaps`age`.

Let us also check the *Qualitative predictor variables*: Access to the
Charles river (`chas`) does seem to affect the prices somewhat.

```{r}
# Set graph theme
theme_set(new = theme_custom())
#
housing %>%
  # Target variable cmedv
  # Predictor Access to Charles River
  select(cmedv, chas) %>%
  GGally::ggpairs(title = "Plot 4", 
                  progress = FALSE,
                  lower = list(continuous = wrap("smooth", 
                                                 alpha = 0.2)))

```

Look at the bar plot above. While not too many
properties can be near the Charles River (for obvious reasons) the box
plots do seem to show some dependency of `cmedv` on `chas`.

::: callout-note
Qualitative predictors for a Quantitative target can be included in the
model using what is called *dummy variables*, where each *level* of the
Qualitative variable is given a **one-hot** kind of encoding. See for
example <https://www.statology.org/dummy-variables-regression/>
:::

### {{< iconify flat-color-icons workflow >}} Correlations using cor.test and purrr

This is somewhat advanced material: We will use the `purrr` package to
develop all correlations with respect to our target variable in one shot
and also plot these correlation test scores in an error-bar plot. See
[Tidy Modelling with R](https://www.tmwr.org/base-r#tidiness-modeling).
This has the advantage of being able to depict all correlations in one
plot. (We will use this approach again here when we trim our linear
models down from the *maximal* one to a workable one of lesser
complexity.). Let us do this.

We develop a *list object* containing all correlation test results with
respect to `cmedv`, tidy these up using `broom::tidy`, and then plot
these:

```{r}
# Set graph theme
theme_set(new = theme_custom())
#

all_corrs <- housing %>% 
  select(where(is.numeric)) %>% 
  # leave off target variable cmedv and IDs
  # get all the remaining ones
  select(-cmedv, -medv) %>%  

  purrr::map(.x = ., # All numeric variables selected in the previous step
             .f = \(.x) cor.test(.x, housing$cmedv)) %>% # Apply the cor.test with `cmedv`
  
  # Tidy up the cor.test outputs into neat columns
  # Need ".id" column to keep track of predictor variable name
  map_dfr(broom::tidy, .id = "predictor") 

all_corrs

all_corrs %>%
  gf_hline(
    yintercept = 0,
    color = "grey",
    linewidth = 2,
    title = "Correlations: Target Variable vs All Predictors",
    subtitle = "Boston Housing Dataset"
  ) %>%
  gf_errorbar(
    conf.high + conf.low ~ reorder(predictor, estimate),
    colour = ~ estimate,
    width = 0.5,
    linewidth = ~ -log10(p.value),
    caption = "Significance = -log10(p.value)"
  ) %>%
  
  # Plot points(smallest geom) last!
  gf_point(estimate ~ reorder(predictor, estimate)) %>%
  gf_labs(x = "Predictors", y = "Correlation with cmedv") %>%
  #gf_theme(theme_minimal()) %>% 
  
  # tilt the x-axis labels for readability
  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %>%
  
  # Colour and linewidth scales + legends
  gf_refine(
    scale_colour_distiller("Correlation", type = "div", palette = "RdBu"),
    scale_linewidth_continuous("Significance", range = c(0.25, 3), 
                               
  # guide_legend(reverse = TRUE): Fat Lines mean higher significance
    )) %>%
  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))

```

We can clearly see that `rm` and `lstat` have strong correlations with
`cmedv` and should make good choices for setting up a minimal linear
regression model. (`medv` is the older errored version of `cmedv`)
:::


### {{< iconify flat-color-icons workflow >}} Model Building

We will first execute the `lm` test with code and evaluate the results.
Then we will do an intuitive walk through of the process and finally,
hand-calculate entire analysis for clear understanding.

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
#### {{< iconify mingcute code-fill >}} Model Code

R offers a very simple command `lm` to execute an Linear Model: Note the
familiar `formula` of stating the variables: ( $y \sim x$; where $y$ =
target, $x$ = predictor)

```{r}

housing_lm <- lm(cmedv ~ rm, data = housing)
summary(housing_lm)

```

The model for $\widehat{cmedv}$ , the prediction for `cmedv`can be
written in the form of $y = mx + c$, as:

```{r}
#| echo: false
#| include: false
# Works, but I have the LaTex already!
equatiomatic::extract_eq(housing_lm, use_coefs = TRUE)

```

$$
\widehat{cmedv} \sim -34.65924 + 9.09967* rm
$$ {#eq-rm-model}

::: callout-important
-   The **effect size** of `rm` on predicting `cmedv` a (slope) value of
    $9.09967$ which is significant at p-value of $<2.2e-16$; for every
    one room increase in `rm`, we have a $USD~90997$ increase in
    median price `cmedv`.
-   The **F-statistic** for the Linear Model is given by $F = 474.3$, which
    is very high. (We will use the F-statistic again when we do Multiple
    Regression.)
-   The `R-squared` value is $R^2 = 0.48$ which means that `rm` is able to
    explain about half of the trend in `cmedv`; there is substantial
    variation in `cmedv` that is still left to explain, an indication that      we should perhaps use a richer model, with more predictors. These aspects are explored in the Tutorials.
:::

We can plot the scatter plot of these two variables with the model also
over-plotted.

```{r}

#| layout-ncol: 3
#| fig-width: 5
#| fig-height: 4

# Set graph theme
theme_set(new = theme_custom())
#
# Tidy Data frame for the model using `broom`
housing_lm_tidy <- 
  housing_lm %>% 
  broom::tidy(conf.int= TRUE, 
              conf.level = 0.95)
housing_lm_tidy
##
housing_lm_augment <- 
  housing_lm %>% 
  broom::augment(se_fit = TRUE,
                 interval = "confidence")
housing_lm_augment
##
intercept <- 
  housing_lm_tidy %>%
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  as.numeric()
##
slope <- 
  housing_lm_tidy %>%
  filter(term == "rm") %>%
  select(estimate) %>%
  as.numeric()
##
housing %>% drop_na() %>% 
  gf_point(
  cmedv ~ rm,
  title = "Price vs Average no. of Rooms",
  ylab = "Median Price",
  xlab = "Avg. No. of Rooms",
  alpha = 0.2
) %>%
  # Plot the model equation
  gf_abline(slope = slope, intercept = intercept, 
            colour = "lightcoral",
            linewidth = 2)  %>%
  
  # Plot the model prediction points on the line
  gf_smooth(method = "lm", geom = "point", 
            color = "grey30", 
            size = 0.5) %>%
  gf_refine(
    annotate(geom = "segment",
    y = 0, yend = 29, x = 7, xend = 7, # manually calculated
    linetype = "dashed",
    color = "dodgerblue",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "last",
      type = "closed")),
    
    annotate(geom = "segment",
    y = 29, yend = 29, x = 2.5, xend = 7, # manually calculated
    linetype = "dashed",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "first",
      type = "closed"),
    color = "dodgerblue")) %>%
  gf_refine(
    scale_x_continuous(limits = c(2.5, 10),
                       expand = c(0, 0)),
    # removes plot panel margins
    scale_y_continuous(limits = c(0, 55),
                       expand = c(0, 0))
  ) %>% gf_theme(theme = theme_custom())

```

For any new value of `rm`, we go up to the vertical blue line and read
off the predicted median price by following the horizontal blue line.
That is how the model is used (by hand).

#### {{< iconify carbon forecast-hail >}} Forecasting with the Linear Model

In practice, we use the `broom` package functions (`tidy`, `glance` and
`augment`) to obtain a clear view of the model parameters and
predictions of `cmedv` for all *existing* values of `rm`. We see
estimates for the intercept and slope (`rm`) for the linear model, along
with the *standard errors* and *p.values* for these estimated
parameters. And we see the fitted values of `cmedv` for the existing
`rm`; these values will naturally lie **on** the straight-line depicting
the model. We will examine this `augment`-ed data more the section on Diagnostics. 

To predict `cmedv` with *new* values of `rm`, we use `predict`. Let us
now try to make predictions with some new data:

```{r}

new <- tibble(rm = seq(3, 10)) # must be named "rm"
new %>% mutate(predictions =
                 stats::predict(
                   object = housing_lm,
                   newdata = .,
                   se.fit = FALSE
                 ))
```

Note that "negative values" for predicted `cmedv` would have no meaning!

#### {{< iconify mdi thinking >}} Linear Model Intuitive {#sec-lm-intuitive}

All that is very well, but what is happening under the hood of the `lm`
command? Consider the `cmedv` (target) variable and the `rm`
feature/predictor variable. What we do is:

1.  Plot a scatter plot `gf_point(cmedv ~ rm, housing)`
2.  Find a line that, in some way, gives us some prediction of `cmedv`
    for any given `rm`
3.  Calculate the errors in prediction and use those to find the "best"
    line.
4.  Use that "best" line henceforth as a model for prediction.

How does one fit the "best" line? Consider a choice of "lines" that we
can use to fit to the data. Here are 6 lines of varying slopes (and
intercepts ) that we can try as candidates for the best fit line:

```{r}
#| echo: false
# This code is not to be dissected in class
# For exposition purposes only
# Discuss the results only (graphs)

set.seed(1234)
housing_sample <- housing_lm_augment %>% 
  slice_sample(n = 15)
mean_cmedv_sample <-
  mean( ~ cmedv, na.rm = TRUE, data = housing_sample)
mean_rm_sample <- mean( ~ rm, na.rm = TRUE, data = housing_sample)

lm_sample <- tibble(
  slope = slope + c(5, 2, 0, -2, -5, -slope),
  intercept = intercept + c(-30, -15, 0, 10, 30, -intercept + mean_cmedv_sample),
  # List column containing `housing_sample`
  # No repetition needed !!!
  # Auto recycle for each of slope + intercept
  sample = list(housing_sample)
)

lm_sample <- lm_sample %>% 
  mutate(line = pmap(
    .l = list(intercept, slope, sample),
    .f = \(intercept, slope, sample) 
            tibble(pred = intercept + slope * sample$rm, 
                   rm = sample$rm))) %>%
  
  mutate(graphs = pmap(
    .l = list(sample, line),
    .f = \(sample, line)
    gf_point(cmedv ~ rm, data = sample, size = 2) %>%
    gf_line(
        pred ~ rm,
        data = line ,
        color = "dodgerblue",
        linewidth = 2
      ) %>%
    
    gf_refine(
        scale_x_continuous(limits = c(2.5, 10),
                           expand = c(0, 0)),
        # removes plot panel margins
        scale_y_continuous(limits = c(0, 55),
                           expand = c(0, 0)))))

```

```{r}
#| echo: false
#| layout-ncol: 3
lm_sample %>% pluck("graphs",1)
lm_sample %>% pluck("graphs",2)
lm_sample %>% pluck("graphs",3)
lm_sample %>% pluck("graphs",4)
lm_sample %>% pluck("graphs",5)
lm_sample %>% pluck("graphs",6)


```

It should be apparent that while we cannot determine which line may be
the best, the **worst** line seems to be the one in the final plot,
which ignores the x-variable `rm` altogether. This corresponds to the
*NULL Hypothesis*, that there is *no relationship* between the two
variables. Any of the other lines could be a decent candidate, so how do
we decide?

```{r}
#| echo: false
#| layout-ncol: 2
#| warning: false
#| 
housing_sample %>% 
  gf_hline(yintercept =  ~ mean_cmedv_sample,
           color = "dodgerblue", linewidth = 2) %>%
  gf_segment(
    data = housing_sample,
    color = "springgreen3",
    mean_cmedv_sample + cmedv ~ rm + rm,
    title = "Fig A: Price vs \nAverage no. of Rooms",
    subtitle = "NULL Hypothesis",
    ylab = "cmedv (Median Price)",
    xlab = "rm (Avg. No. of Rooms)") %>%
  gf_point(cmedv ~ rm) %>%
  gf_text(
    mean_cmedv_sample - 2 ~ 7.5,
    label = expression(paste(mu, "_tot")),
    inherit = F,
    family = "Merri") 
##
housing_sample  %>%
  gf_segment(
    .fitted + cmedv ~ rm + rm,
    color = "orangered1",
    title = "Fig B: Price vs \nAverage no. of Rooms",
    subtitle = "Alternative Hypothesis",
    ylab = "cmedv (Median Price)",
    xlab = "rm (Avg. No. of Rooms)") %>%
  gf_abline(slope = slope,
            intercept = intercept,
            colour = "dodgerblue", linewidth = 2) %>%
  gf_point(cmedv ~ rm) 


```

In Fig A, the *horizontal* [blue line]{style="color: dodgerblue;"} is
the overall mean of `cmedv`, denoted as $\mu_{tot}$. The vertical [green
lines]{style="color: palegreen;"} to the points show the departures of
each point from this overall mean, called
[**residuals**]{style="background-color: yellow;"}. The sum of *squares*
of these residuals in Fig A is called the [**Total Sum of Squares**
(SST)]{style="background-color: yellow;"}.

$$
SST = \Sigma (y - \mu_{tot})^2
$${#eq-SST}

In Fig B, the vertical [red lines]{style="color: red;"} are the
residuals of each point from the potential line of fit. The sum of the
*squares* of these lines is called the [**Total Error Sum of Squares**
(SSE)]{style="background-color: yellow;"}.

$$
SSE = \Sigma [(y - a - b * rm)^2]
$${#eq-SSE}

It should be apparent that if there is any positive linear relationship
between `cmedv` and `rm`,then $SSE < SST$.

How do we get the optimum slope + intercept? If we plot the $SSE$ as a
function of varying slope, we get:

```{r}

#| echo: false
sim_model <- tibble(b = slope + seq(-5,5),
                  a = intercept,
                  dat = list(tibble(cmedv = housing_sample$cmedv, 
                               rm = housing_sample$rm))) %>% 
  mutate(r_squared= pmap_dbl(
    .l = list(a,b,dat),
    .f = \(a,b, dat) sum((dat$cmedv - (b*dat$rm + a))^2))) 
min_r_squared <- sim_model %>% select(r_squared) %>% min()
min_slope <- sim_model %>% filter(r_squared == min_r_squared) %>% select(b) %>% as.numeric()
sim_model %>% 
  gf_point(r_squared ~ b,data = ., size = 2) %>% 
  gf_line(ylab = "SSE", xlab = "slope",title = "Error vs Slope") %>% 
  gf_hline(yintercept = min_r_squared, color = "red") %>%
  gf_segment(min_r_squared + 0 ~ min_slope + min_slope, 
             colour = "red", 
             arrow = arrow(ends = "last", length = unit(1, "mm")))  %>% 
  gf_refine(coord_cartesian(expand = FALSE), 
            expand_limits(y = c(0, 20000), x = c(3.5, 15)))

```

We see that there is a quadratic minimum $SSE$ at the optimum value of
slope and at all other slopes, the $SSE$ is higher. We can use this to
find the optimum slope, which is what the function `lm` does.

#### {{< iconify material-symbols slideshow-sharp >}} Linear Models Manually Demonstrated (Apologies to Spinoza)

Let us hand-calculate the numbers so we know what the test is doing.
Here is the SST: we pretend that there is no relationship between
`cmedv` ans `rm` and compute a **NULL** model:

```{r}
#| label: SST-Total-Sum-of-Squares
# Calculate overall sum squares SST

SST <- deviance(lm(cmedv ~ 1, data = housing))
SST

```

And here is the SSE:

```{r}
#| label: SSE-Within-Group-Sum-of-Squares

SSE <- deviance(housing_lm)
SSE

```

Given that the model leaves **unexplained** variations in `cmedv` to the
extent of $SSE$, we can compute the $SSR$, [the Regression Sum of
Squares]{style="background-color: yellow;"}, the amount of variation in
`cmedv` that the linear model **does** explain:

```{r}
#| label: SSR
SSR <- SST - SSE
SSR

```

We have $SST = 42577.74$, $SSE = 21934.39$ and therefore
$SSR = 20643.35$.

In order to calculate the F-Statistic, we need to compute the variances,
using these sum of squares. We obtain variances by dividing by their
*Degrees of Freedom*:

$$
F_{stat} = \frac{SSR / df_{SSR}}{SSE / df_{SSE}}
$$

where $df_{SSR}$ and $df_{SSE}$ are respectively the degrees of freedom
in SSR and SSE.

Let us calculate these Degrees of Freedom. If we have $n=$
`r dim(housing)[[1]]` observations of data, then:

-   $SST$ clearly has degree of freedom
    $n-1 = `r dim(housing)[[1]] -1`$, since it uses all observations but
    loses one degree to calculate the global mean.
-   $SSE$ was computed using the slope and intercept, so it has
    $(n-2) = `r dim(housing)[[1]] -2`$ as degrees of freedom.
-   And therefore $SSR$ being their difference has just $1$ degree of
    freedom.

Now we are ready to compute the F-statistic:

```{r}
n <- housing %>% count() %>% as.numeric()
df_SSR <- 1
df_SSE <- n -2
F_stat <- (SSR/df_SSR) / (SSE/df_SSE)
F_stat

```

The F-stat is compared with a **critical value** of the F-statistic,
which is computed using the formula for the f-distribution in R. As with
our hypothesis tests, we set the significance level to 0.95, and quote
the two relevant degrees of freedom as parameters to `qf()` which
computes the critical F value as a **quartile**:

```{r}
F_crit <-  qf(p = 0.95,     # Significance level is 5%
              df1 = df_SSR, # Numerator degrees of freedom 
              df2 = df_SSE) # Denominator degrees of freedom
F_crit
F_stat

```

The F_crit value can also be seen in a plot[^4]:

```{r}
mosaic::pdist(dist = "f",
              q = F_crit, 
              df1 = df_SSR, df2 = df_SSE)
```

Any value of F more than the $F_{crit}$ occurs with smaller probability
than 0.05. Our F_stat is much higher than $F_{crit}$, by orders of
magnitude! And so we can say with confidence that `rm` has a significant
effect on `cmedv`.

The value of `R.squared` is also calculated from the previously computed
sums of squares:

$$
R.squared = \frac{SSR}{SST} = \frac{SSY-SSE}{SST}
$$ {#eq-rsquared}

```{r}
r_squared <- (SST - SSE)/SST
r_squared
# Also computable by
# mosaic::rsquared(housing_lm)

```

So `R.squared` = `r (SST - SSE)/SST`

The value of Slope and Intercept are computed using a maximum likelihood
derivation and the knowledge that the means square error is a minimum at
the optimum slope: for a linear model $y \sim mx + c$

$$
slope = \frac{\Sigma[(y - y_{mean})*(x - x_{mean})]}{\Sigma(x - x_{mean})^2}
$$

::: callout-tip
Note that the slope is equal to the ratio of the covariance of x and y
to the variance of x.
:::

and

$$
Intercept = y_{mean} - slope * x_{mean}
$$

```{r}
#| label: slope-and-intercept

slope <- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~ rm, data = housing)
slope
##
intercept <- mosaic::mean(~ cmedv, data = housing) - slope * mosaic::mean(~ rm, data = housing)
intercept

```

So, there we are! All of this is done for us by one simple formula,
`lm()`!

#### {{< iconify iconoir stats-report >}} Using Other Packages {#sec-using-other-packages}

There is a very neat package called `ggstatsplot`[^5] that allows us to
plot very comprehensive statistical graphs. Let us quickly do this:

```{r}
#| message: false
library(ggstatsplot)
housing_lm %>%
  ggstatsplot::ggcoefstats(title = "Linear Model for Boston Housing", 
                           subtitle = "Using ggstatsplot")


```

This chart shows the estimates for the `intercept` and `rm` along with
their error bars, the `t-statistic`, degrees of freedom, and the
`p-value`.

We can also obtain crisp-looking model tables from the new `supernova`
package [^6], which is based on the methods discussed in Judd et al.

```{r}
#| layout-ncol: 2
library(supernova)
supernova::supernova(housing_lm)

```

This table is very neat in that it gives the `Sums of Squares` for both
the NULL (empty) model, and the current model for comparison. The `PRE`
entry is the *Proportional Reduction in Error*, a measure that is
identical with `r.squared`, which shows how much the model reduces the
error compared to the NULL model(48%). The PRE idea is nicely discussed
in Judd et al @sec-references.
:::

[^4]: Michael Crawley, *The R Book, Third Edition 2023. Chapter 9.
    Statistical Modelling*

[^5]: <https://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html>

[^6]: <https://github.com/UCLATALL/supernova>

### {{< iconify flat-color-icons workflow >}} Workflow: Model Checking and Diagnostics {#sec-diagnostics}

We will follow much of the treatment on Linear Model diagnostics, given
[here on the STHDA
website](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#homogeneity-of-variance).

> A first step of this regression diagnostic is to inspect the
> significance of the regression beta coefficients, as well as, the
> R.square that tells us how well the linear regression model fits to
> the data.
>
> For example, the linear regression model makes the assumption that the
> relationship between the predictors (x) and the outcome variable is
> linear. This might not be true. The relationship could be polynomial
> or logarithmic.
>
> Additionally, the data might contain some influential observations,
> such as outliers (or extreme values), that can affect the result of
> the regression.
>
> Therefore, the regression model must be closely diagnosed in order to
> detect potential problems and to check whether the assumptions made by
> the linear regression model are met or not. To do so, we generally
> examine the distribution of **residuals errors**, that can tell us
> more about our data.

### {{< iconify ic twotone-rule >}} Workflow: Checks for Uncertainty

Let us first look at the uncertainties in the estimates of slope and
intercept. These are most easily read off from the `broom::tidy`-ed
model:

```{r}
#| label: uncertainty-and-confidence
# housing_lm_tidy <-  housing_lm %>% broom::tidy()
housing_lm_tidy

```

Plotting this is simple too:

```{r}
#| layout-ncol: 2
#| warn: false
#| message: false


# Set graph theme
theme_set(new = theme_custom())
#
housing_lm_tidy %>%
  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %>% 
  gf_hline(yintercept = 0) %>% 
  gf_errorbar(conf.low + conf.high ~ term, 
              width = 0.1, 
              title = "Model Bar Plot for Estimates with Confidence Intervals") %>% 
  gf_theme(theme = theme_custom())
##
housing_lm_tidy %>% 
  gf_pointrange(estimate + conf.low + conf.high ~ term,
                title = "Model Point-Range Plot for Estimates with Confidence Intervals") %>% 
  gf_hline(yintercept = 0) %>% 
  gf_theme(theme = theme_custom())

```

The point-range plot helps to avoid what has been called ["within-the-bar bias"](https://graphics.cs.wisc.edu/Papers/2014/CG14/Preprint.pdf). The estimate is just a value, which we might plot as a *bar* or as a *point*, with uncertainty error-bars.

Values **within the bar** are not more likely!! This is the bias that the point-range plot avoids. 

### {{< iconify ic twotone-rule >}} Checks for Constant Variance/Heteroscedasticity

Linear Modelling makes 4 fundamental assumptions:("**LINE**")

1.  **Linear** relationship between y and x
2.  Observations are **independent**.
3.  Residuals are **normally** distributed
4.  Variance of the `y` variable is **equal** at all values of `x`.

We can check these using checks and graphs: Here we plot the residuals
against the independent/feature variable and see if there is a gross
variation in their range

```{r}
#| label: Looking-at-Residuals
#| layout-ncol: 2
#| warning: false

housing_lm_augment %>% 
  gf_point(.resid ~ .fitted, title = "Residuals vs Fitted") %>%
  gf_smooth(method = "loess")

housing_lm_augment %>% 
  gf_hline(yintercept = 0, colour = "grey", linewidth = 2) %>%
  gf_point(.resid ~ cmedv, title = "Residuals vs Target Variable") 

housing_lm_augment %>% 
  gf_dhistogram(~ .resid, title = "Histogram of Residuals") %>% 
  gf_fitdistr()

housing_lm_augment %>% 
  gf_qq(~ .resid, title = "Q-Q Residuals") %>% 
  gf_qqline() 

```

The Q-Q plot of residuals also has significant deviations from the
normal quartiles. The residuals are not quite "like the night sky", i.e.
random enough. These point to the need for a richer model, with more
predictors. The "trend line" of residuals vs predictors show a U-shaped
pattern, indicating significant nonlinearity: there is a curved
relationship in the graph. The solution can be a **nonlinear
transformation** of the predictor variables, such as $\sqrt(X)$,
$log(X)$, or even $X^2$. For instance, we might try a model for `cmedv`
using $rm^2$ instead of just `rm` as we have done. This will **still**
be a linear model!

::: callout-tip
Base R has a crisp command to plot these diagnostic graphs. But we will
continue to use `ggformula`.

```{r}
#| layout-ncol: 2
#| layout-nrow: 2
plot(housing_lm)

```
:::

::: callout-tip
One of the [ggplot extension
packages](https://exts.ggplot2.tidyverse.org/gallery/) named `lindia`
also has a crisp command to plot these diagnostic graphs.

```{r}
#| label: using-lindia-package
#| message: false
# Set graph theme
theme_set(new = theme_custom())
#
library(lindia)
gg_diagnose(housing_lm, 
            mode = "base_r", # plots like those with base-r
  theme = theme(axis.title = element_text(size = 6, face = "bold"),
                title = element_text(size = 8))
)

```
:::

::: column-margin
The `r-squared` for a model `lm(cmedv ~ rm^2)` shows some improvement:

```{r}
#| echo: false
lm(cmedv ~ poly(rm,2), data = housing) %>% 
  broom::glance() %>% 
  select(r.squared) %>% 
  as.numeric()

```
:::


## Extras
    
::: callout-note
### Multiple Regression
It is also possible that there is more than
one explanatory variable: this is **multiple regression.**

$$
y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 ...+ \beta_n*x_n
$${#eq-multiple-regression}

where each of the $\beta_i$ are slopes defining the relationship between
y and $x_i$. Note that this is a vector dot-product, or inner-product, taken with a *vector* of input variables $x_i$ and a vector of weights, $\beta_i$. 
Together, the RHS of that equation defines an n-dimensional
*hyperplane*. The model is **linear in the parameters** $\beta_i$, e.g.
these are OK:

$$
\color{black}{
\begin{cases}
 & y_i = \pmb\beta_0 + \pmb\beta_1x_1 + \pmb\beta_2x_1^2 + \epsilon_i\\
 & y_1 = \pmb\beta_0 + \pmb\gamma_1\pmb\delta_1x_1 + exp(\pmb\beta_2)x_2+ \epsilon_i\\
\end{cases}
}
$$

but not, for example, these:

$$
\color{red}{
\begin{cases}
 & y_i = \pmb\beta_0 + \pmb\beta_1x_1^{\beta_2} + \epsilon_i\\
 & y_i = \pmb\beta_0 + exp(\pmb\beta_1x_1) + \epsilon_i\\
\end{cases}
}
$$
:::

There are three ways[^2] to include more predictors:

-   **Backward Selection**: We would typically start with a **maximal
    model**[^3] and progressively simplify the model by knocking off
    predictors that have the least impact on model accuracy.
-   **Forward Selection**: Start with no predictors and systematically
    add them one by one to increase the quality of the model
-   **Mixed Selection**: Wherein we start with no predictors and add
    them to gain improvement, or remove them at as their *significance*
    changes based on other predictors that have been added.

The first two are covered in the other tutorials above; Mixed Selection we will leave for a more advanced course. But for now we will first use just one predictor `rm`(Avg. no. of Rooms) to model housing prices.


[^2]: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani,
    *Introduction to Statistical Learning, Springer, 2021. Chapter 3.
    Linear Regression*. [Available
    Online](https://www.statlearning.com/)

[^3]: Michael Crawley, *The R Book, Third Edition 2023. Chapter 9.
    Statistical Modelling*
    
## {{< iconify fluent-mdl2 decision-solid >}} Conclusions

We have seen how starting from a basic EDA of the data, we have been
able to choose a single Quantitative predictor variable to model a
Quantitative target variable, using Linear Regression. As stated
earlier, we may have wish to use more than one predictor variables, to
build more sophisticated models with improved prediction capability. And
there is more than one way of selecting these predictor variables, which
we will examine in the Tutorials.

Secondly, sometimes it may be necessary to mathematically transform the
variables in the dataset to enable the construction of better models,
something that was not needed here.

We may also encounter cases where the predictor variables seem to work
together; one predictor may influence "how well" another predictor
works, something called an *interaction effect* or a *synergy effect*.
We might then have to modify our formula to include *interaction terms*
that look like $predictor1 \times predictor2$.

So our Linear Modelling workflow might look like this: we have not seen
all stages yet, but that is for another course module or tutorial!

``` {.d2 theme="MixedBerryBlue" layout="dagre"}

title: Our Linear Regression Workflow {
  near: top-center
  shape: text
  style: {
    font-size: 29
    bold: true
    underline: true
  }
}
  Data :  {
  style: {
    stroke: "#53C0D8"
    stroke-width: 5
    shadow: true
  }
}
  
  EDA : {
  style: {
    opacity: 0.6
    fill: red
    3d: true
    stroke: black
  }
}
Check Relationships; 
Build Model; 
Transform Variables {shape: cloud}; Try Multiple Regression\n and/or interaction effects {shape: cloud};
Check Model Diagnostics: {shape: diamond}
Check Model Diagnostics: {tooltip: Check R^2 }
Interpret Model
Apply Model {shape: oval}

    Data -> EDA : "inspect"
    Data -> EDA : "ggformula"
    Data -> EDA : "glimpse"
    Data -> EDA : "skim"
    EDA --> Check Relationships : "corrplot"
    EDA --> Check Relationships : "corrgram"
    EDA -> Check Relationships : "ggformula + purrr"
    EDA -> Check Relationships : "cor.test"
    Check Relationships --> Simple or\nComplex Model Decision
    Simple or\nComplex Model Decision -> Is the Model Possible?
    Is the Model Possible? -> Build Model
    Build Model -> Check Model Diagnostics
    Check Model Diagnostics -> Interpret Model : "All Good"
    Check Model Diagnostics -> Transform Variables : "Inadequate" {

          source-arrowhead.label: 1
          style.stroke: red
          target-arrowhead: {
          shape: diamond
          style.filled: true
          label: 1
          }
    }
    Check Model Diagnostics -> Try Multiple Regression\n and/or interaction effects : " Still Inadequate" {
          source-arrowhead.label: 2
          style.stroke: red
          target-arrowhead: {
          shape: diamond
          style.filled: true
          label: 2
          }
    }
    Transform Variables ->  Build Model {style.stroke: red }
     Try Multiple Regression\n and/or interaction effects ->  Build Model {style.stroke: red }
    Interpret Model -> Apply Model
    
  
```

## {{< iconify ooui references-rtl >}} References {#sec-references}

1.  <https://mlu-explain.github.io/linear-regression/>\
2.  The Boston Housing Dataset, corrected version. StatLib \@ CMU,
    [lib.stat.cmu.edu/datasets/boston_corrected.txt](http://lib.stat.cmu.edu/datasets/boston_corrected.txt)\
3.  <https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R>\
4.  Andrew Gelman, Jennifer Hill, Aki Vehtari. *Regression and Other
    Stories*, Cambridge University Press, 2023.[Available
    Online](https://users.aalto.fi/~ave/ROS.pdf)\
5.  Michael Crawley.(2013). *The R Book,second edition*. Chapter 11.\
6.  Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani,
    *Introduction to Statistical Learning*, Springer, 2021. Chapter 3.
    <https://www.statlearning.com/>\
7.  David C Howell, [Permutation Tests for Factorial ANOVA
    Designs](https://www.uvm.edu/~statdhtx/StatPages/Permutation%20Anova/PermTestsAnova.html)\
8.  Marti Anderson, [Permutation tests for univariate or multivariate
    analysis of variance and
    regression](https://www.academia.edu/50056272/Permutation_tests_for_univariate_or_multivariate_analysis_of_variance_and_regression?auto=download)\
9.  <http://r-statistics.co/Assumptions-of-Linear-Regression.html>\
10. Judd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017.
    "Introduction to Data Analysis." In, 1--9. Routledge.
    <https://doi.org/10.4324/9781315744131-1>. Also see
    <http://www.dataanalysisbook.com/index.html>\
11. Patil, I. (2021). Visualizations with statistical details: The
    'ggstatsplot' approach. Journal of Open Source Software, 6(61),
    3167,[https://doi:10.21105/joss.03167](https://doi:10.21105/joss.03167){.uri}\

::: {#refs style="font-size: 60%;"}
###### {{< iconify lucide package-check >}} R Package Citations

```{r}
#| echo: false
# scan_packages()
cite_packages(
  output = "table",cite.tidyverse = FALSE,
  out.dir = ".",
  out.format = "html",
  pkgs = c("broom", "corrgram", "corrplot", "GGally", "geomtextpath",
           "ggstatsplot", "ISLR", "janitor", "lindia",
           "reghelper", "supernova")
) %>%
  knitr::kable(format = "simple")

```
:::
